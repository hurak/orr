@article{abdelmalekL1SolutionOverdetermined1980,
  title = {L1 {{Solution}} of {{Overdetermined Systems}} of {{Linear Equations}}},
  author = {Abdelmalek, Nabih N.},
  year = {1980},
  month = jun,
  journal = {ACM Trans. Math. Softw.},
  volume = {6},
  number = {2},
  pages = {220--227},
  issn = {0098-3500},
  doi = {10.1145/355887.355894},
  url = {http://doi.acm.org/10.1145/355887.355894},
  urldate = {2018-08-01}
}

@article{abdelmalekLinearL1Approximation1971,
  title = {Linear {{L1 Approximation}} for a {{Discrete Point Set}} and {{L1 Solutions}} of {{Overdetermined Linear Equations}}},
  author = {Abdelmalek, Nabih N.},
  year = {1971},
  month = jan,
  journal = {J. ACM},
  volume = {18},
  number = {1},
  pages = {41--47},
  issn = {0004-5411},
  doi = {10.1145/321623.321628},
  url = {http://doi.acm.org/10.1145/321623.321628},
  urldate = {2018-08-01}
}

@book{absilOptimizationAlgorithmsMatrix2007,
  title = {Optimization {{Algorithms}} on {{Matrix Manifolds}}},
  author = {Absil, P.-A. and Mahony, R. and Sepulchre, Rodolphe},
  year = {2007},
  month = dec,
  publisher = {Princeton University Press},
  address = {Princeton, N.J. ; Woodstock},
  url = {https://sites.uclouvain.be/absil/amsbook/},
  isbn = {978-0-691-13298-3},
  langid = {english}
}

@article{aghassiSolvingAsymmetricVariational2006,
  title = {Solving Asymmetric Variational Inequalities via Convex Optimization},
  author = {Aghassi, Michele and Bertsimas, Dimitris and Perakis, Georgia},
  year = {2006},
  month = sep,
  journal = {Operations Research Letters},
  volume = {34},
  number = {5},
  pages = {481--490},
  issn = {0167-6377},
  doi = {10.1016/j.orl.2005.09.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0167637705001124},
  urldate = {2025-01-02},
  abstract = {Using duality, we reformulate the asymmetric variational inequality (VI) problem over a conic region as an optimization problem. We give sufficient conditions for the convexity of this reformulation. We thereby identify a class of VIs that includes monotone affine VIs over polyhedra, which may be solved by commercial optimization solvers.}
}

@inproceedings{agrawalDifferentiableConvexOptimization2019,
  title = {Differentiable {{Convex Optimization Layers}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32 ({{NeurIPS}} 2019)},
  author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, Zico},
  year = {2019},
  month = oct,
  eprint = {1910.12430},
  address = {Vancouver, Canada},
  url = {http://arxiv.org/abs/1910.12430},
  urldate = {2021-09-17},
  abstract = {Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver's solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.},
  archiveprefix = {arXiv},
  isbn = {978-1-71380-793-3}
}

@inproceedings{ahmadiDSOSSDSOSOptimization2014,
  title = {{{DSOS}} and {{SDSOS}} Optimization: {{LP}} and {{SOCP-based}} Alternatives to Sum of Squares Optimization},
  shorttitle = {{{DSOS}} and {{SDSOS}} Optimization},
  booktitle = {2014 48th {{Annual Conference}} on {{Information Sciences}} and {{Systems}} ({{CISS}})},
  author = {Ahmadi, A. A. and Majumdar, A.},
  year = {2014},
  month = mar,
  pages = {1--5},
  doi = {10.1109/CISS.2014.6814141},
  abstract = {Sum of squares (SOS) optimization has been a powerful and influential addition to the theory of optimization in the past decade. Its reliance on relatively large-scale semidefinite programming, however, has seriously challenged its ability to scale in many practical applications. In this paper, we introduce DSOS and SDSOS optimization as more tractable alternatives to sum of squares optimization that rely instead on linear programming and second order cone programming. These are optimization problems over certain subsets of sum of squares polynomials and positive semidefinite matrices and can be of potential interest in general applications of semidefinite programming where scalability is a limitation.}
}

@techreport{ahmadiSumSquaresSOS,
  type = {Lecture Notes},
  title = {Sum of {{Squares}} ({{SOS}}) {{Techniques}}: {{An Introduction}}},
  author = {Ahmadi, Amir Ali},
  institution = {Princeton University},
  url = {https://www.princeton.edu/~aaa/Public/Teaching/ORF523/S16/ORF523_S16_Lec15.pdf}
}

@article{alarieTwoDecadesBlackbox2021,
  title = {Two Decades of Blackbox Optimization Applications},
  author = {Alarie, St{\'e}phane and Audet, Charles and Gheribi, A{\"i}men E. and Kokkolaras, Michael and Le Digabel, S{\'e}bastien},
  year = {2021},
  month = jan,
  journal = {EURO Journal on Computational Optimization},
  volume = {9},
  pages = {100011},
  issn = {2192-4406},
  doi = {10.1016/j.ejco.2021.100011},
  url = {https://www.sciencedirect.com/science/article/pii/S2192440621001386},
  urldate = {2022-06-02},
  abstract = {This article reviews blackbox optimization applications of direct search optimization methods over the past twenty years. Emphasis is placed on the Mesh Adaptive Direct Search (Mads) derivative-free optimization algorithm. The main focus is on applications in three specific fields: energy, materials science, and computational engineering design. Nevertheless, other applications in science and engineering, including patents, are also considered. The breadth of applications demonstrates the versatility of Mads and highlights the evolution of its accompanying software NOMAD as a standard tool for blackbox optimization.},
  langid = {english}
}

@misc{AlgorithmSCSDocumentation,
  title = {Algorithm --- {{SCS}} 3.2.4 Documentation},
  url = {https://www.cvxgrp.org/scs/algorithm/index.html},
  urldate = {2024-01-19}
}

@phdthesis{allendeMathematicalProgramsEquilibrium2006,
  title = {Mathematical Programs with Equilibrium Constraints: Solution Techniques from Parametric Optimization},
  author = {Allende, Gamayqzel Bouza},
  year = {2006},
  address = {Enschede, NL},
  url = {https://ris.utwente.nl/ws/files/6042785/thesis_Allende.pdf},
  abstract = {Equilibrium constrained problems form a special class of mathematical programs where the decision variables satisfy a finite number of constraints together with an equilibrium condition. Optimization problems with a variational inequality constraint, bilevel problems and semi-infinite programs can be seen as particular cases of equilibrium constrained problems. Such models appear in many practical applications. Equilibrium constraint problems can be written in bilevel form with possi- bly a finite number of extra inequality constraints. This opens the way to solve these programs by applying the so-called Karush-Kuhn-Tucker approach. Here the lower level problem of the bilevel program is replaced by the Karush-Kuhn- Tucker condition, leading to a mathematical program with complementarity con- straints (MPCC). Unfortunately, MPCC problems cannot be solved by classical algorithms since they do not satisfy the standard regularity conditions. To solve MPCCs one has tried to conceive appropriate modifications of standard methods. For example sequential quadratic programming, penalty algorithms, regulariza- tion and smoothing approaches. The aim of this thesis is twofold. First, as a basis, MPCC problems will be investigated from a structural and generical viewpoint. We concentrate on a special parametric smoothing approach to solve these programs. The convergence behavior of this method is studied in detail. Although the smoothing approach is widely used, our results on existence of solutions and on the rate of convergence are new. We also derive (for the first time) genericity results for the set of minimizers (generalized critical points) for one-parametric MPCC. In a second part we will consider the MPCC problem obtained by applying the KKT-approach to equilibrium constrained programs and bilevel problems. We will analyze the generic structure of the resulting MPCC programs and adapt the related smoothing method to these particular cases. All corresponding results are new.},
  school = {Universiteit Twente}
}

@phdthesis{amosDifferentiableOptimizationBasedModeling2019,
  title = {Differentiable {{Optimization-Based Modeling}} for {{Machine Learning}}},
  author = {Amos, Brandon},
  year = {2019},
  url = {https://github.com/bamos/thesis},
  school = {Carnegie Mellon University}
}

@misc{amosDifferentiableOptimizationBasedModeling2019a,
  type = {Thesis Defense},
  title = {Differentiable {{Optimization-Based Modeling}} for {{Machine Learning}}},
  author = {Amos, Brandon},
  year = {2019},
  address = {Carnegie Mellon University}
}

@misc{amosOptNetDifferentiableOptimization2021,
  title = {{{OptNet}}: {{Differentiable Optimization}} as a {{Layer}} in {{Neural Networks}}},
  shorttitle = {{{OptNet}}},
  author = {Amos, Brandon and Kolter, J. Zico},
  year = {2021},
  month = dec,
  number = {arXiv:1703.00443},
  eprint = {1703.00443},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.00443},
  url = {http://arxiv.org/abs/1703.00443},
  urldate = {2024-08-14},
  abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. We explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, the method is learns to play mini-Sudoku (4x4) given just input and output games, with no a-priori information about the rules of the game; this highlights the ability of OptNet to learn hard constraints better than other neural architectures.},
  archiveprefix = {arXiv}
}

@misc{anderssonCasADiDocumentation2023,
  title = {{{CasADi Documentation}}},
  author = {Andersson, Joel and Gillis, Joris and Horn, Greg},
  year = {2023},
  month = nov,
  url = {https://github.com/casadi/casadi/wiki/Onboarding-Guide}
}

@article{andreaniSolutionMathematicalProgramming2001,
  title = {On the Solution of Mathematical Programming Problems with Equilibrium Constraints},
  author = {Andreani, Roberto and Mart{\i}{\textasciiacute}nez, Jos{\'e} Mario},
  year = {2001},
  month = dec,
  journal = {Mathematical Methods of Operations Research},
  volume = {54},
  number = {3},
  pages = {345--358},
  issn = {1432-5217},
  doi = {10.1007/s001860100158},
  url = {https://doi.org/10.1007/s001860100158},
  urldate = {2022-03-22},
  abstract = {Mathematical programming problems with equilibrium constraints (MPEC) are nonlinear programming problems where the constraints have a form that is analogous to first-order optimality conditions of constrained optimization. We prove that, under reasonable sufficient conditions, stationary points of the sum of squares of the constraints are feasible points of the MPEC. In usual formulations of MPEC all the feasible points are nonregular in the sense that they do not satisfy the Mangasarian-Fromovitz constraint qualification of nonlinear programming. Therefore, all the feasible points satisfy the classical Fritz-John necessary optimality conditions. In principle, this can cause serious difficulties for nonlinear programming algorithms applied to MPEC. However, we show that most feasible points do not satisfy a recently introduced stronger optimality condition for nonlinear programming. This is the reason why, in general, nonlinear programming algorithms are successful when applied to MPEC.},
  langid = {english}
}

@misc{anjosConicOptimizationBasics2014,
  title = {Conic {{Optimization The Basics}}, Some {{Fundamental Results}}, and {{Recent Developments}}},
  author = {Anjos, Miguel F.},
  year = {2014},
  month = apr,
  url = {http://cost-td1207.zib.de/sites/default/files/miguel_f_anjos.pdf}
}

@book{aptPrinciplesConstraintProgramming2003,
  title = {Principles of {{Constraint Programming}}},
  author = {Apt, Krzysztof},
  year = {2003},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  url = {https://doi.org/10.1017/CBO9780511615320},
  abstract = {Scheduling, vehicle routing and timetabling are all examples of constraint problems, and methods to solve them rely on the idea of constraint propagation and search. This book meets the need for a modern, multidisciplinary introduction to the field that covers foundations and applications. Written by Krzysztof Apt, an authority on the subject, it will be welcomed by graduate students and professionals. With the insertion of constraint techniques into programming environments, new developments have accelerated the solution process. Constraint programming combines ideas from artificial intelligence, programming languages, databases, and operational research.},
  isbn = {978-0-521-12549-9},
  langid = {english}
}

@phdthesis{axehillIntegerQuadraticProgramming2008,
  title = {Integer {{Quadratic Programming}} for {{Control}} and {{Communication}}},
  author = {Axehill, Daniel},
  year = {2008},
  address = {Link{\"o}ping, Sweden},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-10642},
  urldate = {2022-12-07},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
  langid = {english},
  school = {Link{\"o}ping University}
}

@article{baayenHiddenInvariantConvexity2022,
  title = {Hidden Invariant Convexity for Global and Conic-Intersection Optimality Guarantees in Discrete-Time Optimal Control},
  author = {Baayen, Jorn H. and Postek, Krzysztof},
  year = {2022},
  month = feb,
  journal = {Journal of Global Optimization},
  volume = {82},
  number = {2},
  pages = {263--281},
  issn = {1573-2916},
  doi = {10.1007/s10898-021-01072-5},
  url = {https://doi.org/10.1007/s10898-021-01072-5},
  urldate = {2022-03-25},
  abstract = {Non-convex discrete-time optimal control problems in, e.g., water or power systems, typically involve a large number of variables related through nonlinear equality constraints. The ideal goal is to find a globally optimal solution, and numerical experience indicates that algorithms aiming for Karush--Kuhn--Tucker points often find solutions that are indistinguishable from global optima. In our paper, we provide a theoretical underpinning for this phenomenon, showing that on a broad class of problems the objective can be shown to be an invariant convex function (invex function) of the control decision variables when state variables are eliminated using implicit function theory. In this way, optimality guarantees can be obtained, the exact nature of which depends on the position of the solution within the feasible set. In a numerical example, we show how high-quality solutions are obtained with local search for a river control problem where invexity holds.},
  langid = {english}
}

@book{balasDisjunctiveProgramming2018,
  title = {Disjunctive {{Programming}}},
  author = {Balas, Egon},
  year = {2018},
  month = dec,
  publisher = {Springer},
  address = {Cham},
  url = {https://doi.org/10.1007/978-3-030-00148-3},
  isbn = {978-3-030-00147-6},
  langid = {english}
}

@misc{bambadePROXQPEfficientVersatile2023,
  title = {{{PROXQP}}: An {{Efficient}} and {{Versatile Quadratic Programming Solver}} for {{Real-Time Robotics Applications}} and {{Beyond}}},
  shorttitle = {{{PROXQP}}},
  author = {Bambade, Antoine and Schramm, Fabian and Kazdadi, Sarah El and Caron, St{\'e}phane and Taylor, Adrien and Carpentier, Justin},
  year = {2023},
  month = sep,
  url = {https://inria.hal.science/hal-04198663},
  urldate = {2024-01-18},
  abstract = {Convex Quadratic programming (QP) has become a core component in the modern engineering toolkit, particularly in robotics, where QP problems are legions, ranging from real-time whole-body controllers to planning and estimation algorithms. Many of those QPs need to be solved at high frequency. Meeting timing requirements requires taking advantage of as many structural properties as possible for the problem at hand. For instance, it is generally crucial to resort to warm-starting to exploit the resemblance of consecutive control iterations. While a large range of off-the-shelf QP solvers is available, only a few are suited to exploit problem structure and warm-starting capacities adequately. In this work, we propose the PROXQP algorithm, a new and efficient QP solver that exploits QP structures by leveraging primal-dual augmented Lagrangian techniques. For convex QPs, PROXQP features a global convergence guarantee to the closest feasible QP, an essential property for safe closedloop control. We illustrate its practical performance on various standard robotic and control experiments, including a real-world closed-loop model predictive control application. While originally tailored for robotics applications, we show that PROXQP also performs at the level of state of the art on generic QP problems, making PROXQP suitable for use as an off-the-shelf solver for regular applications beyond robotics.},
  langid = {english}
}

@book{bankNonLinearParametricOptimization1983,
  title = {Non-{{Linear Parametric Optimization}}},
  author = {Bank, B. and Guddat, J. and Klatte, D. and Kummer, B. and Tammer, K.},
  year = {1983},
  publisher = {Birkh{\"a}user},
  address = {Basel},
  url = {https://doi.org/10.1007/978-3-0348-6328-5},
  isbn = {978-3-0348-6330-8}
}

@book{bardPracticalBilevelOptimization1998,
  title = {Practical {{Bilevel Optimization}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Practical {{Bilevel Optimization}}},
  author = {Bard, Jonathan F.},
  year = {1998},
  month = dec,
  series = {Nonconvex {{Optimization}} and {{Its Applications}}},
  number = {30},
  publisher = {Springer},
  address = {Dordrecht ; Boston},
  url = {https://link.springer.com/book/10.1007/978-1-4757-2836-1},
  isbn = {978-0-7923-5458-1},
  langid = {english}
}

@article{barrodaleImprovedAlgorithmDiscrete1973,
  title = {An {{Improved Algorithm}} for {{Discrete}} \$l\_1 \$ {{Linear Approximation}}},
  author = {Barrodale, I. and Roberts, F.},
  year = {1973},
  month = oct,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {10},
  number = {5},
  pages = {839--848},
  issn = {0036-1429},
  doi = {10.1137/0710069},
  url = {https://epubs.siam.org/doi/abs/10.1137/0710069},
  urldate = {2018-08-01},
  abstract = {By modifying the simplex method of linear programming, we are able to present an algorithm for \$l\_1 \$-approximation which appears to, be superior computationally to any other known algorithm for this problem.}
}

@article{bartelsMinimizationTechniquesPiecewise1978,
  title = {Minimization {{Techniques}} for {{Piecewise Differentiable Functions}}: {{The}} \$l\_1\$ {{Solution}} to an {{Overdetermined Linear System}}},
  shorttitle = {Minimization {{Techniques}} for {{Piecewise Differentiable Functions}}},
  author = {Bartels, R. and Conn, A. and Sinclair, J.},
  year = {1978},
  month = apr,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {15},
  number = {2},
  pages = {224--241},
  issn = {0036-1429},
  doi = {10.1137/0715015},
  url = {https://epubs.siam.org/doi/10.1137/0715015},
  urldate = {2018-08-01},
  abstract = {A new algorithm is presented for computing a vector x which satisfies a given m by \$n(m {$>$} n {\textbackslash}geqq 2)\$ linear system in the sense that the \$l\_1 \$ norm is minimized. That is, if A is a matrix having m columns \$a\_1 , {\textbackslash}cdots ,a\_m \$ each of length n, and b is a vector with components \${\textbackslash}beta \_1 , {\textbackslash}cdots ,{\textbackslash}beta \_m \$, then x is selected so that {\textbackslash}[{\textbackslash}phi (x) = {\textbar}{\textbar}A{\textasciicircum}T x - b{\textbar}{\textbar}\_1  = {\textbackslash}sum \_\{i = 1\}{\textasciicircum}m \{{\textbackslash}left{\textbar}a\_i{\textasciicircum}T x - {\textbackslash}beta \_i {\textbackslash}right{\textbar}\} {\textbackslash}] is as small as possible. Such solutions are of interest for the ``robust'' fitting of a linear model to data.The function \${\textbackslash}phi \$ is directly minimized in a finite number of steps using techniques borrowed from Conn's approach toward minimizing piecewise differentiable functions. In these techniques if x is any point and \$A\_{\textbackslash}mathcal \{Z\} \$ stands for the submatrix consisting of those columns \$a\_j\$ from A for which the corresponding residuals \$a\_j{\textasciicircum}T x - {\textbackslash}beta \_j \$ are zero, then the discontinuities in the gradient of \${\textbackslash}phi \$ at x are handled by making use of the projector onto the null space of \$A\_{\textbackslash}mathcal \{Z\}{\textasciicircum}T \$.Attention has been paid both to numerical stability and efficiency in maintaining and updating a factorization of \$A\_{\textbackslash}mathcal\{Z\} \$ from which the necessary projector is obtainable.The algorithm compares favorably with the best so far reported for the linear \$l\_1\$ problem, and it can easily be extended to handle linear constraints.}
}

@book{bauschkeIntroductionConvexityOptimization2023,
  title = {An {{Introduction}} to {{Convexity}}, {{Optimization}}, and {{Algorithms}}},
  author = {Bauschke, Heinz H. and Moursi, Walaa M.},
  year = {2023},
  month = dec,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  url = {https://doi.org/10.1137/1.9781611977806},
  abstract = {This concise, self-contained volume introduces convex analysis and optimization algorithms, with an emphasis on bridging the two areas. It explores cutting-edge algorithms---such as the proximal gradient, Douglas--Rachford, Peaceman--Rachford, and FISTA---that have applications in machine learning, signal processing, image reconstruction, and other fields. An Introduction to Convexity, Optimization, and Algorithms contains algorithms illustrated by Julia examples and more than 200 exercises that enhance the reader's understanding of the topic. Clear explanations and step-by-step algorithmic descriptions facilitate self-study for individuals looking to enhance their expertise in convex analysis and optimization.},
  isbn = {978-1-61197-779-0},
  langid = {english}
}

@book{bazaraaNonlinearProgrammingTheory2006,
  title = {Nonlinear {{Programming}}: {{Theory}} and {{Algorithms}}},
  shorttitle = {Nonlinear {{Programming}}},
  author = {Bazaraa, Mokhtar S. and Sherali, Hanif D. and Shetty, C. M.},
  year = {2006},
  month = may,
  edition = {3},
  publisher = {Wiley-Interscience},
  url = {https://www.wiley.com/en-us/Nonlinear+Programming%3A+Theory+and+Algorithms%2C+3rd+Edition-p-9780471486008},
  abstract = {COMPREHENSIVE COVERAGE OF NONLINEAR PROGRAMMING THEORY AND ALGORITHMS, THOROUGHLY REVISED AND EXPANDEDNonlinear Programming: Theory and Algorithms{\texthorizontalbar}now in an extensively updated Third Edition{\texthorizontalbar}addresses the problem of optimizing an objective function in the presence of equality and inequality constraints. Many realistic problems cannot be adequately represented as a linear program owing to the nature of the nonlinearity of the objective function and/or the nonlinearity of any constraints. The Third Edition begins with a general introduction to nonlinear programming with illustrative examples and guidelines for model construction.Concentration on the three major parts of nonlinear programming is provided:Convex analysis with discussion of topological properties of convex sets, separation and support of convex sets, polyhedral sets, extreme points and extreme directions of polyhedral sets, and linear programmingOptimality conditions and duality with coverage of the nature, interpretation, and value of the classical Fritz John (FJ) and the Karush-Kuhn-Tucker (KKT) optimality conditions; the interrelationships between various proposed constraint qualifications; and Lagrangian duality and saddle point optimality conditionsAlgorithms and their convergence, with a presentation of algorithms for solving both unconstrained and constrained nonlinear programming problemsImportant features of the Third Edition include:New topics such as second interior point methods, nonconvex optimization, nondifferentiable optimization, and moreUpdated discussion and new applications in each chapterDetailed numerical examples and graphical illustrationsEssential coverage of modeling and formulating nonlinear programsSimple numerical problemsAdvanced theoretical exercisesThe book is a solid reference for professionals as well as a useful text for students in the fields of operations research, management science, industrial engineering, applied mathematics, and also in engineering disciplines that deal with analytical optimization techniques. The logical and self-contained format uniquely covers nonlinear programming techniques with a great depth of information and an abundance of valuable examples and illustrations that showcase the most current advances in nonlinear problems.},
  isbn = {978-0-471-48600-8},
  langid = {english}
}

@book{beckFirstOrderMethodsOptimization2017,
  title = {First-{{Order Methods}} in {{Optimization}}},
  author = {Beck, Amir},
  year = {2017},
  month = oct,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611974997},
  url = {https://sites.google.com/site/amirbeck314/books},
  urldate = {2021-03-29},
  abstract = {This book, as the title suggests, is about first-order methods, namely, methods that exploit information on values and gradients/subgradients (but not Hessians) of the functions comprising the model under consideration. First-order methods go back to 1847 with the work of Cauchy on the steepest descent method. With the increase in the amount of applications that can be modeled as large- or even huge-scale optimization problems, there has been a revived interest in using simple methods that require low iteration cost as well as low memory storage.},
  isbn = {978-1-61197-498-0}
}

@book{beckIntroductionNonlinearOptimization2014,
  title = {Introduction to {{Nonlinear Optimization}}: : {{Theory}}, {{Algorithms}}, and {{Applications}} with {{MATLAB}}},
  author = {Beck, Amir},
  year = {2014},
  month = oct,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611973655},
  url = {https://sites.google.com/site/amirbeck314/books},
  urldate = {2022-01-26},
  abstract = {This book emerged from the idea that an optimization training should include three basic components: a strong theoretical and algorithmic foundation, familiarity with various applications, and the ability to apply the theory and algorithms on actual ``real-life'' problems. The book is intended to be the basis of such an extensive training. The mathematical development of the main concepts in nonlinear optimization is done rigorously, where a special effort was made to keep the proofs as simple as possible. The results are presented gradually and accompanied with many illustrative examples. Since the aim is not to give an encyclopedic overview, the focus is on the most useful and important concepts. The theory is complemented by numerous discussions on applications from various scientific fields such as signal processing, economics and localization. Some basic algorithms are also presented and studied to provide some flavor of this important aspect of optimization. Many topics are demonstrated by MATLAB programs, and ideally, the interested reader will find satisfaction in the ability of actually solving problems on his or her own. The book contains several topics that, compared to other classical textbooks, are treated differently. The following are some examples of the less common issues.},
  isbn = {978-1-61197-364-8}
}

@book{beckIntroductionNonlinearOptimization2023,
  title = {Introduction to {{Nonlinear Optimization}}: {{Theory}}, {{Algorithms}}, and {{Applications}} with {{Python}} and {{MATLAB}}},
  shorttitle = {Introduction to {{Nonlinear Optimization}}},
  author = {Beck, Amir},
  year = {2023},
  month = aug,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  edition = {2},
  publisher = {Society for Industrial \& Applied Mathematics},
  address = {Philadelphia, PA, USA},
  url = {https://doi.org/10.1137/1.9781611977622},
  abstract = {Built on the framework of the successful first edition, this book serves as a modern introduction to the field of optimization. The author's objective is to provide the foundations of theory and algorithms of nonlinear optimization as well as to present a variety of applications from diverse areas of applied sciences. Introduction to Nonlinear Optimization gradually yet rigorously builds connections between theory, algorithms, applications, and actual implementation. The book contains several topics not typically included in optimization books, such as optimality conditions in sparsity constrained optimization, hidden convexity, and total least squares. Readers will discover a wide array of applications such as circle fitting, Chebyshev center, the Fermat--Weber problem, denoising, clustering, total least squares, and orthogonal regression. These applications are studied both theoretically and algorithmically, illustrating concepts such as duality. Python and MATLAB programs are used to show how the theory can be implemented. The extremely popular CVX toolbox (MATLAB) and CVXPY module (Python) are described and used.More than 250 theoretical, algorithmic, and numerical exercises enhance the reader's understanding of the topics. (More than 70 of the exercises provide detailed solutions, and many others are provided with final answers.) The theoretical and algorithmic topics are illustrated by Python and MATLAB examples.},
  isbn = {978-1-61197-761-5},
  langid = {english}
}

@article{behrendtTechnicalReportTotally2021,
  title = {Technical {{Report}}: {{A Totally Asynchronous Algorithm}} for {{Tracking Solutions}} to {{Time-Varying Convex Optimization Problems}}},
  shorttitle = {Technical {{Report}}},
  author = {Behrendt, Gabriel and Hale, Matthew},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.06705 [math]},
  eprint = {2110.06705},
  primaryclass = {math},
  url = {http://arxiv.org/abs/2110.06705},
  urldate = {2022-05-10},
  abstract = {This paper presents a decentralized algorithm for a team of agents to track time-varying fixed points that are the solutions to time-varying convex optimization problems. The algorithm is first-order, and it allows for total asynchrony in the communications and computations of all agents, i.e., all such operations can occur with arbitrary timing and arbitrary (finite) delays. Convergence rates are computed in terms of the communications and computations that agents execute, without specifying when they must occur. These rates apply to convergence to the minimum of each individual objective function, as well as agents' long-run behavior as their objective functions change. Then, to improve the usage of limited communication and computation resources, we optimize the timing of agents' operations relative to changes in their objective functions to minimize total fixed point tracking error over time. Simulation results are presented to illustrate these developments in practice and empirically assess their robustness to uncertainties in agents' update laws.},
  archiveprefix = {arXiv}
}

@book{ben-talLecturesModernConvex2001,
  title = {Lectures on {{Modern Convex Optimization}}: {{Analysis}}, {{Algorithms}}, and {{Engineering Applications}}},
  shorttitle = {Lectures on {{Modern Convex Optimization}}},
  author = {{Ben-Tal}, Aharon and Nemirovski, Arkadi},
  year = {2001},
  month = aug,
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia, PA},
  abstract = {Here is a book devoted to well-structured and thus efficiently solvable convex optimization problems, with emphasis on conic quadratic and semidefinite programming. The authors present the basic theory underlying these problems as well as their numerous applications in engineering, including synthesis of filters, Lyapunov stability analysis, and structural design. The authors also discuss the complexity issues and provide an overview of the basic theory of state-of-the-art polynomial time interior point methods for linear, conic quadratic, and semidefinite programming. The book's focus on well-structured convex problems in conic form allows for unified theoretical and algorithmical treatment of a wide spectrum of important optimization problems arising in applications.},
  isbn = {978-0-89871-491-3},
  langid = {english}
}

@unpublished{ben-talLecturesModernConvex2023,
  type = {Lecture Notes},
  title = {Lectures on {{Modern Convex Optimization}} - 2020/2021/2022/2023 {{Analysis}}, {{Algorithms}}, {{Engineering Applications}}},
  shorttitle = {Lectures on {{Modern Convex Optimization}}},
  author = {{Ben-Tal}, Aharon and Nemirovski, Arkadi},
  year = {2023},
  address = {Technion \& Georgia Institute of Technology},
  url = {https://www2.isye.gatech.edu/~nemirovs/LMCOLN2023Spring.pdf}
}

@book{ben-talRobustOptimization2009,
  title = {Robust {{Optimization}}},
  author = {{Ben-Tal}, Aharon and Ghaoui, Laurent El and Nemirovski, Arkadi},
  year = {2009},
  month = aug,
  publisher = {Princeton University Press},
  address = {Princeton},
  abstract = {Robust optimization is still a relatively new approach to optimization problems affected by uncertainty, but it has already proved so useful in real applications that it is difficult to tackle such problems today without considering this powerful methodology. Written by the principal developers of robust optimization, and describing the main achievements of a decade of research, this is the first book to provide a comprehensive and up-to-date account of the subject. Robust optimization is designed to meet some major challenges associated with uncertainty-affected optimization problems: to operate under lack of full information on the nature of uncertainty; to model the problem in a form that can be solved efficiently; and to provide guarantees about the performance of the solution. The book starts with a relatively simple treatment of uncertain linear programming, proceeding with a deep analysis of the interconnections between the construction of appropriate uncertainty sets and the classical chance constraints (probabilistic) approach. It then develops the robust optimization theory for uncertain conic quadratic and semidefinite optimization problems and dynamic (multistage) problems. The theory is supported by numerous examples and computational illustrations. An essential book for anyone working on optimization and decision making under uncertainty, Robust Optimization also makes an ideal graduate textbook on the subject.},
  isbn = {978-0-691-14368-2},
  langid = {english}
}

@article{bergmannManoptJlOptimization2022,
  title = {Manopt.Jl: {{Optimization}} on {{Manifolds}} in {{Julia}}},
  shorttitle = {Manopt.Jl},
  author = {Bergmann, Ronny},
  year = {2022},
  month = feb,
  journal = {Journal of Open Source Software},
  volume = {7},
  number = {70},
  pages = {3866},
  issn = {2475-9066},
  doi = {10.21105/joss.03866},
  url = {https://joss.theoj.org/papers/10.21105/joss.03866},
  urldate = {2022-02-11},
  abstract = {Bergmann, R., (2022). Manopt.jl: Optimization on Manifolds in Julia. Journal of Open Source Software, 7(70), 3866, https://doi.org/10.21105/joss.03866},
  langid = {english}
}

@article{bernsteinOnlinePrimalDualMethods2019,
  title = {Online {{Primal-Dual Methods With Measurement Feedback}} for {{Time-Varying Convex Optimization}}},
  author = {Bernstein, Andrey and Dall'Anese, Emiliano and Simonetto, Andrea},
  year = {2019},
  month = apr,
  journal = {IEEE Transactions on Signal Processing},
  volume = {67},
  number = {8},
  pages = {1978--1991},
  issn = {1941-0476},
  doi = {10.1109/TSP.2019.2896112},
  abstract = {This paper addresses the design and analysis of feedback-based online algorithms to control systems or networked systems based on performance objectives and engineering constraints that may evolve over time. The emerging time-varying convex optimization formalism is leveraged to model optimal operational trajectories of the systems, as well as explicit local and network-level operational constraints. Departing from existing batch and feed-forward optimization approaches, the design of the algorithms capitalizes on an online implementation of primal-dual projected-gradient methods; the gradient steps are, however, suitably modified to accommodate feedback from the system in the form of measurements, hence, the term ``online optimization with feedback.'' By virtue of this approach, the resultant algorithms can cope with model mismatches in the algebraic representation of the system states and outputs, they avoid pervasive measurements of exogenous inputs, and they naturally lend themselves to a distributed implementation. Under suitable assumptions, analytical convergence claims are established in terms of dynamic regret. Furthermore, when the synthesis of the feedback-based online algorithms is based on a regularized Lagrangian function, Q-linear convergence to solutions of the time-varying optimization problem is shown.}
}

@book{bertsekasConstrainedOptimizationLagrange1996,
  title = {Constrained {{Optimization}} and {{Lagrange Multiplier Methods}}},
  author = {Bertsekas, Dimitri P. and Bertsekas, Dimitri P.},
  year = {1996},
  month = jan,
  publisher = {Athena Scientific},
  address = {Belmont, Mass},
  url = {http://www.athenasc.com/lmultbook.html},
  isbn = {978-1-886529-04-5},
  langid = {english}
}

@misc{bertsekasConvexAnalysisOptimization2014,
  type = {Lecture Slides},
  title = {Convex Analysis and Optimization Based on 6.253 Class Lectures at the {{Mass}}. {{Institute}} of {{Technology}}, {{Cambridge}}, {{Mass}}, {{Spring}} 2014},
  author = {Bertsekas, Dimitri P.},
  year = {2014},
  url = {http://web.mit.edu/dimitrib/www/home.html}
}

@book{bertsekasConvexOptimizationAlgorithms2015,
  title = {Convex {{Optimization Algorithms}}},
  author = {Bertsekas, Dimitri P.},
  year = {2015},
  month = feb,
  publisher = {Athena Scientific},
  address = {Nashua},
  url = {http://www.athenasc.com/convexalgorithms.html},
  isbn = {978-1-886529-28-1},
  langid = {english}
}

@book{bertsekasConvexOptimizationTheory2009,
  title = {Convex {{Optimization Theory}}},
  author = {Bertsekas, Dimitri P.},
  year = {2009},
  month = jun,
  publisher = {Athena Scientific},
  address = {Belmont, Mass},
  url = {http://web.mit.edu/dimitrib/www/Convex_Theory_Entire_Book.pdf},
  isbn = {978-1-886529-31-1},
  langid = {english}
}

@incollection{bertsekasIncrementalGradientSubgradient2012,
  title = {Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey},
  booktitle = {Optimization for {{Machine Learning}}},
  author = {Bertsekas, Dimitri P.},
  year = {2012},
  publisher = {MIT Press},
  abstract = {The interplay between optimization and machine learning is one of the most important developments in modern computational science. Optimization formulations and methods are proving to be vital in designing algorithms to extract essential knowledge from huge volumes of data. Machine learning, however, is not simply a consumer of optimization technology but a rapidly evolving field that is itself generating new optimization ideas. This book captures the state of the art of the interaction between optimization and machine learning in a way that is accessible to researchers in both fields.Optimization approaches have enjoyed prominence in machine learning because of their wide applicability and attractive theoretical properties. The increasing complexity, size, and variety of today's machine learning models call for the reassessment of existing assumptions. This book starts the process of reassessment. It describes the resurgence in novel contexts of established frameworks such as first-order methods, stochastic approximations, convex relaxations, interior-point methods, and proximal methods. It also devotes attention to newer themes such as regularized optimization, robust optimization, gradient and subgradient methods, splitting techniques, and second-order methods. Many of these techniques draw inspiration from other fields, including operations research, theoretical computer science, and subfields of optimization. The book will enrich the ongoing cross-fertilization between the machine learning community and these other fields, and within the broader optimization community.},
  isbn = {978-0-262-01646-9},
  langid = {english}
}

@book{bertsekasNonlinearProgramming2016,
  title = {Nonlinear {{Programming}}},
  shorttitle = {Nonlinear {{Programming}}},
  author = {Bertsekas, Dimitri},
  year = {2016},
  month = jun,
  edition = {3rd},
  publisher = {Athena Scientific},
  address = {Belmont, Mass},
  url = {http://www.athenasc.com/nonlinbook.html},
  isbn = {978-1-886529-05-2},
  langid = {english}
}

@book{bertsekasParallelDistributedComputation1997,
  title = {Parallel and {{Distributed Computation}}: {{Numerical Methods}}},
  shorttitle = {Parallel and {{Distributed Computation}}},
  author = {Bertsekas, Dimitri P. and Tsitsiklis, John},
  year = {1997},
  month = jan,
  publisher = {Athena Scientific},
  address = {Belmont, Mass},
  url = {http://www.athenasc.com/pdcbook.html},
  abstract = {This highly acclaimed work, first published by Prentice Hall in 1989, is a comprehensive and theoretically sound treatment of parallel and distributed numerical methods. It focuses on algorithms that are naturally suited for massive parallelization, and it explores the fundamental convergence, rate of convergence, communication, and synchronization issues associated with such algorithms. This is an extensive book, which aside from its focus on parallel and distributed algorithms, contains a wealth of material on a broad variety of computation and optimization topics. Among its special features, the book: 1) Quantifies the performance of parallel algorithms, including the limitations imposed by the communication and synchronization penalties. 2) Describes communication algorithms for a variety of system architectures including tree, mesh, and hypercube. 3) Provides a comprehensive convergence analysis of asynchronous methods and a comparison with their asynchronous counterparts. 4) Cove},
  isbn = {978-1-886529-01-4},
  langid = {english}
}

@article{bertsekasProjectedNewtonMethods1982,
  title = {Projected {{Newton Methods}} for {{Optimization Problems}} with {{Simple Constraints}}},
  author = {Bertsekas, D.},
  year = {1982},
  month = mar,
  journal = {SIAM Journal on Control and Optimization},
  volume = {20},
  number = {2},
  pages = {221--246},
  issn = {0363-0129},
  doi = {10.1137/0320018},
  url = {https://epubs.siam.org/doi/abs/10.1137/0320018},
  urldate = {2019-04-11},
  abstract = {We consider the problem \${\textbackslash}min {\textbackslash}\{ f(x){\textbar}x {\textbackslash}geqq 0{\textbackslash}\} \$, and propose algorithms of the form \$x\_\{k + 1\}  = [x\_k  - {\textbackslash}alpha \_k D\_k {\textbackslash}nabla f(x\_k )]{\textasciicircum} +  \$, where \$[ {\textbackslash}cdot ]{\textasciicircum} +  \$ denotes projection on the positive orthant, \${\textbackslash}alpha \_k \$ is a stepsize chosen by an Armijo-like rule and \$D\_k \$ is a positive definite symmetric matrix which is partly diagonal. We show that \$D\_k \$ can be calculated simply on the basis of second derivatives of f so that the resulting Newton-like algorithm has a typically superlinear rate of convergence. With other choices of \$D\_k \$ convergence at a typically linear rate is obtained. The algorithms are almost as simple as their unconstrained counterparts. They are well suited for problems of large dimension such as those arising in optimal control while being competitive with existing methods for low-dimensional problems. The effectiveness of the Newton-like algorithm is demonstrated via computational examples involving as many as 10,000 variables. Extensions to general linearly constrained problems are also provided. These extensions utilize a notion of an active generalized rectangle patterned after the notion of an active manifold used in manifold suboptimization methods. By contrast with these methods, many constraints can be added or subtracted from the binding set at each iteration without the need to solve a quadratic programming problem.}
}

@article{bertsekasPseudonormalityLagrangeMultiplier2002,
  title = {Pseudonormality and a {{Lagrange Multiplier Theory}} for {{Constrained Optimization}}},
  author = {Bertsekas, D.P. and Ozdaglar, A.E.},
  year = {2002},
  month = aug,
  journal = {Journal of Optimization Theory and Applications},
  volume = {114},
  number = {2},
  pages = {287--343},
  issn = {1573-2878},
  doi = {10.1023/A:1016083601322},
  url = {https://doi.org/10.1023/A:1016083601322},
  urldate = {2022-05-17},
  abstract = {We consider optimization problems with equality, inequality, and abstract set constraints, and we explore various characteristics of the constraint set that imply the existence of Lagrange multipliers. We prove a generalized version of the Fritz--John theorem, and we introduce new and general conditions that extend and unify the major constraint qualifications. Among these conditions, two new properties, pseudonormality and quasinormality, emerge as central within the taxonomy of interesting constraint characteristics. In the case where there is no abstract set constraint, these properties provide the connecting link between the classical constraint qualifications and two distinct pathways to the existence of Lagrange multipliers: one involving the notion of quasiregularity and the Farkas lemma, and the other involving the use of exact penalty functions. The second pathway also applies in the general case where there is an abstract set constraint.},
  langid = {english}
}

@book{bertsimasIntroductionLinearOptimization1997,
  title = {Introduction to {{Linear Optimization}}},
  author = {Bertsimas, Dimitris and Tsitsiklis, John N.},
  year = {1997},
  month = feb,
  publisher = {Athena Scientific},
  address = {Belmont, Mass},
  url = {http://athenasc.com/linoptbook.html},
  abstract = {This book provides a unified, insightful, and modern treatment of linear optimization, that is, linear programming, network flow problems, and discrete optimization. It includes classical topics as well as the state of the art, in both theory and practice.},
  isbn = {978-1-886529-19-9},
  langid = {english}
}

@incollection{bertsimasMomentProblemsSemidefinite2000,
  title = {Moment {{Problems}} and {{Semidefinite Optimization}}},
  booktitle = {Handbook of {{Semidefinite Programming}}},
  author = {Bertsimas, Dimitris and Sethuraman, Jay},
  editor = {Wolkowicz, Henry and Saigal, Romesh and Vandenberghe, Lieven},
  year = {2000},
  month = jan,
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  number = {27},
  pages = {469--509},
  publisher = {Springer US},
  url = {http://link.springer.com/chapter/10.1007/978-1-4615-4381-7_16},
  urldate = {2014-07-01},
  abstract = {Problems involving moments of random variables arise naturally in many areas of mathematics, economics, and operations research. Let us give some examples that motivate the present paper.},
  copyright = {{\copyright}2000 Kluwer Academic Publishers},
  isbn = {978-1-4613-6970-7 978-1-4615-4381-7},
  langid = {english}
}

@article{bertsimasOnlineMixedIntegerOptimization2022,
  title = {Online {{Mixed-Integer Optimization}} in {{Milliseconds}}},
  author = {Bertsimas, Dimitris and Stellato, Bartolomeo},
  year = {2022},
  month = jul,
  journal = {INFORMS Journal on Computing},
  volume = {34},
  number = {4},
  pages = {2229--2248},
  publisher = {INFORMS},
  issn = {1091-9856},
  doi = {10.1287/ijoc.2022.1181},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/ijoc.2022.1181},
  urldate = {2023-09-20},
  abstract = {We propose a method to approximate the solution of online mixed-integer optimization (MIO) problems at very high speed using machine learning. By exploiting the repetitive nature of online optimization, we can greatly speed up the solution time. Our approach encodes the optimal solution into a small amount of information denoted as strategy using the voice of optimization framework. In this way, the core part of the optimization routine becomes a multiclass classification problem that can be solved very quickly. In this work, we extend that framework to real-time and high-speed applications focusing on parametric mixed-integer quadratic optimization. We propose an extremely fast online optimization method consisting of a feedforward neural network evaluation and a linear system solution where the matrix has already been factorized. Therefore, this online approach does not require any solver or iterative algorithm. We show the speed of the proposed method both in terms of total computations required and measured execution time. We estimate the number of floating point operations required to completely recover the optimal solution as a function of the problem dimensions. Compared with state-of-the-art MIO routines, the online running time of our method is very predictable and can be lower than a single matrix factorization time. We benchmark our method against the state-of-the-art solver Gurobi obtaining up to two to three orders of magnitude speedups on examples from fuel cell energy management, sparse portfolio optimization, and motion planning with obstacle avoidance. Summary of Contribution: We propose a technique to approximate the solution of online optimization problems at high speed using machine learning. By exploiting the repetitive nature of online optimization, we learn the mapping between the key problem parameters and an encoding of the optimal solution to greatly speed up the solution time. This allows us to significantly improve the computation time and resources needed to solve online mixed-integer optimization problems. We obtain a simple method with a very low computing time variance, which is crucial in online settings.}
}

@book{bertsimasOptimizationIntegers2005,
  title = {Optimization {{Over Integers}}},
  author = {Bertsimas, Dimitris and Weismantel, Robert},
  year = {2005},
  month = jun,
  publisher = {Dynamic Ideas},
  address = {Belmont},
  url = {https://www.dynamic-ideas.com/books/x0g7bsm2nvnl6j7ebqodcrhsvlgbm7},
  abstract = {The book provides a unified, insightful, and modern treatment of the theory of integer optimization. The book is used in the doctoral level course, "Integer and Combinatorial Optimization" at the Massachusetts Institute of Technology. For solutions to exercises and other instructor resources, please contact Dimitris Bertsimas (dbertsim@mit.edu). The chapters of the book are logically organized in four parts: Part I: Formulations and relaxations includes Chapters 1-5 and discusses how to formulate integer optimization problems, how to enhance the formulations to improve the quality of relaxations, how to obtain ideal formulations, the duality of integer optimization and how to solve the resulting relaxations both practically and theoretically. Part II: Algebra and geometry of integer optimization includes Chapters 6-8 and develops the theory of lattices, oulines ideas from algebraic geometry that have had an impact on integer optimization, and most importantly discusses the geometry of integer optimization, a key feature of the book. These chapters provide the building blocks for developing algorithms. Part III: Algorithms for integer optimization includes Chapters 9-12 and develops cutting plane methods, integral basis methods, enumerative and heuristic methods and approximation algorithms. The key characteristic of our treatment is that our development of the algorithms is naturally based on the algebraic and geometric developments of Part II. Part IV: Extensions of integer optimization includes Chapters 13 and 14, and treats mixed integer optimization and robust discrete optimization. Both areas are practically significant as real world problems have very often both continuous and discrete variables and have elements of uncertainty that need to be addressed in a tractable manner. Distinguishing Characteristics Of This Book:* Develops the theory of integer optimization from a new geometric perspective via integral generating sets; * Emphasizes strong formulations, ways to improve them, integral polyhedra, duality, and relaxations; * Discusses applications of lattices and algebraic geometry to integer optimization, including Grobner bases, optimization over polynomials and counting integer points in polyhedra; * Contains a unified geometric treatment of cutting plane and integral basis methods; * Covers enumerative and heuristic methods, including local search over exponential neighborhoods and simulated annealing; * Presents the major methods to construct approximation algorithms: primal-dual, randomized rounding, semidefinite and enumerative methods; * Provides a unified treatment of mixed integer and robust discrete optimization; * Includes a large number of examples and exercises developed through extensive classroom use.},
  isbn = {978-0-9759146-2-5},
  langid = {english}
}

@book{bertsimasRobustAdaptiveOptimization2022,
  title = {Robust and {{Adaptive Optimization}}},
  author = {Bertsimas, Dimitris and Hertog, Dick Den},
  year = {1 ledna 2022},
  publisher = {Dynamic Ideas},
  address = {Belmont, Massachusetts},
  url = {https://www.dynamic-ideas.com/books/robust-and-adaptive-optimization},
  abstract = {This book provides an original treatment of robust (RO) and adaptive robust optimization (ARO) based on over twenty years of research by each of the authors.  Structure of the Book Part I describes linear RO and the underlying uncertainty sets.  Part II treats modeling, exact and approximate algorithms for ARO. Part III introduces nonlinear RO for concave uncertainty. Part IV outlines nonlinear RO for concave uncertainty. Part V discusses the theory of distributional RO and ARO. Part VI contains a variety of RO and ARO applications including queueing theory, auction design, option pricing and energy unit commitment.  ORIGINAL CHARACTERISTICS OF THE BOOK: Emphasis on modeling in RO and ARO.  Integrated treatment of RO and ARO and of nonlinear RO for concave and convex uncertainty, as opposed to robust conic optimization.  Interplay of probability theory to select parameters in RO and of optimization for tractability.},
  isbn = {978-1-73378-852-6}
}

@article{bertsimasTheoryApplicationsRobust2011,
  title = {Theory and {{Applications}} of {{Robust Optimization}}},
  author = {Bertsimas, Dimitris and Brown, David B. and Caramanis, Constantine},
  year = {2011},
  month = jan,
  journal = {SIAM Review},
  volume = {53},
  number = {3},
  pages = {464--501},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/080734510},
  url = {https://epubs.siam.org/doi/abs/10.1137/080734510},
  urldate = {2023-05-28},
  abstract = {In this paper we consider semidefinite programs (SDPs) whose data depend on some unknown but bounded perturbation parameters. We seek "robust" solutions to such programs, that is, solutions which minimize the (worst-case) objective while satisfying the constraints for every possible value of parameters within the given bounds. Assuming the data matrices are rational functions of the perturbation parameters, we show how to formulate sufficient conditions for a robust solution to exist as SDPs. When the perturbation is "full," our conditions are necessary and sufficient. In this case, we provide sufficient conditions which guarantee that the robust solution is unique and continuous (H{\"o}lder-stable) with respect to the unperturbed problem's data. The approach can thus be used to regularize ill-conditioned SDPs. We illustrate our results with examples taken from linear programming, maximum norm minimization, polynomial interpolation, and integer programming.}
}

@book{bieglerNonlinearProgramming2010,
  title = {Nonlinear {{Programming}}},
  author = {Biegler, Lorenz T.},
  year = {2010},
  month = jan,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898719383},
  url = {https://my.siam.org/Store/Product/viewproduct/?ProductId=1488},
  urldate = {2022-01-26},
  abstract = {Chemical engineering applications have been a source of challenging optimization problems for over 50 years. For many chemical process systems, detailed steady state and dynamic behavior can now be described by a rich set of detailed nonlinear models, and relatively small changes in process design and operation can lead to significant improvements in efficiency, product quality, environmental impact, and profitability.With these characteristics, it is not surprising that systematic optimization strategies have played an important role in chemical engineering practice. In particular, over the past 35 years, nonlinear programming (NLP) has become an indispensable tool for the optimization of chemical processes. These tools are now applied at research and process development stages, in the design stage, and in the online operation of these processes. More recently, the scope of these applications is being extended to cover more challenging, large-scale tasks including process control based on the optimization of nonlinear dynamic models, as well as the incorporation of nonlinear models into strategic planning functions. Moreover, the ability to solve large-scale process optimization models cheaply, even online, is aided by recent breakthroughs in nonlinear programming, including the development of modern barrier methods, deeper understanding of line search and trust region strategies to aid global convergence, efficient exploitation of second derivatives in algorithmic development, and the availability of recently developed and widely used NLP codes, including those for barrier methods [81, 391, 404], sequential quadratic programming (SQP) [161, 159], and reduced gradient methods [119, 245, 285]. Finally, the availability of optimization modeling environments, such as AIMMS, AMPL, and GAMS, as well as the NEOS server, has made the formulation and solution of optimization accessible to a much wider user base. All of these advances have a huge impact in addressing and solving process engineering problems previously thought intractable. In addition to developments in mathematical programming, research in process systems engineering has led to optimization modeling formulations that leverage these algorithmic advances, with specific model structure and characteristics that lead to more efficient solutions. This text attempts to make these recent optimization advances accessible to engineers and practitioners. Optimization texts for engineers usually fall into two categories. First, excellent mathematical programming texts (e.g., [134, 162, 294, 100, 227]) emphasize fundamental properties and numerical analysis, but have few specific examples with relevance to real-world applications, and are less accessible to practitioners. On the other hand, equally good engineering texts (e.g., [122, 305, 332, 53]) emphasize applications with well-known methods and codes, but often without their underlying fundamental properties. While their approach is accessible and quite useful for engineers, these texts do not aid in a deeper understanding of the methods or provide extensions to tackle large-scale problems efficiently.},
  isbn = {978-0-89871-702-0}
}

@article{bieglerRetrospectiveOptimization2004,
  title = {Retrospective on Optimization},
  author = {Biegler, Lorenz T. and Grossmann, Ignacio E.},
  year = {2004},
  month = jul,
  journal = {Computers \& Chemical Engineering},
  volume = {28},
  number = {8},
  pages = {1169--1192},
  issn = {0098-1354},
  doi = {10.1016/j.compchemeng.2003.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0098135403003089},
  urldate = {2023-11-04},
  abstract = {In this paper, we provide a general classification of mathematical optimization problems, followed by a matrix of applications that shows the areas in which these problems have been typically applied in process systems engineering. We then provide a review of solution methods of the major types of optimization problems for continuous and discrete variable optimization, particularly nonlinear and mixed-integer nonlinear programming (MINLP). We also review their extensions to dynamic optimization and optimization under uncertainty. While these areas are still subject to significant research efforts, the emphasis in this paper is on major developments that have taken place over the last 25 years.}
}

@article{bielEfficientStochasticProgramming2022,
  title = {Efficient {{Stochastic Programming}} in {{Julia}}},
  author = {Biel, Martin and Johansson, Mikael},
  year = {2022},
  month = jul,
  journal = {INFORMS Journal on Computing},
  volume = {34},
  number = {4},
  pages = {1885--1902},
  publisher = {INFORMS},
  issn = {1091-9856},
  doi = {10.1287/ijoc.2022.1158},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/ijoc.2022.1158},
  urldate = {2024-05-01},
  abstract = {We present StochasticPrograms.jl, a user-friendly and powerful open-source framework for stochastic programming written in the Julia language. The framework includes both modeling tools and structure-exploiting optimization algorithms. Stochastic programming models can be efficiently formulated using an expressive syntax, and models can be instantiated, inspected, and analyzed interactively. The framework scales seamlessly to distributed environments. Small instances of a model can be run locally to ensure correctness, whereas larger instances are automatically distributed in a memory-efficient way onto supercomputers or clouds and solved using parallel optimization algorithms. These structure-exploiting solvers are based on variations of the classical L-shaped, progressive-hedging, and quasi-gradient algorithms. We provide a concise mathematical background for the various tools and constructs available in the framework along with code listings exemplifying their usage. Both software innovations related to the implementation of the framework and algorithmic innovations related to the structured solvers are highlighted. We conclude by demonstrating strong scaling properties of the distributed algorithms on numerical benchmarks in a multinode setup. Summary of Contribution: This paper presents StochasticPrograms.jl, an open-source framework for stochastic programming implemented in the Julia programming language. The framework includes an expressive syntax for formulating stochastic programming models as well as versatile analysis tools and parallel optimization algorithms. The framework will prove useful to researchers, educators, and industrial users alike. Researchers will benefit from the readily extensible open-source framework, in which they can formulate complex stochastic models or quickly typeset and test novel optimization algorithms. Educators of stochastic programming will benefit from the clean and expressive syntax. Moreover, the framework supports analysis tools and stochastic programming constructs from classical theory and leading textbooks. We strongly believe that the StochasticPrograms.jl framework can reduce the barrier to entry for incoming practitioners of stochastic programming. Industrial practitioners can make use of StochasticPrograms.jl to rapidly formulate complex models, analyze small instances locally, and then run large-scale instances in production. In doing so, they get distributed capabilities for free without changing the code and access to well-tested state-of-the-art implementations of parallel structure-exploiting solvers. As the framework is open-source, anyone from these target audiences can contribute with new functionality to the framework. In conclusion, by providing both an intuitive interface for new users and an extensive development environment for expert users, StochasticPrograms.jl has strong potential to further the field of stochastic programming.}
}

@book{bierlaireOptimizationPrinciplesAlgorithms2018,
  title = {Optimization: {{Principles}} and {{Algorithms}}},
  shorttitle = {Optimization},
  author = {Bierlaire, Michel},
  year = {2018},
  edition = {2},
  publisher = {EPFL Press},
  address = {Lausanne},
  url = {https://transp-or.epfl.ch/books/optimization/html/OptimizationPrinciplesAlgorithms2018.pdf},
  abstract = {Optimization algorithms are critical tools for engineers, but difficult to use since none of them are universal in application. This introductery text builds up the knowledge set, from the basics, so that engineering students can understand the processes that govern optimization processes. Reinforcing theory with practice, the book includes discussions on how to match applications to appropiate methods and why certain approaches are not adapted to specific requirements.Following the success and adoption of the French version of this book with undergraduate and graduate students, the author has translated and substantially updated and reworked this new English edition. Featuring more worked problems and supported by a companion website, readers can learn the practical elements of building algorithms to solve real-life problems.},
  isbn = {978-2-88915-279-7},
  langid = {english}
}

@misc{BilevelProgramming2016,
  title = {Bilevel Programming},
  year = {2016},
  month = sep,
  journal = {YALMIP},
  url = {https://yalmip.github.io/tutorial/bilevelprogramming/},
  urldate = {2022-03-24},
  abstract = {Bilevel programming using the built-in bilevel solver},
  langid = {english}
}

@book{birgeIntroductionStochasticProgramming2011,
  title = {Introduction to {{Stochastic Programming}}},
  author = {Birge, John R. and Louveaux, Fran{\c c}ois},
  year = {27 {\v c}ervna 2011},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  edition = {2},
  publisher = {Springer},
  address = {New York, NY},
  url = {https://doi.org/10.1007/978-1-4614-0237-4},
  abstract = {The aim of stochastic programming is to find optimal decisions in problems~ which involve uncertain data. This field is currently developing rapidly with contributions from many disciplines including operations research, mathematics, and probability. At the same time, it is now being applied in a wide variety of subjects ranging from agriculture to financial planning and from industrial engineering to computer networks. This textbook provides a first course in stochastic programming suitable for students with a basic knowledge of linear programming, elementary analysis, and probability. The authors aim to present a broad overview of the main themes and methods of the subject. Its prime goal is to help students develop an intuition on how to model uncertainty into mathematical problems, what uncertainty changes bring to the decision process, and what techniques help to manage uncertainty in solving the problems.In this extensively updated new edition there is more material on methods and examples including several new approaches for discrete variables, new results on risk measures in modeling and Monte Carlo sampling methods, a new chapter on relationships to other methods including approximate dynamic programming, robust optimization and online methods.The book is highly illustrated with chapter summaries and many examples and exercises. Students, researchers and practitioners in operations research and the optimization area will find it particularly of interest. Review of First Edition:"The discussion on modeling issues, the large number of examples used to illustrate the material, and the breadth of the coverage make~'Introduction to Stochastic Programming' an ideal textbook for the area." (Interfaces, 1998)},
  isbn = {978-1-4614-0236-7}
}

@book{birginPracticalAugmentedLagrangian2014,
  title = {Practical {{Augmented Lagrangian Methods}} for {{Constrained Optimization}}},
  author = {Birgin, E. G. and Mart{\'i}nez, J. M.},
  year = {2014},
  month = may,
  series = {Fundamentals of {{Algorithms}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611973365},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9781611973365},
  urldate = {2022-01-26},
  abstract = {This book is about the Augmented Lagrangian method, a popular technique for solving constrained optimization problems. It is mainly dedicated to engineers, chemists, physicists, economists, and general users of constrained optimization for solving real-life problems. Nevertheless, it describes in rigorous mathematical terms the convergence theory that applies to the algorithms analyzed. Users often need to understand with precision the properties of the solutions that a practical algorithm finds and the way in which these properties are reflected in practice. Many theorems concerning the behavior of practical algorithms will be found in this book. The geometrical and computational meaning of each theoretical result will be highlighted to make the relevant theory accessible to practitioners. Often, the assumptions under which we prove that algorithms work will not be the most general ones but will be those whose interpretation helps one to understand the computational behavior in real-life problems. Moreover, the plausibility of most assumptions will be discussed, presenting simple sufficient conditions under which assumptions hold. This helps one foresee what can be expected from a practical algorithm and which properties are not expected at all.},
  isbn = {978-1-61197-335-8}
}

@misc{blekhermanIntroductionSumsSquares2019,
  type = {{{AMS Short Course}} on {{Sums}} of {{Squares}}},
  title = {Introduction to {{Sums}} of {{Squares}}},
  author = {Blekherman, Greg},
  year = {2019},
  url = {https://www.ams.org/meetings/shortcourse/BlekhermanSlides-Updated.pdf}
}

@book{blekhermanSemidefiniteOptimizationConvex2013,
  title = {Semidefinite {{Optimization}} and {{Convex Algebraic Geometry}}},
  editor = {Blekherman, Grigoriy and Parrilo, Pablo A. and Thomas, Rekha},
  year = {2013},
  month = mar,
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  url = {http://www.mit.edu/~parrilo/sdocag/},
  isbn = {978-1-61197-228-3},
  langid = {english}
}

@article{bloomfieldLeastAbsoluteDeviations1980,
  title = {Least {{Absolute Deviations Curve-Fitting}}},
  author = {Bloomfield, P. and Steiger, W.},
  year = {1980},
  month = jun,
  journal = {SIAM Journal on Scientific and Statistical Computing},
  volume = {1},
  number = {2},
  pages = {290--301},
  issn = {0196-5204},
  doi = {10.1137/0901019},
  url = {https://epubs.siam.org/doi/10.1137/0901019},
  urldate = {2018-08-01},
  abstract = {A method is proposed for least absolute deviations curve fitting. It may be used to obtain least absolute deviations fits of general linear regressions. As a special case it includes a minor variant of a method for fitting straight lines by least absolute deviations that was previously thought to possess no generalization. The method has been tested on a computer and was found on a range of problems to execute in as little as \$\{1 /3 \}\$ the CPU time required by a published algorithm based on linear programming. More important, this advantage appears to increase indefinitely with the number of data points}
}

@book{bloomfieldLeastAbsoluteDeviations1983,
  title = {Least {{Absolute Deviations}}: {{Theory}}, {{Applications}} and {{Algorithms}}},
  shorttitle = {Least {{Absolute Deviations}}},
  author = {Bloomfield, Peter and Steiger, William},
  year = {1983},
  series = {Progress in {{Probability}}},
  publisher = {Birkh{\"a}user Basel},
  url = {//www.springer.com/us/book/9781468485769},
  urldate = {2018-08-01},
  abstract = {Least squares is probably the best known method for fitting linear models and by far the most widely used. Surprisingly, the discrete L 1 analogue, least absolute deviations (LAD) seems to have been considered first. Possibly the LAD criterion was forced into the background because of the com\- putational difficulties associated with it. Recently there has been a resurgence of interest in LAD. It was spurred on by work that has resulted in efficient al\- gorithms for obtaining LAD fits. Another stimulus came from robust statistics. LAD estimates resist undue effects from a feyv, large errors. Therefore. in addition to being robust, they also make good starting points for other iterative, robust procedures. The LAD criterion has great utility. LAD fits are optimal for linear regressions where the errors are double exponential. However they also have excellent properties well outside this narrow context. In addition they are useful in other linear situations such as time series and multivariate data analysis. Finally, LAD fitting embodies a set of ideas that is important in linear optimization theory and numerical analysis. viii PREFACE In this monograph we will present a unified treatment of the role of LAD techniques in several domains. Some of the material has appeared in recent journal papers and some of it is new. This presentation is organized in the following way. There are three parts, one for Theory, one for Applicatior.s and one for Algorithms.},
  isbn = {978-1-4684-8576-9},
  langid = {english}
}

@phdthesis{bookhovenPiecewiselinearModellingAnalysis1981,
  title = {Piecewise-Linear Modelling and Analysis},
  author = {Bookhoven, van, W. M. G.},
  year = {1981},
  address = {Eindhoven, NL},
  url = {https://doi.org/10.6100/IR118197},
  school = {Technische Hogeschool Eindhoven}
}

@article{bootSensitivityAnalysisConvex1963,
  title = {On {{Sensitivity Analysis}} in {{Convex Quadratic Programming Problems}}},
  author = {Boot, J. C. G.},
  year = {1963},
  month = oct,
  journal = {Operations Research},
  publisher = {INFORMS},
  doi = {10.1287/opre.11.5.771},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/opre.11.5.771},
  urldate = {2024-08-14},
  abstract = {The present paper consists of three clearly distinct sections. In the first section we discuss the problem of how the solution vector of a quadratic programming problem changes in case of infinites...},
  copyright = {{\copyright} 1963 INFORMS},
  langid = {english}
}

@book{borweinConvexAnalysisNonlinear2006,
  title = {Convex {{Analysis}} and {{Nonlinear Optimization}}: {{Theory}} and {{Examples}}},
  shorttitle = {Convex {{Analysis}} and {{Nonlinear Optimization}}},
  author = {Borwein, Jonathan and Lewis, Adrian S.},
  year = {2006},
  series = {{{CMS Books}} in {{Mathematics}}},
  edition = {2},
  publisher = {Springer},
  address = {New York},
  url = {https://link.springer.com/book/10.1007/978-0-387-31256-9},
  isbn = {978-0-387-29570-1},
  langid = {english}
}

@book{borziComputationalOptimizationSystems2012,
  title = {Computational {{Optimization}} of {{Systems Governed}} by {{Partial Differential Equations}}},
  author = {Borz{\`i}, Alfio and Schulz, Volker},
  year = {2012},
  month = jan,
  series = {Computational {{Science}} and {{Engineering}}},
  number = {8},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9781611972054},
  isbn = {978-1-61197-204-7},
  langid = {english}
}

@book{boumalIntroductionOptimizationSmooth2023,
  title = {An Introduction to Optimization on Smooth Manifolds},
  author = {Boumal, Nicolas},
  year = {2023},
  publisher = {Cambridge University Press},
  url = {https://www.nicolasboumal.net/book/},
  urldate = {2022-02-11},
  abstract = {This is a book about optimization on smooth manifolds for readers who are comfortable with linear algebra and multivariable calculus. There are no prerequisites in geometry or optimization. Chapters 3 and 5 in particular can serve as a standalone introduction to differential and Riemannian geometry. They focus on embedded submanifolds of linear spaces, with full proofs. A distinguishing feature is that these early chapters do not involve charts. Chapter 8 provides the general theory so that we can build quotient manifolds in Chapter 9. The optimization algorithms in Chapters 4 and 6 apply to the general case, but can already be understood after reading Chapters 3 and 5. Chapter 7 details examples of submanifolds that come up in practice. Chapter 10 covers more advanced Riemannian tools, and Chapter 11 introduces geodesic convexity. In a one-semester graduate course of the mathematics department at Princeton University in 2019 and 2020 (24 lectures of 80 minutes each, two projects, no exercises), I covered much of Chapters 1--6 and select parts of Chapter 7 before the midterm break, then much of Chapters 8--9 and select parts of Chapters 10--11 after the break. Those chapters were shorter at the time, but it still made for a sustained pace. At EPFL in 2021, I discussed mostly Chapters 1--8 in 13 lectures of 90 minutes, plus as many exercise sessions and two projects. The course is popular with applied mathematicians and mathematically inclined engineering students, at the graduate and advanced undergraduate level. You may also be interested in the Manopt toolboxes (Matlab, Python, Julia), and in the book Optimization Algorithms on Matrix Manifolds by Absil, Mahony and Sepulchre (Princeton University Press, 2008), all freely available online. These slides hold a summary of the basic geometric tools and algorithms from Chapters 3 and 5. Here are a one-hour video and a two-hour video introducing the basics of differential geometry and Riemannian geometry for optimization on smooth manifolds, using a variation of the slides linked here. These two videos have mostly the same contents. This version forms the basis for a forthcoming publication with Cambridge University Press.},
  isbn = {978-1-00-916617-1}
}

@book{boydConvexOptimization2004,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  year = {2004},
  month = mar,
  edition = {Seventh printing with corrections 2009},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK},
  url = {https://web.stanford.edu/~boyd/cvxbook/},
  abstract = {Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics.},
  isbn = {978-0-521-83378-3},
  langid = {english}
}

@misc{boydConvexOptimizationApplications,
  title = {Convex {{Optimization Applications}}},
  author = {Boyd, Stephen and Diamond, Steven and Zhang, Junzi and Agrawal, Akshay},
  url = {https://web.stanford.edu/~boyd/papers/pdf/cvx_applications.pdf},
  urldate = {2021-07-30},
  langid = {english}
}

@article{boydDistributedOptimizationStatistical2011,
  title = {Distributed {{Optimization}} and {{Statistical Learning}} via the {{Alternating Direction Method}} of {{Multipliers}}},
  author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
  year = {2011},
  month = jan,
  journal = {Found. Trends Mach. Learn.},
  volume = {3},
  number = {1},
  pages = {1--122},
  issn = {1935-8237},
  doi = {10.1561/2200000016},
  url = {http://dx.doi.org/10.1561/2200000016},
  urldate = {2017-05-16},
  abstract = {Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas--Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for {$\ell$}1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.},
  annotation = {05103}
}

@techreport{boydEE363ReviewSession2008,
  type = {Lecture Notes for {{EE363}}},
  title = {{{EE363 Review Session}} 4: {{Linear Matrix Inequalities}}},
  author = {Boyd, Stephen},
  year = {2008},
  address = {Stanford, CA},
  institution = {Stanford University},
  url = {https://stanford.edu/class/ee363/sessions/s4notes.pdf},
  urldate = {2024-08-22}
}

@book{boydIntroductionAppliedLinear2018,
  title = {Introduction to {{Applied Linear Algebra}}: {{Vectors}}, {{Matrices}}, and {{Least Squares}}},
  shorttitle = {Introduction to {{Applied Linear Algebra}}},
  author = {Boyd, Stephen},
  year = {2018},
  month = aug,
  edition = {1st edition},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK ; New York, NY},
  isbn = {978-1-316-51896-0},
  langid = {english}
}

@unpublished{boydIntroductionAppliedLinear2021,
  type = {Draft},
  title = {Introduction to {{Applied Linear Algebra Vectors}}, {{Matrices}}, and {{Least Squares}} - {{Julia Language Companion}}},
  author = {Boyd, Stephen and Vandeberghe, Lieven},
  year = {2021},
  month = mar,
  url = {https://web.stanford.edu/~boyd/vmls/vmls-julia-companion.pdf}
}

@book{boydLinearMatrixInequalities1994,
  title = {Linear {{Matrix Inequalities}} in {{System}} and {{Control Theory}}},
  author = {Boyd, Stephen and El Ghaoui, Laurent and Feron, Eric and Balakrishnan, Venkataramanan},
  year = {1994},
  month = jan,
  series = {Studies in {{Applied}} and {{Numerical Mathematics}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  url = {https://web.stanford.edu/~boyd/lmibook/},
  urldate = {2021-04-16},
  abstract = {The basic topic of this book is solving problems from system and control theory using convex optimization. We show that a wide variety of problems arising in system and control theory can be reduced to a handful of standard convex and quasiconvex optimization problems that involve matrix inequalities. For a few special cases there are ``analytic solutions'' to these problems, but our main point is that they can be solved numerically in all cases. These standard problems can be solved in polynomial-time (by, e.g., the ellipsoid algorithm of Shor, Nemirovskii, and Yudin), and so are tractable, at least in a theoretical sense. Recently developed interior-point methods for these standard problems have been found to be extremely efficient in practice. Therefore, we consider the original problems from system and control theory as solved. This book is primarily intended for the researcher in system and control theory, but can also serve as a source of application problems for researchers in convex optimization. Although we believe that the methods described in this book have great practical value, we should warn the reader whose primary interest is applied control engineering. This is a research monograph: We present no specific examples or numerical results, and we make only brief comments about the implications of the results for practical control engineering. To put it in a more positive light, we hope that this book will later be considered as the first book on the topic, not the most readable or accessible. The background required of the reader is knowledge of basic system and control theory and an exposure to optimization. Sontag's book Mathematical Control Theory [Son90] is an excellent survey. Further background material is covered in the texts Linear Systems [Kai80] by Kailath, Nonlinear Systems Analysis [Vid92] by Vidyasagar, Optimal Control: Linear Quadratic Methods [AM90] by Anderson and Moore, and Convex Analysis and Minimization Algorithms I [HUL93] by Hiriart-Urruty and Lemar{\'e}chal. We also highly recommend the book Interior-point Polynomial Algorithms in Convex Programming [NN94] by Nesterov and Nemirovskii as a companion to this book. The reader will soon see that their ideas and methods play a critical role in the basic idea presented in this book.},
  isbn = {978-0-89871-485-2}
}

@article{boydMethodCentersMinimizing1993,
  title = {Method of Centers for Minimizing Generalized Eigenvalues},
  author = {Boyd, Stephen and El Ghaoui, Laurent},
  year = {1993},
  month = jul,
  journal = {Linear Algebra and its Applications},
  volume = {188--189},
  pages = {63--111},
  issn = {0024-3795},
  doi = {10.1016/0024-3795(93)90465-Z},
  url = {https://www.sciencedirect.com/science/article/pii/002437959390465Z},
  urldate = {2021-07-04},
  abstract = {We consider the problem of minimizing the largest generalized eigenvalue of a pair of symmetric matrices, each of which depends affinely on the decision variables. Although this problem may appear specialized, it is in fact quite general, and includes for example all linear, quadratic, and linear fractional programs. Many problems arising in control theory can be cast in this form. The problem is nondifferentiable but quasiconvex, so methods such as Kelley's cutting-plane algorithm or the ellipsoid algorithm of Shor, Nemirovsky, and Yudin are guaranteed to minimize it. In this paper we describe relevant background material and a simple interior-point method that solves such problems more efficiently. The algorithm is a variation on Huard's method of centers, using a self-concordant barrier for matrix inequalities developed by Nesterov and Nemirovsky. (Nesterov and Nemirovsky have also extended their potential reduction methods to handle the same problem.) Since the problem is quasiconvex but not convex, devising a nonheuristic stopping criterion (i.e., one that guarantees a given accuracy) is more difficult than in the convex case. We describe several nonheuristic stopping criteria that are based on the dual of a related convex problem and a new ellipsoidal approximation that is slightly sharper, in some cases, than a more general result to Nesterov and Nemirovsky. The algorithm is demonstrated on an example: determining the quadratic Lyapunov function that optimizes a decay-rate estimate for a differential inclusion.},
  langid = {english}
}

@misc{boydMoreAdvancedTopics,
  title = {More {{Advanced Topics}}},
  author = {Boyd, Stephen and Diamond, Steven and Zhang, Junzi and Agrawal, Akshay},
  url = {https://web.stanford.edu/~junziz/papers/cvx_additional_materials.pdf},
  langid = {english}
}

@techreport{boydSolvingSemidefinitePrograms,
  type = {Lecture Notes for {{EE363}}},
  title = {Solving Semidefinite Programs Using Cvx},
  author = {Boyd, Stephen},
  year = {2008},
  address = {Stanford, CA},
  institution = {Stanford University},
  url = {https://stanford.edu/class/ee363/notes/lmi-cvx.pdf},
  urldate = {2024-08-22}
}

@article{boydTutorialGeometricProgramming2007,
  title = {A Tutorial on Geometric Programming},
  author = {Boyd, Stephen and Kim, Seung-Jean and Vandenberghe, Lieven and Hassibi, Arash},
  year = {2007},
  month = apr,
  journal = {Optimization and Engineering},
  volume = {8},
  number = {1},
  pages = {67},
  issn = {1573-2924},
  doi = {10.1007/s11081-007-9001-7},
  url = {https://web.stanford.edu/~boyd/papers/gp_tutorial.html},
  urldate = {2022-02-14},
  abstract = {A geometric program (GP) is a type of mathematical optimization problem characterized by objective and constraint functions that have a special form. Recently developed solution methods can solve even large-scale GPs extremely efficiently and reliably; at the same time a number of practical problems, particularly in circuit design, have been found to be equivalent to (or well approximated by) GPs. Putting these two together, we get effective solutions for the practical problems. The basic approach in GP modeling is to attempt to express a practical problem, such as an engineering analysis or design problem, in GP format. In the best case, this formulation is exact; when this is not possible, we settle for an approximate formulation. This tutorial paper collects together in one place the basic background material needed to do GP modeling. We start with the basic definitions and facts, and some methods used to transform problems into GP format. We show how to recognize functions and problems compatible with GP, and how to approximate functions or data in a form compatible with GP (when this is possible). We give some simple and representative examples, and also describe some common extensions of GP, along with methods for solving (or approximately solving) them.},
  langid = {english}
}

@book{brinkhuisConvexAnalysisOptimization2020,
  title = {Convex {{Analysis}} for {{Optimization}}},
  author = {Brinkhuis, Jan},
  year = {2020},
  month = may,
  series = {Graduate {{Texts}} in {{Operations Research}}},
  publisher = {Springer},
  address = {Cham},
  url = {https://link.springer.com/book/10.1007/978-3-030-41804-5},
  abstract = {Presents a unified novel three-step method for all constructions, formulas and proofs of the important classic notions of convexity Includes numerous exercises and illustrations to stimulate learning by doing and seeing Written in a narrative style with short sections and concise proofs},
  isbn = {978-3-030-41803-8},
  langid = {english}
}

@book{brinkhuisOptimizationInsightsApplications2005,
  title = {Optimization: {{Insights}} and {{Applications}}},
  shorttitle = {Optimization},
  author = {Brinkhuis, Jan and Tikhomirov, Vladimir},
  year = {2005},
  month = sep,
  publisher = {Princeton University Press},
  address = {Princeton, N.J},
  url = {https://press.princeton.edu/books/hardcover/9780691102870/optimization},
  isbn = {978-0-691-10287-0},
  langid = {english}
}

@book{bucurVariationalMethodsShape2005,
  title = {Variational {{Methods}} in {{Shape Optimization Problems}}},
  author = {Bucur, Dorin and Buttazzo, Giuseppe},
  year = {2005},
  month = jul,
  series = {Progress in {{Nonlinear Differential Equations}} and {{Their Applications Book}}},
  number = {65},
  publisher = {Birkh{\"a}user},
  address = {Boston},
  isbn = {978-0-8176-4359-1},
  langid = {english}
}

@incollection{byrnesConvexOptimizationApproach2003,
  title = {A {{Convex Optimization Approach}} to {{Generalized Moment Problems}}},
  booktitle = {Control and {{Modeling}} of {{Complex Systems}}},
  author = {Byrnes, Christopher I. and Lindquist, Anders},
  editor = {Hashimoto, Koichi and Oishi, Yasuaki and Yamamoto, Yutaka},
  year = {2003},
  month = jan,
  series = {Trends in {{Mathematics}}},
  pages = {3--21},
  publisher = {Birkh{\"a}user Boston},
  url = {http://link.springer.com/chapter/10.1007/978-1-4612-0023-9_1},
  urldate = {2014-07-01},
  abstract = {In this paper we present a universal solution to the generalized moment problem, with a nonclassical complexity constraint. We show that this solution can be obtained by minimizing a strictly convex nonlinear functional. This optimization problem is derived in two different ways. We first derive this intrinsically, in a geometric way, by path integration of a one-form which defines the generalized moment problem. It is observed that this one-form is closed and defined on a convex set, and thus exact with, perhaps surprisingly, a strictly convex primitive function. We also derive this convex functional as the dual problem of a problem to maximize a cross entropy functional. In particular, these approaches give a constructive parameterization of all solutions to the Nevanlinna-Pick interpolation problem, with possible higher-order interpolation at certain points in the complex plane, with a degree constraint as well as all solutions to the rational covariance extension problem --- two areas which have been advanced by the work of Hidenori Kimura. Illustrations of these results in system identification and probability are also mentioned.},
  copyright = {{\copyright}2003 Birkh{\"a}user Boston},
  isbn = {978-1-4612-6577-1 978-1-4612-0023-9},
  langid = {english}
}

@article{cadzowMinimumL1L22002,
  title = {Minimum {$\ell$}1, {$\ell$}2, and {$\ell\infty$} {{Norm Approximate Solutions}} to an {{Overdetermined System}} of {{Linear Equations}}},
  author = {Cadzow, James A.},
  year = {2002},
  month = oct,
  journal = {Digital Signal Processing},
  volume = {12},
  number = {4},
  pages = {524--560},
  issn = {1051-2004},
  doi = {10.1006/dspr.2001.0409},
  url = {http://www.sciencedirect.com/science/article/pii/S1051200401904099},
  urldate = {2018-08-01},
  abstract = {Cadzow, J. A., Minimum {$\ell$}1, {$\ell$}2, and {$\ell\infty$} Norm Approximate Solutions to an Overdetermined System of Linear Equations, Digital Signal Processing12 (2002) 524--560 Many practical problems encountered in digital signal processing and other quantitative oriented disciplines entail finding a best approximate solution to an overdetermined system of linear equations. Invariably, the least squares error approximate solution (i.e., minimum {$\ell$}2 norm) is chosen for this task due primarily to the existence of a convenient closed expression for its determination. It should be noted, however, that in many applications a minimum {$\ell$}1 or {$\ell\infty$} norm approximate solution is preferable. For example, in cases where the data being analyzed contain a few data outliers a minimum {$\ell$}1 approximate solution is preferable since it tends to ignore bad data points. In other applications one may wish to determine an approximate solution whose largest error magnitude is the smallest possible (i.e., a minimum {$\ell\infty$} norm approximate solution). Unfortunately, there do not exist convenient closed form expressions for either the minimum {$\ell$}1 or the minimum {$\ell\infty$} norm approximate solution and one must resort to nonlinear programming methods for their determination. Effective algorithms for determining these two solutions are herein presented (see Cadzow, J. A., Data Analysis and Signal Processing: Theory and Applications).}
}

@book{calafioreOptimizationModels2014,
  title = {Optimization {{Models}}},
  author = {Calafiore, Giuseppe C.},
  year = {2014},
  month = oct,
  publisher = {Cambridge University Press},
  address = {Cambridge, UK},
  url = {https://doi.org/10.1017/CBO9781107279667},
  abstract = {Emphasizing practical understanding over the technicalities of specific algorithms, this elegant textbook is an accessible introduction to the field of optimization, focusing on powerful and reliable convex optimization techniques. Students and practitioners will learn how to recognize, simplify, model and solve optimization problems - and apply these principles to their own projects. A clear and self-contained introduction to linear algebra demonstrates core mathematical concepts in a way that is easy to follow, and helps students to understand their practical relevance. Requiring only a basic understanding of geometry, calculus, probability and statistics, and striking a careful balance between accessibility and rigor, it enables students to quickly understand the material, without being overwhelmed by complex mathematics. Accompanied by numerous end-of-chapter problems, an online solutions manual for instructors, and relevant examples from diverse fields including engineering, data science, economics, finance, and management, this is the perfect introduction to optimization for undergraduate and graduate students.},
  isbn = {978-1-107-05087-7},
  langid = {english}
}

@misc{caronOptimalityConditionsNumerical2022,
  title = {Optimality Conditions and Numerical Tolerances in {{QP}} Solvers},
  author = {Caron, St{\'e}phane},
  year = {2022},
  month = nov,
  url = {https://scaron.info/blog/optimality-conditions-and-numerical-tolerances-in-qp-solvers.html}
}

@article{cavalierModelingIntegerProgramming1990,
  title = {Modeling and Integer Programming Techniques Applied to Propositional Calculus},
  author = {Cavalier, Tom M. and Pardalos, Panos M. and Soyster, Allen L.},
  year = {1990},
  month = jan,
  journal = {Computers \& Operations Research},
  volume = {17},
  number = {6},
  pages = {561--570},
  issn = {0305-0548},
  doi = {10.1016/0305-0548(90)90062-C},
  url = {https://www.sciencedirect.com/science/article/pii/030505489090062C},
  urldate = {2022-12-30},
  abstract = {This paper discusses alternative methods for constructing a 0--1 integer programming problem from a propositional calculus problem and the use of the resulting mathematical program to solve the related logic problem. This paper also identifies some special structures associated with the constraint sets and discusses several fundamental results concerning methods of preprocessing the logical inferences into constraints.},
  langid = {english}
}

@article{colombinoOnlineOptimizationFeedback2020,
  title = {Online {{Optimization}} as a {{Feedback Controller}}: {{Stability}} and {{Tracking}}},
  shorttitle = {Online {{Optimization}} as a {{Feedback Controller}}},
  author = {Colombino, Marcello and Dall'Anese, Emiliano and Bernstein, Andrey},
  year = {2020},
  month = mar,
  journal = {IEEE Transactions on Control of Network Systems},
  volume = {7},
  number = {1},
  pages = {422--432},
  issn = {2325-5870},
  doi = {10.1109/TCNS.2019.2906916},
  abstract = {This paper develops and analyzes feedback-based online optimization methods to regulate the output of a linear time invariant (LTI) dynamical system to the optimal solution of a time-varying convex optimization problem. The design of the algorithm is based on continuous-time primal-dual dynamics, properly modified to incorporate feedback from the LTI dynamical system, applied to a proximal augmented Lagrangian function. The resultant closed-loop algorithm tracks the solution of the time-varying optimization problem without requiring knowledge of (time varying) disturbances in the dynamical system. The analysis leverages integral quadratic constraints to provide linear matrix inequality (LMI) conditions that guarantee global exponential stability and bounded tracking error. Analytical results show that under a sufficient time-scale separation between the dynamics of the LTI dynamical system and the algorithm, the LMI conditions can be always satisfied. This paper further proposes a modified algorithm that can track an approximate solution trajectory of the constrained optimization problem under less restrictive assumptions. As an illustrative example, the proposed algorithms are showcased for power transmission systems, to compress the time scales between secondary and tertiary control, and allow to simultaneously power rebalancing and tracking of the DC optimal power flow points.}
}

@misc{ComplementarityConstraintsArtelys,
  title = {Complementarity Constraints --- {{Artelys Knitro}} 13.0 {{User}}'s {{Manual}}},
  url = {https://www.artelys.com/docs/knitro/2_userGuide/complementarity.html},
  urldate = {2022-03-21}
}

@misc{ComplementarityProblemsNEOS,
  title = {Complementarity {{Problems}} {\textbar} {{NEOS}}},
  url = {https://neos-guide.org/content/complementarity-problems},
  urldate = {2022-03-22}
}

@book{confortiIntegerProgramming2014,
  title = {Integer {{Programming}}},
  author = {Conforti, Michele and Cornu{\'e}jols, G{\'e}rard and Zambelli, Giacomo},
  year = {2014},
  month = dec,
  series = {Graduate {{Texts}} in {{Mathematics}}},
  number = {271},
  publisher = {Springer},
  address = {New York},
  isbn = {978-3-319-11007-3},
  langid = {english}
}

@misc{ConicPdf,
  title = {Mosek {{Conic Modeling Cheatsheet}}},
  url = {https://docs.mosek.com/cheatsheets/conic.pdf},
  urldate = {2024-01-27}
}

@misc{ConstraintsJuMP,
  title = {Constraints {$\cdot$} {{JuMP}}},
  url = {https://jump.dev/JuMP.jl/stable/manual/constraints/#Complementarity-constraints},
  urldate = {2022-03-21}
}

@book{cottleLinearComplementarityProblem2009,
  title = {The {{Linear Complementarity Problem}}},
  author = {Cottle, Richard W. and Pang, Jong-Shi and Stone, Richard E.},
  year = {2009},
  month = aug,
  series = {Classics in {{Applied Mathematics}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9780898719000},
  isbn = {978-0-89871-686-3},
  langid = {english}
}

@misc{dahlPolynomialOptimizationUsing2015,
  title = {Polynomial Optimization Using {{MOSEK}} and {{Julia}}},
  author = {Dahl, Joachim},
  year = {2015-07-12/2015-07-17},
  address = {Pittsburgh},
  url = {https://docs.mosek.com/slides/2015/ismp-2015-dahl-polyopt.pdf}
}

@article{daspremontAccelerationMethods2021,
  title = {Acceleration {{Methods}}},
  author = {{d'Aspremont}, Alexandre and Scieur, Damien and Taylor, Adrien},
  year = {2021},
  month = mar,
  journal = {arXiv:2101.09545 [cs, math]},
  eprint = {2101.09545},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/2101.09545},
  urldate = {2021-12-20},
  abstract = {This monograph covers some recent advances on a range of acceleration techniques frequently used in convex optimization. We first use quadratic optimization problems to introduce two key families of methods, momentum and nested optimization schemes, which coincide in the quadratic case to form the Chebyshev method whose complexity is analyzed using Chebyshev polynomials. We discuss momentum methods in detail, starting with the seminal work of Nesterov (1983) and structure convergence proofs using a few master templates, such as that of {\textbackslash}emph\{optimized gradient methods\} which have the key benefit of showing how momentum methods maximize convergence rates. We further cover proximal acceleration techniques, at the heart of the {\textbackslash}emph\{Catalyst\} and {\textbackslash}emph\{Accelerated Hybrid Proximal Extragradient\} frameworks, using similar algorithmic patterns. Common acceleration techniques directly rely on the knowledge of some regularity parameters of the problem at hand, and we conclude by discussing {\textbackslash}emph\{restart\} schemes, a set of simple techniques to reach nearly optimal convergence rates while adapting to unobserved regularity parameters.},
  archiveprefix = {arXiv}
}

@book{delfourShapesGeometriesMetrics2011,
  title = {Shapes and {{Geometries}}: {{Metrics}}, {{Analysis}}, {{Differential Calculus}}, and {{Optimization}}},
  shorttitle = {Shapes and {{Geometries}}},
  author = {Delfour, Michael C.},
  year = {2011},
  month = jan,
  series = {Advances in {{Design}} and {{Control}}},
  edition = {2},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  isbn = {978-0-89871-936-9},
  langid = {english}
}

@article{demoorGeneralizedLinearComplementarity1992,
  title = {The Generalized Linear Complementarity Problem and an Algorithm to Find All Its Solutions},
  author = {De Moor, Bart and Vandenberghe, Lieven and Vandewalle, Joos},
  year = {1992},
  month = may,
  journal = {Mathematical Programming},
  volume = {57},
  number = {1},
  pages = {415--426},
  issn = {1436-4646},
  doi = {10.1007/BF01581091},
  url = {https://doi.org/10.1007/BF01581091},
  urldate = {2022-11-27},
  abstract = {Motivated by a number of typical applications, a generalization of the classicallinear complementarity problem is presented together with an algorithm to determine the complete solution set. The algorithm is based on the double description method for solving linear inequalities and succeeds in describing continuous as well as unbounded solution sets.},
  langid = {english}
}

@book{dempeFoundationsBilevelProgramming2002a,
  title = {Foundations of {{Bilevel Programming}}},
  author = {Dempe, Stephan},
  year = {2002},
  month = may,
  series = {Nonconvex {{Optimization}} and {{Its Applications}}},
  number = {61},
  publisher = {Springer},
  address = {Dordrecht ; Boston},
  isbn = {978-1-4020-0631-9},
  langid = {english}
}

@book{dennisNumericalMethodsUnconstrained1996,
  title = {Numerical {{Methods}} for {{Unconstrained Optimization}} and {{Nonlinear Equations}}},
  author = {Dennis, J. E. and Schnabel, Robert B.},
  year = {1996},
  month = jan,
  series = {Classics in {{Applied Mathematics}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611971200},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9781611971200},
  urldate = {2022-01-26},
  abstract = {We are delighted that SIAM is republishing our original 1983 book after what many in the optimization field have regarded as ``premature termination'' by the previous publisher. At 12 years of age, the book may be a little young to be a ``classic,'' but since its publication it has been well received in the numerical computation community. We are very glad that it will continue to be available for use in teaching, research, and applications. We set out to write this book in the late 1970s because we felt that the basic techniques for solving small to medium-sized nonlinear equations and unconstrained optimization problems had matured and converged to the point where they would remain relatively stable. Fortunately, the intervening years have confirmed this belief. The material that constitutes most of this book---the discussion of Newton-based methods, globally convergent line search and trust region methods, and secant (quasi-Newton) methods for nonlinear equations, unconstrained optimization, and nonlinear least squares---continues to represent the basis for algorithms and analysis in this field. On the teaching side, a course centered around Chapters 4 to 9 forms a basic, in-depth introduction to the solution of nonlinear equations and unconstrained optimization problems. For researchers or users of optimization software, these chapters give the foundations of methods and software for solving small to medium-sized problems of these types.},
  isbn = {978-0-89871-364-0}
}

@article{derinkuyuSprocedureVariants2006,
  title = {On the {{S-procedure}} and {{Some Variants}}},
  author = {Derinkuyu, K{\"u}r{\c s}ad and P{\i}nar, Mustafa {\c C}.},
  year = {2006},
  month = aug,
  journal = {Mathematical Methods of Operations Research},
  volume = {64},
  number = {1},
  pages = {55--77},
  issn = {1432-5217},
  doi = {10.1007/s00186-006-0070-8},
  url = {https://doi.org/10.1007/s00186-006-0070-8},
  urldate = {2022-07-28},
  abstract = {We give a concise review and extension of S-procedure that is an instrumental tool in control theory and robust optimization analysis. We also discuss the approximate S-Lemma as well as its applications in robust optimization.},
  langid = {english}
}

@article{deschutterEquivalenceLinearComplementarity2002,
  title = {On the Equivalence of Linear Complementarity Problems},
  author = {De Schutter, B. and Heemels, W. P. M. H. and Bemporad, A.},
  year = {2002},
  month = aug,
  journal = {Operations Research Letters},
  volume = {30},
  number = {4},
  pages = {211--222},
  issn = {0167-6377},
  doi = {10.1016/S0167-6377(02)00159-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0167637702001591},
  urldate = {2022-11-29},
  abstract = {We show that the Extended Linear Complementarity Problem (ELCP) can be recast as a standard Linear Complementarity Problem (LCP) provided that the surplus variables or the feasible set of the ELCP are bounded. Since many extensions of the LCP are special cases of the ELCP, this implies that these extensions can be rewritten as an LCP as well. Our equivalence proof is constructive and leads to three possible numerical solution methods for a given ELCP: regular ELCP algorithms, mixed integer linear programming algorithms, and regular LCP algorithms.},
  langid = {english}
}

@article{deschutterExtendedLinearComplementarity1995,
  title = {The Extended Linear Complementarity Problem},
  author = {De Schutter, Bart and De Moor, Bart},
  year = {1995},
  month = dec,
  journal = {Mathematical Programming},
  volume = {71},
  number = {3},
  pages = {289--325},
  issn = {1436-4646},
  doi = {10.1007/BF01590958},
  url = {https://doi.org/10.1007/BF01590958},
  urldate = {2022-09-16},
  abstract = {In this paper we define the Extended Linear Complementarity Problem (ELCP), an extension of the well-known Linear Complementarity Problem (LCP). We show that the ELCP can be viewed as a kind of unifying framework for the LCP and its various generalizations. We study the general solution set of an ELCP and we develop an algorithm to find all its solutions. We also show that the general ELCP is an NP-hard problem.},
  langid = {english}
}

@book{deuflhardNewtonMethodsNonlinear2011,
  title = {Newton {{Methods}} for {{Nonlinear Problems}}: {{Affine Invariance}} and {{Adaptive Algorithms}}},
  shorttitle = {Newton {{Methods}} for {{Nonlinear Problems}}},
  author = {Deuflhard, Peter},
  year = {2011},
  month = sep,
  series = {Springer {{Series}} in {{Computational Mathematics}}},
  number = {35},
  publisher = {Springer Science \& Business Media},
  url = {https://link.springer.com/book/10.1007/978-3-642-23899-4},
  abstract = {This book deals with the efficient numerical solution of challenging nonlinear problems in science and engineering, both in finite dimension (algebraic systems) and in infinite dimension (ordinary and partial differential equations). Its focus is on local and global Newton methods for direct problems or Gauss-Newton methods for inverse problems. The term 'affine invariance' means that the presented algorithms and their convergence analysis are invariant under one out of four subclasses of affine transformations of the problem to be solved. Compared to traditional textbooks, the distinguishing affine invariance approach leads to shorter theorems and proofs and permits the construction of fully adaptive algorithms. Lots of numerical illustrations, comparison tables, and exercises make the text useful in computational mathematics classes. At the same time, the book opens many directions for possible future research.},
  googlebooks = {5rVcytMq3DoC},
  isbn = {978-3-642-23899-4},
  langid = {english}
}

@inproceedings{diehlLocalConvergenceGeneralized2019,
  title = {Local {{Convergence}} of {{Generalized Gauss-Newton}} and {{Sequential Convex Programming}}},
  booktitle = {2019 {{IEEE}} 58th {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Diehl, Moritz and Messerer, Florian},
  year = {2019},
  month = dec,
  pages = {3942--3947},
  issn = {2576-2370},
  doi = {10.1109/CDC40024.2019.9029288},
  abstract = {We analyze the convergence properties of two Newton-type algorithms for the solution of unconstrained nonlinear optimization problems with convex substructure: Generalized Gauss-Newton (GGN) and Sequential Convex Programming (SCP). While both algorithms are identical to the classical Gauss-Newton method for the special case of nonlinear least squares, they differ when applied to more general convex outer functions. We show under mild assumptions that GGN and SCP have locally linear convergence with the same contraction rate. The convergence or divergence rate can be characterized as the smallest scalar that satisfies two linear matrix inequalities. We further show that bad convergence or divergence at a given local minimum can be a desirable property in the context of estimation problems with symmetric likelihood functions, because it avoids that the algorithm is attracted by statistically undesirable local minima. Both algorithms and their convergence properties are illustrated with a numerical example.}
}

@misc{diehlSurveyGeneralizedGaussNewton2019,
  title = {A {{Survey}} of {{Generalized Gauss-Newton}} and {{Sequential Convex Programming Methods}}},
  author = {Diehl, Moritz},
  year = {2019},
  month = sep,
  address = {Nice},
  url = {https://fgs-2019.sciencesconf.org/data/diehl.pdf}
}

@article{dirksePathSolverNommonotone1995,
  title = {The Path Solver: A Nommonotone Stabilization Scheme for Mixed Complementarity Problems},
  shorttitle = {The Path Solver},
  author = {Dirkse, Steven P. and Ferris, Michael C.},
  year = {1995},
  month = jan,
  journal = {Optimization Methods and Software},
  volume = {5},
  number = {2},
  pages = {123--156},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556789508805606},
  url = {https://doi.org/10.1080/10556789508805606},
  urldate = {2023-11-28},
  abstract = {The PATH solver is an implementation of a stabilized Newton method for the solution of the Mixed Complementarity Problem. The stabilization scheme employs a path-generation procedure which is used to construct a piecewise-linear path from the current point to the Newton point; a step length acceptance criterion and a non-monotone pathsearch are then used to choose the next iterate. The algorithm is shown to be globally convergent under assumptions which generalize those required to obtain similar results in the smooth case. Several imple{$\acute{\epsilon}$}entation issues are discussed, and extensive computational results obtained from problems commonly found in the literature are given}
}

@book{dodsonProbabilisticDesignOptimization2014,
  title = {Probabilistic {{Design}} for {{Optimization}} and {{Robustness}} for {{Engineers}}},
  author = {Dodson, Bryan and Hammett, Patrick and Klerx, Rene},
  year = {2014},
  month = sep,
  publisher = {Wiley},
  address = {Chichester, West Sussex, United Kingdom},
  url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118796245},
  abstract = {Probabilistic Design for Optimization and Robustness:* Presents the theory of modeling with variation using physical models and methods for practical applications on designs more insensitive to variation.* Provides a comprehensive guide to optimization and robustness for probabilistic design.* Features examples, case studies and exercises throughout. The methods presented can be applied to a wide range of disciplines such as mechanics, electrics, chemistry, aerospace, industry and engineering. This text is supported by an accompanying website featuring videos, interactive animations to aid the readers understanding.},
  isbn = {978-1-118-79619-1},
  langid = {english}
}

@article{donohoMostLargeUnderdetermined2006,
  title = {For Most Large Underdetermined Systems of Linear Equations the Minimal {$l$}1-norm Solution Is Also the Sparsest Solution},
  author = {Donoho, David L.},
  year = {2006},
  month = mar,
  journal = {Communications on Pure and Applied Mathematics},
  volume = {59},
  number = {6},
  pages = {797--829},
  issn = {0010-3640},
  doi = {10.1002/cpa.20132},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.20132},
  urldate = {2018-08-01}
}

@book{dostalOptimalQuadraticProgramming2009,
  title = {Optimal {{Quadratic Programming Algorithms}}: {{With Applications}} to {{Variational Inequalities}}},
  author = {Dost{\'a}l, Zdenek},
  year = {2009},
  series = {Springer {{Optimization}} and {{Its Applications}}},
  publisher = {Springer},
  address = {New York, NY},
  url = {https://doi.org/10.1007/b138610},
  urldate = {2022-05-31},
  abstract = {Solving optimization problems in complex systems often requires the implementation of advanced mathematical techniques. Quadratic programming (QP) is one technique that allows for the optimization of a quadratic function in several variables in the presence of linear constraints. QP problems arise in fields as diverse as electrical engineering, agricultural planning, and optics. Given its broad applicability, a comprehensive understanding of quadratic programming is a valuable resource in nearly every scientific field. Optimal Quadratic Programming Algorithms presents recently developed algorithms for solving large QP problems. The presentation focuses on algorithms which are, in a sense optimal, i.e., they can solve important classes of problems at a cost proportional to the number of unknowns. For each algorithm presented, the book details its classical predecessor, describes its drawbacks, introduces modifications that improve its performance, and demonstrates these improvements through numerical experiments. This self-contained monograph can serve as an introductory text on quadratic programming for graduate students and researchers. Additionally, since the solution of many nonlinear problems can be reduced to the solution of a sequence of QP problems, it can also be used as a convenient introduction to nonlinear programming. The reader is required to have a basic knowledge of calculus in several variables and linear algebra.},
  isbn = {978-0-387-84805-1},
  langid = {english}
}

@misc{duchiChanceConstrainedOptimization2020,
  type = {Lecture Slides for {{EE364A}}: {{Convex Optimization I}}},
  title = {Chance Constrained Optimization},
  author = {Duchi, John},
  year = {Winter Quarter 2020--2021},
  address = {Stanford University},
  url = {https://web.stanford.edu/class/ee364a/},
  urldate = {2021-09-03}
}

@book{dumitrescuPositiveTrigonometricPolynomials2017,
  title = {Positive {{Trigonometric Polynomials}} and {{Signal Processing Applications}}},
  author = {Dumitrescu, Bogdan},
  year = {2017},
  month = mar,
  series = {Signals and {{Communication Technology}}},
  edition = {2},
  publisher = {Springer},
  langid = {english}
}

@article{dunningJuMPModelingLanguage2017,
  title = {{{JuMP}}: {{A Modeling Language}} for {{Mathematical Optimization}}},
  shorttitle = {{{JuMP}}},
  author = {Dunning, Iain and Huchette, Joey and Lubin, Miles},
  year = {2017},
  month = jan,
  journal = {SIAM Review},
  volume = {59},
  number = {2},
  pages = {295--320},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/15M1020575},
  url = {https://epubs.siam.org/doi/abs/10.1137/15M1020575},
  urldate = {2020-12-28},
  abstract = {JuMP is an open-source modeling language that allows users to express a wide range of optimization problems (linear, mixed-integer, quadratic, conic-quadratic, semidefinite, and nonlinear) in a high-level, algebraic syntax. JuMP takes advantage of advanced features of the Julia programming language to offer unique functionality while achieving performance on par with commercial modeling tools for standard tasks. In this work we will provide benchmarks, present the novel aspects of the implementation, and discuss how JuMP can be extended to new problem classes and composed with state-of-the-art tools for visualization and interactivity.}
}

@article{durisChebyshevSolutionsLinear1968,
  title = {Chebyshev and \$l{\textasciicircum}1 \$-{{Solutions}} of {{Linear Equations Using Least Squares Solutions}}},
  author = {Duris, C. and Sreedharan, V.},
  year = {1968},
  month = sep,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {5},
  number = {3},
  pages = {491--505},
  issn = {0036-1429},
  doi = {10.1137/0705040},
  url = {https://epubs.siam.org/doi/10.1137/0705040},
  urldate = {2018-08-01}
}

@techreport{eavesEquivalenceLCPPLS1979,
  title = {Equivalence of {{LCP}} and {{PLS}}},
  author = {Eaves, B. C. and Lemke, C. E.},
  year = {1979},
  address = {Stanford, CA},
  institution = {Stanford University},
  url = {https://apps.dtic.mil/sti/pdfs/ADA080890.pdf},
  urldate = {2022-12-04}
}

@article{eavesLinearComplementarityProblem1971,
  title = {The {{Linear Complementarity Problem}}},
  author = {Eaves, B. Curtis},
  year = {1971},
  month = may,
  journal = {Management Science},
  volume = {17},
  number = {9},
  pages = {612--634},
  publisher = {INFORMS},
  issn = {0025-1909},
  doi = {10.1287/mnsc.17.9.612},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.17.9.612},
  urldate = {2023-11-29},
  abstract = {This study centers on the task of efficiently finding a solution of the linear complementarity problem: Ix - My = q, x {$\geq$} 0, y {$\geq$} 0, x {$\perp$} y. The main results are: (1) It is shown that Lemke's algorithm will solve (or show no solution exists) the problem for M {$\in$} L where L is a class of matrices, which properly includes (i) certain copositive matrices, (ii) certain matrices with nonnegative principal minors, (iii) matrices for bimatrix games. (2) If M {$\in$} L, if the system Ix - My = q, x {$\geq$} 0, y {$\geq$} 0 is feasible and nondegenerate, then the corresponding linear complementarity problem has an odd number of solutions. If M {$\in$} L and q {$>$} 0 then the solution is unique. (3) If for some M and every q {$\geq$} 0 the problem has a unique solution then M {$\in$} L and the problem has a solution for every q. (4) If M has nonnegative principal minors and if the linear complementarity with M and q has a nondegenerate complementary solution then the solution is unique. (5) If yTMy + yTq is bounded below on y {$\geq$} 0 then the linear complementarity problem with M and q has a solution and Lemke's algorithm can be used to find such a solution. If, in addition, the problem is nondegenerate, then it has an odd number of solutions. (6) A procedure based on Lemke's algorithm is developed which either computes stationary points for general quadratic programs or else shows that the program has no optimum. (7) If a quadratic program has an optimum and satisfies a nondegeneracy condition then there are an odd number of stationary points.}
}

@book{elghaouiAdvancesLinearMatrix2000,
  title = {Advances in {{Linear Matrix Inequality Methods}} in {{Control}}},
  editor = {El Ghaoui, Laurent and Niculescu, Silviu-lulian},
  year = {2000},
  month = jan,
  series = {Advances in {{Design}} and {{Control}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  url = {http://epubs.siam.org/doi/book/10.1137/1.9780898719833},
  urldate = {2016-05-24},
  abstract = {Linear matrix inequalities (LMIs) have emerged recently as a useful tool for solving a number of control problems. The basic idea of the LMI method in control is to interpret a given control problem as a semidefinite programming (SDP) problem, i.e., an optimization problem with linear objective and positive semidefinite constraints involving symmetric matrices that are affine in the decision variables. The LMI formalism is relevant for many reasons. First, writing a given problem in this form brings an efficient, numerical solution. Also, the approach is particularly suited to problems with ``uncertain'' data and multiple (possibly conflicting) specifications. Finally, this approach seems to be widely applicable, not only in control, but also in other areas where uncertainty arises. Purpose and intended audience Since the early 1990s, with the developement of interior-point methods for solving SDP problems, the LMI approach has witnessed considerable attention in the control area (see the regularity of the invited sessions in the control conferences and workshops). Up to now, two self-contained books related to this subject have appeared. The book Interior Point Polynomial Methods in Convex Programming: Theory and Applications, by Nesterov and Nemirovskii, revolutionarized the field of optimization by showing that a large class of nonlinear convex programming problems (including SDP) can be solved very efficiently. A second book, also published by SIAM in 1994, Linear Matrix Inequalities in System and Control Theory, by Boyd, El Ghaoui, Feron, and Balakrishnan, shows that the advances in convex optimization can be successfully applied to a wide variety of difficult control problems. At this point, a natural question arises: Why another book on LMIs? One aim of this book is to describe, for the researcher in the control area, several important advances made both in algorithms and software and in the important issues in LMI control pertaining to analysis, design, and applications. Another aim is to identify several important issues, both in control and optimization, that need to be addressed in the future. We feel that these challenging issues require an interdisciplinary research effort, which we sought to foster. For example, Chapter 1 uses an optimization formalism, in the hope of encouraging researchers in optimization to look at some of the important ideas in LMI control (e.g., deterministic uncertainty, robustness) and seek nonclassical applications and challenges in the control area. Bridges go both ways, of course: for example, the ``primal-dual'' point of view that is so successful in optimization is also important in control.},
  isbn = {978-0-89871-438-8},
  annotation = {00397}
}

@book{EncyclopediaOptimization,
  title = {Encyclopedia of {{Optimization}}},
  editor = {Floudas, Christodoulos A. and Pardalos, Panos M.},
  year = {2009},
  edition = {2},
  publisher = {Springer},
  address = {New York, NY},
  url = {https://doi.org/10.1007/978-0-387-74759-0},
  urldate = {2022-07-29},
  abstract = {The goal of the Encyclopedia of Optimization is to introduce the reader to a complete set of topics that show the spectrum of research, the richness of ideas, and the breadth of applications that has come from this field. In 2000, the first edition was widely acclaimed and received high praise. J.B. Rosen crowned it ``an indispensable resource'' and Dingzhu Du lauded it as ``the standard most important reference in this very dynamic research field''. Top authors such as Herbert Hauptman (winner of the Nobel Prize) and Leonid Khachiyan (the Ellipsoid theorist) contributed and the second edition keeps these seminal entries. The second edition builds on the success of the former edition with more than 150 completely new entries, designed to ensure that the reference addresses recent areas where optimization theories and techniques have advanced. Particularly heavy attention resulted in health science and transportation, with entries such as ``Algorithms for Genomics'', ``Optimization and Radiotherapy Treatment Design'', and ``Crew Scheduling''.},
  isbn = {978-0-387-74758-3},
  langid = {english}
}

@book{facchineiFiniteDimensionalVariationalInequalities2003,
  title = {Finite-{{Dimensional Variational Inequalities}} and {{Complementarity Problems}}, {{Volume I}}},
  author = {Facchinei, Francisco and Pang, Jong-Shi},
  year = {2003},
  month = feb,
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  publisher = {Springer},
  address = {New York, NY},
  url = {https://doi.org/10.1007/b97543},
  abstract = {The ?nite-dimensional nonlinear complementarity problem (NCP) is a s- tem of ?nitely many nonlinear inequalities in ?nitely many nonnegative variables along with a special equation that expresses the complementary relationship between the variables and corresponding inequalities. This complementarity condition is the key feature distinguishing the NCP from a general inequality system, lies at the heart of all constrained optimi- tion problems in ?nite dimensions, provides a powerful framework for the modeling of equilibria of many kinds, and exhibits a natural link between smooth and nonsmooth mathematics. The ?nite-dimensional variational inequality (VI), which is a generalization of the NCP, provides a broad unifying setting for the study of optimization and equilibrium problems and serves as the main computational framework for the practical solution of a host of continuum problems in the mathematical sciences. The systematic study of the ?nite-dimensional NCP and VI began in the mid-1960s; in a span of four decades, the subject has developed into a very fruitful discipline in the ?eld of mathematical programming. The - velopments include a rich mathematical theory, a host of e?ective solution algorithms, a multitude of interesting connections to numerous disciplines, and a wide range of important applications in engineering and economics. As a result of their broad associations, the literature of the VI/CP has bene?ted from contributions made by mathematicians (pure, applied, and computational), computer scientists, engineers of many kinds (civil, ch- ical, electrical, mechanical, and systems), and economists of diverse exp- tise (agricultural, computational, energy, ?nancial, and spatial).},
  isbn = {978-0-387-95580-3},
  langid = {english}
}

@book{facchineiFiniteDimensionalVariationalInequalities2003a,
  title = {Finite-{{Dimensional Variational Inequalities}} and {{Complementarity Problems}}, {{Volume II}}},
  author = {Facchinei, Francisco and Pang, Jong-Shi},
  year = {2003},
  month = feb,
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  publisher = {Springer},
  address = {New York, NY},
  url = {https://doi.org/10.1007/b97544},
  abstract = {The ?nite-dimensional nonlinear complementarity problem (NCP) is a s- tem of ?nitely many nonlinear inequalities in ?nitely many nonnegative variables along with a special equation that expresses the complementary relationship between the variables and corresponding inequalities. This complementarity condition is the key feature distinguishing the NCP from a general inequality system, lies at the heart of all constrained optimi- tion problems in ?nite dimensions, provides a powerful framework for the modeling of equilibria of many kinds, and exhibits a natural link between smooth and nonsmooth mathematics. The ?nite-dimensional variational inequality (VI), which is a generalization of the NCP, provides a broad unifying setting for the study of optimization and equilibrium problems and serves as the main computational framework for the practical solution of a host of continuum problems in the mathematical sciences. The systematic study of the ?nite-dimensional NCP and VI began in the mid-1960s; in a span of four decades, the subject has developed into a very fruitful discipline in the ?eld of mathematical programming. The - velopments include a rich mathematical theory, a host of e?ective solution algorithms, a multitude of interesting connections to numerous disciplines, and a wide range of important applications in engineering and economics. As a result of their broad associations, the literature of the VI/CP has bene?ted from contributions made by mathematicians (pure, applied, and computational), computer scientists, engineers of many kinds (civil, ch- ical, electrical, mechanical, and systems), and economists of diverse exp- tise (agricultural, computational, energy, ?nancial, and spatial).},
  isbn = {978-0-387-95581-0},
  langid = {english}
}

@article{ferreauQpOASESParametricActiveset2014,
  title = {{{qpOASES}}: A Parametric Active-Set Algorithm for~Quadratic Programming},
  shorttitle = {{{qpOASES}}},
  author = {Ferreau, Hans Joachim and Kirches, Christian and Potschka, Andreas and Bock, Hans Georg and Diehl, Moritz},
  year = {2014},
  month = dec,
  journal = {Mathematical Programming Computation},
  volume = {6},
  number = {4},
  pages = {327--363},
  issn = {1867-2957},
  doi = {10.1007/s12532-014-0071-1},
  url = {https://doi.org/10.1007/s12532-014-0071-1},
  urldate = {2022-05-01},
  abstract = {Many practical applications lead to optimization problems that can either be stated as quadratic programming (QP) problems or require the solution of QP problems on a lower algorithmic level. One relatively recent approach to solve QP problems are parametric active-set methods that are based on tracing the solution along a linear homotopy between a QP problem with known solution and the QP problem to be solved. This approach seems to make them particularly suited for applications where a-priori information can be used to speed-up the QP solution or where high solution accuracy is required. In this paper we describe the open-source C++ software package qpOASES, which implements a parametric active-set method in a reliable and efficient way. Numerical tests show that qpOASES can outperform other popular academic and commercial QP solvers on small- to medium-scale convex test examples of the Maros-M{\'e}sz{\'a}ros QP collection. Moreover, various interfaces to third-party software packages make it easy to use, even on embedded computer hardware. Finally, we describe how qpOASES can be used to compute critical points of nonconvex QP problems.},
  langid = {english}
}

@techreport{ferrisComplementarityRelatedProblems1998,
  title = {Complementarity and Related Problems: {{A}} Survey},
  author = {Ferris, Michael C and Kanzow, Christian},
  year = {1998},
  month = nov,
  pages = {24},
  abstract = {This survey gives an introduction to some of the recent developments in the eld of complementarity and related problems. After presenting two typical examples and the basic existence and uniqueness results, we focus on some new trends for solving nonlinear complementarity problems. Extensions to mixed complementarity problems, variational inequalities and mathematical programs with equilibrium constraints are also discussed.},
  langid = {english}
}

@article{ferrisEngineeringEconomicApplications1997,
  title = {Engineering and {{Economic Applications}} of {{Complementarity Problems}}},
  author = {Ferris, M. C. and Pang, J. S.},
  year = {1997},
  month = jan,
  journal = {SIAM Review},
  volume = {39},
  number = {4},
  pages = {669--713},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/S0036144595285963},
  url = {https://epubs.siam.org/doi/abs/10.1137/S0036144595285963},
  urldate = {2022-03-22},
  abstract = {This paper gives an extensive documentation of applications of finite-dimensional nonlinear complementarity problems in engineering and equilibrium modeling. For most applications, we describe the problem briefly, state the defining equations of the model, and give functional expressions for the complementarity formulations. The goal of this documentation is threefold: (i) to summarize the essential applications of the nonlinear complementarity problem known to date, (ii) to provide a basis for the continued research on the nonlinear complementarity problem, and (iii) to supply a broad collection of realistic complementarity problems for use in algorithmic experimentation and other studies.}
}

@book{ferrisLinearProgrammingMATLAB2008,
  title = {Linear {{Programming}} with {{MATLAB}}},
  author = {Ferris, Michael C. and Mangasarian, Olvi L. and Wright, Stephen J.},
  year = {2008},
  month = jan,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9780898718775},
  abstract = {This textbook provides a self-contained introduction to linear programming using MATLAB{\textregistered} software to elucidate the development of algorithms and theory. Early chapters cover linear algebra basics, the simplex method, duality, the solving of large linear problems, sensitivity analysis, and parametric linear programming. In later chapters, the authors discuss quadratic programming, linear complementarity, interior-point methods, and selected applications of linear programming to approximation and classification problems. Exercises are interwoven with the theory presented in each chapter, and two appendices provide additional information on linear algebra, convexity, and nonlinear functions and on available MATLAB commands, respectively. Readers can access MATLAB codes and associated mex files at a Web site maintained by the authors.},
  isbn = {978-0-89871-643-6},
  langid = {english}
}

@article{fischerNewtontypeMethodPositivesemidefinite1995,
  title = {A {{Newton-type}} Method for Positive-Semidefinite Linear Complementarity Problems},
  author = {Fischer, A.},
  year = {1995},
  month = sep,
  journal = {Journal of Optimization Theory and Applications},
  volume = {86},
  number = {3},
  pages = {585--608},
  issn = {1573-2878},
  doi = {10.1007/BF02192160},
  url = {https://doi.org/10.1007/BF02192160},
  urldate = {2023-11-28},
  abstract = {The paper presents a damped and perturbed Newton-type method for solving linear complementarity problems with positive-semidefinite matricesM. In particular, the following properties hold: all occurring subproblems are linear equations; each subproblem is uniquely solvable without any assumption; every accumulation point generated by the method solves the linear complementarity problem. The additional property ofM to be an R0-matrix is sufficient, but not necessary, for the boundedness of the iterates. Provided thatM is positive definite on a certain subspace, the method converges Q-quadratically.},
  langid = {english}
}

@article{fletcherNonlinearProgrammingNonsmooth1989,
  title = {Nonlinear Programming and Nonsmooth Optimization by Successive Linear Programming},
  author = {Fletcher, R. and {de la Maza}, E. Sainz},
  year = {1989},
  month = jan,
  journal = {Mathematical Programming},
  volume = {43},
  number = {1},
  pages = {235--256},
  issn = {1436-4646},
  doi = {10.1007/BF01582292},
  url = {https://doi.org/10.1007/BF01582292},
  urldate = {2024-09-28},
  abstract = {Methods are considered for solving nonlinear programming problems using an exactl1 penalty function. LP-like subproblems incorporating a trust region constraint are solved successively both to estimate the active set and to provide a foundation for proving global convergence. In one particular method, second order information is represented by approximating the reduced Hessian matrix, and Coleman-Conn steps are taken. A criterion for accepting these steps is given which enables the superlinear convergence properties of the Coleman-Conn method to be retained whilst preserving global convergence and avoiding the Maratos effect. The methods generalize to solve a wide range of composite nonsmooth optimization problems and the theory is presented in this general setting. A range of numerical experiments on small test problems is described.},
  langid = {english}
}

@book{fletcherPracticalMethodsOptimization2000,
  title = {Practical {{Methods}} of {{Optimization}}},
  author = {Fletcher, R.},
  year = {2000},
  month = may,
  edition = {2},
  publisher = {John Wiley \& Sons},
  url = {https://www.wiley.com/en-ie/Practical+Methods+of+Optimization%2C+2nd+Edition-p-9780471494638},
  urldate = {2022-03-08},
  abstract = {Fully describes optimization methods that are currently most valuable in solving real-life problems. Since optimization has applications in almost every branch of science and technology, the text emphasizes their practical aspects in conjunction with the heuristics useful in making them perform more reliably and efficiently. To this end, it presents comparative numerical studies to give readers a feel for possibile applications and to illustrate the problems in assessing evidence. Also provides theoretical background which provides insights into how methods are derived. This edition offers revised coverage of basic theory and standard techniques, with updated discussions of line search methods, Newton and quasi-Newton methods, and conjugate direction methods, as well as a comprehensive treatment of restricted step or trust region methods not commonly found in the literature. Also includes recent developments in hybrid methods for nonlinear least squares; an extended discussion of linear programming, with new methods for stable updating of LU factors; and a completely new section on network programming. Chapters include computer subroutines, worked examples, and study questions.},
  isbn = {978-0-471-49463-8},
  langid = {english}
}

@article{fletcherSolvingMathematicalPrograms2004,
  title = {Solving Mathematical Programs with Complementarity Constraints as Nonlinear Programs},
  author = {Fletcher, Roger and Leyffer, Sven},
  year = {2004},
  month = feb,
  journal = {Optimization Methods and Software},
  volume = {19},
  number = {1},
  pages = {15--40},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556780410001654241},
  url = {https://doi.org/10.1080/10556780410001654241},
  urldate = {2022-03-21},
  abstract = {We consider solving mathematical programs with complementarity constraints (MPCCs) as nonlinear programs (NLPs) using standard NLP solvers. This approach is appealing because it allows existing off-the-shelf NLP solvers to tackle large instances of MPCCs. Numerical experience on MacMPEC, a large collection of MPCC test problems is presented. Our experience indicates that sequential quadratic programming (SQP) methods are very well suited for solving MPCCs and at present outperform interior-point solvers both in terms of speed and reliability. All NLP solvers also compare very favorably to special MPCC solvers on tests published in the literature.}
}

@book{floudasNonlinearMixedIntegerOptimization1995,
  title = {Nonlinear and {{Mixed-Integer Optimization}}: {{Fundamentals}} and {{Applications}}},
  shorttitle = {Nonlinear and {{Mixed-Integer Optimization}}},
  author = {Floudas, Christodoulos A.},
  year = {1995},
  publisher = {Oxford University Press},
  address = {New York},
  url = {https://doi.org/10.1093/oso/9780195100563.001.0001},
  isbn = {978-0-19-510056-3},
  langid = {english}
}

@book{forresterEngineeringDesignSurrogate2008,
  title = {Engineering {{Design}} via {{Surrogate Modelling}}: {{A Practical Guide}}},
  shorttitle = {Engineering {{Design}} via {{Surrogate Modelling}}},
  author = {Forrester, Alexander and Sobester, Andras and Keane, Andy},
  year = {2008},
  month = sep,
  publisher = {Wiley},
  address = {Chichester, West Sussex, England ; Hoboken, NJ},
  url = {https://www.wiley.com/en-us/Engineering+Design+via+Surrogate+Modelling:+A+Practical+Guide-p-9780470060681},
  abstract = {Surrogate models expedite the search for promising designs by standing in for expensive design evaluations or simulations. They provide a global model of some metric of a design (such as weight, aerodynamic drag, cost, etc.), which can then be optimized efficiently. Engineering Design via Surrogate Modelling is a self-contained guide to surrogate models and their use in engineering design. The fundamentals of building, selecting, validating, searching and refining a surrogate are presented in a manner accessible to novices in the field. Figures are used liberally to explain the key concepts and clearly show the differences between the various techniques, as well as to emphasize the intuitive nature of the conceptual and mathematical reasoning behind them.More advanced and recent concepts are each presented in stand-alone chapters, allowing the reader to concentrate on material pertinent to their current design problem, and concepts are clearly demonstrated using simple design problems. This collection of advanced concepts (visualization, constraint handling, coping with noisy data, gradient-enhanced modelling, multi-fidelity analysis and multiple objectives) represents an invaluable reference manual for engineers and researchers active in the area.Engineering Design via Surrogate Modelling is complemented by a suite of Matlab codes, allowing the reader to apply all the techniques presented to their own design problems. By applying statistical modelling to engineering design, this book bridges the wide gap between the engineering and statistics communities. It will appeal to postgraduates and researchers across the academic engineering design community as well as practising design engineers.Provides an inclusive and practical guide to using surrogates in engineering design.Presents the fundamentals of building, selecting, validating, searching and refining a surrogate model.Guides the reader through the practical implementation of a surrogate-based design process using a set of case studies from real engineering design challenges.Accompanied by a companion website featuring Matlab software at http://www.wiley.com/go/forrester},
  isbn = {978-0-470-06068-1},
  langid = {english}
}

@incollection{fourerAlgebraicModelingLanguages2016,
  title = {Algebraic {{Modeling Languages}} for {{Optimization}}},
  booktitle = {Encyclopedia of {{Operations Research}} and {{Management Science}}},
  author = {Fourer, Robert},
  editor = {Gass, Saul I. and Fu, Michael C.},
  year = {2016},
  month = jan,
  pages = {43--51},
  publisher = {Springer},
  address = {Boston, MA},
  doi = {10.1007/978-1-4419-1153-7_25},
  url = {https://doi.org/10.1007/978-1-4419-1153-7_25},
  urldate = {2024-01-26},
  isbn = {978-1-4419-1153-7},
  langid = {english}
}

@article{fragniereOptimizationModelingLanguages1999,
  title = {Optimization {{Modeling Languages}}},
  author = {Fragniere, Emmanuel},
  year = {1999},
  month = sep,
  langid = {english}
}

@incollection{frieszFiniteDimensionalVariational2010,
  title = {Finite {{Dimensional Variational Inequalities}} and {{Nash Equilibria}}},
  booktitle = {Dynamic {{Optimization}} and {{Differential Games}}},
  author = {Friesz, Terry L.},
  editor = {Friesz, Terry L.},
  year = {2010},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  pages = {219--265},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-0-387-72778-3_5},
  url = {https://doi.org/10.1007/978-0-387-72778-3_5},
  urldate = {2025-01-02},
  abstract = {In this chapter, we lay the foundation for turning our focus from dynamic optimization, which has been the subject of preceding chapters, to the notion of a dynamic game. To fully appreciate the material presented in subsequent chapters, we must in the present chapter review some of the essential features of the theory of finite-dimensional variational inequalities and static noncooperative mathematical games. Today many economists and engineers are exposed to the notion of a game-theoretic equilibrium that we study in this chapter, namely Nash equilibrium. Yet, the relationship of such equilibria to certain nonextremal problems known as fixed-point problems, variational inequalities and nonlinear complementarity problems is not widely understood. It is the fact that, as we shall see, Nash and Nash-like equilibria are related to and frequently equivalent to nonextremal problems that makes the computation and qualitative investigation of such equilibria so tractable. Although the static games discussed in this chapter are really steady states of dynamic games, we are, for the most part, indifferent in this chapter to any underlying dynamics. We also comment that readers familiar with finite-dimensional variational inequalities and static Nash games may wish to skip this chapter.},
  isbn = {978-0-387-72778-3},
  langid = {english}
}

@misc{frisonGiafHpipm2023,
  title = {Giaf/Hpipm},
  author = {Frison, Gianluca},
  year = {2023},
  month = apr,
  url = {https://github.com/giaf/hpipm},
  urldate = {2023-04-08},
  abstract = {High-performance interior-point-method QP and QCQP solvers}
}

@article{frisonHPIPMHighperformanceQuadratic2020,
  title = {{{HPIPM}}: A High-Performance Quadratic Programming Framework for Model Predictive Control},
  shorttitle = {{{HPIPM}}},
  author = {Frison, Gianluca and Diehl, Moritz},
  year = {2020},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {21st {{IFAC World Congress}}},
  volume = {53},
  number = {2},
  pages = {6563--6569},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2020.12.073},
  url = {https://www.sciencedirect.com/science/article/pii/S2405896320303293},
  urldate = {2023-04-08},
  abstract = {This paper introduces HPIPM, a high-performance framework for quadratic programming (QP), designed to provide building blocks to efficiently and reliably solve model predictive control problems. HPIPM currently supports three QP types, and provides interior point method (IPM) solvers as well (partial) condensing routines. In particular, the IPM for optimal control QPs is intended to supersede the HPMPC solver, and it largely improves robustness while keeping the focus on speed. Numerical experiments show that HPIPM reliably solves challenging QPs, and that it outperforms other state-of-the-art solvers in speed.},
  langid = {english}
}

@misc{fukudaCcdlibReferenceManual2021,
  title = {Ccdlib {{Reference Manual}}},
  author = {Fukuda, Komei},
  year = {2021},
  month = feb,
  url = {https://people.inf.ethz.ch/fukudak/cdd_home/cddlibman2021.pdf},
  urldate = {2022-12-16}
}

@article{gabrelRecentAdvancesRobust2014,
  title = {Recent Advances in Robust Optimization: {{An}} Overview},
  shorttitle = {Recent Advances in Robust Optimization},
  author = {Gabrel, Virginie and Murat, C{\'e}cile and Thiele, Aur{\'e}lie},
  year = {2014},
  month = jun,
  journal = {European Journal of Operational Research},
  volume = {235},
  number = {3},
  pages = {471--483},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2013.09.036},
  url = {https://www.sciencedirect.com/science/article/pii/S0377221713007911},
  urldate = {2023-05-28},
  abstract = {This paper provides an overview of developments in robust optimization since 2007. It seeks to give a representative picture of the research topics most explored in recent years, highlight common themes in the investigations of independent research teams and highlight the contributions of rising as well as established researchers both to the theory of robust optimization and its practice. With respect to the theory of robust optimization, this paper reviews recent results on the cases without and with recourse, i.e., the static and dynamic settings, as well as the connection with stochastic optimization and risk theory, the concept of distributionally robust optimization, and findings in robust nonlinear optimization. With respect to the practice of robust optimization, we consider a broad spectrum of applications, in particular inventory and logistics, finance, revenue management, but also queueing networks, machine learning, energy systems and the public good. Key developments in the period from 2007 to present include: (i) an extensive body of work on robust decision-making under uncertainty with uncertain distributions, i.e., ``robustifying'' stochastic optimization, (ii) a greater connection with decision sciences by linking uncertainty sets to risk theory, (iii) further results on nonlinear optimization and sequential decision-making and (iv) besides more work on established families of examples such as robust inventory and revenue management, the addition to the robust optimization literature of new application areas, especially energy systems and the public good.},
  langid = {english}
}

@book{gabrielComplementarityModelingEnergy2013,
  title = {Complementarity {{Modeling}} in {{Energy Markets}}},
  author = {Gabriel, Steven A. and Conejo, Antonio J. and Fuller, J. David and Hobbs, Benjamin F. and Ruiz, Carlos},
  year = {2013},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  publisher = {Springer},
  address = {New York, NY},
  url = {https://doi.org/10.1007/978-1-4419-6123-5},
  abstract = {This addition to the ISOR series introduces complementarity models in a straightforward and approachable manner and uses them to carry out an in-depth analysis of energy markets, including formulation issues and solution techniques. In a nutshell, complementarity models generalize: a. optimization problems via their Karush-Kuhn-Tucker conditions b. on-cooperative games in which each player may be solving a separate but related optimization problem with potentially overall system constraints (e.g., market-clearing conditions) c. conomic and engineering problems that aren't specifically derived from optimization problems (e.g., spatial price equilibria) d. roblems in which both primal and dual variables (prices) appear in the original formulation (e.g., The National Energy Modeling System (NEMS) or its precursor, PIES). As such, complementarity models are a very general and flexible modeling format. A natural question is why concentrate on energy markets for this complementarity approach? s it turns out, energy or other markets that have game theoretic aspects are best modeled by complementarity problems. The reason is that the traditional perfect competition approach no longer applies due to deregulation and restructuring of these markets and thus the corresponding optimization problems may no longer hold. Also, in some instances it is important in the original model formulation to involve both primal variables (e.g., production) as well as dual variables (e.g., market prices) for public and private sector energy planning. Traditional optimization problems can not directly handle this mixing of primal and dual variables but complementarity models can and this makes them all that more effective for decision-makers.},
  isbn = {978-1-4419-6122-8}
}

@article{gandolfoStablePathFollowingControl2017,
  title = {Stable {{Path-Following Control}} for a {{Quadrotor Helicopter Considering Energy Consumption}}},
  author = {Gandolfo, Daniel C. and Salinas, Lucio R. and Brand{\~a}o, Alexandre and Toibero, Juan M.},
  year = {2017},
  month = jul,
  journal = {IEEE Transactions on Control Systems Technology},
  volume = {25},
  number = {4},
  pages = {1423--1430},
  issn = {1558-0865},
  doi = {10.1109/TCST.2016.2601288},
  abstract = {A substantial interest in aerial robots has grown in recent years. However, the energetic cost of flying is one of the key challenges nowadays. Rotorcrafts are heavier-than-air flying machines that use lift generated by one or several rotors (vertically oriented propellers), and because of this, they spend a large proportion of their available energy to maintain their own weight in the air. In this brief, this concept is used to evaluate the relationship between navigation speed and energy consumption in a miniature quadrotor helicopter, which travels over a desired path. A novel path-following controller is proposed in which the speed of the rotorcraft is a dynamic profile that varies with the geometric requirements of the desired path. The stability of the control law is proved using the Lyapunov theory. The experimental results using a real quadrotor show the good performance of the proposed controller, and the percentages of involved energy are quantified using a model of a lithium polymer battery that was previously identified.}
}

@book{garnettBayesianOptimization2023,
  title = {Bayesian {{Optimization}}},
  author = {Garnett, Roman},
  year = {23 b{\v r}ezna 2023},
  publisher = {Cambridge University Press},
  address = {Cambridge, United Kingdom; New York, NY},
  url = {https://bayesoptbook.com},
  abstract = {"Bayesian optimization is a methodology that has proven success in the sciences, engineering, and beyond for optimizing expensive objective functions. This self-contained text targets graduate students and researchers in machine learning and statistics -and practitioners from other fields - wishing to harness the power of Bayesian optimization"--},
  isbn = {978-1-108-42578-0}
}

@misc{GettingStartedSum,
  type = {Documentation and Examples on Using {{Sum}} of {{Squares}} Solvers, Tutorial and Examples of {{Sum}} of {{Squares}} Programming},
  title = {Getting Started with {{Sum}} of {{Squares}}},
  journal = {Getting started with Sum of Squares},
  url = {https://sums-of-squares.github.io/sos/},
  urldate = {2022-01-13},
  abstract = {Documentation and examples on using Sum of Squares solvers, tutorial and examples of Sum of Squares programming},
  langid = {american}
}

@incollection{giannessiTheoremsAlternativeOptimization2009,
  title = {Theorems of the Alternative and Optimization},
  booktitle = {Encyclopedia of {{Optimization}}},
  author = {Giannessi, Franco},
  editor = {Floudas, Christodoulos A. and Pardalos, Panos M.},
  year = {2009},
  pages = {3895--3902},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-0-387-74759-0_680},
  url = {https://doi.org/10.1007/978-0-387-74759-0_680},
  urldate = {2022-07-29},
  isbn = {978-0-387-74759-0},
  langid = {english}
}

@book{gillNumericalLinearAlgebra1990,
  title = {Numerical {{Linear Algebra}} and {{Optimization}}, {{Vol}}. 1},
  author = {Gill, Philip E. and Murray, Walter and Wright, Margaret H.},
  year = {1990},
  month = sep,
  publisher = {CRC Press},
  address = {Redwood City, Calif},
  isbn = {978-0-201-12649-5},
  langid = {english}
}

@book{gillPracticalOptimization2019,
  title = {Practical {{Optimization}}},
  author = {Gill, Philip E. and Murray, Walter and Wright, Margaret H.},
  year = {2019},
  series = {Classics in {{Applied Mathematics}}},
  publisher = {SIAM},
  address = {Philadelphia, PA},
  url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611975604},
  urldate = {2022-03-16},
  abstract = {In the intervening years since this book was published in 1981, the field of optimization has been exceptionally lively. This fertility has involved not only progress in theory, but also faster numerical algorithms and extensions into unexpected or previously unknown areas such as semidefinite programming. Despite these changes, many of the important principles and much of the intuition can be found in this Classics version of Practical Optimization. This book     provides model algorithms and pseudocode, useful tools for users who prefer to write their own code as well as for those who want to understand externally provided code;     presents algorithms in a step-by-step format, revealing the overall structure of the underlying procedures and thereby allowing a high-level perspective on the fundamental differences; and     contains a wealth of techniques and strategies that are well suited for optimization in the twenty-first century and particularly in the now-flourishing fields of data science, ``big data,'' and machine learning. Practical Optimization is appropriate for advanced undergraduates, graduate students, and researchers interested in methods for solving optimization problems.},
  isbn = {978-1-61197-559-8},
  langid = {english}
}

@article{gillSNOPTSQPAlgorithm2005,
  title = {{{SNOPT}}: {{An SQP Algorithm}} for {{Large-Scale Constrained Optimization}}},
  shorttitle = {{{SNOPT}}},
  author = {Gill, Philip E. and Murray, Walter and Saunders, Michael A.},
  year = {2005},
  month = jan,
  journal = {SIAM Review},
  volume = {47},
  number = {1},
  pages = {99--131},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/S0036144504446096},
  url = {https://epubs.siam.org/doi/abs/10.1137/S0036144504446096},
  urldate = {2021-04-08},
  abstract = {Sequential quadratic programming (SQP) methods have proved highly effective for solving constrained optimization problems with smooth nonlinear functions in the objective and constraints. Here we consider problems with general inequality constraints (linear and nonlinear). We assume that first derivatives are available and that the constraint gradients are sparse. Second derivatives are assumed to be unavailable or too expensive to calculate.  We discuss an SQP algorithm that uses a smooth augmented Lagrangian merit function and makes explicit provision for infeasibility in the original problem and the QP subproblems. The Hessian of the Lagrangian is approximated using a limited-memory quasi-Newton method.  SNOPT is a particular implementation that uses a reduced-Hessian semidefinite QP solver (SQOPT) for the QP subproblems. It is designed for problems with many thousands of constraints and variables but is best suited for problems with a moderate number of degrees of freedom (say, up to 2000). Numerical results are given for most of the CUTEr and COPS test collections (about 1020 examples of all sizes up to 40000 constraints and variables, and up to 20000 degrees of freedom).}
}

@article{gorissenPracticalGuideRobust2015,
  title = {A Practical Guide to Robust Optimization},
  author = {Gorissen, Bram L. and Yan{\i}ko{\u g}lu, {\.I}hsan and {den Hertog}, Dick},
  year = {2015},
  month = jun,
  journal = {Omega},
  volume = {53},
  pages = {124--137},
  issn = {0305-0483},
  doi = {10.1016/j.omega.2014.12.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0305048314001698},
  urldate = {2021-11-26},
  abstract = {Robust optimization is a young and active research field that has been mainly developed in the last 15 years. Robust optimization is very useful for practice, since it is tailored to the information at hand, and it leads to computationally tractable formulations. It is therefore remarkable that real-life applications of robust optimization are still lagging behind; there is much more potential for real-life applications than has been exploited hitherto. The aim of this paper is to help practitioners to understand robust optimization and to successfully apply it in practice. We provide a brief introduction to robust optimization, and also describe important dos and donts for using it in practice. We use many small examples to illustrate our discussions.},
  langid = {english}
}

@article{gorokhovikPiecewiseAffineFunctions1994,
  title = {Piecewise Affine Functions and Polyhedral Sets},
  author = {Gorokhovik, V. V. and Zorko, O. I. and Birkhoff, G.},
  year = {1994},
  month = jan,
  journal = {Optimization},
  volume = {31},
  number = {3},
  pages = {209--221},
  publisher = {Taylor \& Francis},
  issn = {0233-1934},
  doi = {10.1080/02331939408844018},
  url = {https://doi.org/10.1080/02331939408844018},
  urldate = {2023-11-16},
  abstract = {In this paper we present a number of characterizations of piecewise affine and piecewise linear functions defined on finite dimesional normed vector spaces. In particular we prove that a real-valued function is piecewise affine [resp. piecewise linear] if both its epigraph and its hypograph are (nonconvex) polyhedral sets[resp..Polyhedral cones]. Also,We show that the collection of all piecewise affine[resp.piecewise linear] functions. Furthermore, we prove that a function is piecewise affine[resp.piecewise linear] if it can be represented as a difference of two convex [resp.,sublinear] polyhedral fucntions.}
}

@misc{grantCVXMatlabSoftware2020,
  title = {{{CVX}}: {{Matlab Software}} for {{Disciplined Convex Programming}}},
  shorttitle = {{{CVX}}},
  author = {Grant, Michael and Boyd, Stephen},
  year = {2020},
  month = dec,
  url = {http://cvxr.com/cvx/},
  urldate = {2020-12-28},
  howpublished = {CVX Research}
}

@inproceedings{grantGraphImplementationsNonsmooth2008,
  title = {Graph {{Implementations}} for {{Nonsmooth Convex Programs}}},
  booktitle = {Recent {{Advances}} in {{Learning}} and {{Control}}},
  author = {Grant, Michael C. and Boyd, Stephen P.},
  editor = {Blondel, Vincent D. and Boyd, Stephen P. and Kimura, Hidenori},
  year = {2008},
  series = {Lecture {{Notes}} in {{Control}} and {{Information Sciences}}},
  pages = {95--110},
  publisher = {Springer},
  address = {London},
  doi = {10.1007/978-1-84800-155-8_7},
  abstract = {We describe graph implementations, a generic method for representing a convex function via its epigraph, described in a disciplined convex programming framework. This simple and natural idea allows a very wide variety of smooth and nonsmooth convex programs to be easily specified and efficiently solved, using interiorpoint methods for smooth or cone convex programs.},
  isbn = {978-1-84800-155-8},
  langid = {english}
}

@misc{greenbergStrictlyComplementarySolutions,
  title = {Strictly Complementary Solutions in Linear Programming},
  author = {Greenberg, Harvey J.},
  url = {https://sbrg.ucsd.edu/sites/sbrg.ucsd.edu/files/Attachments/Images/classes/convex_presentations/SCSolutions.pdf}
}

@book{grossmannAdvancedOptimizationProcess2021,
  title = {Advanced {{Optimization}} for {{Process Systems Engineering}}},
  author = {Grossmann, Ignacio E.},
  year = {2021},
  month = mar,
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/9781108917834},
  url = {https://www.cambridge.org/highereducation/books/advanced-optimization-for-process-systems-engineering/8F1FBC76FB26A317402AE396759E12A4},
  urldate = {2023-07-12},
  abstract = {Based on the author's forty years of teaching experience, this unique textbook covers both basic and advanced concepts of optimization theory and methods for process systems engineers. Topics covered include continuous, discrete and logic optimization (linear, nonlinear, mixed-integer and generalized disjunctive programming), optimization under uncertainty (stochastic programming and flexibility analysis), and decomposition techniques (Lagrangean and Benders decomposition). Assuming only a basic background in calculus and linear algebra, it enables easy understanding of mathematical reasoning, and numerous examples throughout illustrate key concepts and algorithms. End-of-chapter exercises involving theoretical derivations and small numerical problems, as well as in modeling systems like GAMS, enhance understanding and help put knowledge into practice. Accompanied by two appendices containing web links to modeling systems and models related to applications in PSE, this is an essential text for single-semester, graduate courses in process systems engineering in departments of chemical engineering.},
  isbn = {978-1-108-91783-4},
  langid = {english}
}

@book{gueninGentleIntroductionOptimization2014,
  title = {A {{Gentle Introduction}} to {{Optimization}}},
  author = {Guenin, B. and K{\"o}nemann, J. and Tun{\c c}el, L.},
  year = {2014},
  month = jul,
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9781107282094},
  url = {https://www.cambridge.org/highereducation/books/a-gentle-introduction-to-optimization/F3E04AD76ADD3C202A2206902CD2D051},
  urldate = {2025-01-21},
  abstract = {Optimization is an essential technique for solving problems in areas as diverse as accounting, computer science and engineering. Assuming only basic linear algebra and with a clear focus on the fundamental concepts, this textbook is the perfect starting point for first- and second-year undergraduate students from a wide range of backgrounds and with varying levels of ability. Modern, real-world examples motivate the theory throughout. The authors keep the text as concise and focused as possible, with more advanced material treated separately or in starred exercises. Chapters are self-contained so that instructors and students can adapt the material to suit their own needs and a wide selection of over 140 exercises gives readers the opportunity to try out the skills they gain in each section. Solutions are available for instructors. The book also provides suggestions for further reading to help students take the next step to more advanced material.},
  isbn = {9781107282094},
  langid = {english}
}

@misc{GurobiOptimizerExample,
  title = {Gurobi {{Optimizer Example Tour}}},
  year = {2023},
  url = {https://www.gurobi.com/documentation/}
}

@misc{GurobiOptimizerReference2023,
  title = {Gurobi {{Optimizer Reference Manual}}},
  year = {2023},
  url = {https://www.gurobi.com/documentation/},
  howpublished = {Gurobi Optimization, LLC}
}

@article{guzelisCanonicalRepresentationPiecewiseaffine1991,
  title = {A Canonical Representation for Piecewise-Affine Maps and Its Applications to Circuit Analysis},
  author = {Guzelis, C. and Goknar, I.C.},
  year = {1991},
  month = nov,
  journal = {IEEE Transactions on Circuits and Systems},
  volume = {38},
  number = {11},
  pages = {1342--1354},
  issn = {1558-1276},
  doi = {10.1109/31.99163},
  abstract = {A canonical representation for a rather general class of piecewise-affine maps has been developed. This canonical representation extends the canonical representation proposed by L.O. Chua and S.M. Hang (Proc. IEEE, vol.65, no.6, p.915-29, 1977) into PWA (piecewise-affine) partitions, which arise frequently in driving-point, transfer characteristics, and state equations. Thus, a universal canonical representation that is capable of characterizing circuit equations, state equations, and driving-point and transfer characteristics of piecewise-affine circuits in a compact global analytic form has been obtained. The canonical forms developed seem to be promising tools for computational purposes as well as for analytical studies of PWA circuits.{$<>$}}
}

@article{haleAsynchronousMultiagentPrimalDual2017,
  title = {Asynchronous {{Multiagent Primal-Dual Optimization}}},
  author = {Hale, Matthew T. and Nedi{\'c}, Angelia and Egerstedt, Magnus},
  year = {2017},
  month = sep,
  journal = {IEEE Transactions on Automatic Control},
  volume = {62},
  number = {9},
  pages = {4421--4435},
  issn = {1558-2523},
  doi = {10.1109/TAC.2017.2662019},
  abstract = {We present a framework for asynchronously solving convex optimization problems over networks of agents which are augmented by the presence of a centralized cloud computer. This framework uses a Tikhonov-regularized primal-dual approach in which the agents update the system's primal variables and the cloud updates its dual variables. To minimize coordination requirements placed upon the system, the times of communications and computations among the agents are allowed to be arbitrary, provided they satisfy mild conditions. Communications from the agents to the cloud are likewise carried out without any coordination in their timing. However, we require that the cloud keeps the dual variable's value synchronized across the agents, and a counterexample is provided that demonstrates that this level of synchrony is indeed necessary for convergence. Convergence rate estimates are provided in both the primal and dual spaces, and simulation results are presented that demonstrate the operation and convergence of the proposed algorithm.}
}

@misc{hallEngineeringBusinessApplications2019,
  title = {Engineering and {{Business Applications}} of {{Sum}} of {{Squares Polynomials}}},
  author = {Hall, Georgina},
  year = {2019},
  month = jun,
  number = {arXiv:1906.07961},
  eprint = {1906.07961},
  primaryclass = {cs, eess, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.07961},
  url = {http://arxiv.org/abs/1906.07961},
  urldate = {2023-12-20},
  abstract = {Optimizing over the cone of nonnegative polynomials, and its dual counterpart, optimizing over the space of moments that admit a representing measure, are fundamental problems that appear in many different applications from engineering and computational mathematics to business. In this paper, we review a number of these applications. These include, but are not limited to, problems in control (e.g., formal safety verification), finance (e.g., option pricing), statistics and machine learning (e.g., shape-constrained regression and optimal design), and game theory (e.g., Nash equilibria computation in polynomial games). We then show how sum of squares techniques can be used to tackle these problems, which are hard to solve in general. We conclude by highlighting some directions that could be pursued to further disseminate sum of squares techniques within more applied fields. Among other things, we briefly address the current challenge that scalability represents for optimization problems that involve sum of squares polynomials and discuss recent trends in software development.},
  archiveprefix = {arXiv}
}

@misc{hallLCQPowSolverLinear2022,
  title = {{{LCQPow}} -- {{A Solver}} for {{Linear Complementarity Quadratic Programs}}},
  author = {Hall, Jonas and Nurkanovic, Armin and Messerer, Florian and Diehl, Moritz},
  year = {2022},
  month = nov,
  number = {arXiv:2211.16341},
  eprint = {2211.16341},
  primaryclass = {math},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2211.16341},
  urldate = {2022-12-03},
  abstract = {In this paper we introduce an open-source software package written in C++ for efficiently finding solutions to quadratic programming problems with linear complementarity constraints. These problems arise in a wide range of applications in engineering and economics, and they are challenging to solve due to their structural violation of standard constraint qualifications, and highly nonconvex, nonsmooth feasible sets. This work extends a previously presented algorithm based on a sequential convex programming approach applied to a standard penalty reformulation. We examine the behavior of local convergence and introduce new algorithmic features. Competitive performance profiles are presented in comparison to state-of-the-art solvers and solution variants in both existing and new benchmarks.},
  archiveprefix = {arXiv}
}

@article{hallSequentialConvexProgramming2022,
  title = {A {{Sequential Convex Programming Approach}} to {{Solving Quadratic Programs}} and {{Optimal Control Problems With Linear Complementarity Constraints}}},
  author = {Hall, Jonas and Nurkanovi{\'c}, Armin and Messerer, Florian and Diehl, Moritz},
  year = {2022},
  journal = {IEEE Control Systems Letters},
  volume = {6},
  pages = {536--541},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2021.3083467},
  abstract = {Mathematical programs with complementarity constraints are notoriously difficult to solve due to their nonconvexity and lack of constraint qualifications in every feasible point. This letter focuses on the subclass of quadratic programs with linear complementarity constraints. A novel approach to solving a penalty reformulation using sequential convex programming and a homotopy on the penalty parameter is introduced. Linearizing the necessarily nonconvex penalty function yields convex quadratic subproblems, which have a constant Hessian matrix throughout all iterates. This allows solution computation with a single KKT matrix factorization. Furthermore, a globalization scheme is introduced in which the underlying merit function is minimized analytically, and guarantee of descent is provided at each iterate. The algorithmic features and possible computational speedups are illustrated in a numerical experiment.}
}

@book{haneveldStochasticProgrammingModeling2020,
  title = {Stochastic {{Programming}}: {{Modeling Decision Problems Under Uncertainty}}},
  shorttitle = {Stochastic {{Programming}}},
  author = {Haneveld, Willem K. Klein and van der Vlerk, Maarten H. and Romeijnders, Ward},
  year = {5 listopadu 2020},
  series = {Graduate {{Texts}} in {{Operations Research}}},
  publisher = {Springer},
  address = {Cham},
  url = {https://doi.org/10.1007/978-3-030-29219-5},
  abstract = {This book provides an essential introduction to Stochastic Programming, especially intended for graduate students. The book begins by exploring a linear programming problem with random parameters, representing a decision problem under uncertainty. Several models for this problem are presented, including the main ones used in Stochastic Programming: recourse models and chance constraint models. The book not only discusses the theoretical properties of these models and algorithms for solving them, but also explains the intrinsic differences between the models. In the book's closing section, several case studies are presented, helping students apply the theory covered to practical problems. The book is based on lecture notes developed for an Econometrics and Operations Research course for master students at the University of Groningen, the Netherlands - the longest-standing Stochastic Programming course worldwide.},
  isbn = {978-3-030-29221-8}
}

@article{hansonSufficiencyKuhnTuckerConditions1981,
  title = {On Sufficiency of the {{Kuhn-Tucker}} Conditions},
  author = {Hanson, Morgan A},
  year = {1981},
  month = apr,
  journal = {Journal of Mathematical Analysis and Applications},
  volume = {80},
  number = {2},
  pages = {545--550},
  issn = {0022-247X},
  doi = {10.1016/0022-247X(81)90123-2},
  url = {https://www.sciencedirect.com/science/article/pii/0022247X81901232},
  urldate = {2022-03-31},
  abstract = {The increase in kitchen waste (KW) generation and irresponsible rice straw (RS) disposal has accentuated problems regarding environmental pollution and resource wasting. Although anaerobic co-digestion has been shown to be a more efficient strategy for bioenergy production than single digestion, there is still significant resistance to its application and diffusion owing to RS collection and many related factors in practice. Therefore, this study proposes a cooperative scheme for the cross-regional problem involving authorities and disposal plants. Subsequently, a bi-level multi-objective programming is developed to express the complexity of conflicts relationship in a multi-hierarchical problem and to achieve economic development, environmental protection and societal satisfaction in the bioenergy optimization process. A practical case study is then investigated to demonstrate the applicability of methodology in co-digestion deployment and resource recovery improvement, whereby scenario analyses are conducted by adjusting associated parameters. The practical application observed that (1) RS purchase was positively correlated with carbon reduction and social satisfaction targets, while excessive pursuit of social satisfaction resulted in a rebounding of carbon emissions; (2) the most popular mix ratios in disposal plants were 4:1 and 3:1, but they favor the latter when RS is readily available in sufficient quantities. Finally, some targeted suggestions are proposed for all stakeholders, which are expected to be a reference for potential users in other areas. Projected gradient descent has been proved efficient in many optimization and machine learning problems. The weighted {$\ell$}1{$<$}math{$><$}msub is="true"{$><$}mrow is="true"{$><$}mi is="true"{$>\ell<$}/mi{$><$}/mrow{$><$}mrow is="true"{$><$}mn is="true"{$>$}1{$<$}/mn{$><$}/mrow{$><$}/msub{$><$}/math{$>$} ball has been shown effective in sparse system identification and features selection. In this paper we propose three new efficient algorithms for projecting any vector of finite length onto the weighted {$\ell$}1{$<$}math{$><$}msub is="true"{$><$}mrow is="true"{$><$}mi is="true"{$>\ell<$}/mi{$><$}/mrow{$><$}mrow is="true"{$><$}mn is="true"{$>$}1{$<$}/mn{$><$}/mrow{$><$}/msub{$><$}/math{$>$} ball. The first two algorithms have a linear worst case complexity. The third one has a highly competitive performances in practice but the worst case has a quadratic complexity. These new algorithms are efficient tools for machine learning methods based on projected gradient descent such as compressed sensing, feature selection. We illustrate this effectiveness by adapting an efficient compressed sensing algorithm to weighted projections. We demonstrate the efficiency of our new algorithms on benchmarks using very large vectors. For instance, it requires only 8 ms, on an Intel I7 3rd generation, for projecting vectors of size 107. Federated learning (FL) has been widely used to train machine learning models over massive data in edge computing. However, the existing FL solutions may cause long training time and/or high resource (e.g., bandwidth) cost, and thus cannot be directly applied for resource-constrained edge nodes, such as base stations and access points. In this paper, we propose a novel communication-efficient asynchronous federated learning (CE-AFL) mechanism, in which the parameter server will aggregate the local model updates only from a certain fraction {$\alpha<$}math{$><$}mi is="true"{$>\alpha<$}/mi{$><$}/math{$>$}, with 0{$<\alpha<$}1{$<$}math{$><$}mrow is="true"{$><$}mn is="true"{$>$}0{$<$}/mn{$><$}mo linebreak="goodbreak" linebreakstyle="after" is="true"{$>\&$}lt;{$<$}/mo{$><$}mi is="true"{$>\alpha<$}/mi{$><$}mo linebreak="goodbreak" linebreakstyle="after" is="true"{$>\&$}lt;{$<$}/mo{$><$}mn is="true"{$>$}1{$<$}/mn{$><$}/mrow{$><$}/math{$>$}, of all edge nodes by their arrival order in each epoch. As a case study, we design efficient algorithms to determine the optimal value of {$\alpha<$}math{$><$}mi is="true"{$>\alpha<$}/mi{$><$}/math{$>$} for two cases of CE-AFL, single learning task and multiple learning tasks, under bandwidth constraints. We formally prove the convergence of the proposed algorithm. We evaluate the performance of our algorithm with experiments on Jetson TX2, deep learning workstation and extensive simulations. Both experimental results and simulation results on the classical models and datasets show the effectiveness of our proposed mechanism and algorithms. For example, CE-AFL can reduce the training time by about 69\% while achieving similar accuracy, and improve the accuracy of the trained models by about 18\% under resource constraints, compared with the state-of-the-art solutions. In this paper, a new concept of generalized convexity is introduced for (not necessarily) differentiable optimization problem with E{$<$}math{$><$}mi is="true"{$>$}E{$<$}/mi{$><$}/math{$>$}-differentiable functions. Namely, for an E{$<$}math{$><$}mi is="true"{$>$}E{$<$}/mi{$><$}/math{$>$}-differentiable function, the concept of E{$<$}math{$><$}mi is="true"{$>$}E{$<$}/mi{$><$}/math{$>$}-B{$<$}math{$><$}mi is="true"{$>$}B{$<$}/mi{$><$}/math{$>$}-invexity is defined. The E{$<$}math{$><$}mi is="true"{$>$}E{$<$}/mi{$><$}/math{$>$}-differentiable E{$<$}math{$><$}mi is="true"{$>$}E{$<$}/mi{$><$}/math{$>$}-B{$<$}math{$><$}mi is="true"{$>$}B{$<$}/mi{$><$}/math{$>$}-invexity notion unify the concepts of convexity, invexity, E{$<$}math{$><$}mi is="true"{$>$}E{$<$}/mi{$><$}/math{$>$}-convexity, E{$<$}math{$><$}mi is="true"{$>$}E{$<$}/mi{$><$}/math{$>$}-invexity and B{$<$}math{$><$}mi is="true"{$>$}B{$<$}/mi{$><$}/math{$>$}-invexity. Further, the sufficiency of the so-called E{$<$}math{$><$}mi is="true"{$>$}E{$<$}/mi{$><$}/math{$>$}-Karush-Kuhn--Tucker optimality conditions are established for the considered E{$<$}math{$><$}mi is="true"{$>$}E{$<$}/mi{$><$}/math{$>$}-differentiable optimization problem with both inequality and equality constraints under E{$<$}math{$><$}mi is="true"{$>$}E{$<$}/mi{$><$}/math{$>$}-B{$<$}math{$><$}mi is="true"{$>$}B{$<$}/mi{$><$}/math{$>$}-invexity hypotheses. Moreover, the example of a nonsmooth programming problem with E{$<$}math{$><$}mi is="true"{$>$}E{$<$}/mi{$><$}/math{$>$}-differentiable functions is constructed to illustrate the aforesaid results. This paper aims to find a solution of interval uncertainty to multiobjective variational problems. For this, we consider an interval-valued multiobjective variational problem. Then, by using the modified F{$<$}math{$><$}mi is="true"{$>$}F{$<$}/mi{$><$}/math{$>$}-objective function method, we construct associated interval-valued multiobjective variational problem with the modified F{$<$}math{$><$}mi is="true"{$>$}F{$<$}/mi{$><$}/math{$>$}-objective functions. We establish a relationship between LU{$<$}math{$><$}mrow is="true"{$><$}mi is="true"{$>$}L{$<$}/mi{$><$}mi is="true"{$>$}U{$<$}/mi{$><$}/mrow{$><$}/math{$>$}-pareto optimal solution of original problem and its associated modified problem by using the concept of LU{$<$}math{$><$}mrow is="true"{$><$}mi is="true"{$>$}L{$<$}/mi{$><$}mi is="true"{$>$}U{$<$}/mi{$><$}/mrow{$><$}/math{$>$}-F{$<$}math{$><$}mi is="true"{$>$}F{$<$}/mi{$><$}/math{$>$}-convexity and LU{$<$}math{$><$}mrow is="true"{$><$}mi is="true"{$>$}L{$<$}/mi{$><$}mi is="true"{$>$}U{$<$}/mi{$><$}/mrow{$><$}/math{$>$}-F{$<$}math{$><$}mi is="true"{$>$}F{$<$}/mi{$><$}/math{$>$}-pseudoconvexity. Further, we define LU{$<$}math{$><$}mrow is="true"{$><$}mi is="true"{$>$}L{$<$}/mi{$><$}mi is="true"{$>$}U{$<$}/mi{$><$}/mrow{$><$}/math{$>$}-Lagrange function and its saddle point to discuss the efficient solution of original problem through it. We provide an example to validate our results numerically.},
  langid = {english}
}

@book{hanssonOptimizationLearningControl2023,
  title = {Optimization for {{Learning}} and {{Control}}},
  author = {Hansson, Anders and Andersen, Martin},
  year = {2023},
  address = {Hoboken, NJ, USA},
  url = {https://www.wiley.com/en-us/Optimization+for+Learning+and+Control-p-9781119809135},
  abstract = {Optimization for Learning and Control~Comprehensive resource providing a masters' level introduction to optimization theory and algorithms for learning and control~Optimization for Learning and Control describes how optimization is used in these domains, giving a thorough introduction to both unsupervised learning, supervised learning, and reinforcement learning, with an emphasis on optimization methods for large-scale learning and control problems.~Several applications areas are also discussed, including signal processing, system identification, optimal control, and machine learning.~Today, most of the material on the optimization aspects of deep learning that is accessible for students at a Masters' level is focused on surface-level computer programming; deeper knowledge about the optimization methods and the trade-offs that are behind these methods is not provided. The objective of this book is to make this scattered knowledge, currently mainly available in publications in academic journals, accessible for Masters' students in a coherent way. The focus is on basic algorithmic principles and trade-offs.~Optimization for Learning and Control covers sample topics such as:* Optimization theory and optimization methods, covering classes of optimization problems like least squares problems, quadratic problems, conic optimization problems and rank optimization.* First-order methods, second-order methods, variable metric methods, and methods for nonlinear least squares problems.* Stochastic optimization methods, augmented Lagrangian methods, interior-point methods, and conic optimization methods.* Dynamic programming for solving optimal control problems and its generalization to reinforcement learning.* How optimization theory is used to develop theory and tools of statistics and learning, e.g., the maximum likelihood method, expectation maximization, k-means clustering, and support vector machines.* How calculus of variations is used in optimal control and for deriving the family of exponential distributions.~Optimization for Learning and Control is an ideal resource on the subject for scientists and engineers learning about which optimization methods are useful for learning and control problems; the text will also appeal to industry professionals using machine learning for different practical applications.},
  isbn = {978-1-119-80913-5}
}

@book{haslingerIntroductionShapeOptimization1987,
  title = {Introduction to {{Shape Optimization}}: {{Theory}}, {{Approximation}}, and {{Computation}}},
  shorttitle = {Introduction to {{Shape Optimization}}},
  author = {Haslinger, J. and M{\"a}kinen, R. A. E.},
  year = {2003},
  series = {Advances in {{Design}} and {{Control}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9780898718690},
  isbn = {978-0-89871-536-1},
  langid = {english}
}

@article{hauswirthTimescaleSeparationAutonomous2021,
  title = {Timescale {{Separation}} in {{Autonomous Optimization}}},
  author = {Hauswirth, Adrian and Bolognani, Saverio and Hug, Gabriela and D{\"o}rfler, Florian},
  year = {2021},
  journal = {IEEE Transactions on Automatic Control},
  volume = {66},
  number = {2},
  pages = {611--624},
  issn = {1558-2523},
  doi = {10.1109/TAC.2020.2989274},
  abstract = {Autonomous optimization refers to the design of feedback controllers that steer a physical system to a steady state that solves a predefined, possibly constrained, optimization problem. As such, no exogenous control inputs such as set points or trajectories are required. Instead, these controllers are modeled after optimization algorithms that take the form of dynamical systems. The interconnection of this type of optimization dynamics with a physical system is however not guaranteed to be stable unless both dynamics act on sufficiently different timescales. In this paper, we quantify the required timescale separation and give prescriptions that can be directly used in the design of this type of feedback controllers. Using ideas from singular perturbation analysis, we derive stability bounds for different feedback laws that are based on common continuous-time optimization schemes. In particular, we consider gradient descent and its variations, including projected gradient, and Newton gradient. We further give stability bounds for momentum methods and saddle-point flows. Finally, we discuss how optimization algorithms such as subgradient and accelerated gradient descent, while well-behaved in offline settings, are unsuitable for autonomous optimization due to their general lack of robustness.}
}

@inproceedings{hedengrenOpensourceModelingPlatforms2023,
  title = {Open-Source Modeling Platforms},
  booktitle = {{{FOCAPO}}/{{CPC}} 2023},
  author = {Hedengren, John and Nicholson, Bethany},
  year = {2023},
  address = {San Antonio, TX},
  url = {http://focapo-cpc.org/index.cfm},
  abstract = {A review of current trends in scientific computing reveals a broad shift to open-source and higher-level programming languages such as Python and growing career opportunities over the next decade. Open-source modeling tools accelerate innovation in equation-based and data-driven applications. Significant resources have been deployed to develop datadriven tools (PyTorch, TensorFlow, Scikit-learn) from tech companies that rely on machine learning services to accelerate business needs. The data and applications of the software are proprietary but the foundational tools are open. Opensource equation-based tools such as Pyomo, CasADi, Gekko, and JuMP are also gaining momentum according to user community and development pace metrics. The future of open-source modeling tools is in specialization and interfaces to other specialized packages. Integration of data-driven (empirical) and equation-based (principles, knowledge-driven) tools is emerging. New compute hardware, productivity software, and training resources have the potential to radically accelerate progress. However, long-term support mechanisms are still needed to sustain momentum and maintenance for key foundational packages.},
  langid = {english}
}

@article{heltonLinearMatrixInequality2003,
  title = {Linear {{Matrix Inequality Representation}} of {{Sets}}},
  author = {Helton, J. William and Vinnikov, Victor},
  year = {2003},
  month = jun,
  journal = {math/0306180},
  eprint = {math/0306180},
  url = {http://arxiv.org/abs/math/0306180},
  urldate = {2010-04-21},
  abstract = {This article concerns the question: which subsets of \$\{{\textbackslash}mathbb R\}{\textasciicircum}m\$ can be represented with Linear Matrix Inequalities, LMIs? This gives some perspective on the scope and limitations of one of the most powerful techniques commonly used in control theory. Also before having much hope of representing engineering problems as LMIs by automatic methods one needs a good idea of which problems can and cannot be represented by LMIs. Little is currently known about such problems. In this article we give a necessary condition, we call "rigid convexity", which must hold for a set \$\{{\textbackslash}cC\} {\textbackslash}in \{{\textbackslash}mathbb R\}{\textasciicircum}m\$ in order for \$\{{\textbackslash}cC\}\$ to have an LMI representation. Rigid convexity is proved to be necessary and sufficient when \$m=2\$. This settles a question formally stated by Pablo Parrilo and Berndt Sturmfels in [PSprep].},
  archiveprefix = {arXiv}
}

@article{henrionConstraintQualifications1992,
  title = {On Constraint Qualifications},
  author = {Henrion, R.},
  year = {1992},
  month = jan,
  journal = {Journal of Optimization Theory and Applications},
  volume = {72},
  number = {1},
  pages = {187--197},
  issn = {1573-2878},
  doi = {10.1007/BF00939955},
  url = {https://doi.org/10.1007/BF00939955},
  urldate = {2022-03-25},
  abstract = {The linear independence constraint qualification (LICQ) and the weaker Mangasarian-Fromovitz constraint qualification (MFCQ) are well-known concepts in nonlinear optimization. A theorem is proved suggesting that the set of feasible points for which MFCQ essentially differs from LICQ is small in a specified sense. As an auxiliary result, it is shown that, under MFCQ, the constraint set (even in semi-infinite optimization) is locally representable in epigraph form.},
  langid = {english}
}

@article{henrionGloptiPolyMomentsOptimization2009,
  title = {{{GloptiPoly}} 3: Moments, Optimization and Semidefinite Programming},
  shorttitle = {{{GloptiPoly}} 3},
  author = {Henrion, Didier and Lasserre, Jean-Bernard and L{\"o}fberg, Johan},
  year = {2009},
  month = oct,
  journal = {Optimization Methods and Software},
  volume = {24},
  number = {4-5},
  pages = {761--779},
  issn = {1055-6788},
  doi = {10.1080/10556780802699201},
  url = {https://doi.org/10.1080/10556780802699201},
  urldate = {2018-04-09},
  abstract = {We describe a major update of our Matlab freeware GloptiPoly for parsing generalized problems of moments and solving them numerically with semidefinite programming.}
}

@article{henrionLinearConicOptimization2014,
  title = {Linear Conic Optimization for Nonlinear Optimal Control},
  author = {Henrion, Didier and Pauwels, Edouard},
  year = {2014},
  month = jul,
  journal = {arXiv:1407.1650 [math]},
  eprint = {1407.1650},
  primaryclass = {math},
  url = {http://arxiv.org/abs/1407.1650},
  urldate = {2022-02-02},
  abstract = {Infinite-dimensional linear conic formulations are described for nonlinear optimal control problems. The primal linear problem consists of finding occupation measures supported on optimal relaxed controlled trajectories, whereas the dual linear problem consists of finding the largest lower bound on the value function of the optimal control problem. Various approximation results relating the original optimal control problem and its linear conic formulations are developed. As illustrated by a couple of simple examples, these results are relevant in the context of finite-dimensional semidefinite programming relaxations used to approximate numerically the solutions of the infinite-dimensional linear conic problems.},
  archiveprefix = {arXiv}
}

@article{henrionLinearMatrixInequalities2002,
  title = {Linear Matrix Inequalities for Robust Strictly Positive Real Design},
  author = {Henrion, D.},
  year = {2002},
  journal = {Circuits and Systems I: Fundamental Theory and Applications, IEEE Transactions on},
  volume = {49},
  number = {7},
  pages = {1017--1020},
  issn = {1057-7122},
  doi = {10.1109/TCSI.2002.800838},
  abstract = {A necessary and sufficient condition is proposed for the existence of a polynomial p(s) such that the rational function p(s)/q(s) is robustly strictly positive real when q(s) is a given Hurwitz polynomial with polytopic uncertainty. It turns out that the whole set of candidates p(s) is a convex subset of the cone of positive semidefinite matrices, resulting in a straightforward strictly positive real design algorithm based on linear matrix inequalities}
}

@phdthesis{henrionMeasuresLinearMatrix2012,
  type = {Czech Professorship Inaugural Lecture Manuscript},
  title = {Measures and Linear Matrix Inequalities in Polynomial Optimal Control},
  author = {Henrion, Didier},
  year = {2012},
  address = {Prague},
  url = {http://homepages.laas.fr/henrion/Papers/henrionprof.pdf},
  urldate = {2014-04-18},
  school = {Czech Technical University in Prague}
}

@book{henrionMomentSOSHierarchy2020,
  title = {The {{Moment-SOS Hierarchy}}},
  author = {Henrion, Didier and Korda, Milan and Lasserre, Jean Bernard},
  year = {2020},
  month = nov,
  series = {Series on {{Optimization}} and {{Its Applications}}},
  volume = {4},
  publisher = {World Scientific},
  url = {https://doi.org/10.1142/q0252},
  isbn = {978-1-78634-853-1}
}

@book{henrionPositivePolynomialsControl2005a,
  title = {Positive {{Polynomials}} in {{Control}}},
  editor = {Henrion, Didier and Garulli, Andrea},
  year = {2005},
  month = jan,
  edition = {2005th edition},
  publisher = {Springer},
  address = {Berlin ; New York, N.Y},
  isbn = {978-3-540-23948-2},
  langid = {english}
}

@inproceedings{hercegMultiParametricToolbox2013a,
  title = {Multi-{{Parametric Toolbox}} 3.0},
  booktitle = {2013 {{European Control Conference}} ({{ECC}})},
  author = {Herceg, Martin and Kvasnica, Michal and Jones, Colin N. and Morari, Manfred},
  year = {2013},
  month = jul,
  pages = {502--510},
  doi = {10.23919/ECC.2013.6669862},
  abstract = {The Multi-Parametric Toolbox is a collection of algorithms for modeling, control, analysis, and deployment of constrained optimal controllers developed under Matlab. It features a powerful geometric library that extends the application of the toolbox beyond optimal control to various problems arising in computational geometry. The new version 3.0 is a complete rewrite of the original toolbox with a more flexible structure that offers faster integration of new algorithms. The numerical side of the toolbox has been improved by adding interfaces to state of the art solvers and by incorporation of a new parametric solver that relies on solving linear-complementarity problems. The toolbox provides algorithms for design and implementation of real-time model predictive controllers that have been extensively tested.}
}

@inproceedings{hijaziGravityMathematicalModeling2018,
  title = {Gravity: {{A Mathematical Modeling Language}} for {{Optimization}} and {{Machine Learning}}},
  shorttitle = {Gravity},
  booktitle = {{{NIPS}} 2018 {{Workshop MLOSS}}},
  author = {Hijazi, Hassan and Wang, Guanglei and Coffrin, Carleton},
  year = {2018},
  month = oct,
  url = {https://github.com/coin-or/Gravity},
  urldate = {2022-08-26},
  abstract = {Gravity is an open source, scalable, memory efficient modeling language for solving mathematical models in Optimization and Machine Learning. Gravity exploits structure to reduce function...},
  langid = {english}
}

@book{hiriart-urrutyFundamentalsConvexAnalysis2001,
  title = {Fundamentals of {{Convex Analysis}}},
  author = {{Hiriart-Urruty}, Jean-Baptiste and Lemar{\'e}chal, Claude},
  year = {2004},
  series = {Grundlehren {{Text Editions}}},
  edition = {1st, corrected 2nd printing},
  publisher = {Springer},
  address = {Berlin; Heidelberg},
  url = {https://doi.org/10.1007/978-3-642-56468-0},
  abstract = {This book is an abridged version of our two-volume opus Convex Analysis and Minimization Algorithms [18], about which we have received very positive feedback from users, readers, lecturers ever since it was published - by Springer-Verlag in 1993. Its pedagogical qualities were particularly appreciated, in the combination with a rather advanced technical material. Now [18] hasa dual but clearly defined nature: - an introduction to the basic concepts in convex analysis, - a study of convex minimization problems (with an emphasis on numerical al- rithms), and insists on their mutual interpenetration. It is our feeling that the above basic introduction is much needed in the scientific community. This is the motivation for the present edition, our intention being to create a tool useful to teach convex anal\- ysis. We have thus extracted from [18] its "backbone" devoted to convex analysis, namely ChapsIII-VI and X. Apart from some local improvements, the present text is mostly a copy of the corresponding chapters. The main difference is that we have deleted material deemed too advanced for an introduction, or too closely attached to numerical algorithms. Further, we have included exercises, whose degree of difficulty is suggested by 0, I or 2 stars *. Finally, the index has been considerably enriched. Just as in [18], each chapter is presented as a "lesson", in the sense of our old masters, treating of a given subject in its entirety.},
  isbn = {978-3-540-42205-1},
  langid = {english}
}

@article{hodgeOptimizationConditionalValueatrisk2000,
  title = {Optimization of Conditional Value-at-Risk},
  author = {Hodge, Miriam},
  year = {2000},
  month = mar,
  journal = {Journal of Risk},
  url = {https://www.risk.net/node/2161159},
  urldate = {2021-09-03},
  langid = {english}
}

@article{horstNoteFunctionsWhose1982,
  title = {A Note on Functions Whose Local Minima Are Global},
  author = {Horst, R.},
  year = {1982},
  month = mar,
  journal = {Journal of Optimization Theory and Applications},
  volume = {36},
  number = {3},
  pages = {457--463},
  issn = {1573-2878},
  doi = {10.1007/BF00934358},
  url = {https://doi.org/10.1007/BF00934358},
  urldate = {2018-12-18},
  abstract = {In this note, we introduce a new class of generalized convex functions and show that a real functionf which is continuous on a compact convex subsetM of {$\mathbb{R}$} n and whose set of global minimizers onM is arcwise-connected has the property that every local minimum is global if, and only if,f belongs to that class of functions.},
  langid = {english}
}

@article{huchetteNonconvexPiecewiseLinear2022,
  title = {Nonconvex {{Piecewise Linear Functions}}: {{Advanced Formulations}} and {{Simple Modeling Tools}}},
  shorttitle = {Nonconvex {{Piecewise Linear Functions}}},
  author = {Huchette, Joey and Vielma, Juan Pablo},
  year = {2022},
  month = may,
  journal = {Operations Research},
  publisher = {INFORMS},
  issn = {0030-364X},
  doi = {10.1287/opre.2019.1973},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/opre.2019.1973},
  urldate = {2022-08-02},
  abstract = {We present novel mixed-integer programming (MIP) formulations for optimization over nonconvex piecewise linear functions. We exploit recent advances in the systematic construction of MIP formulations to derive new formulations for univariate functions using a geometric approach and for bivariate functions using a combinatorial approach. All formulations are strong, small (so-called logarithmic formulations), and have other desirable computational properties. We present extensive experiments in which they exhibit substantial computational performance improvements over existing approaches. To accompany these advanced formulations, we present PiecewiseLinearOpt, an extension of the JuMP modeling language in Julia that implements our models (alongside other formulations from the literature) through a high-level interface, hiding the complexity of the formulations from the end user.}
}

@book{hurlimannMathematicalModelingBasics2024,
  title = {Mathematical {{Modeling Basics}}},
  author = {H{\"u}rlimann, Tony},
  year = {2024},
  month = oct,
  url = {https://matmod.ch/lpl/doc/modelbook.pdf},
  langid = {english}
}

@article{chandrasekaranRelativeEntropyOptimization2017,
  title = {Relative Entropy Optimization and Its Applications},
  author = {Chandrasekaran, Venkat and Shah, Parikshit},
  year = {2017},
  month = jan,
  journal = {Mathematical Programming},
  volume = {161},
  number = {1},
  pages = {1--32},
  issn = {1436-4646},
  doi = {10.1007/s10107-016-0998-2},
  url = {https://doi.org/10.1007/s10107-016-0998-2},
  urldate = {2024-08-16},
  abstract = {In this expository article, we study optimization problems specified via linear and relative entropy inequalities. Such relative entropy programs (REPs) are convex optimization problems as the relative entropy function is jointly convex with respect to both its arguments. Prominent families of convex programs such as geometric programs (GPs), second-order cone programs, and entropy maximization problems are special cases of REPs, although REPs are more general than these classes of problems. We provide solutions based on REPs to a range of problems such as permanent maximization, robust optimization formulations of GPs, and hitting-time estimation in dynamical systems. We survey previous approaches to some of these problems and the limitations of those methods, and we highlight the more powerful generalizations afforded by REPs. We conclude with a discussion of quantum analogs of the relative entropy function, including a review of the similarities and distinctions with respect to the classical case. We also describe a stylized application of quantum relative entropy optimization that exploits the joint convexity of the quantum relative entropy function.},
  langid = {english}
}

@article{changDistributedConstrainedOptimization2013,
  title = {Distributed {{Constrained Optimization}} by {{Consensus-Based Primal-Dual Perturbation Method}}},
  author = {Chang, Tsung-Hui and Nedi{\'c}, Angelia and Scaglione, Anna},
  year = {2013},
  month = apr,
  journal = {arXiv:1304.5590 [cs, math]},
  eprint = {1304.5590},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/1304.5590},
  urldate = {2014-01-13},
  abstract = {Various distributed optimization methods have been developed for solving problems which have simple local constraint sets and whose objective function is the sum of local cost functions of distributed agents in a network. Motivated by emerging applications in smart grid and distributed sparse regression, this paper studies distributed optimization methods for solving general problems which have a coupled global cost function and have inequality constraints. We consider a network scenario where each agent has no global knowledge and can access only its local mapping and constraint functions. To solve this problem in a distributed manner, we propose a consensus-based distributed primal-dual perturbation (PDP) algorithm. In the algorithm, agents employ the average consensus technique to estimate the global cost and constraint functions via exchanging messages with neighbors, and meanwhile use a local primal-dual perturbed subgradient method to approach a global optimum. The proposed PDP method not only can handle smooth inequality constraints but also non-smooth constraints such as some sparsity promoting constraints arising in sparse optimization. We prove that the proposed PDP algorithm converges to an optimal primal-dual solution of the original problem, under standard problem and network assumptions. Numerical results illustrating the performance of the proposed algorithm for a distributed demand response control problem in smart grid are also presented.},
  archiveprefix = {arXiv}
}

@book{chenAppliedIntegerProgramming2010,
  title = {Applied {{Integer Programming}}: {{Modeling}} and {{Solution}}},
  shorttitle = {Applied {{Integer Programming}}},
  author = {Chen, Der-San and Batson, Robert G. and Dang, Yu},
  year = {2010},
  month = jan,
  publisher = {Wiley},
  address = {Hoboken, N.J},
  abstract = {An accessible treatment of the modeling and solution of integer programming problems, featuring modern applications and software In order to fully comprehend the algorithms associated with integer programming, it is important to understand not only how algorithms work, but also why they work. Applied Integer Programming features a unique emphasis on this point, focusing on problem modeling and solution using commercial software. Taking an application-oriented approach, this book addresses the art and science of mathematical modeling related to the mixed integer programming (MIP) framework and discusses the algorithms and associated practices that enable those models to be solved most efficiently. The book begins with coverage of successful applications, systematic modeling procedures, typical model types, transformation of non-MIP models, combinatorial optimization problem models, and automatic preprocessing to obtain a better formulation. Subsequent chapters present algebraic and geometric basic concepts of linear programming theory and network flows needed for understanding integer programming. Finally, the book concludes with classical and modern solution approaches as well as the key components for building an integrated software system capable of solving large-scale integer programming and combinatorial optimization problems. Throughout the book, the authors demonstrate essential concepts through numerous examples and figures. Each new concept or algorithm is accompanied by a numerical example, and, where applicable, graphics are used to draw together diverse problems or approaches into a unified whole. In addition, features of solution approaches found in today's commercial software are identified throughout the book. Thoroughly classroom-tested, Applied Integer Programming is an excellent book for integer programming courses at the upper-undergraduate and graduate levels. It also serves as a well-organized reference for professionals, software developers, and analysts who work in the fields of applied mathematics, computer science, operations research, management science, and engineering and use integer-programming techniques to model and solve real-world optimization problems.},
  isbn = {978-0-470-37306-4},
  langid = {english}
}

@inproceedings{chenConstrainedAttitudeControl2017,
  title = {Constrained Attitude Control, {{PSD}} Lifts, and Semidefinite Programming},
  booktitle = {2017 {{IEEE}} 56th {{Annual Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Chen, Dian-Jing and Mesbahi, Mehran},
  year = {2017},
  month = dec,
  pages = {2134--2139},
  doi = {10.1109/CDC.2017.8263962},
  abstract = {This paper studies theoretical properties of the positive semidefinite cone lift (PSD lift) and utilizes the PSD lift to solve the constrained attitude control problem for a rigid body. We first introduce the PSD lift and the mapping between the PSD lift to the convex hull of the rotation matrices conv SO(3), commonly used in attitude estimation and control, as well as in areas such as computer vision. We next study conditions under which this mapping defines an affine isomorphism. We focus particularly on the set of extreme points of the PSD lift and show that this set is in fact a group; connections with SO(3) and unit quaternions H are also explored. Moreover, the differentiation operation on this group, which is related to the angular velocity of the rigid body, will be discussed. Lastly, we apply the PSD lift in the constrained attitude control problem and approach it via a semidefinite program (SDP). Furthermore, we provide insights into the physical realization and the optimization results through an example.}
}

@article{chiangGeometricProgrammingCommunication2005,
  title = {Geometric {{Programming}} for {{Communication Systems}}},
  author = {Chiang, Mung},
  year = {2005},
  month = jul,
  journal = {Foundations and Trends{\textregistered} in Communications and Information Theory},
  volume = {2},
  number = {1--2},
  pages = {1--154},
  publisher = {Now Publishers, Inc.},
  issn = {1567-2190, 1567-2328},
  doi = {10.1561/0100000005},
  url = {https://www.nowpublishers.com/article/Details/CIT-005},
  urldate = {2022-02-14},
  abstract = {Geometric Programming for Communication Systems},
  langid = {english}
}

@article{chinchuluunSurveyRecentDevelopments2007,
  title = {A Survey of Recent Developments in Multiobjective Optimization},
  author = {Chinchuluun, Altannar and Pardalos, Panos M.},
  year = {2007},
  month = oct,
  journal = {Annals of Operations Research},
  volume = {154},
  number = {1},
  pages = {29--50},
  issn = {1572-9338},
  doi = {10.1007/s10479-007-0186-0},
  url = {https://doi.org/10.1007/s10479-007-0186-0},
  urldate = {2023-07-26},
  abstract = {Multiobjective Optimization (MO) has many applications in such fields as the Internet, finance, biomedicine, management science, game theory and engineering. However, solving MO problems is not an easy task. Searching for all Pareto optimal solutions is expensive and a time consuming process because there are usually exponentially large (or infinite) Pareto optimal solutions. Even for simple problems determining whether a point belongs to the Pareto set is \${\textbackslash}mathcal\{NP\}\$-hard. In this paper, we discuss recent developments in MO. These include optimality conditions, applications, global optimization techniques, the new concept of epsilon Pareto optimal solution, and heuristics.},
  langid = {english}
}

@article{chuaSectionwisePiecewiselinearFunctions1977,
  title = {Section-Wise Piecewise-Linear Functions: {{Canonical}} Representation, Properties, and Applications},
  shorttitle = {Section-Wise Piecewise-Linear Functions},
  author = {Chua, L.O. and Kang, Sung Mo},
  year = {1977},
  month = jun,
  journal = {Proceedings of the IEEE},
  volume = {65},
  number = {6},
  pages = {915--929},
  issn = {1558-2256},
  doi = {10.1109/PROC.1977.10589},
  abstract = {This paper presents a new closed form analytical formula for representing n-dimensional surfaces and scalar functions of n variables which are piecewise-linear over each cross section obtained by freezing any combination of n - 1 of the n coordinates. This new section-wise piecewise-linear representation can be easily programmed with efficient computer storage. It is a global representation in the sense that a single formula is used to compute for f(x1,x2,...,xn) for all values of (x1, x2,...., xn). Since this representation is expressed in closed analytic form, it allows standard mathematical operations and manipulations to be carried out in theorectical studies, In particular, it led to the possibility of deriving explicit closed form expressions for system parameters and design formulas. Examples are given which illustrate the potential applications of this representation in the modeling and analysis of nonlinear devices, circuits and systems.}
}

@article{johanssonDecentralizedNegotiationOptimal2008,
  title = {On Decentralized Negotiation of Optimal Consensus},
  author = {Johansson, Bj{\"o}rn and Speranzon, Alberto and Johansson, Mikael and Johansson, Karl Henrik},
  year = {2008},
  month = apr,
  journal = {Automatica},
  volume = {44},
  number = {4},
  pages = {1175--1179},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2007.09.003},
  url = {http://www.sciencedirect.com/science/article/pii/S0005109807003962},
  urldate = {2014-01-13},
  abstract = {A consensus problem consists of finding a distributed control strategy that brings the state or output of a group of agents to a common value, a consensus point. In this paper, we propose a negotiation algorithm that computes an optimal consensus point for agents modeled as linear control systems subject to convex input constraints and linear state constraints. By primal decomposition and incremental subgradient methods, it is shown that the algorithm can be implemented such that each agent exchanges only a small amount of information per iteration with its neighbors.}
}

@inproceedings{johanssonSubgradientMethodsConsensus2008,
  title = {Subgradient Methods and Consensus Algorithms for Solving Convex Optimization Problems},
  booktitle = {47th {{IEEE Conference}} on {{Decision}} and {{Control}}, 2008. {{CDC}} 2008},
  author = {Johansson, B. and Keviczky, T. and Johansson, M. and Johansson, K.H.},
  year = {2008},
  pages = {4185--4190},
  doi = {10.1109/CDC.2008.4739339},
  abstract = {In this paper we propose a subgradient method for solving coupled optimization problems in a distributed way given restrictions on the communication topology. The iterative procedure maintains local variables at each node and relies on local subgradient updates in combination with a consensus process. The local subgradient steps are applied simultaneously as opposed to the standard sequential or cyclic procedure. We study convergence properties of the proposed scheme using results from consensus theory and approximate subgradient methods. The framework is illustrated on an optimal distributed finite-time rendezvous problem.}
}

@unpublished{johnsonQuasiNewtonOptimizationOrigin2021,
  type = {Notes for 18335 at {{MIT}}},
  title = {Quasi-{{Newton}} Optimization: {{Origin}} of the {{BFGS}} Update},
  author = {Johnson, Steven G.},
  year = {2021},
  month = may,
  url = {https://github.com/mitmath/18335/blob/spring21/notes/BFGS.pdf},
  urldate = {2024-02-09},
  langid = {english}
}

@book{jonesPracticalGoalProgramming2010,
  title = {Practical {{Goal Programming}}},
  author = {Jones, Dylan and Tamiz, Mehrdad},
  year = {2010},
  month = mar,
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  number = {141},
  publisher = {Springer},
  address = {New York, NY},
  url = {https://doi.org/10.1007/978-1-4419-5771-9},
  abstract = {Practical Goal Programming is intended to allow academics and practitioners to be able to build effective goal programming models, ~to detail the current state of the art, and to lay the foundation for its future development and continued application to new and varied fields.~ Suitable as both a text and reference, its nine chapters first provide a brief history, fundamental definitions, and underlying philosophies, and then detail the goal programming variants and define them algebraically.~ Chapter 3 details the step-by-step formulation of the basic goal programming model, and Chapter 4 explores more advanced modeling issues and highlights some recently proposed extensions.Chapter 5 then details the solution methodologies of goal programming, concentrating on computerized solution by the Excel Solver and LINGO packages for each of the three main variants, and ~includes a discussion of the viability of the use of specialized goal programming packages.~ Chapter 6 discusses the linkages between Pareto Efficiency and goal programming.~ Chapters 3 to 6 are supported by a set of ten exercises, and an Excel spreadsheet giving the basic solution of each example is available at an accompanying website.Chapter 7 details the current state of the art in terms of the integration of goal programming with other techniques, and the text concludes with two case studies which were chosen to demonstrate the application of goal programming in practice and to illustrate the principles developed in Chapters 1 to 7. ~Chapter 8 details an application in healthcare, and Chapter 9 describes applications in portfolio selection.},
  isbn = {978-1-4419-5770-2},
  langid = {english}
}

@article{julianCompleteCanonicalPiecewiselinear2003,
  title = {The Complete Canonical Piecewise-Linear Representation: Functional Form for Minimal Degenerate Intersections},
  shorttitle = {The Complete Canonical Piecewise-Linear Representation},
  author = {Julian, P.},
  year = {2003},
  month = mar,
  journal = {IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
  volume = {50},
  number = {3},
  pages = {387--396},
  issn = {1558-1268},
  doi = {10.1109/TCSI.2003.808904},
  url = {https://ieeexplore.ieee.org/document/1193020},
  urldate = {2023-11-16},
  abstract = {In this paper, we develop a functional form for canonical piecewise-linear (CPWL) functions defined over a minimal degenerate intersection of order k. This type of intersection is known to be the building block of CPWL representations. Accordingly, the representation proposed is the main component in a general CPWL representation and constitutes the first functional form that is valid in a domain that has arbitrary dimension and is subdivided by a generic partition.}
}

@article{julianHighlevelCanonicalPiecewise1999,
  title = {High-Level Canonical Piecewise Linear Representation Using a Simplicial Partition},
  author = {Julian, P. and Desages, A. and Agamennoni, O.},
  year = {1999},
  month = apr,
  journal = {IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
  volume = {46},
  number = {4},
  pages = {463--480},
  issn = {1558-1268},
  doi = {10.1109/81.754847},
  abstract = {In this work, we propose a set of high-level canonical piecewise linear (HL-CPWL) functions to form a representation basis for the set of piecewise linear functions f: D/spl rarr/R/sup 1/ defined over a simplicial partition of a rectangular compact set D in R/sup n/. In consequence, the representation proposed uses the minimum number of parameters. The basis functions are obtained recursively by multiple compositions of a unique generating function /spl gamma/, resulting in several types of nested absolute-value functions. It is shown that the representation in a domain in R/sup n/ requires functions up to nesting level n. As a consequence of the choice of the basis functions, an efficient numerical method for the resolution of the parameters of the high-level (HL) canonical representation results. Finally, an application to the approximation of continuous functions is shown.}
}

@misc{JuMP2024,
  title = {{{JuMP}}},
  year = {2024},
  month = nov,
  url = {https://jump.dev/JuMP.jl/stable/JuMP.pdf},
  urldate = {2024-12-25},
  howpublished = {JuMP-dev}
}

@article{kahlertGeneralizedCanonicalPiecewiselinear1990,
  title = {A Generalized Canonical Piecewise-Linear Representation},
  author = {Kahlert, C. and Chua, L.O.},
  year = {1990},
  month = mar,
  journal = {IEEE Transactions on Circuits and Systems},
  volume = {37},
  number = {3},
  pages = {373--383},
  issn = {1558-1276},
  doi = {10.1109/31.52731},
  abstract = {An extension of the well-known canonical representation for continuous piecewise-linear functions is introduced. This form is capable of describing all piecewise-linear functions in two dimensions, being no longer subject to any restrictions. Moreover, it is shown that just one nesting of absolute-value functions is sufficient to cover the whole class.{$<>$}}
}

@book{kaliszewskiMultipleCriteriaDecision2016,
  title = {Multiple {{Criteria Decision Making}} by {{Multiobjective Optimization}}: {{A Toolbox}}},
  shorttitle = {Multiple {{Criteria Decision Making}} by {{Multiobjective Optimization}}},
  author = {Kaliszewski, Ignacy and Miroforidis, Janusz and Podkopaev, Dmitry},
  year = {2016},
  month = aug,
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  publisher = {Springer},
  address = {Cham},
  url = {https://doi.org/10.1007/978-3-319-32756-3},
  abstract = {This textbook approaches optimization from a multi-aspect, multi-criteria perspective. By using a Multiple Criteria Decision Making (MCDM) approach, it avoids the limits and oversimplifications that can come with optimization models with one criterion. The book is presented in a concise form, addressing how to solve decision problems in sequences of intelligence, modelling, choice and review phases, often iterated, to identify the most preferred decision variant. The approach taken is human-centric, with the user taking the final decision is a sole and sovereign actor in the decision making process. To ensure generality, no assumption about the Decision Maker preferences or behavior is made. The presentation of these concepts is illustrated by numerous examples, figures, and problems to be solved with the help of downloadable spreadsheets. This electronic companion contains models of problems to be solved built in Excel spreadsheet files. Optimization models are too often oversimplifications of decision problems met in practice. For instance, modeling company performance by an optimization model in which the criterion function is short-term profit to be maximized, does not fully reflect the essence of business management. The company's managing staff is accountable not only for operational decisions, but also for actions which shall result in the company ability to generate a decent profit in the future. This calls for management decisions and actions which ensure short-term profitability, but also maintaining long-term relations with clients, introducing innovative products, financing long-term investments, etc. Each of those additional, though indispensable actions and their effects can be modeled separately, case by case, by an optimization model with a criterion function adequately selected. However, in each case the same set of constraints represents the range of company admissible actions. The aim and the scope of this textbook is to present methodologies and methods enabling modeling of such actions jointly.},
  langid = {english}
}

@article{kevenaarExtensionsChuaExplicit1994,
  title = {Extensions to {{Chua}}'s Explicit Piecewise-Linear Function Descriptions},
  author = {Kevenaar, T.A.M. and Leenaerts, D.M.W. and {van Bokhoven}, W.M.G.},
  year = {1994},
  month = apr,
  journal = {IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
  volume = {41},
  number = {4},
  pages = {308--314},
  issn = {1558-1268},
  doi = {10.1109/81.285686},
  abstract = {In this paper, we extend the class of explicit piecewise linear descriptions in the modulus form that was recently introduced by Chua cum suis. This is achieved by transforming a special form of a more general implicit description given by Van Bokhoven (1981) using the modulus transformation. This results in a set of base functions that can be used to compose the more general explicit description. It is shown that with this set all previously presented explicit PL descriptions can be covered. Furthermore, we pose some problems that occur when trying to construct an even more general description.{$<>$}}
}

@book{khamisOptimizationAlgorithmsAI2023,
  title = {Optimization {{Algorithms}}: {{AI}} Techniques for Design, Planning, and Control Problems},
  author = {Khamis, Alaa},
  year = {2023},
  edition = {Manning Early Access Program (MEAP)},
  publisher = {Manning},
  url = {https://www.manning.com/books/optimization-algorithms},
  urldate = {2023-10-22},
  abstract = {Solve design, planning, and control problems using modern machine learning and AI techniques. In Optimization Algorithms: AI techniques for design, planning, and control problems you will learn: Machine learning methods for search and optimization problems{$<$}/li{$>$} The core concepts of search and optimization{$<$}/li{$>$} Deterministic and stochastic optimization techniques{$<$}/li{$>$} Graph search algorithms{$<$}/li{$>$} Nature-inspired search and optimization algorithms{$<$}/li{$>$} Efficient trade-offs between search space exploration and exploitation{$<$}/li{$>$} State-of-the-art Python libraries for search and optimization{$<$}/li{$>$} {$<$}/ul{$>$} Optimization problems are everywhere in daily life. What's the fastest route from one place to another? How do you calculate the optimal price for a product? How should you plant crops, allocate resources, and schedule surgeries? Optimization Algorithms introduces the AI algorithms that can solve these complex and poorly-structured problems. Inside you'll find a wide range of optimization methods, from deterministic and stochastic derivative-free optimization to nature-inspired search algorithms and machine learning methods. Don't worry---there's no complex mathematical notation. You'll learn through in-depth case studies that cut through academic complexity to demonstrate how each algorithm works in the real world.},
  isbn = {978-1-63343-883-5},
  langid = {english}
}

@book{khanSetvaluedOptimizationIntroduction2014,
  title = {Set-Valued {{Optimization}}: {{An Introduction}} with {{Applications}}},
  shorttitle = {Set-Valued {{Optimization}}},
  author = {Khan, Akhtar A. and Tammer, Christiane and Z{\u a}linescu, Constantin},
  year = {28 {\v r}{\'i}jna 2014},
  series = {Vector {{Optimization}}},
  publisher = {Springer},
  address = {Berlin; Heidelberg},
  url = {https://doi.org/10.1007/978-3-642-54265-7},
  isbn = {978-3-642-54264-0}
}

@misc{kiesslingAlmostFeasibleSequential2024,
  title = {An {{Almost Feasible Sequential Linear Programming Algorithm}}},
  author = {Kiessling, David and Vanaret, Charlie and Astudillo, Alejandro and Decre, Wilm and Swevers, Jan},
  year = {2024},
  month = jan,
  number = {arXiv:2401.13840},
  eprint = {2401.13840},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.13840},
  url = {http://arxiv.org/abs/2401.13840},
  urldate = {2024-06-28},
  abstract = {This paper proposes an almost feasible Sequential Linear Programming (afSLP) algorithm. In the first part, the practical limitations of previously proposed Feasible Sequential Linear Programming (FSLP) methods are discussed along with illustrative examples. Then, we present a generalization of FSLP based on a tolerance-tube method that addresses the shortcomings of FSLP. The proposed algorithm afSLP consists of two phases. Phase I starts from random infeasible points and iterates towards a relaxation of the feasible set. Once the tolerance-tube around the feasible set is reached, phase II is started and all future iterates are kept within the tolerance-tube. The novel method includes enhancements to the originally proposed tolerance-tube method that are necessary for global convergence. afSLP is shown to outperform FSLP and the state-of-the-art solver IPOPT on a SCARA robot optimization problem.},
  archiveprefix = {arXiv}
}

@book{kinderlehrerIntroductionVariationalInequalities2000,
  title = {An {{Introduction}} to {{Variational Inequalities}} and {{Their Applications}}},
  author = {Kinderlehrer, David and Stampacchia, Guido},
  year = {2000},
  series = {Classics in {{Applied Mathematics}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia, PA},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9780898719451},
  abstract = {This unabridged republication of the 1980 text, an established classic in the field, is a resource for many important topics in elliptic equations and systems and is the first modern treatment of free boundary problems. Variational inequalities (equilibrium or evolution problems typically with convex constraints) are carefully explained in An Introduction to Variational Inequalities and Their Applications. They are shown to be extremely useful across a wide variety of subjects, ranging from linear programming to free boundary problems in partial differential equations. Exciting new areas like finance and phase transformations along with more historical ones like contact problems have begun to rely on variational inequalities, making this book a necessity once again.},
  googlebooks = {B1cPRJ3qiw0C},
  isbn = {978-0-89871-466-1},
  langid = {english}
}

@book{kingModelingStochasticProgramming2012,
  title = {Modeling with {{Stochastic Programming}}},
  author = {King, Alan J. and Wallace, Stein W.},
  year = {2012},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  publisher = {Springer},
  address = {New York, NY},
  url = {https://doi.org/10.1007/978-0-387-87817-1},
  abstract = {While there are several texts on how to solve and analyze stochastic programs, this is the first text to address basic questions about how to model uncertainty, and how to reformulate a deterministic model so that it can be analyzed in a stochastic setting. This text would be suitable as a stand-alone or supplement for a second course in OR/MS or in optimization-oriented engineering disciplines where the instructor wants to explain where models come from and what the fundamental issues are. The book is easy-to-read, highly illustrated with lots of examples and discussions. It will be suitable for graduate students and researchers working in operations research, mathematics, engineering and related departments where there is interest in learning how to model uncertainty. Alan King is a Research Staff Member at IBM's Thomas J. Watson Research Center in New York. Stein W. Wallace is a Professor of Operational Research at Lancaster University Management School in England.},
  isbn = {978-0-387-87816-4}
}

@article{klotzPracticalGuidelinesSolving2013,
  title = {Practical Guidelines for Solving Difficult Mixed Integer Linear Programs},
  author = {Klotz, Ed and Newman, Alexandra M.},
  year = {2013},
  month = oct,
  journal = {Surveys in Operations Research and Management Science},
  volume = {18},
  number = {1},
  pages = {18--32},
  issn = {1876-7354},
  doi = {10.1016/j.sorms.2012.12.001},
  url = {https://www.sciencedirect.com/science/article/pii/S1876735413000020},
  urldate = {2021-11-26},
  abstract = {Even with state-of-the-art hardware and software, mixed integer programs can require hours, or even days, of run time and are not guaranteed to yield an optimal (or near-optimal, or any!) solution. In this paper, we present suggestions for appropriate use of state-of-the-art optimizers and guidelines for careful formulation, both of which can vastly improve performance.},
  langid = {english}
}

@article{klotzPracticalGuidelinesSolving2013a,
  title = {Practical Guidelines for Solving Difficult Linear Programs},
  author = {Klotz, Ed and Newman, Alexandra M.},
  year = {2013},
  month = oct,
  journal = {Surveys in Operations Research and Management Science},
  volume = {18},
  number = {1},
  pages = {1--17},
  issn = {1876-7354},
  doi = {10.1016/j.sorms.2012.11.001},
  url = {https://www.sciencedirect.com/science/article/pii/S1876735412000189},
  urldate = {2025-01-24},
  abstract = {The advances in state-of-the-art hardware and software have enabled the inexpensive, efficient solution of many large-scale linear programs previously considered intractable. However, a significant number of large linear programs can require hours, or even days, of run time and are not guaranteed to yield an optimal (or near-optimal) solution. In this paper, we present suggestions for diagnosing and removing performance problems in state-of-the-art linear programming solvers, and guidelines for careful model formulation, both of which can vastly improve performance.}
}

@book{kochenderferAlgorithmsOptimization2019,
  title = {Algorithms for {{Optimization}}},
  author = {Kochenderfer, Mykel J. and Wheeler, Tim A.},
  year = {2019},
  month = mar,
  publisher = {The MIT Press},
  url = {https://algorithmsbook.com/optimization/},
  urldate = {2020-12-29},
  abstract = {A comprehensive introduction to optimization with a focus on practical algorithms for the design of engineering systems.                 This book offers a comprehensive introduction to optimization with a focus on practical algorithms. The book approaches optimization from an engineering perspective, where the objective is to design a system that optimizes a set of metrics subject to constraints. Readers will learn about computational approaches for a range of challenges, including searching high-dimensional spaces, handling problems where there are multiple competing objectives, and accommodating uncertainty in the metrics. Figures, examples, and exercises convey the intuition behind the mathematical approaches. The text provides concrete implementations in the Julia programming language. Topics covered include derivatives and their generalization to multiple dimensions; local descent and first- and second-order methods that inform local descent; stochastic methods, which introduce randomness into the optimization process; linear constrained optimization, when both the objective function and the constraints are linear; surrogate models, probabilistic surrogate models, and using probabilistic surrogate models to guide optimization; optimization under uncertainty; uncertainty propagation; expression optimization; and multidisciplinary design optimization. Appendixes offer an introduction to the Julia language, test functions for evaluating algorithm performance, and mathematical concepts used in the derivation and analysis of the optimization methods discussed in the text. The book can be used by advanced undergraduates and graduate students in mathematics, statistics, computer science, any engineering field, (including electrical engineering and aerospace engineering), and operations research, and as a reference for professionals.},
  isbn = {978-0-262-03942-0},
  langid = {english}
}

@book{korteCombinatorialOptimizationTheory2018,
  title = {Combinatorial {{Optimization}}: {{Theory}} and {{Algorithms}}},
  shorttitle = {Combinatorial {{Optimization}}},
  author = {Korte, Bernhard and Vygen, Jens},
  year = {2018},
  month = mar,
  series = {Algorithms and {{Combinatorics}}},
  edition = {6},
  number = {21},
  publisher = {Springer},
  address = {New York, NY},
  url = {http://www.or.uni-bonn.de/~vygen/co.html},
  isbn = {978-3-662-56038-9},
  langid = {english}
}

@book{kozielSurrogateBasedModelingOptimization2013,
  title = {Surrogate-{{Based Modeling}} and {{Optimization}}: {{Applications}} in {{Engineering}}},
  shorttitle = {Surrogate-{{Based Modeling}} and {{Optimization}}},
  editor = {Koziel, Slawomir and Leifsson, Leifur},
  year = {2013},
  month = jun,
  publisher = {Springer},
  address = {New York},
  url = {https://doi.org/10.1007/978-1-4614-7551-4},
  abstract = {Contemporary engineering design is heavily based on computer simulations. Accurate, high-fidelity simulations are used not only for design verification but, even more importantly, to adjust parameters of the system to have it meet given performance requirements. Unfortunately, accurate simulations are often computationally very expensive with evaluation times as long as hours or even days per design, making design automation using conventional methods impractical. These and other problems can be alleviated by the development and employment of so-called surrogates that reliably represent the expensive, simulation-based model of the system or device of interest but they are much more reasonable and analytically tractable.~~This volume features surrogate-based modeling and optimization techniques, and their applications for solving difficult and computationally expensive engineering design problems. It begins by~presenting~the basic concepts and formulations of the surrogate-based modeling and optimization paradigm and then~discusses relevant modeling techniques, optimization algorithms and design procedures, as well as state-of-the-art developments. The chapters are self-contained with basic concepts and formulations along with applications and examples. The book will be useful to~researchers in engineering and mathematics, in particular those who employ computationally heavy simulations in their design work.},
  isbn = {978-1-4614-7550-7},
  langid = {english}
}

@book{kreinMarkovMomentProblem1977,
  title = {The {{Markov Moment Problem}} and {{Extremal Problems}}},
  author = {Krein, M. G. and Nudelman, A. A.},
  year = {1977},
  month = dec,
  publisher = {American Mathematical Society},
  address = {Providence, R.I},
  abstract = {In this book, an extensive circle of questions originating in the classical work of P. L. Chebyshev and A. A. Markov is considered from the more modern point of view. It is shown how results and methods of the generalized moment problem are interlaced with various questions of the geometry of convex bodies, algebra, and function theory. From this standpoint, the structure of convex and conical hulls of curves is studied in detail and isoperimetric inequalities for convex hulls are established; a theory of orthogonal and quasiorthogonal polynomials is constructed; problems on limiting values of integrals and on least deviating functions (in various metrics) are generalized and solved; problems in approximation theory and interpolation and extrapolation in various function classes (analytic, absolutely monotone, almost periodic, etc.) are solved, as well as certain problems in optimal control of linear objects.},
  isbn = {978-0-8218-4500-4},
  langid = {english}
}

@article{lasserreGlobalOptimizationPolynomials2001,
  title = {Global {{Optimization}} with {{Polynomials}} and the {{Problem}} of {{Moments}}},
  author = {Lasserre, J.},
  year = {2001},
  month = jan,
  journal = {SIAM Journal on Optimization},
  volume = {11},
  number = {3},
  pages = {796--817},
  issn = {1052-6234},
  doi = {10.1137/S1052623400366802},
  url = {http://epubs.siam.org/doi/abs/10.1137/S1052623400366802},
  urldate = {2014-07-01},
  abstract = {We consider the problem of finding the unconstrained global minimum of a real-valued polynomial p(x): \{{\textbackslash}mathbb\{R\}\}{\textasciicircum}n{\textbackslash}to \{{\textbackslash}mathbb\{R\}\}\$, as well as the global minimum of p(x), in a compact set K defined by polynomial inequalities. It is shown that this problem reduces to solving an (often finite) sequence of convex linear matrix inequality (LMI) problems. A notion of Karush--Kuhn--Tucker polynomials is introduced in a global optimality condition. Some illustrative examples are provided.,  We consider the problem of finding the unconstrained global minimum of a real-valued polynomial p(x): \{{\textbackslash}mathbb\{R\}\}{\textasciicircum}n{\textbackslash}to \{{\textbackslash}mathbb\{R\}\}\$, as well as the global minimum of p(x), in a compact set K defined by polynomial inequalities. It is shown that this problem reduces to solving an (often finite) sequence of convex linear matrix inequality (LMI) problems. A notion of Karush--Kuhn--Tucker polynomials is introduced in a global optimality condition. Some illustrative examples are provided.}
}

@book{lasserreIntroductionPolynomialSemiAlgebraic2015,
  title = {An {{Introduction}} to {{Polynomial}} and {{Semi-Algebraic Optimization}}},
  author = {Lasserre, Jean Bernard},
  year = {2015},
  month = feb,
  series = {Cambridge {{Texts}} in {{Applied Mathematics Book}}},
  number = {52},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  url = {https://www.cambridge.org/cz/academic/subjects/mathematics/optimization-or-and-risk-analysis/introduction-polynomial-and-semi-algebraic-optimization?format=AR&isbn=9781316236611},
  isbn = {978-1-107-06057-9},
  langid = {english}
}

@book{lasserreMomentsPositivePolynomials2009,
  title = {Moments, {{Positive Polynomials}} and {{Their Applications}}},
  author = {Lasserre, Jean Bernard},
  year = {2009},
  month = oct,
  series = {Series on {{Optimization}} and {{Its Applications}}},
  publisher = {Imperial College Press},
  address = {London : Signapore ; Hackensack, NJ},
  url = {https://www.worldscientific.com/worldscibooks/10.1142/p665},
  isbn = {978-1-84816-445-1},
  langid = {english}
}

@incollection{laurentSumsSquaresMoment2009,
  title = {Sums of {{Squares}}, {{Moment Matrices}} and {{Optimization Over Polynomials}}},
  booktitle = {Emerging {{Applications}} of {{Algebraic Geometry}}},
  author = {Laurent, Monique},
  editor = {Putinar, Mihai and Sullivant, Seth},
  year = {2009},
  month = jan,
  series = {The {{IMA Volumes}} in {{Mathematics}} and Its {{Applications}}},
  number = {149},
  pages = {157--270},
  publisher = {Springer New York},
  url = {http://link.springer.com/chapter/10.1007/978-0-387-09686-5_7},
  urldate = {2014-07-01},
  abstract = {We consider the problem of minimizing a polynomial over a semialgebraic set defined by polynomial equations and inequalities, which is NP-hard in general. Hierarchies of semidefinite relaxations have been proposed in the literature, involving positive semidefinite moment matrices and the dual theory of sums of squares of polynomials. We present these hierarchies of approximations and their main properties: asymptotic/finite convergence, optimality certificate, and extraction of global optimum solutions. We review the mathematical tools underlying these properties, in particular, some sums of squares representation results for positive polynomials, some results about moment matrices (in particular, of Curto and Fialkow), and the algebraic eigenvalue method for solving zero-dimensional systems of polynomial equations. We try whenever possible to provide detailed proofs and background.},
  copyright = {{\copyright}2009 Springer Science+Business Media, LLC},
  isbn = {978-0-387-09685-8 978-0-387-09686-5},
  langid = {english}
}

@misc{LectureNotesSlides,
  title = {Lecture {{Notes}} and/or {{Slides}} for the 2019 {{AMS Short Course}} on {{Sum}} of {{Squares}}: {{Theory}} and {{Applications}}},
  journal = {American Mathematical Society},
  url = {https://www.ams.org/meetings/shortcourse/lecturenotes-2019},
  urldate = {2022-01-13},
  abstract = {Advancing research. Creating connections.},
  langid = {english}
}

@book{leenaertsPiecewiseLinearModeling1998,
  title = {Piecewise {{Linear Modeling}} and {{Analysis}}},
  author = {Leenaerts, Domine M. W. and Van Bokhoven, Wim M. G.},
  year = {1998},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4757-6190-0},
  url = {http://link.springer.com/10.1007/978-1-4757-6190-0},
  urldate = {2022-11-11},
  isbn = {978-1-4419-5046-8 978-1-4757-6190-0},
  langid = {english}
}

@misc{legatJuMPMathOptInterfaceOptimizationframework2019,
  type = {Conference Presentation},
  title = {{{JuMP}} and {{MathOptInterface}}: {{An}} Optimizationframework Extensible by Design},
  author = {Legat, Beno{\^i}t},
  year = {2019},
  month = jun,
  url = {https://dial.uclouvain.be/pr/boreal/object/boreal%3A217884/datastream/PDF_01/view}
}

@article{legatMathOptInterfaceDataStructure2020,
  title = {{{MathOptInterface}}: A Data Structure for Mathematical Optimization Problems},
  shorttitle = {{{MathOptInterface}}},
  author = {Legat, Benoit and Dowson, Oscar and Garcia, Joaquim Dias and Lubin, Miles},
  year = {2020},
  month = sep,
  journal = {arXiv:2002.03447 [math]},
  eprint = {2002.03447},
  primaryclass = {math},
  url = {http://arxiv.org/abs/2002.03447},
  urldate = {2021-04-08},
  abstract = {We introduce MathOptInterface, an abstract data structure for representing mathematical optimization problems based on combining pre-defined functions and sets. MathOptInterface is significantly more general than existing data structures in the literature, encompassing, for example, a spectrum of problems classes from integer programming with indicator constraints to bilinear semidefinite programming. We also outline an automated rewriting system between equivalent formulations of a constraint. MathOptInterface has been implemented in practice, forming the foundation of a recent rewrite of JuMP, an open-source algebraic modeling language in the Julia language. The regularity of the MathOptInterface representation leads naturally to a general file format for mathematical optimization we call MathOptFormat. In addition, the automated rewriting system provides modeling power to users while making it easy to connect new solvers to JuMP.},
  archiveprefix = {arXiv}
}

@misc{legatMultivariatePolynomialsJulia2022,
  title = {Multivariate Polynomials in {{Julia}}},
  author = {Legat, Beno{\^i}t},
  year = {2022},
  month = jul
}

@techreport{levinCSISyEMath2019,
  type = {Lecture Notes},
  title = {{{CS}}/{{ISyE}}/{{Math}} 730: {{Nonlinear Optimization}} 2 {{Lecture Notes}}, {{Lecturer}}: {{Wright Steven}}},
  author = {Levin, Owen},
  year = 2019,
  institution = {University of Wisconsin--Madison},
  url = {https://pages.cs.wisc.edu/~olevin/Optimization_Notes.pdf}
}

@book{lobatoMultiObjectiveOptimizationProblems2017,
  title = {Multi-{{Objective Optimization Problems}}: {{Concepts}} and {{Self-Adaptive Parameters}} with {{Mathematical}} and {{Engineering Applications}}},
  shorttitle = {Multi-{{Objective Optimization Problems}}},
  author = {Lobato, Fran S{\'e}rgio and Jr, Valder Steffen},
  year = {2017},
  month = jul,
  series = {{{SpringerBriefs}} in {{Mathematics}}},
  publisher = {Springer},
  address = {Cham},
  url = {https://doi.org/10.1007/978-3-319-58565-9},
  abstract = {This book is aimed at undergraduate and graduate students in applied mathematics or computer science, as a tool for solving real-world design problems. The present work covers fundamentals in multi-objective optimization and applications in mathematical and engineering system design using a new optimization strategy, namely the Self-Adaptive Multi-objective Optimization Differential Evolution (SA-MODE) algorithm. This strategy is proposed in order to reduce the number of evaluations of the objective function through dynamic update of canonical Differential Evolution parameters (population size, crossover probability and perturbation rate). The methodology is applied to solve mathematical functions considering test cases from the literature and various engineering systems design, such as cantilevered beam design, biochemical reactor, crystallization process, machine tool spindle design, rotary dryer design, among others.},
  isbn = {978-3-319-58564-2},
  langid = {english}
}

@book{locatelliGlobalOptimizationTheory2013,
  title = {Global {{Optimization}}: {{Theory}}, {{Algorithms}}, and {{Applications}}},
  shorttitle = {Global {{Optimization}}},
  author = {Locatelli, Marco and Schoen, Fabio},
  year = {2013},
  month = nov,
  series = {{{MPS-SIAM Series}} on {{Optimization}}},
  number = {15},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  url = {https://my.siam.org/Store/Product/viewproduct/?ProductId=24184829},
  isbn = {978-1-61197-266-5},
  langid = {english}
}

@article{lofbergPrePostProcessingSumofSquares2009,
  title = {Pre- and {{Post-Processing Sum-of-Squares Programs}} in {{Practice}}},
  author = {Lofberg, Johan},
  year = {2009},
  month = may,
  journal = {IEEE Transactions on Automatic Control},
  volume = {54},
  number = {5},
  pages = {1007--1011},
  issn = {1558-2523},
  doi = {10.1109/TAC.2009.2017144},
  abstract = {Checking non-negativity of polynomials using sum-of-squares has recently been popularized and found many applications in control. Although the method is based on convex programming, the optimization problems rapidly grow and result in huge semidefinite programs. Additionally, they often become increasingly ill-conditioned. To alleviate these problems, it is important to exploit properties of the analyzed polynomial, and post-process the obtained solution. This technical note describes how the sum-of-squares module in the MATLAB toolbox YALMIP handles these issues.}
}

@misc{lofbergSemidefiniteProgramming2016,
  title = {Semidefinite Programming},
  author = {Lofberg, Johan},
  year = {2016},
  month = sep,
  url = {https://yalmip.github.io/tutorial/semidefiniteprogramming/},
  urldate = {2024-08-22},
  abstract = {Who wudda thought? Optimization over positive definite symmetric matrices is easy.}
}

@misc{lofbergYALMIP,
  title = {{{YALMIP}}},
  author = {L{\"o}fberg, Johan},
  url = {https://yalmip.github.io/}
}

@inproceedings{lofbergYALMIPToolboxModeling2004,
  title = {{{YALMIP}} : A Toolbox for Modeling and Optimization in {{MATLAB}}},
  shorttitle = {{{YALMIP}}},
  booktitle = {2004 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {L{\"o}fberg, Johan},
  year = {2004},
  month = sep,
  pages = {284--289},
  doi = {10.1109/CACSD.2004.1393890},
  abstract = {The MATLAB toolbox YALMIP is introduced. It is described how YALMIP can be used to model and solve optimization problems typically occurring in systems and control theory. In this paper, free MATLAB toolbox YALMIP, developed initially to model SDPs and solve these by interfacing eternal solvers. The toolbox makes development of optimization problems in general, and control oriented SDP problems in particular, extremely simple. In fact, learning 3 YALMIP commands is enough for most users to model and solve the optimization problems}
}

@article{lootsmaUniquenessSolutionsLinear1999,
  title = {Uniqueness of Solutions of Linear Relay Systems},
  author = {Lootsma, Y. J. and {van der Schaft}, A. J. and {\c C}aml{\i}bel, M. K.},
  year = {1999},
  month = mar,
  journal = {Automatica},
  volume = {35},
  number = {3},
  pages = {467--478},
  issn = {0005-1098},
  doi = {10.1016/S0005-1098(98)90177-7},
  url = {https://www.sciencedirect.com/science/article/pii/S0005109898901777},
  urldate = {2022-12-04},
  abstract = {Conditions are given for uniqueness of solutions of linear time-invariant systems under relay feedback. From a hybrid dynamical point of view this entails the deterministic specification of the discrete transition rules. The results are based on the formulation of relay systems as complementarity systems, and use the constructive theory of the Linear Complementarity Problem.},
  langid = {english}
}

@book{luenbergerLinearNonlinearProgramming2016,
  title = {Linear and {{Nonlinear Programming}}},
  author = {Luenberger, David G. and Ye, Yinyu},
  year = {2016},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  edition = {4},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-319-18842-3},
  url = {https://www.springer.com/gp/book/9783319188416},
  urldate = {2020-12-29},
  abstract = {This new edition covers the central concepts of practical optimization techniques, with an emphasis on methods that are both state-of-the-art and popular. One major insight is the connection between the purely analytical character of an optimization problem and the behavior of algorithms used to solve a problem. This was a major theme of the first edition of this book and the fourth edition expands and further illustrates this relationship. As in the earlier editions, the material in this fourth edition is organized into three separate parts. Part I is a self-contained introduction to linear programming. The presentation in this part is fairly conventional, covering the main elements of the underlying theory of linear programming, many of the most effective numerical algorithms, and many of its important special applications. Part II, which is independent of Part I, covers the theory of unconstrained optimization, including both derivations of the appropriate optimality conditions and an introduction to basic algorithms. This part of the book explores the general properties of algorithms and defines various notions of convergence. Part III extends the concepts developed in the second part to constrained optimization problems. Except for a few isolated sections, this part is also independent of Part I. It is possible to go directly into Parts II and III omitting Part I, and, in fact, the book has been used in this way in many universities.New to this edition is a chapter devoted to Conic Linear Programming, a powerful generalization of Linear Programming. Indeed, many conic structures are possible and useful in a variety of applications. It must be recognized, however, that conic linear programming is an advanced topic, requiring special study. Another important topic is an accelerated steepest descent method that exhibits superior convergence properties, and for this reason, has become quite popular. The proof of the convergence property for both standard and accelerated steepest descent methods are presented in Chapter 8. As in previous editions, end-of-chapter exercises appear for all chapters.From the reviews of the Third Edition:``{\dots} this very well-written book is a classic textbook in Optimization. It should be present in the bookcase of each student, researcher, and specialist from the host of disciplines from which practical optimization applications are drawn.'' (Jean-Jacques Strodiot, Zentralblatt MATH, Vol. 1207, 2011)},
  isbn = {978-3-319-18841-6},
  langid = {english}
}

@book{luenbergerLinearNonlinearProgramming2021,
  title = {Linear and {{Nonlinear Programming}}},
  author = {Luenberger, David G. and Ye, Yinyu},
  year = {2021},
  month = nov,
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  edition = {5},
  number = {228},
  publisher = {Springer},
  address = {Cham, Switzerland},
  url = {https://doi.org/10.1007/978-3-030-85450-8},
  abstract = {The 5th edition of this classic textbook covers the central concepts of practical optimization techniques, with an emphasis on methods that are both state-of-the-art and popular. One major insight is the connection between the purely analytical character of an optimization problem and the behavior of algorithms used to solve that problem. End-of-chapter exercises are provided for all chapters. The material is organized into three separate parts. Part I offers a self-contained introduction to linear programming. The presentation in this part is fairly conventional, covering the main elements of the underlying theory of linear programming, many of the most effective numerical algorithms, and many of its important special applications. Part II, which is independent of Part I, covers the theory of unconstrained optimization, including both derivations of the appropriate optimality conditions and an introduction to basic algorithms. This part of the book explores the general properties of algorithms and defines various notions of convergence. In turn, Part III extends the concepts developed in the second part to constrained optimization problems. Except for a few isolated sections, this part is also independent of Part I. As such, Parts II and III can easily be used without reading Part I and, in fact, the book has been used in this way at many universities. New to this edition are popular topics in data science and machine learning, such as the Markov Decision Process, Farkas' lemma, convergence speed analysis, duality theories and applications, various first-order methods, stochastic gradient method, mirror-descent method, Frank-Wolf method, ALM/ADMM method, interior trust-region method for non-convex optimization, distributionally robust optimization, online linear programming, semidefinite programming for sensor-network localization, and infeasibility detection for nonlinear optimization.},
  isbn = {978-3-030-85449-2},
  langid = {english}
}

@book{luenbergerOptimizationVectorSpace1997,
  title = {Optimization by {{Vector Space Methods}}},
  author = {Luenberger, David G.},
  year = {1997},
  month = jan,
  series = {Wiley {{Professional Paperback Series}}},
  edition = {Reprint of the 1969 edition},
  publisher = {Wiley-Interscience},
  address = {New York},
  url = {https://www.wiley.com/en-us/Optimization+by+Vector+Space+Methods-p-9780471181170},
  abstract = {Engineers must make decisions regarding the distribution of expensive resources in a manner that will be economically beneficial. This problem can be realistically formulated and logically analyzed with optimization theory. This book shows engineers how to use optimization theory to solve complex problems. Unifies the large field of optimization with a few geometric principles. Covers functional analysis with a minimum of mathematics. Contains problems that relate to the applications in the book.},
  isbn = {978-0-471-18117-0},
  langid = {english}
}

@book{luoMathematicalProgramsEquilibrium1996,
  title = {Mathematical {{Programs}} with {{Equilibrium Constraints}}},
  author = {Luo, Zhi-Quan and Pang, Jong-Shi and Ralph, Daniel},
  year = {1996},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511983658},
  url = {https://www.cambridge.org/core/books/mathematical-programs-with-equilibrium-constraints/03981C32ABDD55A4001BF58BA0C57444},
  urldate = {2022-03-22},
  abstract = {This book provides a solid foundation and an extensive study for an important class of constrained optimization problems known as Mathematical Programs with Equilibrium Constraints (MPEC), which are extensions of bilevel optimization problems. The book begins with the description of many source problems arising from engineering and economics that are amenable to treatment by the MPEC methodology. Error bounds and parametric analysis are the main tools to establish a theory of exact penalisation, a set of MPEC constraint qualifications and the first-order and second-order optimality conditions. The book also describes several iterative algorithms such as a penalty-based interior point algorithm, an implicit programming algorithm and a piecewise sequential quadratic programming algorithm for MPECs. Results in the book are expected to have significant impacts in such disciplines as engineering design, economics and game equilibria, and transportation planning, within all of which MPEC has a central role to play in the modelling of many practical problems.},
  isbn = {978-0-521-57290-3}
}

@article{madsenNewCharacterizationsL11994,
  title = {New {{Characterizations}} of {{L1 Solutions}} to {{Overdetermined Systems}} of {{Linear Equations}}},
  author = {Madsen, Kaj and Nielsen, Hans Bruun and Pinar, Mustafa {\c C}.},
  year = {1994},
  month = oct,
  journal = {Oper. Res. Lett.},
  volume = {16},
  number = {3},
  pages = {159--166},
  issn = {0167-6377},
  doi = {10.1016/0167-6377(94)90027-2},
  url = {http://dx.doi.org/10.1016/0167-6377(94)90027-2},
  urldate = {2018-08-01},
  abstract = {New characterizations of the @?"1 solutions to overdetermined system of linear equations are given. The first is a polyhedral characterization of the solution set in terms of a special sign vector using a simple property of the @?"1 solutions. The second characterization is based on a smooth approximation of the @?"1 function using a ''Huber'' function. This allows a description of the solution set of the @?"1 problem from any solution to the approximating problem for sufficiently small positive values of an approximation parameter. A sign approximation property of the Huber problem is also considered and a characterization of this property is given.}
}

@article{magnaniConvexPiecewiselinearFitting2008,
  title = {Convex Piecewise-Linear Fitting},
  author = {Magnani, Alessandro and Boyd, Stephen P.},
  year = {2008},
  month = mar,
  journal = {Optimization and Engineering},
  volume = {10},
  number = {1},
  pages = {1--17},
  issn = {1389-4420, 1573-2924},
  doi = {10.1007/s11081-008-9045-3},
  url = {http://www.springerlink.com/content/hvtl08lr4524014j/},
  urldate = {2012-02-24}
}

@article{magronApproximatingParetoCurves2014,
  title = {Approximating {{Pareto}} Curves Using Semidefinite Relaxations},
  author = {Magron, Victor and Henrion, Didier and Lasserre, Jean-Bernard},
  year = {2014},
  month = sep,
  journal = {Operations Research Letters},
  volume = {42},
  number = {6--7},
  pages = {432--437},
  issn = {0167-6377},
  doi = {10.1016/j.orl.2014.07.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0167637714001084},
  urldate = {2016-05-31},
  abstract = {We approximate as closely as desired the Pareto curve associated with bicriteria polynomial optimization problems. We use three formulations (including the weighted sum approach and the Chebyshev approximation) and each of them is viewed as a parametric polynomial optimization problem. For each case is associated a hierarchy of semidefinite relaxations and from an optimal solution of each relaxation one approximates the Pareto curve by solving an inverse problem (first two cases) or by building a polynomial underestimator (third case).}
}

@misc{majumdarSurveyRecentScalability2019,
  title = {A {{Survey}} of {{Recent Scalability Improvements}} for {{Semidefinite Programming}} with {{Applications}} in {{Machine Learning}}, {{Control}}, and {{Robotics}}},
  author = {Majumdar, Anirudha and Hall, Georgina and Ahmadi, Amir Ali},
  year = {2019},
  month = dec,
  number = {arXiv:1908.05209},
  eprint = {1908.05209},
  primaryclass = {cs, eess, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.05209},
  url = {http://arxiv.org/abs/1908.05209},
  urldate = {2023-09-15},
  abstract = {Historically, scalability has been a major challenge to the successful application of semidefinite programming in fields such as machine learning, control, and robotics. In this paper, we survey recent approaches for addressing this challenge including (i) approaches for exploiting structure (e.g., sparsity and symmetry) in a problem, (ii) approaches that produce low-rank approximate solutions to semidefinite programs, (iii) more scalable algorithms that rely on augmented Lagrangian techniques and the alternating direction method of multipliers, and (iv) approaches that trade off scalability with conservatism (e.g., by approximating semidefinite programs with linear and second-order cone programs). For each class of approaches we provide a high-level exposition, an entry-point to the corresponding literature, and examples drawn from machine learning, control, or robotics. We also present a list of software packages that implement many of the techniques discussed in the paper. Our hope is that this paper will serve as a gateway to the rich and exciting literature on scalable semidefinite programming for both theorists and practitioners.},
  archiveprefix = {arXiv}
}

@book{mangasarianNonlinearProgramming1994,
  title = {Nonlinear {{Programming}}},
  author = {Mangasarian, Olvi L.},
  year = {1994},
  series = {Classics in {{Applied Mathematics}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  url = {https://doi.org/10.1137/1.9781611971255},
  urldate = {2022-05-09},
  abstract = {This reprint of the 1969 book of the same name is a concise, rigorous, yet accessible, account of the fundamentals of constrained optimization theory. Many problems arising in diverse fields such as machine learning, medicine, chemical engineering, structural design, and airline scheduling can be reduced to a constrained optimization problem. This book provides readers with the fundamentals needed to study and solve such problems. Beginning with a chapter on linear inequalities and theorems of the alternative, basics of convex sets and separation theorems are then derived based on these theorems. This is followed by a chapter on convex functions that includes theorems of the alternative for such functions. These results are used in obtaining the saddlepoint optimality conditions of nonlinear programming without differentiability assumptions. Properties of differentiable convex functions are derived and then used in two key chapters of the book, one on optimality conditions for differentiable nonlinear programs and one on duality in nonlinear programming. Generalizations of convex functions to pseudoconvex and quasiconvex functions are given and then used to obtain generalized optimality conditions and duality results in the presence of nonlinear equality constraints. The book has four useful self-contained appendices on vectors and matrices, topological properties of n-dimensional real space, continuity and minimization, and differentiable functions.},
  isbn = {978-0-89871-341-1},
  langid = {english}
}

@book{marshallPositivePolynomialsSums2008,
  title = {Positive {{Polynomials}} and {{Sums}} of {{Squares}}},
  author = {Marshall, Murray},
  year = {1 dubna 2008},
  series = {Mathematical {{Surveys}} and {{Monographs}}},
  volume = {146},
  publisher = {American Mathematical Society},
  address = {Providence, R.I},
  abstract = {The study of positive polynomials brings together algebra, geometry and analysis. The subject is of fundamental importance in real algebraic geometry, when studying the properties of objects defined by polynomial inequalities. Hilbert's 17th problem and its solution in the first half of the 20th century were landmarks in the early days of the subject. More recently, new connections to the moment problem and to polynomial optimization have been discovered. The moment problem relates linear maps on the multidimensional polynomial ring to positive Borel measures.},
  isbn = {978-0-8218-4402-1}
}

@book{martinsEngineeringDesignOptimization2022,
  title = {Engineering {{Design Optimization}}},
  author = {Martins, Joaquim R. R. A. and Ning, Andrew},
  year = {2022},
  month = jan,
  publisher = {Cambridge University Press},
  address = {Cambridge ; New York, NY},
  url = {https://mdobook.github.io/},
  abstract = {Based on course-tested material, this rigorous yet accessible graduate textbook covers both fundamental and advanced optimization theory and algorithms. It covers a wide range of numerical methods and topics, including both gradient-based and gradient-free algorithms, multidisciplinary design optimization, and uncertainty, with instruction on how to determine which algorithm should be used for a given application. It also provides an overview of models and how to prepare them for use with numerical optimization, including derivative computation. Over 400 high-quality visualizations and numerous examples facilitate understanding of the theory, and practical tips address common issues encountered in practical engineering design optimization and how to address them. Numerous end-of-chapter homework problems, progressing in difficulty, help put knowledge into practice. Accompanied online by a solutions manual for instructors and source code for problems, this is ideal for a one- or two-semester graduate course on optimization in aerospace, civil, mechanical, electrical, and chemical engineering departments.},
  isbn = {978-1-108-83341-7},
  langid = {english}
}

@misc{MathOptInterface2024,
  title = {{{MathOptInterface}}},
  year = {2024},
  month = jan,
  url = {https://github.com/jump-dev/MathOptInterface.jl},
  urldate = {2023-01-16},
  abstract = {An abstraction layer for mathematical optimization solvers.},
  howpublished = {JuMP-dev}
}

@book{matousekUnderstandingUsingLinear2007,
  title = {Understanding and {{Using Linear Programming}}},
  author = {Matou{\v s}ek, Ji{\v r}{\'i} and G{\"a}rtner, Bernd},
  year = {2007},
  series = {Universitext},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  url = {https://link.springer.com/book/10.1007/978-3-540-30717-4},
  urldate = {2022-05-31},
  abstract = {This is an introductory textbook of linear programming, written mainly for students of computer science and mathematics. The book is concise, but at the same time, the main results are covered with complete proofs and in sufficient detail, ready for presentation in class.},
  isbn = {978-3-540-30697-9},
  langid = {english}
}

@article{mccormickComputabilityGlobalSolutions1976,
  title = {Computability of Global Solutions to Factorable Nonconvex Programs: {{Part I}} --- {{Convex}} Underestimating Problems},
  shorttitle = {Computability of Global Solutions to Factorable Nonconvex Programs},
  author = {McCormick, Garth P.},
  year = {1976},
  month = dec,
  journal = {Mathematical Programming},
  volume = {10},
  number = {1},
  pages = {147--175},
  issn = {1436-4646},
  doi = {10.1007/BF01580665},
  url = {https://doi.org/10.1007/BF01580665},
  urldate = {2025-01-12},
  abstract = {For nonlinear programming problems which are factorable, a computable procedure for obtaining tight underestimating convex programs is presented. This is used to exclude from consideration regions where the global minimizer cannot exist.},
  langid = {english}
}

@article{mehdiloozadFindingMaximalElement2015,
  title = {Finding a Maximal Element of a Convex Set through Its Characteristic Cone: {{An}} Application to Finding a Strictly Complementary Solution},
  shorttitle = {Finding a Maximal Element of a Convex Set through Its Characteristic Cone},
  author = {Mehdiloozad, Mahmood and Tone, Kaoru and Askarpour, Rahim and Ahmadi, Mohammad Bagher},
  year = {2015},
  month = mar,
  journal = {arXiv:1503.09014 [math]},
  eprint = {1503.09014},
  primaryclass = {math},
  url = {http://arxiv.org/abs/1503.09014},
  urldate = {2022-05-10},
  abstract = {In order to express a polyhedron as the (Minkowski) sum of a polytope and a polyhedral cone, Motzkin (1936) made a transition from the polyhedron to a polyhedral cone. Based on his excellent idea, we represent a set by a characteristic cone. By using this representation, we then reach four main results: (i) expressing a closed convex set containing no line as the direct sum of the convex hull of its extreme points and conical hull of its extreme directions, (ii) establishing a convex programming (CP) based framework for determining a maximal element-an element with the maximum number of positive components-of a convex set, (iii) developing a linear programming problem for finding a relative interior point of a polyhedron, and (iv) proposing two procedures for the identification of a strictly complementary solution in linear programming.},
  archiveprefix = {arXiv}
}

@inproceedings{mentaStabilityDynamicFeedback2018,
  title = {Stability of {{Dynamic Feedback}} Optimization with {{Applications}} to {{Power Systems}}},
  booktitle = {2018 56th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}} ({{Allerton}})},
  author = {Menta, Sandeep and Hauswirth, Adrian and Bolognani, Saverio and Hug, Gabriela and D{\"o}rfler, Florian},
  year = {2018},
  pages = {136--143},
  doi = {10.1109/ALLERTON.2018.8635640},
  abstract = {We consider the problem of optimizing the steady state of a dynamical system in closed loop. Conventionally, the design of feedback optimization control laws assumes that the system is stationary. However, in reality, the dynamics of the (slow) iterative optimization routines can interfere with the (fast) system dynamics. We provide a study of the stability and convergence of these feedback optimization setups in closed loop with the underlying plant, via a custom-tailored singular perturbation analysis result. Our study is particularly geared towards applications in power systems and the question whether recently developed online optimization schemes can be deployed without jeopardizing dynamic system stability.}
}

@inproceedings{messererDeterminingExactLocal2020,
  title = {Determining the {{Exact Local Convergence Rate}} of {{Sequential Convex Programming}}},
  booktitle = {2020 {{European Control Conference}} ({{ECC}})},
  author = {Messerer, Florian and Diehl, Moritz},
  year = {2020},
  month = may,
  pages = {1280--1285},
  doi = {10.23919/ECC51009.2020.9143749},
  abstract = {Sequential Convex Programming (SCP) is an iterative algorithm for solving Nonlinear Programs (NLP) with "convex-over-nonlinear" substructure. At every iteration it solves a convex, but generally nonlinear, approximation to the original NLP, exploiting its outer convexities. It is already known that SCP has linear local convergence, though without a tight characterization of the rate. Sequential Convex Quadratic Programming (SCQP) is another "convex-over-nonlinear" substructure exploiting algorithm, for which a tight characterization of the convergence rate has already been obtained. In this paper we show under mild assumptions that in fact both methods have the same local linear convergence - or divergence - rate. We can therefore determine the convergence rate of SCP, which is tightly characterized by two simple matrix inequalities evaluated at the solution. We further reason why - as a heuristic - SCP should in general show more robust global convergence than SCQP. We illustrate this, as well as the local convergence rate, with a numerical example.}
}

@book{miettinenNonlinearMultiobjectiveOptimization1998,
  title = {Nonlinear {{Multiobjective Optimization}}},
  author = {Miettinen, Kaisa},
  year = {1998},
  month = sep,
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  publisher = {Springer},
  address = {Boston},
  abstract = {Problems with multiple objectives and criteria are generally known as multiple criteria optimization or multiple criteria decision-making (MCDM) problems. So far, these types of problems have typically been modelled and solved by means of linear programming. However, many real-life phenomena are of a nonlinear nature, which is why we need tools for nonlinear programming capable of handling several conflicting or incommensurable objectives. In this case, methods of traditional single objective optimization and linear programming are not enough; we need new ways of thinking, new concepts, and new methods - nonlinear multiobjective optimization.  Nonlinear Multiobjective Optimization provides an extensive, up-to-date, self-contained and consistent survey, review of the literature and of the state of the art on nonlinear (deterministic) multiobjective optimization, its methods, its theory and its background. The amount of literature on multiobjective optimization is immense. The treatment in this book is based on approximately 1500 publications in English printed mainly after the year 1980.  Problems related to real-life applications often contain irregularities and nonsmoothnesses. The treatment of nondifferentiable multiobjective optimization in the literature is rather rare. For this reason, this book contains material about the possibilities, background, theory and methods of nondifferentiable multiobjective optimization as well.  This book is intended for both researchers and students in the areas of (applied) mathematics, engineering, economics, operations research and management science; it is meant for both professionals and practitioners in many different fields of application. The intention has been to provide a consistent summary that may help in selecting an appropriate method for the problem to be solved. It is hoped the extensive bibliography will be of value to researchers.},
  isbn = {978-0-7923-8278-2},
  langid = {english}
}

@article{millerChanceConstrainedProgramming1965,
  title = {Chance {{Constrained Programming}} with {{Joint Constraints}}},
  author = {Miller, Bruce L. and Wagner, Harvey M.},
  year = {1965},
  month = dec,
  journal = {Operations Research},
  volume = {13},
  number = {6},
  pages = {930--945},
  publisher = {INFORMS},
  issn = {0030-364X},
  doi = {10.1287/opre.13.6.930},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/opre.13.6.930},
  urldate = {2024-05-01},
  abstract = {This paper considers the mathematical properties of chance constrained programming problems where the restriction is on the joint probability of a multivariate random event. One model that is considered arises when the right-handside constants of the linear constraints are random. Another model treated here occurs when the coefficients of the linear programming variables are described by a multinormal distribution. It is shown that under certain restrictions both situations can be viewed as a deterministic nonlinear programming problem. Since most computational methods for solving nonlinear programming models require the constraints be concave, this paper explores whether the resultant problem meets the concavity assumption. For many probability laws of practical importance, the constraint in the first type of model is shown to violate concavity. However, a simple logarithmic transformation does produce a concave restriction for an important class of problems. The paper also surveys the ``generalized linear programming'' method for solving such problems when the logarithmic transformation is justified. For the second type model, the constraint is demonstrated to be nonconcave.}
}

@misc{mitchellStrictComplementarity2018,
  type = {Lecture},
  title = {Strict Complementarity},
  author = {Mitchell, John E.},
  year = {2018},
  month = apr,
  url = {http://eaton.math.rpi.edu/faculty/Mitchell/courses/matp4700/notesMATP4700/lecture25/25C_strictcomplementaritybeamer.pdf}
}

@misc{ModelPredictiveControl2020,
  title = {Model {{Predictive Control Toolbox}}},
  year = {2020},
  url = {https://www.mathworks.com/products/model-predictive-control.html},
  howpublished = {The Mathworks}
}

@inproceedings{morbidiMinimumenergyPathGeneration2016,
  title = {Minimum-Energy Path Generation for a Quadrotor {{UAV}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Morbidi, Fabio and Cano, Roel and Lara, David},
  year = {2016},
  month = may,
  pages = {1492--1498},
  doi = {10.1109/ICRA.2016.7487285},
  abstract = {A major limitation of existing battery-powered quadrotor UAVs is their reduced flight endurance. To address this issue, by leveraging the electrical model of a brushless DC motor, we explicitly determine minimum-energy paths between a predefined initial and final configuration of a quadrotor by solving an optimal control problem with respect to the angular accelerations of the four propellers. As a variation on this problem, if the total energy consumption between two boundary states is fixed, minimum-time and/or minimum-control-effort trajectories are computed for the aerial vehicle. The theory is illustrated for the DJI Phantom 2 quadrotor in three realistic scenarios.}
}

@misc{MOSEKModelingCookbook2024,
  title = {{{MOSEK Modeling Cookbook}}},
  year = {2024},
  month = sep,
  url = {https://docs.mosek.com/MOSEKModelingCookbook-a4paper.pdf},
  howpublished = {Mosek ApS}
}

@misc{MOSEKOptimizationSuite2023,
  title = {{{MOSEK Optimization Suite}}},
  year = {2024},
  month = jan,
  url = {https://docs.mosek.com/10.1/intro.pdf},
  howpublished = {MOSEK ApS}
}

@misc{MOSEKOptimizationToolbox2024,
  title = {{{MOSEK Optimization Toolbox}} for {{MATLAB}}},
  year = {2024},
  month = jan,
  url = {https://docs.mosek.com/10.1/toolbox.pdf},
  howpublished = {MOSEK ApS}
}

@misc{MOSEKOptimizerAPI2024,
  title = {{{MOSEK Optimizer API}} for {{Julia}}},
  year = {2024},
  month = oct,
  url = {https://docs.mosek.com/10.2/juliaapi.pdf},
  howpublished = {MOSEK ApS}
}

@misc{MultiparametricProgramming2016,
  title = {Multiparametric Programming},
  year = {2016},
  month = sep,
  journal = {YALMIP},
  url = {https://yalmip.github.io/tutorial/multiparametricprogramming/},
  urldate = {2022-03-24},
  abstract = {This tutorial requires MPT.},
  langid = {english}
}

@book{murtyLinearComplementarityLinear1997,
  title = {Linear {{Complementarity}}, {{Linear}} and {{Non-linear Programming}}},
  author = {Murty, Katta G. and Yu, Vincent F.},
  year = {1997},
  edition = {Internet edition},
  address = {Berlin},
  url = {http://www-personal.umich.edu/~murty/books/linear_complementarity_webbook/},
  isbn = {978-3-88538-403-8}
}

@article{narulaMinimumSumAbsolute1982,
  title = {The {{Minimum Sum}} of {{Absolute Errors Regression}}: {{A State}} of the {{Art Survey}}},
  shorttitle = {The {{Minimum Sum}} of {{Absolute Errors Regression}}},
  author = {Narula, Subhash C. and Wellington, John F.},
  year = {1982},
  journal = {International Statistical Review / Revue Internationale de Statistique},
  volume = {50},
  number = {3},
  eprint = {1402501},
  eprinttype = {jstor},
  pages = {317--326},
  issn = {0306-7734},
  doi = {10.2307/1402501},
  url = {http://www.jstor.org/stable/1402501},
  urldate = {2018-08-01},
  abstract = {The minimum sum of absolute errors regression is resistant to outliers and error distributions with long tails. With the development of several efficient algorithms, computer codes and some statistical inference procedures, it is fast becoming a viable and desirable alternative to the popular least squares regression. The objective of this paper is to present a state of the art survey for the minimum sum of absolute errors regression. /// Notre m{\'e}thode de la r{\'e}gression de la somme minimale des erreurs absolues est "r{\'e}sistante" aux points hors courbe et aux accidents de distribution. Avec le d{\'e}veloppement de plusieurs algorithmes efficaces, de codes d'ordinateur, et de certaines proc{\'e}dures statistiques d'inf{\'e}rence, ce moyen devient rapidement une alternative viable et prefer{\'e}e au moyen populaire de la r{\'e}gression par les moindres carr{\'e}s. Le but de la communication est de pr{\'e}senter l'{\'e}tat actuel des recherches concernant la regression de la somme minimale des erreurs absolues.}
}

@incollection{nedicDistributedAsynchronousIncremental2001,
  title = {Distributed Asynchronous Incremental Subgradient Methods},
  booktitle = {Studies in {{Computational Mathematics}}},
  author = {Nedi{\'c}, A. and Bertsekas, D. P. and Borkar, V. S.},
  editor = {Dan Butnariu, Yair Censor {and} Simeon Reich},
  year = {2001},
  series = {Inherently {{Parallel Algorithms}} in {{Feasibility}} and {{Optimization}} and Their {{Applications}}},
  volume = {Volume 8},
  pages = {381--407},
  publisher = {Elsevier},
  url = {http://www.sciencedirect.com/science/article/pii/S1570579X01800239},
  urldate = {2014-01-13},
  abstract = {We propose and analyze a distributed asynchronous subgradient method for minimizing a convex function that consists of the sum of a large number of component functions. This type of minimization arises in a dual context from Lagrangian relaxation of the coupling constraints of large scale separable problems. The idea is to distribute the computation of the component subgradients among a set of processors, which communicate only with a coordinator. The coordinator performs the subgradient iteration incrementally and asynchronously, by taking steps along the subgradients of the component funtions that are available at the update time. The incremental approach has performed very well in centralized computation, and the parallel implementation should improve its performance substantially, particularly for typical problems where computation of the component subgradients is relatively costly.},
  isbn = {1570-579X}
}

@article{nedicDistributedOptimizationControl2018,
  title = {Distributed {{Optimization}} for {{Control}}},
  author = {Nedi{\'c}, Angelia and Liu, Ji},
  year = {2018},
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume = {1},
  number = {1},
  pages = {77--103},
  doi = {10.1146/annurev-control-060117-105131},
  url = {https://doi.org/10.1146/annurev-control-060117-105131},
  urldate = {2021-09-10},
  abstract = {Advances in wired and wireless technology have necessitated the development of theory, models, and tools to cope with the new challenges posed by large-scale control and optimization problems over networks. The classical optimization methodology works under the premise that all problem data are available to a central entity (a computing agent or node). However, this premise does not apply to large networked systems, where each agent (node) in the network typically has access only to its private local information and has only a local view of the network structure. This review surveys the development of such distributed computational models for time-varying networks. To emphasize the role of the network structure in these approaches, we focus on a simple direct primal (sub)gradient method, but we also provide an overview of other distributed methods for optimization in networks. Applications of the distributed optimization framework to the control of power systems, least squares solutions to linear equations, and model predictive control are also presented.}
}

@article{nedicIncrementalSubgradientMethods2001,
  title = {Incremental {{Subgradient Methods}} for {{Nondifferentiable Optimization}}},
  author = {Nedic, Angelia and Bertsekas, Dimitri P.},
  year = {2001},
  month = jan,
  journal = {SIAM Journal on Optimization},
  volume = {12},
  number = {1},
  pages = {109--138},
  issn = {1052-6234, 1095-7189},
  doi = {10.1137/S1052623499362111},
  url = {http://epubs.siam.org/doi/abs/10.1137/S1052623499362111},
  urldate = {2014-01-13}
}

@article{nemirovskiConvexApproximationsChance2007,
  title = {Convex {{Approximations}} of {{Chance Constrained Programs}}},
  author = {Nemirovski, Arkadi and Shapiro, Alexander},
  year = {2007},
  month = jan,
  journal = {SIAM Journal on Optimization},
  volume = {17},
  number = {4},
  pages = {969--996},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1052-6234},
  doi = {10.1137/050622328},
  url = {https://epubs.siam.org/doi/abs/10.1137/050622328},
  urldate = {2021-09-03},
  abstract = {We consider a chance constrained problem, where one seeks to minimize a convex objective over solutions satisfying, with a given close to one probability, a system of randomly perturbed convex constraints. This problem may happen to be computationally intractable; our goal is to build its computationally tractable approximation, i.e., an efficiently solvable deterministic optimization program with the feasible set contained in the chance constrained problem. We construct a general class of such convex conservative approximations of the corresponding chance constrained problem. Moreover, under the assumptions that the constraints are affine in the perturbations and the entries in the perturbation vector are independent-of-each-other random variables, we build a large deviation-type approximation, referred to as ``Bernstein approximation,'' of the chance constrained problem. This approximation is convex and efficiently solvable. We propose a simulation-based scheme for bounding the optimal value in the chance constrained problem and report numerical experiments aimed at comparing the Bernstein and well-known scenario approximation approaches. Finally, we extend our construction to the case of ambiguous chance constrained problems, where the random perturbations are independent with the collection of distributions known to belong to a given convex compact set rather than to be known exactly, while the chance constraint should be satisfied for every distribution given by this set.}
}

@book{nemirovskiIntroductionLinearOptimization2024,
  title = {Introduction to {{Linear Optimization}}},
  author = {Nemirovski, Arkadi},
  year = {2024},
  month = mar,
  publisher = {World Scientific Publishing Company},
  address = {New Jersey},
  url = {https://www.worldscientific.com/worldscibooks/10.1142/13457#t=aboutBook},
  abstract = {The book presents a graduate level, rigorous, and self-contained introduction to linear optimization (LO), the presented topics being},
  isbn = {9789811278730},
  langid = {english}
}

@book{nesterovIntroductoryLecturesConvex2004,
  title = {Introductory {{Lectures}} on {{Convex Optimization}}: {{A Basic Course}}},
  shorttitle = {Introductory {{Lectures}} on {{Convex Optimization}}},
  author = {Nesterov, Yurii},
  year = {2004},
  series = {Applied {{Optimization}}},
  publisher = {Springer US},
  url = {https://www.springer.com/gp/book/9781402075537},
  urldate = {2019-03-05},
  abstract = {It was in the middle of the 1980s, when the seminal paper by Kar\- markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op\- timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre\- diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc\- tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop\- ing field, which got the name "polynomial-time interior-point methods", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].},
  isbn = {978-1-4020-7553-7},
  langid = {english}
}

@book{nesterovLecturesConvexOptimization2018,
  title = {Lectures on {{Convex Optimization}}},
  author = {Nesterov, Yurii},
  year = {2018},
  month = dec,
  series = {Springer {{Optimization}} and {{Its Applications}}},
  edition = {2},
  publisher = {Springer},
  address = {Cham},
  url = {https://doi.org/10.1007/978-3-319-91578-4},
  abstract = {This book provides a comprehensive, modern introduction to convex optimization, a field that is becoming increasingly important in applied mathematics, economics and finance, engineering, and computer science, notably in data science and machine learning.Written by a leading expert in the field, this book includes recent advances in the algorithmic theory of convex optimization, naturally complementing the existing literature. It contains a unified and rigorous presentation of the acceleration techniques for minimization schemes of first- and second-order. It provides readers with a full treatment of the smoothing technique, which has tremendously extended the abilities of gradient-type methods. Several powerful approaches in structural optimization, including optimization in relative scale and polynomial-time interior-point methods, are also discussed in detail. Researchers in theoretical optimization as well as professionals working on optimization problems will findthis book very useful. It presents many successful examples of how to develop very fast specialized minimization algorithms. Based on the author's lectures, it can naturally serve as the basis for introductory and advanced courses in convex optimization for students in engineering, economics, computer science and mathematics.},
  isbn = {978-3-319-91577-7},
  langid = {english}
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Wright, Stephen},
  year = {2006},
  month = jul,
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  edition = {2},
  publisher = {Springer},
  address = {New York},
  url = {https://doi.org/10.1007/978-0-387-40065-5},
  abstract = {Optimization is an important tool used in decision science and for the analysis of physical systems used in engineering. One can trace its roots to the Calculus of Variations and the work of Euler and Lagrange. This natural and reasonable approach to mathematical programming covers numerical methods for finite-dimensional optimization problems. It begins with very simple ideas progressing through more complicated concepts, concentrating on methods for both unconstrained and constrained optimization.},
  isbn = {978-0-387-30303-1},
  langid = {english}
}

@article{odonoghueConicOptimizationOperator2016a,
  title = {Conic {{Optimization}} via {{Operator Splitting}} and {{Homogeneous Self-Dual Embedding}}},
  author = {O'Donoghue, Brendan and Chu, Eric and Parikh, Neal and Boyd, Stephen},
  year = {2016},
  month = jun,
  journal = {Journal of Optimization Theory and Applications},
  volume = {169},
  number = {3},
  pages = {1042--1068},
  issn = {1573-2878},
  doi = {10.1007/s10957-016-0892-3},
  url = {https://doi.org/10.1007/s10957-016-0892-3},
  urldate = {2024-01-19},
  abstract = {We introduce a first-order method for solving very large convex cone programs. The method uses an operator splitting method, the alternating directions method of multipliers, to solve the homogeneous self-dual embedding, an equivalent feasibility problem involving finding a nonzero point in the intersection of a subspace and a cone. This approach has several favorable properties. Compared to interior-point methods, first-order methods scale to very large problems, at the cost of requiring more time to reach very high accuracy. Compared to other first-order methods for cone programs, our approach finds both primal and dual solutions when available or a certificate of infeasibility or unboundedness otherwise, is parameter free, and the per-iteration cost of the method is the same as applying a splitting method to the primal or dual alone. We discuss efficient implementation of the method in detail, including direct and indirect methods for computing projection onto the subspace, scaling the original problem data, and stopping criteria. We describe an open-source implementation, which handles the usual (symmetric) nonnegative, second-order, and semidefinite cones as well as the (non-self-dual) exponential and power cones and their duals. We report numerical results that show speedups over interior-point cone solvers for large problems, and scaling to very large general cone programs.},
  langid = {english}
}

@article{odonoghueOperatorSplittingHomogeneous2021,
  title = {Operator {{Splitting}} for a {{Homogeneous Embedding}} of the {{Linear Complementarity Problem}}},
  author = {O'Donoghue, Brendan},
  year = {2021},
  month = jan,
  journal = {SIAM Journal on Optimization},
  volume = {31},
  number = {3},
  pages = {1999--2023},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1052-6234},
  doi = {10.1137/20M1366307},
  url = {https://epubs.siam.org/doi/abs/10.1137/20M1366307},
  urldate = {2024-04-13},
  abstract = {We establish connections between the facial reduction algorithm of Borwein and Wolkowicz and the self-dual homogeneous model of Goldman and Tucker when applied to conic optimization problems. Specifically, we show that the self-dual homogeneous model returns facial reduction certificates when it fails to return a primal-dual optimal solution or a certificate of infeasibility. Using this observation, we give an algorithm based on facial reduction for solving the primal problem that, in principle, always succeeds. (An analogous algorithm is easily stated for the dual problem.) This algorithm has the appealing property that it only performs facial reduction when it is required, not when it is possible; e.g., if a primal-dual optimal solution exists, it will be found in lieu of a facial reduction certificate even if Slater's condition fails. For the case of linear, second-order, and semidefinite optimization, we show that the algorithm can be implemented by assuming oracle access to the central-path limit point of an extended embedding, a strictly feasible conic problem with a strictly feasible dual. We then give numerical experiments illustrating barriers to practical implementation.}
}

@article{odonoghueSplittingMethodOptimal2013,
  title = {A {{Splitting Method}} for {{Optimal Control}}},
  author = {O'Donoghue, B. and Stathopoulos, G. and Boyd, S.},
  year = {2013},
  month = nov,
  journal = {IEEE Transactions on Control Systems Technology},
  volume = {21},
  number = {6},
  pages = {2432--2442},
  issn = {1558-0865},
  doi = {10.1109/TCST.2012.2231960},
  abstract = {We apply an operator splitting technique to a generic linear-convex optimal control problem, which results in an algorithm that alternates between solving a quadratic control problem, for which there are efficient methods, and solving a set of single-period optimization problems, which can be done in parallel, and often have analytical solutions. In many cases, the resulting algorithm is division-free (after some off-line pre-computations) and can be implemented in fixed-point arithmetic, for example on a field-programmable gate array (FPGA). We demonstrate the method on several examples from different application areas.}
}

@misc{OptimizationToolbox2023,
  title = {Optimization {{Toolbox}}},
  shorttitle = {Optimization {{Toolbox}}},
  year = {2023},
  url = {https://www.mathworks.com/products/optimization.html},
  howpublished = {The Mathworks, Inc.}
}

@article{ottaNPProNewtonProjection2024,
  title = {{{NPPro}}: A {{Newton}} Projection with Proportioning Solver for Quadratic Programming with Affine Constraints},
  shorttitle = {{{NPPro}}},
  author = {Otta, Pavel and {\v S}antin, Ond{\v r}ej and Havlena, Vladim{\'i}r},
  year = {2024},
  month = dec,
  journal = {Optimization Methods and Software},
  publisher = {Taylor \& Francis},
  url = {https://www.tandfonline.com/doi/abs/10.1080/10556788.2024.2436180},
  urldate = {2025-01-10},
  abstract = {The second-order method for the solution of quadratic programming (QP) with affine constraints is introduced in this paper. It belongs to an active-set method family. However, it uses a projection ...},
  copyright = {{\copyright} 2025 Informa UK Limited, trading as Taylor \& Francis Group},
  langid = {english}
}

@article{outrataMathematicalProgramsComplementarity2000,
  title = {On Mathematical Programs with Complementarity Constraints},
  author = {Outrata, Ji{\v r}{\'i} V.},
  year = {2000},
  month = jan,
  journal = {Optimization Methods and Software},
  volume = {14},
  number = {1-2},
  pages = {117--137},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556780008805796},
  url = {https://doi.org/10.1080/10556780008805796},
  urldate = {2022-03-21},
  abstract = {The paper deals with mathematical programs, where a complementarity problem arises among the constraints. The main attention is paid to optimality conditions and the respective constraint qualification. In addition, a simple numerical approach is proposed based on the exact penalization of the complementarity constraint.}
}

@misc{ozdaglarDistributedDynamicsOptimization2011,
  title = {Distributed Dynamics and Optimization in Multi-Agent Systems},
  author = {Ozdaglar, Asu},
  year = {2011},
  month = oct,
  url = {http://users.ece.utexas.edu/~cmcaram/pubs/Ozdaglar-Winedale.pdf}
}

@article{ozdaglarRelationPseudonormalityQuasiregularity2004,
  title = {The Relation between Pseudonormality and Quasiregularity in Constrained Optimization},
  author = {Ozdaglar, Asuman E. and Bertsekas, Dimitri P.},
  year = {2004},
  month = oct,
  journal = {Optimization Methods and Software},
  volume = {19},
  number = {5},
  pages = {493--506},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556780410001709420},
  url = {https://doi.org/10.1080/10556780410001709420},
  urldate = {2022-11-01},
  abstract = {We consider optimization problems with equality, inequality, and abstract set constraints. We investigate the relations between various characteristics of the constraint set related to the existence of Lagrange multipliers. For problems with no abstract set constraint, the classical condition of quasiregularity provides the connecting link between the most common constraint qualifications and existence of Lagrange multipliers. In earlier work, we introduced a new and general condition, pseudonormality, that is central within the theory of constraint qualifications, exact penalty functions, and existence of Lagrange multipliers. In this paper, we explore the relations between pseudonormality, quasiregularity, and existence of Lagrange multipliers, and show that, unlike pseudonormality, quasiregularity cannot play the role of a general constraint qualification in the presence of an abstract set constraint. In particular, under a regularity assumption on the abstract constraint set, we show that pseudonormality implies quasiregularity. However, contrary to pseudonormality, quasiregularity does not imply the existence of Lagrange multipliers, except under additional assumptions.}
}

@article{palacios-gomezNonlinearOptimizationSuccessive1982,
  title = {Nonlinear {{Optimization}} by {{Successive Linear Programming}}},
  author = {{Palacios-Gomez}, F. and Lasdon, L. and Engquist, M.},
  year = {1982},
  month = oct,
  journal = {Management Science},
  volume = {28},
  number = {10},
  pages = {1106--1120},
  publisher = {INFORMS},
  issn = {0025-1909},
  doi = {10.1287/mnsc.28.10.1106},
  url = {https://pubsonline.informs.org/doi/10.1287/mnsc.28.10.1106},
  urldate = {2024-06-27},
  abstract = {Successive Linear Programming (SLP), which is also known as the Method of Approximation Programming, solves nonlinear optimization problems via a sequence of linear programs. This paper reports on promising computational results with SLP that contrast with the poor performance indicated by previously published comparative tests. The paper provides a detailed description of an efficient, reliable SLP algorithm along with a convergence theorem for linearly constrained problems and extensive computational results. It also discusses several alternative strategies for implementing SLP. The computational results show that SLP compares favorably with the Generalized Reduced Gradient Code GRG2 and with MINOS/GRG. It appears that SLP will be most successful when applied to large problems with low degrees of freedom.}
}

@article{pandalaQpSWIFTRealTimeSparse2019,
  title = {{{qpSWIFT}}: {{A Real-Time Sparse Quadratic Program Solver}} for {{Robotic Applications}}},
  shorttitle = {{{qpSWIFT}}},
  author = {Pandala, Abhishek Goud and Ding, Yanran and Park, Hae-Won},
  year = {2019},
  month = oct,
  journal = {IEEE Robotics and Automation Letters},
  volume = {4},
  number = {4},
  pages = {3355--3362},
  issn = {2377-3766},
  doi = {10.1109/LRA.2019.2926664},
  url = {https://ieeexplore.ieee.org/document/8754693},
  urldate = {2024-01-19},
  abstract = {In this letter, we present qpSWIFT, a real-time quadratic program (QP) solver. Motivated by the need for a robust embedded QP solver in robotic applications, qpSWIFT employs standard primal-dual interior-point method, along with Mehrotra predictor--corrector steps and Nesterov--Todd scaling. The sparse structure of the resulting Karush--Kuhn--Tucker linear system in the QP formulation is exploited, and sparse direct methods are utilized to solve the linear system of equations. To further accelerate the factorization process, we only modify the corresponding rows of the matrix factors that change during iterations and cache the nonzero Cholesky pattern. qpSWIFT is library free, written in ANSI-C and its performance is benchmarked through standard problems that could be cast as QP. Numerical results show that qpSWIFT outperforms state-of-the-art solvers for small scale problems. To evaluate the performance of the solver, a real-time implementation of the solver in the model predictive control framework through experiments on a quadrupedal robot are presented.}
}

@article{pangDifferentialVariationalInequalities2008,
  title = {Differential Variational Inequalities},
  author = {Pang, Jong-Shi and Stewart, David E.},
  year = {2008},
  month = jun,
  journal = {Mathematical Programming},
  volume = {113},
  number = {2},
  pages = {345--424},
  issn = {1436-4646},
  doi = {10.1007/s10107-006-0052-x},
  url = {https://doi.org/10.1007/s10107-006-0052-x},
  urldate = {2025-01-03},
  abstract = {This paper introduces and studies the class of differential variational inequalities (DVIs) in a finite-dimensional Euclidean space. The DVI provides a powerful modeling paradigm for many applied problems in which dynamics, inequalities, and discontinuities are present; examples of such problems include constrained time-dependent physical systems with unilateral constraints, differential Nash games, and hybrid engineering systems with variable structures. The DVI unifies several mathematical problem classes that include ordinary differential equations (ODEs) with smooth and discontinuous right-hand sides, differential algebraic equations (DAEs), dynamic complementarity systems, and evolutionary variational inequalities. Conditions are presented under which the DVI can be converted, either locally or globally, to an equivalent ODE with a Lipschitz continuous right-hand function. For DVIs that cannot be so converted, we consider their numerical resolution via an Euler time-stepping procedure, which involves the solution of a sequence of finite-dimensional variational inequalities. Borrowing results from differential inclusions (DIs) with upper semicontinuous, closed and convex valued multifunctions, we establish the convergence of such a procedure for solving initial-value DVIs. We also present a class of DVIs for which the theory of DIs is not directly applicable, and yet similar convergence can be established. Finally, we extend the method to a boundary-value DVI and provide conditions for the convergence of the method. The results in this paper pertain exclusively to systems with ``index'' not exceeding two and which have absolutely continuous solutions.},
  langid = {english}
}

@incollection{panikTheoremsAlternativeLinear1993,
  title = {Theorems of the {{Alternative}} for {{Linear Systems}}},
  booktitle = {Fundamentals of {{Convex Analysis}}: {{Duality}}, {{Separation}}, {{Representation}}, and {{Resolution}}},
  author = {Panik, Michael J.},
  editor = {Panik, Michael J.},
  year = {1993},
  series = {Theory and {{Decision Library}}},
  pages = {133--164},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-015-8124-0_6},
  url = {https://doi.org/10.1007/978-94-015-8124-0_6},
  urldate = {2022-07-29},
  abstract = {We now turn to an assortment of theorems which are generally characterized as ``theorems of the alternative'' or, in particular instances, so-called ``transposition theorems.'' Specifically, a theorem of the alternative involves two mutually exclusive systems of linear inequalities and/or equalities denoted simply as (I), (II). Moreover, it asserts that either system (I) has a solution or system (II) has a solution, but never both. A transposition theorem, which is a special type of theorem of the alternative, asserts for systems (I), (II) the disjoint alternatives of solvability or contradiction given that in one system a particular vector (usually the null vector) is a linear combination of vectors from the other.},
  isbn = {978-94-015-8124-0},
  langid = {english}
}

@misc{papachristodoulouSOSTOOLSSumsSquares2021,
  title = {{{SOSTOOLS Sums}} of {{Squares Optimization Toolbox}} for {{Matlab}}: {{User}}'s {{Guide}}},
  author = {Papachristodoulou, Antonis and Anderson, James and Valmborbida, Giorgio and Prajna, Stephen and Seiler, Peter and Parrilo, Pablo A. and Peet, Matthew P. and Jagt, Declan},
  year = {2021},
  month = sep,
  url = {https://github.com/oxfordcontrol/SOSTOOLS/blob/SOSTOOLS400/docs/sostools.pdf},
  howpublished = {University of Oxford Control Group}
}

@inproceedings{papachristodoulouTutorialSumSquares2005,
  title = {A Tutorial on Sum of Squares Techniques for Systems Analysis},
  booktitle = {Proceedings of the 2005 {{American Control Conference}}},
  author = {Papachristodoulou, A. and Prajna, S.},
  year = {2005},
  month = jun,
  pages = {2686-2700 vol. 4},
  publisher = {IEEE},
  address = {Portland, OR, USA},
  issn = {2378-5861},
  doi = {10.1109/ACC.2005.1470374},
  abstract = {This tutorial is about new system analysis techniques that were developed in the past few years based on the sum of squares decomposition. We present stability and robust stability analysis tools for different classes of systems: systems described by nonlinear ordinary differential equations or differential algebraic equations, hybrid systems with nonlinear subsystems and/or nonlinear switching surfaces, and time-delay systems described by nonlinear functional differential equations. We also discuss how different analysis questions such as model validation and safety verification can be answered for uncertain nonlinear and hybrid systems.}
}

@article{pappSumofSquaresOptimizationSemidefinite2019,
  title = {Sum-of-{{Squares Optimization}} without {{Semidefinite Programming}}},
  author = {Papp, D{\'a}vid and Yildiz, Sercan},
  year = {2019},
  month = jan,
  journal = {SIAM Journal on Optimization},
  volume = {29},
  number = {1},
  pages = {822--851},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1052-6234},
  doi = {10.1137/17M1160124},
  url = {https://epubs.siam.org/doi/abs/10.1137/17M1160124},
  urldate = {2021-04-22},
  abstract = {We propose a homogeneous primal-dual interior-point method to solve sum-of-squares optimization problems by combining nonsymmetric conic optimization techniques and polynomial interpolation. The approach optimizes directly over the sum-of-squares cone and its dual, circumventing the semidefinite programming (SDP) reformulation, which requires a large number of auxiliary variables when the degree of sum-of-squares polynomials is large. As a result, it has substantially lower theoretical time and space complexity than the conventional SDP-based approach as the degree increases. Although our approach avoids the SDP reformulation, an optimal solution to the semidefinite program can be recovered with little additional effort. Computational results confirm that the proposed method is several orders of magnitude faster than the SDP-based approach for optimization problems over high-degree sum-of-squares polynomials.}
}

@book{pardalosNonConvexMultiObjectiveOptimization2017,
  title = {Non-{{Convex Multi-Objective Optimization}}},
  author = {Pardalos, Panos M. and {\v Z}ilinskas, Antanas and {\v Z}ilinskas, Julius},
  year = {2017},
  month = aug,
  series = {Springer {{Optimization}} and {{Its Applications}}},
  number = {123},
  publisher = {Springer},
  address = {Cham},
  url = {https://doi.org/10.1007/978-3-319-61007-8},
  abstract = {Recent results on non-convex multi-objective optimization problems and methods are presented in this book, with particular attention to expensive black-box objective functions. Multi-objective optimization methods facilitate designers, engineers, and researchers to make decisions on appropriate trade-offs between various conflicting goals. A variety of deterministic and stochastic multi-objective optimization methods are developed in this book. Beginning with basic concepts and a review of non-convex single-objective optimization problems; this book moves on to cover multi-objective branch and bound algorithms, worst-case optimal algorithms (for Lipschitz functions and bi-objective problems), statistical models based algorithms, and probabilistic branch and bound approach. Detailed descriptions of new algorithms for non-convex multi-objective optimization, their theoretical substantiation, and examples for practical applications to the cell formation problem in manufacturing engineering, the process design in chemical engineering, and business process management are included to aide researchers and graduate students in mathematics, computer science, engineering, economics, and business management.},
  isbn = {978-3-319-61005-4},
  langid = {english}
}

@article{parikhProximalAlgorithms2014,
  title = {Proximal {{Algorithms}}},
  author = {Parikh, Neal and Boyd, Stephen},
  year = {2014},
  month = jan,
  journal = {Found. Trends Optim.},
  volume = {1},
  number = {3},
  pages = {127--239},
  issn = {2167-3888},
  doi = {10.1561/2400000003},
  url = {http://web.stanford.edu/~boyd/papers/prox_algs.html},
  urldate = {2019-03-05},
  abstract = {This monograph is about a class of optimization algorithms called proximal algorithms. Much like Newton's method is a standard tool for solving unconstrained smooth optimization problems of modest size, proximal algorithms can be viewed as an analogous tool for nonsmooth, constrained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical algorithms like Newton's method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed-form solutions or can be solved very quickly with standard or simple specialized methods. Here, we discuss the many different interpretations of proximal operators and algorithms, describe their connections to many other topics in optimization and applied mathematics, survey some popular algorithms, and provide a large number of examples of proximal operators that commonly arise in practice.}
}

@phdthesis{parriloStructuredSemidefinitePrograms2000,
  title = {Structured {{Semidefinite Programs}} and {{Semialgebraic Geometry Methods}} in {{Robustness}} and {{Optimization}}},
  author = {Parrilo, Pablo A.},
  year = {2000},
  address = {Pasadena, California},
  url = {http://www.mit.edu/~parrilo/pubs/files/thesis.pdf},
  school = {California Institute of Technology}
}

@phdthesis{parviniahmadiDistributedOptimizationControl2022,
  title = {Distributed {{Optimization}} for {{Control}} and {{Estimation}}},
  author = {Parvini Ahmadi, Shervin},
  year = {2022},
  month = mar,
  address = {Link{\"o}ping},
  doi = {10.3384/9789179291983},
  url = {http://liu.diva-portal.org/smash/record.jsf?pid=diva2%3A1632653&dswid=-595},
  urldate = {2023-01-13},
  langid = {english},
  school = {Link{\"o}ping University}
}

@article{petersonReviewConstraintQualifications1973,
  title = {A {{Review}} of {{Constraint Qualifications}} in {{Finite-Dimensional Spaces}}},
  author = {Peterson, David W.},
  year = {1973},
  month = jul,
  journal = {SIAM Review},
  volume = {15},
  number = {3},
  pages = {639--654},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/1015075},
  url = {https://epubs.siam.org/doi/abs/10.1137/1015075},
  urldate = {2022-03-25},
  abstract = {Constraint qualifications used in conjunction with the Kuhn--Tucker necessary conditions in the solution of a nonlinear programming problem are collected from numerous sources and interrelated as a lattice on the basis of their mutual implications. Several new implications are described, a hitherto open question is resolved, an error is corrected, several new constraint qualifications are identified, and examples are provided to verify the absence of implication in each instance where no implication is indicated in the lattice.}
}

@book{pistikopoulosMultiparametricOptimizationControl2020,
  title = {Multi-Parametric {{Optimization}} and {{Control}}},
  author = {Pistikopoulos, Efstratios N. and Diangelakis, Nikolaos A. and Oberdieck, Richard},
  year = {2020},
  month = nov,
  series = {Wiley {{Series}} in {{Operations Research}} and {{Management Science}}},
  publisher = {Wiley},
  url = {https://www.wiley.com/en-us/Multi+parametric+Optimization+and+Control-p-9781119265184},
  urldate = {2022-03-24},
  abstract = {Recent developmentsinmulti-parametric optimization and control Multi-Parametric Optimization and Controlprovidescomprehensive coverageof recent methodological developments for optimal model-based control through parametric optimization.It also sharesreal-world research applicationsto support deeper understanding of the material. Researchers and practitionerscan use the book as reference. It is also suitable as a primary or a supplementary textbook.Each chapterlooks at the theories related to atopic along with a relevant case study. Topiccomplexity increasesgradually as readersprogressthrough the chapters. Thefirst part of the bookpresents an overview of the state-of-the-art multi-parametric optimization theory and algorithms in multi-parametric programming.The secondexaminesthe connection between multi-parametric programming and model-predictive controlfrom the linear quadratic regulator over hybrid systems to periodic systems and robust control. The third part of the bookaddressesmulti-parametric optimization in process systems engineering.Astep-by-step procedureis introducedforembeddingtheprogramming withinthesystem engineering, which leadsthereader intothetopic of thePAROC framework and software platform. PAROCis an integrated framework and platform for the optimization and advanced model-based control of process systems. Uses casestudiestoillustrate real-world applicationsfora better understanding of theconcepts presented Covers the fundamentals of optimization and model predictive control Provides information on key topics, such as the basic sensitivity theorem, linear programming, quadratic programming, mixed-integer linear programming, optimal control of continuous systems, and multi-parametric optimal control Anappendixsummarizesthe history of multi-parametric optimization algorithms. It also coversthe use of the parametric optimization toolbox (POP), which iscomprehensive softwareforefficiently solving multi-parametric programming problems.},
  isbn = {978-1-119-26518-4},
  langid = {american}
}

@article{polikSurveySLemma2007,
  title = {A {{Survey}} of the {{S-Lemma}}},
  author = {P{\'o}lik, Imre and Terlaky, Tam{\'a}s},
  year = {2007},
  month = jan,
  journal = {SIAM Review},
  volume = {49},
  number = {3},
  pages = {371--418},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/S003614450444614X},
  url = {https://epubs.siam.org/doi/abs/10.1137/S003614450444614X},
  urldate = {2022-07-28},
  abstract = {In this survey we review the many faces of the S-lemma, a result about the correctness of the S-procedure. The basic idea of this widely used method came from control theory but it has important consequences in quadratic and semidefinite optimization, convex geometry, and linear algebra as well. These were all active research areas, but as there was little interaction between researchers in these different areas, their results remained mainly isolated. Here we give a unified analysis of the theory by providing three different proofs for the S-lemma and revealing hidden connections with various areas of mathematics. We prove some new duality results and present applications from control theory, error estimation, and computational geometry.}
}

@unpublished{powellModernApproachTeaching2023,
  type = {Lecture Notes},
  title = {A {{Modern Approach}} to {{Teaching Introduction}} to {{Optimization}}},
  author = {Powell, Warren B},
  year = {2023},
  month = nov,
  url = {https://castle.princeton.edu/wp-content/uploads/2023/11/Powell-Modern-approach-to-teaching-optimization_submitted-November-9-2023.pdf},
  abstract = {Optimization'' is widely taught in departments such as operations research, industrial engineering, and sometimes applied math, as focusing on complex, multidimensional (and often very high-dimensional) problems that can be formulated as linear, nonlinear or integer programs. Introductory courses are often centered on linear programming, the simplex algorithm and duality theory.},
  langid = {english}
}

@article{powellWeNeedModernize2024,
  title = {We Need to Modernize How We Introduce Students to Optimization},
  author = {Powell, Warren B.},
  year = {2024},
  journal = {OR/MS Today Informs},
  number = {2},
  pages = {24--28},
  url = {https://castle.princeton.edu/wp-content/uploads/2024/06/Powell-Teaching_Optimization_Powell_Feature-ORMS-Today-June-7-2024.pdf},
  urldate = {2024-06-15}
}

@inproceedings{prajnaIntroducingSOSTOOLSGeneral2002,
  title = {Introducing {{SOSTOOLS}}: A General Purpose Sum of Squares Programming Solver},
  shorttitle = {Introducing {{SOSTOOLS}}},
  booktitle = {Proceedings of the 41st {{IEEE Conference}} on {{Decision}} and {{Control}}, 2002.},
  author = {Prajna, S. and Papachristodoulou, A. and Parrilo, P.A.},
  year = {2002},
  month = dec,
  volume = {1},
  pages = {741-746 vol.1},
  issn = {0191-2216},
  doi = {10.1109/CDC.2002.1184594},
  abstract = {SOSTOOLS is a MATLAB toolbox for constructing and solving sum of squares programs. It can be used in combination with semidefinite programming software, such as SeDuMi, to solve many continuous and combinatorial optimization problems, as well as various control-related problems. The paper provides an overview on sum of squares programming, describes the primary features of SOSTOOLS, and shows how SOSTOOLS is used to solve sum of squares programs. Some applications from different areas are presented to show the wide applicability of sum of squares programming in general and SOSTOOLS in particular.}
}

@inproceedings{rabbatDistributedOptimizationSensor2004,
  title = {Distributed {{Optimization}} in {{Sensor Networks}}},
  booktitle = {Proceedings of the 3rd {{International Symposium}} on {{Information Processing}} in {{Sensor Networks}}},
  author = {Rabbat, Michael and Nowak, Robert},
  year = {2004},
  series = {{{IPSN}} '04},
  pages = {20--27},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/984622.984626},
  url = {http://doi.acm.org/10.1145/984622.984626},
  urldate = {2014-01-13},
  abstract = {Wireless sensor networks are capable of collecting an enormous amount of data over space and time. Often, the ultimate objective is to derive an estimate of a parameter or function from these data. This paper investigates a general class of distributed algorithms for "in-network" data processing, eliminating the need to transmit raw data to a central point. This can provide significant reductions in the amount of communication and energy required to obtain an accurate estimate. The estimation problems we consider are expressed as the optimization of a cost function involving data from all sensor nodes. The distributed algorithms are based on an incremental optimization process. A parameter estimate is circulated through the network, and along the way each node makes a small adjustment to the estimate based on its local data. Applying results from the theory of incremental subgradient optimization, we show that for a broad class of estimation problems the distributed algorithms converge to within an e-ball around the globally optimal value. Furthermore, bounds on the number incremental steps required for a particular level of accuracy provide insight into the trade-off between estimation performance and communication overhead. In many realistic scenarios, the distributed algorithms are much more efficient, in terms of energy and communications, than centralized estimation schemes. The theory is verified through simulated applications in robust estimation, source localization, cluster analysis and density estimation.},
  isbn = {1-58113-846-6}
}

@article{raghunathanMathematicalProgramsEquilibrium2003,
  title = {Mathematical Programs with Equilibrium Constraints ({{MPECs}}) in Process Engineering},
  author = {Raghunathan, Arvind U and Biegler, Lorenz T},
  year = {2003},
  month = oct,
  journal = {Computers \& Chemical Engineering},
  volume = {27},
  number = {10},
  pages = {1381--1392},
  issn = {0098-1354},
  doi = {10.1016/S0098-1354(03)00092-9},
  url = {https://www.sciencedirect.com/science/article/pii/S0098135403000929},
  urldate = {2022-03-22},
  abstract = {Mathematical programs with equilibrium constraints (MPECs) form a relatively new and interesting subclass of nonlinear programming problems. In this paper we propose a novel method of solving MPECs by appropriate reformulation of the equilibrium conditions. The reformulation can be easily incorporated in a certain class of interior point algorithms for nonlinear optimization. The algorithm used in the study follows a primal-dual interior point approach and shows encouraging results on a test suite of MPECs. The algorithm is also able to perform optimization of distillation columns with phase changes and tray optimization using only continuous variables. We also consider a number of topics to improve performance of the algorithm and to identify classes of process engineering problems that can be handled as MPECs.},
  langid = {english}
}

@unpublished{ramosMathematicalProgramsEquilibrium2017,
  title = {Mathematical {{Programs}} with {{Equilibrium Constraints}}: {{A}} Sequential Optimality Condition, New Constraint Qualifications and Algorithmic Consequences},
  author = {Ramos, A.},
  year = {2017},
  month = may,
  address = {Department of Mathematics, Federal University of Paran{\' }a, Curitiba, PR, Brazil.},
  url = {http://www.optimization-online.org/DB_FILE/2016/04/5423.pdf},
  abstract = {Mathematical programs with equilibrium (or complementarity) constraints, MPECs for short, are a dif- ficult class of constrained optimization problems. The feasible set has a very special structure and violates most of the standard constraint qualifications (CQs). Thus, the Karush-Kuhn-Tucker (KKT) conditions are not necessarily satisfied by minimizers and the convergence assumptions of many methods for solving constrained optimization problems are not fulfilled. Therefore it is necessary, both from a theoretical and numerical point of view, to consider suitable optimality conditions, tailored CQs and specially designed al- gorithms for solving MPECs. In this paper, we present a new sequential optimality condition useful for the convergence analysis for several methods of solving MPECs, such as relaxations schemes, complementarity- penalty methods and interior-relaxation methods. We also introduce a variant of the augmented Lagrangian method for solving MPEC whose stopping criterion is based on this sequential condition and it has strong convergence properties. Furthermore, a new CQ for M-stationary which is weaker than the recently intro- duced MPEC relaxed constant positive linear dependence (MPEC-RCPLD) associated to such sequential condition is presented. Relations between the old and new CQs as well as the algorithmic consequences will be discussed.}
}

@article{rebennackPiecewiseLinearFunction2020,
  title = {Piecewise {{Linear Function Fitting}} via {{Mixed-Integer Linear Programming}}},
  author = {Rebennack, Steffen and Krasko, Vitaliy},
  year = {2020},
  month = apr,
  journal = {INFORMS Journal on Computing},
  volume = {32},
  number = {2},
  pages = {507--530},
  publisher = {INFORMS},
  issn = {1091-9856},
  doi = {10.1287/ijoc.2019.0890},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/ijoc.2019.0890},
  urldate = {2024-08-14},
  abstract = {Piecewise linear (PWL) functions are used in a variety of applications. Computing such continuous PWL functions, however, is a challenging task. Software packages and the literature on PWL function fitting are dominated by heuristic methods. This is true for both fitting discrete data points and continuous univariate functions. The only exact methods rely on nonconvex model formulations. Exact methods compute continuous PWL function for a fixed number of breakpoints minimizing some distance function between the original function and the PWL function. An optimal PWL function can only be computed if the breakpoints are allowed to be placed freely and are not fixed to a set of candidate breakpoints. In this paper, we propose the first convex model for optimal continuous univariate PWL function fitting. Dependent on the metrics chosen, the resulting formulations are either mixed-integer linear programming or mixed-integer quadratic programming problems. These models yield optimal continuous PWL functions for a set of discrete data. On the basis of these convex formulations, we further develop an exact algorithm to fit continuous univariate functions. Computational results for benchmark instances from the literature demonstrate the superiority of the proposed convex models compared with state-of-the-art nonconvex models.}
}

@misc{ReferenceManualArtelys,
  title = {Reference Manual --- {{Artelys Knitro}} 14.0 {{User}}'s {{Manual}}},
  year = {2023},
  url = {https://www.artelys.com/app/docs/knitro/3_referenceManual.html#chap-referencemanual},
  urldate = {2024-01-27}
}

@article{regisPropertiesPositiveSpanning2016,
  title = {On the Properties of Positive Spanning Sets and Positive Bases},
  author = {Regis, Rommel G.},
  year = {2016},
  month = mar,
  journal = {Optimization and Engineering},
  volume = {17},
  number = {1},
  pages = {229--262},
  issn = {1573-2924},
  doi = {10.1007/s11081-015-9286-x},
  url = {https://doi.org/10.1007/s11081-015-9286-x},
  urldate = {2019-05-03},
  abstract = {The concepts of positive span and positive basis are important in derivative-free optimization. In fact, a well-known result is that if the gradient of a continuously differentiable objective function on {$\mathbb{R}n$}Rn{\textbackslash}mathbb\{R\}{\textasciicircum}n is nonzero at a point, then one of the vectors in any positive basis (or any positive spanning set) of {$\mathbb{R}n$}Rn{\textbackslash}mathbb\{R\}{\textasciicircum}n is a descent direction for the objective function from that point. This article summarizes the basic results and explores additional properties of positive spanning sets, positively independent sets and positive bases that are potentially useful in the design of derivative-free optimization algorithms. In particular, it provides construction procedures for these special sets of vectors that were not previously mentioned in the literature. It also proves that invertible linear transformations preserve positive independence and the positive spanning property. Moreover, this article introduces the notion of linear equivalence between positive spanning sets and between positively independent sets to simplify the analysis of their structures. Linear equivalence turns out to be a generalization of the concept of structural equivalence between positive bases that was introduced by Coope and Price (SIAM J Optim 11:859--869, 2001). Furthermore, this article clarifies which properties of linearly independent sets, spanning sets and ordinary bases carry over to positively independent sets, positive spanning sets, and positive bases. For example, a linearly independent set can always be extended to a basis of a linear space but a positively independent set cannot always be extended to a positive basis. Also, the maximum size of a linearly independent set in {$Rn$}RnR{\textasciicircum}n is n but there is no limit to the size of a positively independent set in {$\mathbb{R}n$}Rn{\textbackslash}mathbb\{R\}{\textasciicircum}n when {$n\geq$}3n{$\geq$}3n {\textbackslash}ge 3. Whenever possible, the results are proved for the more general case of frames of convex cones instead of focusing only on positive bases of linear spaces. In addition, this article discusses some algorithms for determining whether a given set of vectors is positively independent or whether it positively spans a linear subspace of {$\mathbb{R}n$}Rn{\textbackslash}mathbb\{R\}{\textasciicircum}n. Finally, it provides an algorithm for extending any finite set of vectors to a positive spanning set of {$\mathbb{R}n$}Rn{\textbackslash}mathbb\{R\}{\textasciicircum}n using only a relatively small number of additional vectors.},
  langid = {english}
}

@book{renegarMathematicalViewInteriorPoint2001,
  title = {A {{Mathematical View}} of {{Interior-Point Methods}} in {{Convex Optimization}}},
  author = {Renegar, James},
  year = {2001},
  month = jan,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898718812},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9780898718812},
  urldate = {2022-01-26},
  abstract = {This book aims at developing a thorough understanding of the most general theory for interior-point methods, a class of algorithms for convex optimization problems. The study of these algorithms has dominated the continuous optimization literature for nearly 15 years, beginning with the paper by Karmarkar [10]. In that time, the theory has matured tremendously, perhaps most notably due to the path-breaking, broadly encompassing, and enormously influential work of Nesterov and Nemirovskii [15]. Much of the literature on the general theory of interior-point methods is difficult to understand, even for specialists. My hope is that this book will make the most general theory accessible to a wide audience---especially Ph.D. students, the next generation of optimizers. The book might be used for graduate courses in optimization, both in mathematics and engineering departments. Moreover, it is self-contained and thus appropriate for individual study by students as well as by seasoned researchers who wish to better assimilate the most general interior-point method theory. This book grew out of my lecture notes for the Ph.D. course on interior-point methods at Cornell University. The impetus for writing the book came from an invitation to give the annual lecture series at the Center for Operations Research and Econometrics (CORE) at Louvain-la-Neuve, Belgium, in fall, 1996. The book is being published by CORE as well as in the SIAM-MPS series. The writing of this book has been a particularly satisfying experience. It has brought into sharp focus the beauty and coherence of the interior-point method theory as developed and influenced through the efforts of many researchers. I hope those researchers will not be offended by my choice to cite few references (and I hope they largely agree that the ones I have cited are the ones that should be cited, given the material covered herein). The reader can look to recent articles and books for extensive bibliographies (cf. [21], [22], [24], [25]); a great place to stay up to date is Interior-Point Methods Online (http://www-unix.mcs.anl.gov/otc/InteriorPoint/). In citing few references, my intent is not to give the impression of research originality. Indeed, most of the results in this monograph are undoubtedly embedded somewhere in [15], [16], and [17] (which in turn were influenced by other works). My only claim to originality is in presenting a simplifying perspective.},
  isbn = {978-0-89871-502-6}
}

@book{rockafellarConvexAnalysis1970,
  title = {Convex {{Analysis}}},
  author = {Rockafellar, R. Tyrrell},
  year = {1970},
  series = {Princeton {{Mathematical Series}}},
  edition = {Tenth printing, 1997},
  publisher = {Princeton University Press},
  url = {https://press.princeton.edu/books/paperback/9780691015866/convex-analysis},
  abstract = {Available for the first time in paperback, R. Tyrrell Rockafellar's classic study presents readers with a coherent branch of nonlinear mathematical analysis that is especially suited to the study of optimization problems. Rockafellar's theory differs from classical analysis in that differentiability assumptions are replaced by convexity assumptions. The topics treated in this volume include: systems of inequalities, the minimum or maximum of a convex function over a convex set, Lagrange multipliers, minimax theorems and duality, as well as basic results about the structure of convex sets and the continuity and differentiability of convex functions and saddle- functions. This book has firmly established a new and vital area not only for pure mathematics but also for applications to economics and engineering. A sound knowledge of linear algebra and introductory real analysis should provide readers with sufficient background for this book. There is also a guide for the reader who may be using the book as an introduction, indicating which parts are essential and which may be skipped on a first reading.},
  googlebooks = {1TiOka9bx3sC},
  isbn = {978-0-691-01586-6},
  langid = {english}
}

@book{rockafellarVariationalAnalysis1997,
  title = {Variational {{Analysis}}},
  author = {Rockafellar, R. Tyrrell and Wets, Roger J.-B. and Wets, Maria},
  year = {1998},
  series = {Grundlehren Der Mathematischen {{Wissenschaften}}},
  edition = {Corrected 3rd printing 2009},
  number = {317},
  publisher = {Springer},
  address = {Berlin; Heidelberg},
  url = {https://doi.org/10.1007/978-3-642-02431-3},
  isbn = {978-3-540-62772-2},
  langid = {english}
}

@book{roosTheoryAlgorithmsLinear1997,
  title = {Theory and {{Algorithms}} for {{Linear Optimization}}: {{An Interior Point Approach}}},
  shorttitle = {Theory and {{Algorithms}} for {{Linear Optimization}}},
  author = {Roos, C. and Terlaky, T. and Vial, J.-Ph},
  year = {1997},
  month = feb,
  publisher = {Wiley},
  address = {Chichester Weinheim},
  url = {https://www.wiley.com/en-ie/Theory+and+Algorithms+for+Linear+Optimization%3A+An+Interior+Point+Approach-p-9780470865934},
  isbn = {978-0-471-95676-1},
  langid = {english}
}

@book{rossiHandbookConstraintProgramming2006,
  title = {Handbook of {{Constraint Programming}}},
  editor = {Rossi, Francesca and van Beek, Peter and Walsh, Toby},
  year = {2006},
  month = oct,
  series = {Foundations of {{Artificial Intelligence}}},
  publisher = {Elsevier Science},
  address = {Amsterdam Heidelberg},
  url = {https://www.sciencedirect.com/bookseries/foundations-of-artificial-intelligence/vol/2/suppl/C},
  abstract = {Constraint programming is a powerful paradigm for solving combinatorial search problems that draws on a wide range of techniques from artificial intelligence, computer science, databases, programming languages, and operations research. Constraint programming is currently applied with success to many domains, such as scheduling, planning, vehicle routing, configuration, networks, and bioinformatics.The aim of this handbook is to capture the full breadth and depth of the constraint programming field and to be encyclopedic in its scope and coverage. While there are several excellent books on constraint programming, such books necessarily focus on the main notions and techniques and cannot cover also extensions, applications, and languages. The handbook gives a reasonably complete coverage of all these lines of work, based on constraint programming, so that a reader can have a rather precise idea of the whole field and its potential. Of course each line of work is dealt with in a survey-like style, where some details may be neglected in favor of coverage. However, the extensive bibliography of each chapter will help the interested readers to find suitable sources for the missing details. Each chapter of the handbook is intended to be a self-contained survey of a topic, and is written by one or more authors who are leading researchers in the area.The intended audience of the handbook is researchers, graduate students, higher-year undergraduates and practitioners who wish to learn about the state-of-the-art in constraint programming. No prior knowledge about the field is necessary to be able to read the chapters and gather useful knowledge. Researchers from other fields should find in this handbook an effective way to learn about constraint programming and to possibly use some of the constraint programming concepts and techniques in their work, thus providing a means for a fruitful cross-fertilization among different research areas.The handbook is organized in two parts. The first part covers the basic foundations of constraint programming, including the history, the notion of constraint propagation, basic search methods, global constraints, tractability and computational complexity, and important issues in modeling a problem as a constraint problem. The second part covers constraint languages and solver, several useful extensions to the basic framework (such as interval constraints, structured domains, and distributed CSPs), and successful application areas for constraint programming.- Covers the whole field of constraint programming- Survey-style chapters- Five chapters on applications},
  isbn = {978-0-444-52726-4},
  langid = {english}
}

@book{roubicekRelaxationOptimizationTheory2020,
  title = {Relaxation in {{Optimization Theory}} and {{Variational Calculus}}},
  author = {Roub{\'i}{\v c}ek, Tom{\'a}{\v s}},
  year = {2020},
  month = dec,
  series = {De {{Gruyter Series}} in {{Nonlinear Analysis}} and {{Applications}}},
  edition = {2nd Revised and Extended edition},
  volume = {4},
  publisher = {De Gruyter},
  address = {Berlin ; Boston},
  url = {https://doi.org/10.1515/9783110590852},
  abstract = {The relaxation method has enjoyed an intensive development during many decades and this new edition of this comprehensive text reflects in particular the main achievements in the past 20 years. Moreover, many further improvements and extensions are included, both in the direction of optimal control and optimal design as well as in numerics and applications in materials science, along with an updated treatment of the abstract parts of the theory.},
  isbn = {978-3-11-058962-7},
  langid = {english}
}

@book{ryuLargeScaleConvexOptimization2022,
  title = {Large-{{Scale Convex Optimization}}: {{Algorithms}} \& {{Analyses}} via {{Monotone Operators}}},
  shorttitle = {Large-{{Scale Convex Optimization}}},
  author = {Ryu, Ernest K. and Yin, Wotao},
  year = {2022},
  month = dec,
  edition = {New edition},
  publisher = {Cambridge University Press},
  address = {Cambridge, United Kingdom New York, NY, USA Port Melbourne, VIC, Australia New Delhi, India Singapore},
  url = {https://doi.org/10.1017/9781009160865},
  abstract = {Starting from where a first course in convex optimization leaves off, this text presents a unified analysis of first-order optimization methods -- including parallel-distributed algorithms -- through the abstraction of monotone operators. With the increased computational power and availability of big data over the past decade, applied disciplines have demanded that larger and larger optimization problems be solved. This text covers the first-order convex optimization methods that are uniquely effective at solving these large-scale optimization problems. Readers will have the opportunity to construct and analyze many well-known classical and modern algorithms using monotone operators, and walk away with a solid understanding of the diverse optimization algorithms. Graduate students and researchers in mathematical optimization, operations research, electrical engineering, statistics, and computer science will appreciate this concise introduction to the theory of convex optimization algorithms.},
  isbn = {978-1-00-916085-8},
  langid = {english}
}

@inproceedings{saundersonSemidefiniteRelaxationsOptimization2014,
  title = {Semidefinite Relaxations for Optimization Problems over Rotation Matrices},
  booktitle = {53rd {{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Saunderson, James and Parrilo, Pablo A. and Willsky, Alan S.},
  year = {2014},
  month = dec,
  pages = {160--166},
  issn = {0191-2216},
  doi = {10.1109/CDC.2014.7039375},
  abstract = {Optimization problems with variables constrained to be in SO(d)-orthogonal matrices with determinant one-arise in attitude estimation, molecular imaging, and computer vision applications, among others. Recently it has been shown that the convex hull of SO(d) can be described in terms of linear matrix inequalities. This allows us to devise new semidefinite programming-based reformulations and relaxations of problems involving rotation matrices. In this paper we illustrate the use of these techniques for two different types of optimization problems over SO(d). The first type of problem arises in jointly estimating the attitude and spin-rate of a spin-stabilized satellite. We show how to exactly reformulate such problems as semidefinite programs. The second type of problem arises when estimating the orientations of a network of objects (such as cameras, satellites or molecules) from noisy relative orientation measurements. For this class of problems we formulate new semidefinite relaxations that are tighter than those existing in the literature, and show that they are exact when the underlying graph is a tree.}
}

@book{sawaragiTheoryMultiobjectiveOptimization1985,
  title = {Theory of {{Multiobjective Optimization}}},
  author = {Sawaragi, Yoshikazu and Nakayama, Hirotaka and Tanino, Tetsuzo},
  year = {1985},
  month = sep,
  series = {Mathematics in {{Science}} and {{Engineering}}},
  number = {176},
  publisher = {Academic Press},
  address = {Orlando},
  abstract = {In this book, we study theoretical and practical aspects of computing methods for mathematical modelling of nonlinear systems. A number of computing techniques are considered, such as methods of operator approximation with any given accuracy; operator interpolation techniques including a non-Lagrange interpolation; methods of system representation subject to constraints associated with concepts of causality, memory and stationarity; methods of system representation with an accuracy that is the best within a given class of models; methods of covariance matrix estimation;methods for low-rank matrix approximations; hybrid methods based on a combination of iterative procedures and best operator approximation; andmethods for information compression and filtering under condition that a filter model should satisfy restrictions associated with causality and different types of memory.As a result, the book represents a blend of new methods in general computational analysis,and specific, but also generic, techniques for study of systems theory ant its particularbranches, such as optimal filtering and information compression.},
  isbn = {978-0-12-620370-7}
}

@book{shapiroLecturesStochasticProgramming2021,
  title = {Lectures on {{Stochastic Programming}}: {{Modeling}} and {{Theory}}},
  shorttitle = {Lectures on {{Stochastic Programming}}},
  author = {Shapiro, Alexander and Dentcheva, Darinka and Ruszczski, Andrzej},
  year = {1 kv{\v e}tna 2021},
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  edition = {2},
  publisher = {SIAM},
  address = {Philadelphia},
  url = {https://doi.org/10.1137/1.9781611976595},
  abstract = {Lectures on Stochastic Programming: Modeling and Theory, Third Edition covers optimization problems involving uncertain parameters for which stochastic models are available. These problems occur in almost all areas of science and engineering.This substantial revision of the previous edition presents a modern theory of stochastic programming, including expanded coverage of sample complexity, risk measures, and distributionally robust optimization:Chapter 6 is updated and the interchangeability principle for risk measures is discussed in detail.Two new chapters, 'Distributionally Robust Stochastic Programming' (DRSP) and 'Computational Methods' provide readers with a solid understanding of emerging topics.Chapter 8 presents new material on formulation and numerical approaches to solving periodical multistage stochastic programs.This book is written for researchers and graduate students working on theory and applications of optimization.},
  isbn = {978-1-61197-658-8}
}

@article{sheraliGlobalOptimizationAlgorithm1992,
  title = {A Global Optimization Algorithm for Polynomial Programming Problems Using a {{Reformulation-Linearization Technique}}},
  author = {Sherali, Hanif D. and Tuncbilek, Cihan H.},
  year = {1992},
  month = mar,
  journal = {Journal of Global Optimization},
  volume = {2},
  number = {1},
  pages = {101--112},
  issn = {1573-2916},
  doi = {10.1007/BF00121304},
  url = {https://doi.org/10.1007/BF00121304},
  urldate = {2024-09-06},
  abstract = {This paper is concerned with the development of an algorithm to solve continuous polynomial programming problems for which the objective function and the constraints are specified polynomials. A linear programming relaxation is derived for the problem based on a Reformulation Linearization Technique (RLT), which generates nonlinear (polynomial) implied constraints to be included in the original problem, and subsequently linearizes the resulting problem by defining new variables, one for each distinct polynomial term. This construct is then used to obtain lower bounds in the context of a proposed branch and bound scheme, which is proven to converge to a global optimal solution. A numerical example is presented to illustrate the proposed algorithm.},
  langid = {english}
}

@article{sheraliRLTUnifiedApproach2007,
  title = {{{RLT}}: {{A}} Unified Approach for Discrete and Continuous Nonconvex Optimization},
  shorttitle = {{{RLT}}},
  author = {Sherali, Hanif D.},
  year = {2007},
  month = feb,
  journal = {Annals of Operations Research},
  volume = {149},
  number = {1},
  pages = {185--193},
  issn = {1572-9338},
  doi = {10.1007/s10479-006-0107-7},
  url = {https://doi.org/10.1007/s10479-006-0107-7},
  urldate = {2024-09-06},
  langid = {english}
}

@book{shohatProblemMoments1943,
  title = {The {{Problem}} of {{Moments}}},
  author = {Shohat, J. A. and Tamarkin, J. D.},
  year = {1943},
  month = dec,
  edition = {4th edition},
  publisher = {American Mathematical Society},
  address = {Providence, R.I.},
  abstract = {The book was first published in 1943 and then was reprinted several times with corrections. It presents the development of the classical problem of moments for the first 50 years, after its introduction by Stieltjes in the 1890s. In addition to initial developments by Stieltjes, Markov, and Chebyshev, later contributions by Hamburger, Nevanlinna, Hausdorff, Stone, and others are discussed. The book also contains some results on the trigonometric moment problem and a chapter devoted to approximate quadrature formulas.},
  isbn = {978-0-8218-1501-4},
  langid = {english}
}

@article{scheelMathematicalProgramsComplementarity2000,
  title = {Mathematical {{Programs}} with {{Complementarity Constraints}}: {{Stationarity}}, {{Optimality}}, and {{Sensitivity}}},
  shorttitle = {Mathematical {{Programs}} with {{Complementarity Constraints}}},
  author = {Scheel, Holger and Scholtes, Stefan},
  year = {2000},
  month = feb,
  journal = {Mathematics of Operations Research},
  volume = {25},
  number = {1},
  pages = {1--22},
  issn = {0364-765X},
  abstract = {We study mathematical programs with complementarity constraints. Several stationarity concepts, based on a piecewise smooth formulation, are presented and compared. The concepts are related to stationarity conditions for certain smooth programs as well as to stationarity concepts for a nonsmooth exact penalty function. Further, we present Fiacco-McCormick type second order optimality conditions and an extension of the stability results of Robinson and Kojima to mathematical programs with complementarity constraints.}
}

@unpublished{schererLinearMatrixInequalities2015,
  type = {Lecture Notes},
  title = {Linear Matrix Inequalities in Control},
  author = {Scherer, Carsten W. and Weiland, Siep},
  year = {2015},
  month = jan,
  url = {https://www.imng.uni-stuttgart.de/mst/files/LectureNotes.pdf},
  urldate = {2021-04-16},
  annotation = {00492}
}

@unpublished{schmidtGentleIncompleteIntroduction2021,
  title = {A {{Gentle}} and {{Incomplete Introduction}} to {{Bilevel Optimization}}},
  author = {Schmidt, Martin and Beck, Yasmine},
  year = {2021},
  month = nov,
  url = {http://www.optimization-online.org/DB_FILE/2021/06/8450.pdf}
}

@book{schrijverCombinatorialOptimizationPolyhedra2002,
  title = {Combinatorial {{Optimization}}: {{Polyhedra}} and {{Efficiency}}},
  shorttitle = {Combinatorial {{Optimization}}},
  author = {Schrijver, Alexander},
  year = {10 prosince 2002},
  series = {Algorithms and {{Combinatorics}}},
  volume = {24},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  url = {https://link.springer.com/book/9783540443896},
  abstract = {This book offers an in-depth overview of polyhedral methods and efficient algorithms in combinatorial optimization.These methods form a broad, coherent and powerful kernel in combinatorial optimization, with strong links to discrete mathematics, mathematical programming and computer science. In eight parts, various areas are treated, each starting with an elementary introduction to the area, with short, elegant proofs of the principal results, and each evolving to the more advanced methods and results, with full proofs of some of the deepest theorems in the area. Over 4000 references to further research are given, and historical surveys on the basic subjects are presented.},
  isbn = {978-3-540-44389-6}
}

@misc{schwanPIQPProximalInteriorPoint2023,
  title = {{{PIQP}}: {{A Proximal Interior-Point Quadratic Programming Solver}}},
  shorttitle = {{{PIQP}}},
  author = {Schwan, Roland and Jiang, Yuning and Kuhn, Daniel and Jones, Colin N.},
  year = {2023},
  month = sep,
  number = {arXiv:2304.00290},
  eprint = {2304.00290},
  primaryclass = {math},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2304.00290},
  urldate = {2024-01-18},
  abstract = {This paper presents PIQP, a high-performance toolkit for solving generic sparse quadratic programs (QP). Combining an infeasible Interior Point Method (IPM) with the Proximal Method of Multipliers (PMM), the algorithm can handle ill-conditioned convex QP problems without the need for linear independence of the constraints. The open-source implementation is written in C++ with interfaces to C, Python, Matlab, and R leveraging the Eigen3 library. The method uses a pivoting-free factorization routine and allocation-free updates of the problem data, making the solver suitable for embedded applications. The solver is evaluated on the Maros-M{\textbackslash}'esz{\textbackslash}'aros problem set and optimal control problems, demonstrating state-of-the-art performance for both small and large-scale problems, outperforming commercial and open-source solvers.},
  archiveprefix = {arXiv}
}

@book{sokolowskiIntroductionShapeOptimization1992,
  title = {Introduction to {{Shape Optimization}}: {{Shape Sensitivity Analysis}}},
  shorttitle = {Introduction to {{Shape Optimization}}},
  author = {Sokolowski, Jan and Zolesio, Jean-Paul},
  year = {1992},
  month = may,
  series = {Springer {{Series}} in {{Computational Mathematics}}},
  number = {16},
  publisher = {Springer},
  address = {Berlin ; New York},
  url = {https://link.springer.com/book/10.1007/978-3-642-58106-9},
  abstract = {This book is motivated largely by a desire to solve shape optimization prob\- lems that arise in applications, particularly in structural mechanics and in the optimal control of distributed parameter systems. Many such problems can be formulated as the minimization of functionals defined over a class of admissible domains. Shape optimization is quite indispensable in the design and construction of industrial structures. For example, aircraft and spacecraft have to satisfy, at the same time, very strict criteria on mechanical performance while weighing as little as possible. The shape optimization problem for such a structure consists in finding a geometry of the structure which minimizes a given functional (e. g. such as the weight of the structure) and yet simultaneously satisfies specific constraints (like thickness, strain energy, or displacement bounds). The geometry of the structure can be considered as a given domain in the three-dimensional Euclidean space. The domain is an open, bounded set whose topology is given, e. g. it may be simply or doubly connected. The boundary is smooth or piecewise smooth, so boundary value problems that are defined in the domain and associated with the classical partial differential equations of mathematical physics are well posed. In general the cost functional takes the form of an integral over the domain or its boundary where the integrand depends smoothly on the solution of a boundary value problem.},
  isbn = {978-3-540-54177-6},
  langid = {english}
}

@incollection{solodovConstraintQualifications2011,
  title = {Constraint {{Qualifications}}},
  booktitle = {Wiley {{Encyclopedia}} of {{Operations Research}} and {{Management Science}}},
  author = {Solodov, Mikhail V.},
  year = {2011},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9780470400531.eorms0978},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470400531.eorms0978},
  urldate = {2022-03-25},
  abstract = {We discuss assumptions on the constraint functions that allow constructive description of the geometry of a given set around a given point in terms of the constraints derivatives. Consequences for characterizing solutions of variational and optimization problems are discussed. In the optimization case, these include primal and primal-dual first- and second-order necessary optimality conditions.},
  isbn = {978-0-470-40053-1},
  langid = {english}
}

@article{solodovNewProjectionMethod1999,
  title = {A {{New Projection Method}} for {{Variational Inequality Problems}}},
  author = {Solodov, M. V. and Svaiter, B. F.},
  year = {1999},
  month = jan,
  journal = {SIAM Journal on Control and Optimization},
  volume = {37},
  number = {3},
  pages = {765--776},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0363-0129},
  doi = {10.1137/S0363012997317475},
  url = {https://epubs.siam.org/doi/10.1137/S0363012997317475},
  urldate = {2025-01-02},
  abstract = {We consider the forward-backward splitting method for finding a zero of the sum of two maximal monotone mappings. This method is known to converge when the inverse of the forward mapping is strongly monotone. We propose a modification to this method, in the spirit of the extragradient method for monotone variational inequalities, under which the method converges assuming only the forward mapping is (Lipschitz) continuous on some closed convex subset of its domain. The modification entails an additional forward step and a projection step at each iteration. Applications of the modified method to decomposition in convex programming and monotone variational inequalities are discussed.}
}

@article{stathopoulosOperatorSplittingMethods2016,
  title = {Operator {{Splitting Methods}} in {{Control}}},
  author = {Stathopoulos, Giorgos and Shukla, Harsh and Szucs, Alexander and Pu, Ye and Jones, Colin N.},
  year = {2016},
  month = aug,
  journal = {Foundations and Trends{\textregistered} in Systems and Control},
  volume = {3},
  number = {3},
  pages = {249--362},
  publisher = {Now Publishers, Inc.},
  issn = {2325-6818, 2325-6826},
  doi = {10.1561/2600000008},
  url = {https://www.nowpublishers.com/article/Details/SYS-008},
  urldate = {2021-03-29},
  abstract = {Operator Splitting Methods in Control},
  langid = {english}
}

@book{stefanovSeparableOptimizationTheory2022,
  title = {Separable {{Optimization}}: {{Theory}} and {{Methods}}},
  shorttitle = {Separable {{Optimization}}},
  author = {Stefanov, Stefan M.},
  year = {27 listopadu 2022},
  series = {Springer {{Optimization}} and {{Its Applications}}},
  edition = {2},
  publisher = {Springer},
  address = {Cham},
  url = {https://doi.org/10.1007/978-3-030-78401-0},
  abstract = {In this book, the theory, methods and applications of separable optimization are considered. Some general results are presented, techniques of approximating the separable problem by linear programming problem, and dynamic programming are also studied. Convex separable programs subject to inequality/ equality constraint(s) and bounds on variables are also studied and convergent iterative algorithms of polynomial complexity are proposed.~ As an application, these algorithms are used in the implementation of stochastic quasigradient methods to some separable stochastic programs. The problems of numerical approximation of tabulated functions and numerical solution of overdetermined systems of linear algebraic equations and some systems of nonlinear equations are solved by separable convex unconstrained minimization problems. Some properties of the Knapsack polytope are also studied. This second edition includes a substantial amount of new and revised content. Three new chapters, 15-17, are included. Chapters 15-16 are devoted to the further analysis of the Knapsack problem. Chapter 17 is focused on the analysis of a nonlinear transportation problem. Three new Appendices (E-G) are also added to this edition and present technical details that help round out the coverage. ~ Optimization problems and methods for solving the problems considered are interesting not only from the viewpoint of optimization theory, optimization methods and their applications, but also from the viewpoint of other fields of science, especially the artificial intelligence and machine learning fields within computer science. This book is intended for the researcher, practitioner, or engineer who is interested in the detailed treatment of separable programming and wants to take advantage of the latest theoretical and~algorithmic results. It~may also be~used as a textbook for a special topics course or as a~supplementary textbook for graduate courses~on nonlinear and convex optimization.},
  isbn = {978-3-030-78403-4}
}

@article{stellatoOSQPOperatorSplitting2020,
  title = {{{OSQP}}: An Operator Splitting Solver for Quadratic Programs},
  shorttitle = {{{OSQP}}},
  author = {Stellato, Bartolomeo and Banjac, Goran and Goulart, Paul and Bemporad, Alberto and Boyd, Stephen},
  year = {2020},
  month = dec,
  journal = {Mathematical Programming Computation},
  volume = {12},
  number = {4},
  pages = {637--672},
  issn = {1867-2949, 1867-2957},
  doi = {10.1007/s12532-020-00179-2},
  url = {http://link.springer.com/10.1007/s12532-020-00179-2},
  urldate = {2023-09-08},
  abstract = {We present a general-purpose solver for convex quadratic programs based on the alternating direction method of multipliers, employing a novel operator splitting technique that requires the solution of a quasi-definite linear system with the same coefficient matrix at almost every iteration. Our algorithm is very robust, placing no requirements on the problem data such as positive definiteness of the objective function or linear independence of the constraint functions. It can be configured to be division-free once an initial matrix factorization is carried out, making it suitable for real-time applications in embedded systems. In addition, our technique is the first operator splitting method for quadratic programs able to reliably detect primal and dual infeasible problems from the algorithm iterates. The method also supports factorization caching and warm starting, making it particularly efficient when solving parametrized problems arising in finance, control, and machine learning. Our open-source C implementation OSQP has a small footprint, is library-free, and has been extensively tested on many problem instances from a wide variety of application areas. It is typically ten times faster than competing interior-point methods, and sometimes much more when factorization caching or warm start is used. OSQP has already shown a large impact with tens of thousands of users both in academia and in large corporations.},
  langid = {english}
}

@book{stoerConvexityOptimizationFinite1970,
  title = {Convexity and {{Optimization}} in {{Finite Dimensions I}}},
  author = {Stoer, Josef and Witzgall, Christoph},
  year = {1970},
  series = {Grundlehren Der Mathematischen {{Wissenschaften}}},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  url = {https://doi.org/10.1007/978-3-642-46216-0},
  urldate = {2022-07-29},
  isbn = {978-3-642-46218-4},
  langid = {english}
}

@misc{SumSquaresProgramming2022,
  title = {Sum of {{Squares Programming}} for {{Julia}}.},
  year = {2022},
  month = aug,
  url = {https://github.com/jump-dev/SumOfSquares.jl},
  urldate = {2022-08-10},
  abstract = {Sum of Squares Programming for Julia},
  howpublished = {JuMP-dev}
}

@phdthesis{taccariOptimizationMethodsPiecewise2010,
  title = {Optimization {{Methods}} for {{Piecewise Affine Model Fitting}}},
  author = {Taccari, Leonardo},
  year = {2010},
  school = {Politecnico di Milano}
}

@book{taillardDesignHeuristicAlgorithms2023,
  title = {Design of {{Heuristic Algorithms}} for {{Hard Optimization}}: {{With Python Codes}} for the {{Travelling Salesman Problem}}},
  shorttitle = {Design of {{Heuristic Algorithms}} for {{Hard Optimization}}},
  author = {Taillard, {\'E}ric D.},
  year = {2023},
  publisher = {Springer Nature},
  doi = {10.1007/978-3-031-13714-3},
  url = {https://library.oapen.org/handle/20.500.12657/59365},
  urldate = {2022-12-08},
  abstract = {This open access book demonstrates all the steps required to design heuristic algorithms for difficult optimization. The classic problem of the travelling salesman is used as a common thread to illustrate all the techniques discussed. This problem is ideal for introducing readers to the subject because it is very intuitive and its solutions can be graphically represented. The book features a wealth of illustrations that allow the concepts to be understood at a glance. The book approaches the main metaheuristics from a new angle, deconstructing them into a few key concepts presented in separate chapters: construction, improvement, decomposition, randomization and learning methods. Each metaheuristic can then be presented in simplified form as a combination of these concepts. This approach avoids giving the impression that metaheuristics is a non-formal discipline, a kind of cloud sculpture. Moreover, it provides concrete applications of the travelling salesman problem, which illustrate in just a few lines of code how to design a new heuristic and remove all ambiguities left by a general framework. Two chapters reviewing the basics of combinatorial optimization and complexity theory make the book self-contained. As such, even readers with a very limited background in the field will be able to follow all the content.},
  isbn = {978-3-031-13714-3},
  langid = {english}
}

@article{takapouiSimpleEffectiveHeuristic2020,
  title = {A Simple Effective Heuristic for Embedded Mixed-Integer Quadratic Programming},
  author = {Takapoui, Reza and Moehle, Nicholas and Boyd, Stephen and Bemporad, Alberto},
  year = {2020},
  month = jan,
  journal = {International Journal of Control},
  volume = {93},
  number = {1},
  pages = {2--12},
  publisher = {Taylor \& Francis},
  issn = {0020-7179},
  doi = {10.1080/00207179.2017.1316016},
  url = {https://doi.org/10.1080/00207179.2017.1316016},
  urldate = {2020-11-04},
  abstract = {In this paper, we propose a fast optimisation algorithm for approximately minimising convex quadratic functions over the intersection of affine and separable constraints (i.e. the Cartesian product of possibly nonconvex real sets). This problem class contains many NP-hard problems such as mixed-integer quadratic programming. Our heuristic is based on a variation of the alternating direction method of multipliers (ADMM), an algorithm for solving convex optimisation problems. We discuss the favourable computational aspects of our algorithm, which allow it to run quickly even on very modest computational platforms such as embedded processors. We give several examples for which an approximate solution should be found very quickly, such as management of a hybrid-electric vehicle drivetrain and control of switched-mode power converters. Our numerical experiments suggest that our method is very effective in finding a feasible point with small objective value; indeed, we see that in many cases, it finds the global solution.}
}

@misc{TalkiitPdf,
  title = {Complementarity Problems and Complementarity Constraint},
  author = {Anitescu, Michai},
  url = {https://web.cels.anl.gov/~anitescu/Presentations/2006/talkiit.pdf},
  urldate = {2023-11-29}
}

@book{terlakyAdvancesTrendsOptimization2017,
  title = {Advances and {{Trends}} in {{Optimization}} with {{Engineering Applications}}},
  author = {Terlaky, Tam{\'a}s and Anjos, Miguel F. and Ahmed, Shabbir},
  year = {2017},
  month = apr,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611974683},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9781611974683},
  urldate = {2022-02-02},
  abstract = {Optimization is an area of critical importance in engineering and applied sciences. When designing products, materials, factories, production processes, manufacturing or service systems, and financial products, engineers strive for the best possible solutions, the most economical use of limited resources, and the greatest efficiency. As system complexity increases, these goals mandate the use of state-of-the-art optimization methodology and computational tools.},
  isbn = {978-1-61197-467-6}
}

@article{tillmannCardinalityMinimizationConstraints2024,
  title = {Cardinality {{Minimization}}, {{Constraints}}, and {{Regularization}}: {{A Survey}}},
  shorttitle = {Cardinality {{Minimization}}, {{Constraints}}, and {{Regularization}}},
  author = {Tillmann, Andreas M. and Bienstock, Daniel and Lodi, Andrea and Schwartz, Alexandra},
  year = {2024},
  month = aug,
  journal = {SIAM Review},
  volume = {66},
  number = {3},
  pages = {403--477},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/21M142770X},
  url = {https://epubs.siam.org/doi/10.1137/21M142770X},
  urldate = {2024-11-14},
  abstract = {In this study, we consider a rich class of mathematical programs with  equilibrium constraints  (MPECs) involving  both integer and continuous variables. Such a class, which subsumes mathematical programs with complementarity constraints, as well as bilevel programs involving lower level convex programs  is, in general, extremely hard to solve due to complementarity constraints and integrality requirements. For its solution, we design an (exact) algorithmic framework based on branch-and-bound (B\&B) that treats each node of the B\&B tree as a separate optimization problem and potentially changes its formulation and solution approach by designing, for example, a separate B\&B tree. The framework is implemented and computationally evaluated on a specific instance of MPEC, namely  a competitive facility location  problem  that takes into account the queueing process that determines the equilibrium assignment of users to open facilities, and a generalization of models for which, to date, no exact method has been proposed.}
}

@article{torielloFittingPiecewiseLinear2012,
  title = {Fitting Piecewise Linear Continuous Functions},
  author = {Toriello, Alejandro and Vielma, Juan Pablo},
  year = {2012},
  month = may,
  journal = {European Journal of Operational Research},
  volume = {219},
  number = {1},
  pages = {86--95},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2011.12.030},
  url = {https://www.sciencedirect.com/science/article/pii/S0377221711011246},
  urldate = {2024-10-30},
  abstract = {We consider the problem of fitting a continuous piecewise linear function to a finite set of data points, modeled as a mathematical program with convex objective. We review some fitting problems that can be modeled as convex programs, and then introduce mixed-binary generalizations that allow variability in the regions defining the best-fit function's domain. We also study the additional constraints required to impose convexity on the best-fit function.}
}

@article{tsitsiklisDistributedAsynchronousDeterministic1986,
  title = {Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms},
  author = {Tsitsiklis, J. and Bertsekas, D. and Athans, M.},
  year = {1986},
  month = sep,
  journal = {IEEE Transactions on Automatic Control},
  volume = {31},
  number = {9},
  pages = {803--812},
  issn = {1558-2523},
  doi = {10.1109/TAC.1986.1104412},
  abstract = {We present a model for asynchronous distributed computation and then proceed to analyze the convergence of natural asynchronous distributed versions of a large class of deterministic and stochastic gradient-like algorithms. We show that such algorithms retain the desirable convergence properties of their centralized counterparts, provided that the time between consecutive interprocessor communications and the communication delays are not too large.}
}

@inproceedings{udellConvexOptimizationJulia2014,
  title = {Convex {{Optimization}} in {{Julia}}},
  booktitle = {2014 {{First Workshop}} for {{High Performance Technical Computing}} in {{Dynamic Languages}}},
  author = {Udell, M. and Mohan, K. and Zeng, D. and Hong, J. and Diamond, S. and Boyd, S.},
  year = {2014},
  month = nov,
  pages = {18--28},
  doi = {10.1109/HPTCDL.2014.5},
  abstract = {This paper describes Convex1, a convex optimization modeling framework in Julia. Convex translates problems from a user-friendly functional language into an abstract syntax tree describing the problem. This concise representation of the global structure of the problem allows Convex to infer whether the problem complies with the rules of disciplined convex programming (DCP), and to pass the problem to a suitable solver. These operations are carried out in Julia using multiple dispatch, which dramatically reduces the time required to verify DCP compliance and to parse a problem into conic form. Convex then automatically chooses an appropriate backend solver to solve the conic form problem.}
}

@article{uhligRecurringTheoremPairs1979,
  title = {A Recurring Theorem about Pairs of Quadratic Forms and Extensions: A Survey},
  shorttitle = {A Recurring Theorem about Pairs of Quadratic Forms and Extensions},
  author = {Uhlig, Frank},
  year = {1979},
  month = jun,
  journal = {Linear Algebra and its Applications},
  volume = {25},
  pages = {219--237},
  issn = {0024-3795},
  doi = {10.1016/0024-3795(79)90020-X},
  url = {https://www.sciencedirect.com/science/article/pii/002437957990020X},
  urldate = {2022-07-28},
  abstract = {This is a historical and mathematical survey of work on necessary and sufficient conditions for a pair of quadratic forms to admit a positive definite linear combination and various extensions thereof.},
  langid = {english}
}

@incollection{uryasevConditionalValueatRiskOptimization2001,
  title = {Conditional {{Value-at-Risk}}: {{Optimization Approach}}},
  shorttitle = {Conditional {{Value-at-Risk}}},
  booktitle = {Stochastic {{Optimization}}: {{Algorithms}} and {{Applications}}},
  author = {Uryasev, Stanislav and Rockafellar, R. Tyrrell},
  editor = {Uryasev, Stanislav and Pardalos, Panos M.},
  year = {2001},
  series = {Applied {{Optimization}}},
  pages = {411--435},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4757-6594-6_17},
  url = {https://doi.org/10.1007/978-1-4757-6594-6_17},
  urldate = {2021-09-03},
  abstract = {A new approach for optimization or hedging of a portfolio of finance instruments to reduce the risks of high losses is suggested and tested with several applications. As a measure of risk, Conditional Value-at-Risk (CVaR) is used. For several important cases, CVaR coincides with the expected shortfall (expected loss exceeding Values-at-Risk). However, generally, CVaR and the expected shortfall are different risk measures. CVaR is a coh erent risk measure both for continuous and discrete distributions. CVaR is a more consistent measure of risk than VaR. Portfolios with low CVaR also have low VaR because CVaR is greater than VaR. The approach is based on a new representation of the performance function, which allows simultaneous calculation of VaR and minimization of CVaR. It can be used in conjunction with analytical or scenario based optimization algorithms If the number of scenarios is fixed, the problem is reduced to a Linear Programming or Nonsmooth Optimization Problem. These techniques allow optimizing portfolios with large numbers of instruments. The approach is tested with two examples: (1) portfolio optimization and comparison with the Minimum Variance approach; (2) hedging of a portfolio of options. The suggested methodology can be used for optimizing of portfolios by investment companies, brokerage firms, mutual funds, and any businesses that evaluate risks. Although the approach is used for portfolio analysis, it is very general and can be applied to any financial or non-financial problems involving optimization of percentiles.},
  isbn = {978-1-4757-6594-6},
  langid = {english}
}

@book{uryasevProbabilisticConstrainedOptimization2000,
  title = {Probabilistic {{Constrained Optimization}}: {{Methodology}} and {{Applications}}},
  shorttitle = {Probabilistic {{Constrained Optimization}}},
  author = {Uryasev, Stanislav},
  year = {2000},
  month = nov,
  series = {Nonconvex {{Optimization}} and {{Its Applications}}},
  publisher = {Springer},
  address = {New York, NY},
  url = {https://doi.org/10.1007/978-1-4757-3150-7},
  abstract = {Probabilistic and percentile/quantile functions play an important role in several applications, such as finance (Value-at-Risk), nuclear safety, and the environment. Recently, significant advances have been made in sensitivity analysis and optimization of probabilistic functions, which is the basis for construction of new efficient approaches. This book presents the state of the art in the theory of optimization of probabilistic functions and several engineering and finance applications, including material flow systems, production planning, Value-at-Risk, asset and liability management, and optimal trading strategies for financial derivatives (options).  Audience: The book is a valuable source of information for faculty, students, researchers, and practitioners in financial engineering, operation research, optimization, computer science, and related areas.},
  isbn = {978-0-7923-6644-7},
  langid = {english}
}

@misc{UserGuideArtelys,
  title = {User Guide --- {{Artelys Knitro}} 14.0 {{User}}'s {{Manual}}},
  year = {2023},
  url = {https://www.artelys.com/app/docs/knitro/2_userGuide.html},
  urldate = {2024-01-27}
}

@article{vanantwerpTutorialLinearBilinear2000,
  title = {A Tutorial on Linear and Bilinear Matrix Inequalities},
  author = {VanAntwerp, Jeremy G. and Braatz, Richard D.},
  year = {2000},
  month = aug,
  journal = {Journal of Process Control},
  volume = {10},
  number = {4},
  pages = {363--385},
  issn = {0959-1524},
  doi = {10.1016/S0959-1524(99)00056-6},
  url = {http://www.sciencedirect.com/science/article/pii/S0959152499000566},
  urldate = {2016-05-24},
  abstract = {This is a tutorial on the mathematical theory and process control applications of linear matrix inequalities (LMIs) and bilinear matrix inequalities (BMIs). Many convex inequalities common in process control applications are shown to be LMIs. Proofs are included to familiarize the reader with the mathematics of LMIs and BMIs. LMIs and BMIs are applied to several important process control applications including control structure selection, robust controller analysis and design, and optimal design of experiments. Software for solving LMI and BMI problems is reviewed.},
  annotation = {00368}
}

@misc{vandebergheLecturePiecewiselinearOptimization2013,
  type = {Lecture Slides},
  title = {Lecture 2: {{Piecewise-linear}} Optimization},
  author = {Vandeberghe, Lieven},
  year = {2013},
  url = {https://www.seas.ucla.edu/~vandenbe/ee236a/lectures/pwl.pdf},
  urldate = {2023-11-16}
}

@article{vandenbergheSemidefiniteProgramming1996,
  title = {Semidefinite {{Programming}}},
  author = {Vandenberghe, Lieven and Boyd, Stephen},
  year = {1996},
  month = mar,
  journal = {SIAM Review},
  volume = {38},
  number = {1},
  pages = {49--95},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/1038003},
  url = {https://epubs.siam.org/doi/abs/10.1137/1038003},
  urldate = {2024-09-06},
  abstract = {This paper studies the semidefinite programming SDP problem, i.e., the optimization problem of a linear function of a symmetric matrix subject to linear equality constraints and the additional condition that the matrix be positive semidefinite. First the classical cone duality is reviewed as it is specialized to SDP is reviewed. Next an interior point algorithm is presented that converges to the optimal solution in polynomial time. The approach is a direct extension of Ye's projective method for linear programming. It is also argued that many known interior point methods for linear programs can be transformed in a mechanical way to algorithms for SDP with proofs of convergence and polynomial time complexity carrying over in a similar fashion. Finally, the significance of these results is studied in a variety of combinatorial optimization problems including the general 0-1 integer programs, the maximum clique and maximum stable set problems in perfect graphs, the maximum k-partite subgraph problem in graphs, and various graph partitioning and cut problems. As a result, barrier oracles are presented for certain combinatorial optimization problems (in particular, clique and stable set problem for perfect graphs) whose linear programming formulation requires exponentially many inequalities. Existence of such barrier oracles refutes the commonly believed notion that to solve a combinatorial optimization problem with interior point methods, its linear programming formulation is eeded explicitly.}
}

@book{vandeputDataScienceSupply2021,
  title = {Data {{Science}} for {{Supply Chain Forecasting}}},
  author = {Vandeput, Nicolas},
  year = {2021},
  month = mar,
  edition = {2},
  publisher = {De Gruyter},
  address = {Berlin; Boston},
  url = {https://doi.org/10.1515/9783110671124},
  abstract = {Using data science in order to solve a problem requires a scientific mindset more than coding skills. Data Science for Supply Chain Forecasting, Second Edition contends that a true scientific method which includes experimentation, observation, and constant questioning, must be applied to supply chains to achieve excellence in demand forecasting.~This second edition adds more than 45 percent extra content with four new chapters, including an introduction to neural networks and the forecast value added framework. Part I focuses on traditional statistical forecasting models, Part II on machine learning, and the all-new Part III discusses demand forecasting process management. The various chapters focus on both (demand) forecasting models and new concepts such as metrics, underfitting, overfitting, outliers, feature optimization, and external demand drivers. The book is replete with do-it-yourself sections with implementations provided in Python (and Excel for the statistical models) to show the readers how to apply these models themselves.This hands-on book, covering the entire range of forecasting--from the basics all the way to leading-edge models--will benefit supply chain practitioners, demand planners, forecasters, and analysts looking to go the extra mile with demand forecasting.},
  isbn = {978-3-11-067110-0},
  langid = {english}
}

@book{vandeputInventoryOptimizationModels2020,
  title = {Inventory {{Optimization}}: {{Models}} and {{Simulations}}},
  shorttitle = {Inventory {{Optimization}}},
  author = {Vandeput, Nicolas},
  year = {2020},
  month = aug,
  publisher = {De Gruyter},
  address = {Berlin; Boston},
  url = {https://doi.org/10.1515/9783110673944},
  abstract = {In this book . . . Nicolas Vandeput hacks his way through the maze of quantitative supply chain optimizations. This book illustrates how the quantitative optimization of 21st century supply chains should be crafted and executed. . . . Vandeput is at the forefront of a new and better way of doing supply chains, and thanks to a richly illustrated book, where every single situation gets its own illustrating code snippet, so could you.--Joannes Vermorel, CEO, Lokad~Inventory Optimization argues that mathematical inventory models can only take us so far with supply chain management. In order to optimize inventory policies, we have to use probabilistic simulations. The book explains how to implement these models and simulations step-by-step, starting from simple deterministic ones to complex multi-echelon optimization.~The first two parts of the book discuss classical mathematical models, their limitations, and assumptions, and a quick but effective introduction to Python is provided. Part 3 contains more advanced models that will allow you to optimize your profits, estimate your lost sales and use advanced demand distributions. It also provides an explanation of how you can optimize a multi-echelon supply chain based on a simple yet powerful framework. Part 4 discusses inventory optimization thanks to simulations under custom discrete demand probability functions.~Inventory managers, demand planners, supply planners, and academics interested in gaining cost-effective solutions will benefit from the "do-it-yourself" examples and Python programs included in each chapter.},
  isbn = {978-3-11-067391-3},
  langid = {english}
}

@book{vanderbeiLinearProgramming2020,
  title = {Linear {{Programming}}},
  author = {Vanderbei, Robert J.},
  year = {2020},
  month = apr,
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  edition = {5},
  number = {285},
  publisher = {Springer},
  address = {Cham, Switzerland},
  url = {https://vanderbei.princeton.edu/LPbook/},
  isbn = {978-3-030-39414-1},
  langid = {english}
}

@misc{vielmaConicOptimizationJulia2020,
  type = {Conference Presentation},
  title = {Conic {{Optimization}} in {{Julia}} and {{JuMP}}},
  author = {Vielma, Juan Pablo},
  year = {2020},
  url = {https://juan-pablo-vielma.github.io/presentations/JULIACON_2020.pdf},
  langid = {english}
}

@inproceedings{vielmaConicOptimizationJulia2020a,
  title = {Conic {{Optimization}} in {{Julia}} and {{JuMP}}},
  booktitle = {{{JuliaCon}} 2020},
  author = {Vielma, Juan Pablo},
  year = {2020},
  url = {https://juan-pablo-vielma.github.io/presentations/JULIACON_2020.pdf},
  langid = {english}
}

@inproceedings{vielmaMixedIntegerConic2019,
  title = {Mixed {{Integer Conic Optimization}} Using {{Julia}} and {{JuMP}}},
  booktitle = {3rd {{Los Alamos National Laboratory Grid Science Winter School}} \& {{Conference}}},
  author = {Vielma, Juan Pablo},
  year = {2019},
  month = jan,
  address = {Santa Fe, NM},
  url = {https://cnls.lanl.gov/external/gsslides2019/Vielma.pdf},
  urldate = {2023-02-20}
}

@book{walkerShapeThingsPractical2015,
  title = {The {{Shape}} of {{Things}}: {{A Practical Guide}} to {{Differential Geometry}} and the {{Shape Derivative}}},
  shorttitle = {The {{Shape}} of {{Things}}},
  author = {Walker, Shawn W.},
  year = {2015},
  month = dec,
  series = {Advances in {{Design}} and {{Control}}},
  number = {28},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  url = {https://my.siam.org/Store/Product/viewproduct/?ProductId=26840633},
  isbn = {978-1-61197-395-2},
  langid = {english}
}

@misc{weisserPolynomialMomentOptimization2019,
  type = {Workshop},
  title = {Polynomial and {{Moment Optimization}} in {{Julia}} and {{JuMP}}},
  author = {Weisser, Tillmann and Legat, Beno{\^i}t and Coey, Chris and Kapelevich, Lea and Vielma, Juan Pablo},
  year = {2019},
  month = jul,
  address = {Baltimore, Maryland, USA},
  abstract = {Polynomial and moment optimization problems are infinite dimensional optimization problems that can model a wide range of problems in engineering and statistics. In this minisymposium we show how the Julia and JuMP ecosystems are particularly well suited for the effortless construction of these problems and the development of state-of-the-art solvers for them. Polynomial and moment optimization problems are infinite dimensional optimization problems that can model a wide range of problems such as shape-constrained polynomial regression, optimal control of dynamical systems, region of attraction, polynomial matrix decomposition, smooth maximum-likelihood density estimation, AC power systems, experimental design, and computation of Nash equilibria. In this minisymposium we show how the Julia and JuMP ecosystems are particularly well suited for constructing and solving these problems. In particular, we show how the JuMP extensions SumOfSquares/PolyJuMP allow for an effortless construction of these problems and how they provide a flexible and customizable building block for additional packages such as JuliaMoments. We also show how various features of the Julia programming language are used in the state-of-the-art solvers Hypatia.jl and Aspasia.jl. Finally, we showcase specific uses of these tools for applications in engineering and statistics.}
}

@techreport{wernerOptimalizace2021,
  type = {{Elektronick{\'a} skripta p{\v r}edm{\v e}tu B0B33OPT}},
  title = {{Optimalizace}},
  author = {Werner, Tom{\'a}{\v s}},
  year = {2021},
  month = sep,
  address = {Praha},
  institution = {FEL {\v C}VUT v Praze},
  url = {https://cw.fel.cvut.cz/wiki/_media/courses/b0b33opt/opt.pdf},
  langid = {{\v c}esk{\'y}}
}

@book{williamsLogicIntegerProgramming2009,
  title = {Logic and {{Integer Programming}}},
  author = {Williams, H. Paul},
  year = {2009},
  month = apr,
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  publisher = {Springer},
  langid = {english}
}

@book{williamsModelBuildingMathematical2013,
  title = {Model {{Building}} in {{Mathematical Programming}}},
  author = {Williams, H. Paul},
  year = {2013},
  month = mar,
  edition = {5},
  publisher = {Wiley},
  address = {Hoboken, N.J},
  isbn = {978-1-118-44333-0},
  langid = {english}
}

@book{wolseyIntegerProgramming2021,
  title = {Integer {{Programming}}},
  author = {Wolsey, Laurence A.},
  year = {2021},
  edition = {2},
  publisher = {Wiley},
  address = {Hoboken, NJ},
  isbn = {978-1-119-60653-6},
  langid = {english}
}

@book{wrightOptimizationDataAnalysis2022,
  title = {Optimization for {{Data Analysis}}},
  author = {Wright, Stephen J. and Recht, Benjamin},
  year = {2022},
  month = mar,
  publisher = {Cambridge University Press},
  url = {https://www.cambridge.org/core/books/optimization-for-data-analysis/C02C3708905D236AA354D1CE1739A6A2},
  urldate = {2022-03-31},
  abstract = {Optimization techniques are at the core of data science, including data analysis and machine learning. An understanding of basic optimization techniques and their fundamental properties provides important grounding for students, researchers, and practitioners in these areas. This text covers the fundamentals of optimization algorithms in a compact, self-contained way, focusing on the techniques most relevant to data science. An introductory chapter demonstrates that many standard problems in data science can be formulated as optimization problems. Next, many fundamental methods in optimization are described and analyzed, including: gradient and accelerated gradient methods for unconstrained optimization of smooth (especially convex) functions; the stochastic gradient method, a workhorse algorithm in machine learning; the coordinate descent approach; several key algorithms for constrained optimization problems; algorithms for minimizing nonsmooth functions arising in data science; foundations of the analysis of nonsmooth functions and optimization duality; and the back-propagation approach, relevant to neural networks.},
  isbn = {978-1-316-51898-4},
  langid = {english}
}

@book{wrightPrimalDualInteriorPointMethods1997,
  title = {Primal-{{Dual Interior-Point Methods}}},
  author = {Wright, Stephen J.},
  year = {1997},
  series = {Other {{Titles}} in {{Applied Mathematics}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  abstract = {In the past decade, primal-dual algorithms have emerged as the most important and useful algorithms from the interior-point class. This book presents the major primal-dual algorithms for linear programming in straightforward terms. A thorough description of the theoretical properties of these methods is given, as are a discussion of practical and computational aspects and a summary of current software. This is an excellent, timely, and well-written work. The major primal-dual algorithms covered in this book are path-following algorithms (short- and long-step, predictor-corrector), potential-reduction algorithms, and infeasible-interior-point algorithms. A unified treatment of superlinear convergence, finite termination, and detection of infeasible problems is presented. Issues relevant to practical implementation are also discussed, including sparse linear algebra and a complete specification of Mehrotra's predictor-corrector algorithm. Also treated are extensions of primal-dual algorithms to more general problems such as monotone complementarity, semidefinite programming, and general convex programming problems.},
  isbn = {978-0-89871-382-4},
  langid = {english}
}

@incollection{yangDistributedOptimizationGames2010,
  title = {Distributed {{Optimization}} and {{Games}}: {{A Tutorial Overview}}},
  shorttitle = {Distributed {{Optimization}} and {{Games}}},
  booktitle = {Networked {{Control Systems}}},
  author = {Yang, Bo and Johansson, Mikael},
  editor = {Bemporad, Alberto and Heemels, Maurice and Johansson, Mikael},
  year = {2010},
  month = jan,
  series = {Lecture {{Notes}} in {{Control}} and {{Information Sciences}}},
  number = {406},
  pages = {109--148},
  publisher = {Springer London},
  url = {http://link.springer.com/chapter/10.1007/978-0-85729-033-5_4},
  urldate = {2014-01-14},
  abstract = {This chapter provides a tutorial overview of distributed optimization and game theory for decision-making in networked systems. We discuss properties of first-order methods for smooth and non-smooth convex optimization, and review mathematical decomposition techniques. A model of networked decision-making is introduced in which a communication structure is enforced that determines which nodes are allowed to coordinate with each other, and several recent techniques for solving such problems are reviewed. We then continue to study the impact of noncooperative games, in which no communication and coordination are enforced. Special attention is given to existence and uniqueness of Nash equilibria, as well as the efficiency loss in not coordinating nodes. Finally, we discuss methods for studying the dynamics of distributed optimization algorithms in continuous time.},
  copyright = {{\copyright}2010 Springer-Verlag London Limited},
  isbn = {978-0-85729-032-8 978-0-85729-033-5}
}

@misc{yeConicLinearProgramming2017,
  title = {Conic Linear Programming},
  author = {Ye, Yinyu},
  year = {2017},
  month = oct,
  url = {https://web.stanford.edu/class/msande314/sdpmain.pdf}
}

@article{zhangPrimaldualInteriorPoint1993,
  title = {Primal-Dual Interior Point Approach for computing{\emph{l}}{$<$}{{Subscript}}{$>$}1{$<$}/{{Subscript}}{$>$}-Solutions and{\emph{l}}{$<$}{{Subscript}}{$>\infty<$}/{{Subscript}}{$>$}-Solutions of Overdetermined Linear Systems},
  author = {Zhang, Y.},
  year = {1993},
  month = may,
  journal = {Journal of Optimization Theory and Applications},
  volume = {77},
  number = {2},
  pages = {323--341},
  issn = {0022-3239, 1573-2878},
  doi = {10.1007/BF00940715},
  url = {https://link.springer.com/article/10.1007/BF00940715},
  urldate = {2018-08-01},
  abstract = {This paper concerns solving an overdetermined linear systemA T x=b in the leastl1-norm orl{$\infty$}-norm sense, whereA{$\in\mathbb{R}$}m{\texttimes}n,m},
  langid = {english}
}

@book{zlobecStableParametricProgramming2013,
  title = {Stable {{Parametric Programming}}},
  author = {Zlobec, S.},
  year = {2013},
  month = nov,
  series = {Applied {{Optimization}}},
  number = {57},
  publisher = {Springer Science \& Business Media},
  abstract = {Optimality and stability are two important notions in applied mathematics. This book is a study of these notions and their relationship in linear and convex parametric programming models. It begins with a survey of basic optimality conditions in nonlinear programming. Then new results in convex programming, using LFS functions, for single-objective, multi-objective, differentiable and non-smooth programs are introduced. Parametric programming models are studied using basic tools of point-to-set topology. Stability of the models is introduced, essentially, as continuity of the feasible set of decision variables under continuous perturbations of the parameters. Perturbations that preserve this continuity are regions of stability. It is shown how these regions can be identified. The main results on stability are characterizations of locally and globally optimal parameters for stable and also for unstable perturbations. The results are straightened for linear models and bi-level programs. Some of the results are extended to abstract spaces after considering parameters as `controls'. Illustrations from diverse fields, such as data envelopment analysis, management, von Stackelberg games of market economy, and navigation problems are given and several case studies are solved by finding optimal parameters. The book has been written in an analytic spirit. Many results appear here for the first time in book form.  Audience: The book is written at the level of a first-year graduate course in optimization for students with varied backgrounds interested in modeling of real-life problems. It is expected that the reader has been exposed to a prior elementary course in optimization, such as linear or non-linear programming. The last section of the book requires some knowledge of functional analysis.},
  googlebooks = {l2UECAAAQBAJ},
  isbn = {978-1-4615-0011-7},
  langid = {english}
}

@book{zotero-undefined,
  title = {Convex {{Optimization}}: {{Euclidean Distance Geometry}}},
  author = {Dattorro, Jon},
  year = {2019},
  month = oct,
  edition = {2},
  publisher = {Meboo Publishing USA},
  url = {https://meboo.convexoptimization.com/Meboo.html},
  isbn = {978-0-578-16140-2}
}
