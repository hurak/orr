[
  {
    "objectID": "reduction_order_controller.html",
    "href": "reduction_order_controller.html",
    "title": "Controller order reduction",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Controller order reduction"
    ]
  },
  {
    "objectID": "dynamic_programming.html",
    "href": "dynamic_programming.html",
    "title": "Dynamic programming and discrete-time optimal control",
    "section": "",
    "text": "In the previous two chapters we explained direct and indirect approaches to discrete-time optimal control. While the direct approach conveniently allows for almost arbitrary constraints, it only provides a control trajectory (a finite sequence of values of the control variable). If feedback is needed, the optimization must be performed in every sampling period, thus implementing the concept of receding horizon control, RHC, aka model predictive control, MPC. The indirect approach, in contrast, can lead to a (state) feedback control law, but this only happens in special cases such as a control of a linear system minimizing a quadratic cost (LQR) while assuming no bound constraints on the control or state variables. In the general case it leads to a two-point boundary value problem (TP-BVP), which can only be solved numerically, and trajectories are produced as outcomes.\nIn this chapter we present yet another approach — dynamic programming (DP). It also allows imposing constraints (in fact, even constraints such as integrality of variables, which are not compatible with our derivative-based optimization toolset exploited so far), and yet it directly leads to feedback controllers.\nWhile in the case of a linear system with a quadratic cost function, dynamic programming provides another route to the theoretical results that we already know — Riccati equation based solution to the LQR problem —, in the the case of a general nonlinear dynamical system with a general cost functions, the feedback controller comes in the form of a look-up table. This format of a feedback controller gives some hint about disadvantages of DP, namely, both computation and then the use of these look-up tables do not scale well with the dimension of the state space (aka curse of dimensionality). Various approximation schemes exist — one such branch is known as reinforcement learning.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "DP and discrete-time optimal control"
    ]
  },
  {
    "objectID": "dynamic_programming.html#bellmans-principle-of-optimality-and-dynamic-programming",
    "href": "dynamic_programming.html#bellmans-principle-of-optimality-and-dynamic-programming",
    "title": "Dynamic programming and discrete-time optimal control",
    "section": "Bellman’s principle of optimality and dynamic programming",
    "text": "Bellman’s principle of optimality and dynamic programming\nWe start by considering the following example.\n\nExample 1 (Reusing the plan for a trip from Prague to Ostrava) We are planning a car trip from Prague to Ostrava and you are searching for a route that minimizes the total time. Using the online planner we learn that the fastest route from Prague to Ostrava is — as bizarre as it sounds — via (actually around) Brno.\n\n\nNow, is it possible to reuse this plan for our friends from Brno who are also heading for Ostrava?\n\n\nThe answer is yes, as the planner confirms. Surely did not even need the planner to answer such trivial question. And yet it demonstrates the key wisdom of the whole chapter — the Bellman’s principle of optimality —, which we now state formally.\n\n\nTheorem 1 (Bellman’s principle of optimality) An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\n\nWe now investigate this idea a bit more quantitatively using a simple computational example of finding a shortest path in a graph.\n\nExample 2 (Shortest path in a graph) We consider a directional graph with nodes A, B, C, D, and E and edges with the prescribed lengths as in the figure below.\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA-&gt;B\n\n\n3\n\n\n\nD\n\nD\n\n\n\nA-&gt;D\n\n\n1\n\n\n\nC\n\nC\n\n\n\nB-&gt;C\n\n\n2\n\n\n\nE\n\nE\n\n\n\nB-&gt;E\n\n\n1\n\n\n\nD-&gt;E\n\n\n3\n\n\n\nG\n\nG\n\n\n\nD-&gt;G\n\n\n2\n\n\n\nF\n\nF\n\n\n\nC-&gt;F\n\n\n3\n\n\n\nE-&gt;F\n\n\n3\n\n\n\nH\n\nH\n\n\n\nE-&gt;H\n\n\n2\n\n\n\nG-&gt;H\n\n\n4\n\n\n\nI\n\nI\n\n\n\nF-&gt;I\n\n\n4\n\n\n\nH-&gt;I\n\n\n2\n\n\n\n\n\n\n\n\nThe task is now to find the shortest path from A to I. What are possible solution strategies? We can start enumerating all the possible paths and calculate their costs (by summing the costs of the participating edges). Needless to say, this strategy based on enumeration scales very badly with the growing number of nodes.\nAlternatively, we solve the problem using dynamic programming and relying on Bellman’s principle of optimality. Before we proceed, we need to define the concept of a stage. It is perhaps less common and natural when it comes to solving graph problems, but we introduce it with anticipation of discrete-time optimal control problems. By the kth stage we understand the node at which the kth decision needs to be made. In our case, starting at A, 4 decisions need to be made to reach the final node. But let’s agree that we also denote the final node as the stage, the 5th one, even if no decision is to be made here. The total number of stages is then N=5.\nThe crucial attribute of the strategy based on dynamic programming is that we proceed backwards. We start at the very final stage. At this stage, there is just one node and there is nothing we can do, but note that it also makes sense to formulate problems with several possible nodes at the final stage, each with a different (terminal) costs — we will actually use once we switch to the optimal control setting. Now we proceed backwards to the last but one, that is, the (N-1)th stage.\nThese are F and H nodes at this 4the stage. In these two nodes there is again no freedom as for the actions, but for each of them we can record their respective cost to go: 4 for the F node and 2 for the H node. These costs reflect how costly it is to reach the terminal node from them.\nThings are only getting interesting if we now proceed to the 3rd stage. We now have to consider three possible nodes: C, E and G. For the C and G nodes there is still just one action and we can only record their costs to go. The cost for the C node can be computed as the cost for the immediate transition from C to F plus the cost for the F node, which we recorded previously, that is, 3+4=7. We record the value of 7 with the C node. Similarly for the G node. For the E node there are two possible actions — two possible decisions to be made, two possible paths to choose from. Either to the left (or, actually, up in our orientation of the graph), which would bring us to the node F, or to the right (or down), which would bring us to the node H. We compute the costs to go for both decisions and choose the decision with a smaller cost. Here the cost of the decision to go to the left is composed of the cost of the transition to F plus the cost to go from F, that is, 3+4=7. The cost to go for the decision to go right is composed of the transition cost from E to H plus the cost to go from H, that is, 2+2=4. Obviously, the optimal decision is to go right, that is, to the node H. Here, on top of the value of the optimal (smallest) cost to go from the node we also record the optimal decision (go to the right/down). We do it by coloring the edge in blue.\nNote that in principle we should have highlighted the edges from F to I, from C to F, and from G to H. It was unnecessary here since there were the only possible edges emanating from these nodes.\nWe proceed backwards to the 2nd stage, and we compute the costs to go for the nodes B and D. Again we record their optimal values and the actual optimal decisions.\nOne last shift backwards and we are at the initial node A, for which we can do the same computation of the costs to go. Note that here coincidently both decisions have the same cost to go, hence both possible decisions/actions are optimal and we can just toss a coin.\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n8\n\n\n\nB\n\nB\n5\n\n\n\nA-&gt;B\n\n\n3\n\n\n\nD\n\nD\n7\n\n\n\nA-&gt;D\n\n\n1\n\n\n\nC\n\nC\n7\n\n\n\nB-&gt;C\n\n\n2\n\n\n\nE\n\nE\n4\n\n\n\nB-&gt;E\n\n\n1\n\n\n\nD-&gt;E\n\n\n3\n\n\n\nG\n\nG\n6\n\n\n\nD-&gt;G\n\n\n2\n\n\n\nF\n\nF\n4\n\n\n\nC-&gt;F\n\n\n3\n\n\n\nE-&gt;F\n\n\n3\n\n\n\nH\n\nH\n2\n\n\n\nE-&gt;H\n\n\n2\n\n\n\nG-&gt;H\n\n\n4\n\n\n\nI\n\nI\n0\n\n\n\nF-&gt;I\n\n\n4\n\n\n\nH-&gt;I\n\n\n2\n\n\n\n\n\n\n\n\nMaybe it is not immediately clear from the graph, but when viewed as an itinerary for a trip, it provides a feedback controller. Even if for whichever reason we find ourselves out of the optimal path, we can always have a look at the graph — it will guide us along the path that is optimal from that given node. For example, if we happen to be in node C, we do have a plan. Well, here is misleadingly simple as there is no decision to be made, but you get the point.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "DP and discrete-time optimal control"
    ]
  },
  {
    "objectID": "dynamic_programming.html#bellmans-principle-of-optimality-applied-to-the-discrete-time-optimal-control-problem",
    "href": "dynamic_programming.html#bellmans-principle-of-optimality-applied-to-the-discrete-time-optimal-control-problem",
    "title": "Dynamic programming and discrete-time optimal control",
    "section": "Bellman’s principle of optimality applied to the discrete-time optimal control problem",
    "text": "Bellman’s principle of optimality applied to the discrete-time optimal control problem\nLet’s recapitulate here the problem of optimal control for a discrete-time system. In particular, we consider the system modelled by \n\\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\n defined on the discrete time interval [i,N], with the initial state \\bm x_i fixed (\\bm x_i = \\mathbf x_i) We aim at minimizing the cost function \nJ_i^N\\left(\\bm x_i, \\bm u_i, \\bm u_{i+1}, \\ldots, \\bm u_{N-1}\\right) = \\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1}L_k(\\bm x_k,\\bm u_k).\n\nBefore we proceed, some comments on the notation are in order. Indeed, a well tuned and systematically used notation is instrumental in dynamic programming.\n\n\n\n\n\n\nWe omit the final time from the notation for the cost function\n\n\n\nWhile the cost function does depend on the final time too, in most if not all our analyses we assume that it is fixed and understood from the context. Hence we will not explicitly indicate the dependence on the final time. We will write just J_i(\\ldots) instead of J_i^N(\\ldots). This may help reduce the notational clutter as we are going to need the upper index for something else soon.\n\n\n\n\n\n\n\n\nWe omit the state trajectory from the notation for the cost function and leave just the initial state\n\n\n\nThe cost function is clearly a function of the full sequence \\bm x_i, \\bm x_{i+1},\\ldots, \\bm x_N of the state vectors too. In the previous chapters we handled it systematically (either by considering them as optimization variables in the simultaneous direct approach or by introducing Lagrange multipliers in the indirect approach). But here we want to emphasize the fact that starting with \\bm x_{i+1}, the whole state trajectory is uniquelly determined by the initial state \\bm x_i and the corresponding control trajectory \\bm u_i, \\bm u_{i+1},\\ldots, \\bm u_{N-1}. Therefore, we write the cost function as a function of the initial state, the initial time (we already agreed above not to emphasize the final time), and the sequence of controls.\n\n\n\n\n\n\n\n\nWe use the lower index to display dependence on time\n\n\n\nThe dependence on the discrete time is reflected by the lower indices: not only in \\bm x_k and \\bm u_k but also in \\mathbf f_k(), L_k() and J_k(). We could perhaps write these as \\mathbf f(\\cdot,\\cdot,k), L(\\cdot,\\cdot,k) and J(\\cdot,\\cdot,k) to better indicate that k is really an argument for these functions, but we prefer making it compatible with the way we indicate the time dependence of \\bm x_k and \\bm u_k.\n\n\nHaving introduced the cost function parameterized by the initial state, initial time and the full sequence of controls, we now introduce the optimal cost function\n\n\\boxed{\n    J^\\star_i(\\bm x_i) = \\min_{\\bm u_i,\\ldots, \\bm u_{N-1}} J_i\\left(\\bm x_i, \\bm u_i, \\bm u_{i+1}, \\ldots, \\bm u_{N-1}\\right).}\n\\tag{1}\nThe sequence of controls in the above minimization may be subject to some constraints, but we do not indicate them here for the sake of notational simplicity.\n\n\n\n\n\n\nDifference between the J_i and J^\\star_i functions\n\n\n\nUnderstanding the difference is crucial. While the cost function J_i depends on the (initial) state, the (initial) time and the sequence of controls applied over the whole interval, the optimal cost function J^\\star_i only depends on the (initial) state and the (initial) time.\n\n\nAssume now that we can find an optimal control sequence from any given state \\bm x_{k+1} at time k+1 on, i.e., we can find \\bm u_{k+1}^\\star,\\bm u_{k+2}^\\star,\\ldots, \\bm u_{N-1}^\\star yielding the optimal cost J_{k+1}^\\star(\\bm x_{k+1}). We will soon show how to actually find it, but for the time being we just assume we can have it. We now show how it can be used to find the optimal cost J_k^\\star(\\bm x_k) at state \\bm x_k and time k.\nLet’s now consider the following strategy: with the system at state \\bm x_k and time k we apply some control \\bm u_k, not necessarily an optimal one, which brings the system to the state \\bm x_{k+1} in the next time k+1. But from then on we use the control sequence \\bm u_{k+1}^\\star,\\bm u_{k+2}^\\star,\\ldots, \\bm u_{N-1}^\\star that is optimal from \\bm x_{k+1}. The corresponding cost is \nL_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1}).\n\\tag{2}\nBellman’s principle of optimality states that if we optimize the above expression over \\bm u_k, we get the optimal cost J_k^\\star(\\bm x_k) at time k \n\\boxed{J_k^\\star(\\bm x_k) = \\min_{\\bm u_k}\\left(L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1})\\right).}\n\\tag{3}\nHence, at a given state \\bm x_{k} and time k, the optimization is performed over only one (possibly vector) control \\bm u_k and not the whole trajectory as the definition of the optimal cost in Equation 1 suggests! What a simplification!\n\n\n\n\n\n\nImportant\n\n\n\nThe minimization needs to be performed over the whole sum L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1}), because \\bm x_{k+1} is a function of \\bm u_k (recall that \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k)). We can also write Equation 3 as \nJ_k^\\star(\\bm x_k) = \\min_{\\bm u_k}\\left(L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\mathbf f_k(\\bm x_k,\\bm u_k))\\right),\n which makes it more apparent.\n\n\nOnce we have the optimal cost function J^\\star_{k}, the optimal control \\bm u_k^\\star(x_k) at a given time k and state \\bm x_k is obtained by \n\\boxed{\n    \\bm u_k^\\star(\\bm x_k) = \\arg \\min_{\\bm u_k}\\left(L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\mathbf f_k(\\bm x_k,\\bm u_k))\\right).}\n\n\nAlternative formulation of dynamic programming using Q-functions\nThe cost function in Equation 2 is sometimes called Q-function and we denote it Q_k^\\star(\\bm x_k,\\bm u_k). We write its definition here for convenience \nQ^\\star_k(\\bm x_k,\\bm u_k) = L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1}).\n\nIt is a cost of choosing the control \\bm u_k at state \\bm x_k at time k and then following the optimal control from the next time on.\nThe optimal cost function J_k^\\star(\\bm x_k) can be recovered from the optimal Q-function Q_k^\\star(\\bm x_k,\\bm u_k) by taking the minimum over \\bm u_k \nJ_k^\\star(\\bm x_k) = \\min_{\\bm u_k} Q_k^\\star(\\bm x_k,\\bm u_k).\n\nBellman’s principle of optimality can be then expressed using the optimal Q-function as \n\\boxed{Q_k^\\star(\\bm x_k,\\bm u_k) = L_k(\\bm x_k,\\bm u_k) + \\min_{\\bm u_{k+1}} Q_{k+1}^\\star(\\bm x_{k+1},\\bm u_{k+1})}.\n\nOptimal control is then obtained from the optimal Q-function as the minimizing control \n\\boxed{\\bm u_k^\\star(\\bm x_k) = \\arg \\min_{\\bm u_k} Q_k^\\star(\\bm x_k,\\bm u_k).}",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "DP and discrete-time optimal control"
    ]
  },
  {
    "objectID": "cont_indir_references_2.html",
    "href": "cont_indir_references_2.html",
    "title": "References",
    "section": "",
    "text": "The content of this lecture is standard and is discussed by a number of books and online resources. When preparing our own material, we took much inspiration from [1]. In particular, we are trying to follow Kirk’s way of relating the optimal control problem to the calculus of variations. Although not available online, the printed book is fairly afordable. We also used [2], in particular the chapters 3 (application of calculus of variations to general problem of optimal control) and chapter 4 (Pontryagin’s principle). Online version of the book is freely available.\nOther recommendable classics are [3] and [4]. The popular [5] is a bit less detailed when it comes to the topics of this particular chapter/lecture.\nWe did not discuss the proof of Pontryagin’s principle and we do not even command the students to go through the proof in the book. Admittedly, it is rather challenging. But if you are courageous, have a look at [2]. Understanding the very statement of the theorem, its roots in calculus of variations, and how it removes the deficiencies of the calculus of variations will suffice for our purposes.\nThe transition from the calculus of variations to the optimal control, especially when it comes to the definition of Hamiltonian, is somewhat tricky. Unfortunately, it is not discussed satisfactorily in the literature. Even [2] leaves it as an (unsolved) exercise (3.5 and 3.6) to the student. Other major textbooks avoid the topic altogether. We find an exceptionally insightful treatment in the paper [6], in particular in the section “The first fork in the road: Hamilton” on page 39.\nThe time-optimal control for linear systems, in particular bang-bang control for a double integrator is described in section 4.4.1 and 4.4.2 in [2]. But the material is quite standard and can be found in many other books and lecture notes.\nWhat is often not emphasized in textbooks, however, is the fact that without any modifications, the bang bang control is rather troublesome from an implementation viewpoint – it leads to chattering. A dedicated research thread has evolved, driven by the needs of hard disk drive industry, which is called (a)proximate time-optimal control (PTOS). Many dozens of papers can be found with this keyword in the title. For instance, [7], and [8].\nAmong numerous other resources available freely online, the lecture notes [9, Sec. 12.3], and [10] can also be recommended.\n\n\n\n\n Back to topReferences\n\n[1] D. E. Kirk, Optimal Control Theory: An Introduction, Reprint of the 1970 edition. Dover Publications, 2004.\n\n\n[2] D. Liberzon, Calculus of Variations and Optimal Control Theory: A Concise Introduction. Princeton University Press, 2011. Available: http://liberzon.csl.illinois.edu/teaching/cvoc/cvoc.html\n\n\n[3] A. E. Bryson Jr. and Y.-C. Ho, Applied Optimal Control: Optimization, Estimation and Control, Revised edition. CRC Press, 1975.\n\n\n[4] M. Athans and P. L. Falb, Optimal Control: An Introduction to the Theory and Its Applications, Reprint of the 1966 edition. Dover Publications, 2006.\n\n\n[5] F. L. Lewis, D. Vrabie, and V. L. Syrmo, Optimal Control, 3rd ed. John Wiley & Sons, 2012. Accessed: Mar. 09, 2022. [Online]. Available: https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf\n\n\n[6] H. J. Sussmann and J. C. Willems, “300 years of optimal control: From the brachystochrone to the maximum principle,” IEEE Control Systems, vol. 17, no. 3, pp. 32–44, Jun. 1997, doi: 10.1109/37.588098.\n\n\n[7] M. L. Workman, R. L. Kosut, and G. F. Franklin, “Adaptive Proximate Time-Optimal Servomechanisms: Continuous Time Case,” in 1987 American Control Conference, Minneapolis, MN, Jun. 1987, pp. 589–594. doi: 10.23919/ACC.1987.4789386.\n\n\n[8] L. Y. Pao and G. F. Franklin, “Proximate time-optimal control of third-order servomechanisms,” IEEE Transactions on Automatic Control, vol. 38, no. 4, pp. 560–580, Apr. 1993, doi: 10.1109/9.250524.\n\n\n[9] S. Gros and M. Diehl, “Numerical Optimal Control (Draft).” Systems Control; Optimization Laboratory IMTEK, Faculty of Engineering, University of Freiburg, Apr. 2022. Available: https://www.syscop.de/files/2020ss/NOC/book-NOCSE.pdf\n\n\n[10] L. C. Evans, “An Introduction to Mathematical Optimal Control Theory,” Department of Mathematics, University of California, Berkeley. Available: https://math.berkeley.edu/~evans/control.course.pdf",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "References"
    ]
  },
  {
    "objectID": "cont_indir_references.html",
    "href": "cont_indir_references.html",
    "title": "References",
    "section": "",
    "text": "Indirect approach to optimal control is based on calculus of variations (and its later extension in the form of Pontryagin’s principle of maximum). Calculus of variations is an advanced mathematical discipline that requires non-trivial foundations and effort to master. In our course, however, we take the liberty of aiming for intuitive understanding rather than mathematical rigor. At roughly the same level, the calculus of variations is introduced in several books on optimal control, such as the affordable and readable [1], the comprehensible classic [2], the popular and online available [3], or the accessible and yet rigorous (with a draft freely available online) [4].\nFor those interested in a having a standard reference for the calculus of variations, the classic reference is [5]. It is slim, but admittedly rather terse.\nWith anticipation of what is coming, we provide here a reference to the paper [6], which shows how the celebrated Pontryagin’s principle of maximum extends the calculus of variations significantly. But we will only discuss this in the next chapter.\n\n\n\n\n Back to topReferences\n\n[1] D. E. Kirk, Optimal Control Theory: An Introduction, Reprint of the 1970 edition. Dover Publications, 2004.\n\n\n[2] A. E. Bryson Jr. and Y.-C. Ho, Applied Optimal Control: Optimization, Estimation and Control, Revised edition. CRC Press, 1975.\n\n\n[3] F. L. Lewis, D. Vrabie, and V. L. Syrmo, Optimal Control, 3rd ed. John Wiley & Sons, 2012. Accessed: Mar. 09, 2022. [Online]. Available: https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf\n\n\n[4] D. Liberzon, Calculus of Variations and Optimal Control Theory: A Concise Introduction. Princeton University Press, 2011. Available: http://liberzon.csl.illinois.edu/teaching/cvoc/cvoc.html\n\n\n[5] I. M. Gelfand and S. V. Fomin, Calculus of Variations, Reprint of the 1963 edition. Mineola, N.Y: Dover Publications, 2020.\n\n\n[6] H. J. Sussmann and J. C. Willems, “300 years of optimal control: From the brachystochrone to the maximum principle,” IEEE Control Systems, vol. 17, no. 3, pp. 32–44, Jun. 1997, doi: 10.1109/37.588098.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "References"
    ]
  },
  {
    "objectID": "discr_dir_LQR.html",
    "href": "discr_dir_LQR.html",
    "title": "Finite-horizon LQR as a QP",
    "section": "",
    "text": "Here we specialize the general procedure from the previous section to the case of a Linear system and a Quadratic cost. We start by considering a simple problem of Regulation, wherein the goal is to bring the system either exactly or approximately to zero final state, that is, \\mathbf x^\\text{ref}=\\mathbf 0 and we want \\bm x_N=\\mathbf x^\\text{ref} or \\bm x_N\\approx\\mathbf x^\\text{ref}, respectively. The problem is known as the LQR problem. \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_0,\\ldots, \\bm u_{N-1}, \\bm x_{0},\\ldots, \\bm x_N} &\\quad  \\frac{1}{2} \\bm x_N^\\top \\mathbf S \\bm x_N + \\frac{1}{2} \\sum_{k=0}^{N-1} \\left(\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k \\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{k+1} = \\mathbf A\\bm x_k + \\mathbf B\\bm u_k,\\quad k = 0, \\ldots, N-1, \\\\\n                    &\\quad \\bm x_0 = \\mathbf x_0,\\\\\n                    &\\quad \\bm x_N = \\mathbf 0\\;  (\\text{or}\\, \\bm x_N \\approx \\mathbf 0).\n\\end{aligned}\nReferring to the two options for the last constraint,",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Finite-horizon LQR as a QP"
    ]
  },
  {
    "objectID": "discr_dir_LQR.html#simultaneous-sparse-formulation",
    "href": "discr_dir_LQR.html#simultaneous-sparse-formulation",
    "title": "Finite-horizon LQR as a QP",
    "section": "Simultaneous (sparse) formulation",
    "text": "Simultaneous (sparse) formulation\nBelow we rewrite the latter problem, that is, \\bm x_N\\approx\\mathbf 0, in the “unrolled” form, where we stack the state and control variables into “long” vectors \\bar{\\bm x} and \\bar{\\bm u}. Doing the same for the former is straightforward. \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u},\\bar{\\bm x}} & \\frac{1}{2}\\left(\\begin{bmatrix} \\bm x_1^\\top & \\bm x_2^\\top & \\ldots & \\bm x_N^\\top \\end{bmatrix}\n\\underbrace{\\begin{bmatrix}\\mathbf Q & & & \\\\ & \\mathbf Q & &\\\\ & &\\ddots & \\\\ & & & \\mathbf S \\end{bmatrix}}_{\\overline{\\mathbf Q}}\n\\underbrace{\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix}}_{\\bar{\\bm x}}\\right.\\\\\n&\\qquad +\\left.\n\\begin{bmatrix} \\bm u_0^\\top & \\bm u_1^\\top & \\ldots & \\bm u_{N-1}^\\top \\end{bmatrix}\n\\underbrace{\\begin{bmatrix}\\mathbf R & & & \\\\ & \\mathbf R & &\\\\ & &\\ddots & \\\\ & & & \\mathbf R \\end{bmatrix}}_{\\overline{\\mathbf R}}\n\\underbrace{\\begin{bmatrix} \\bm u_0 \\\\ \\bm u_1 \\\\ \\vdots \\\\ \\bm u_{N-1} \\end{bmatrix}}_{\\bar{\\bm u}}\\right)\n+ \\underbrace{\\frac{1}{2}\\mathbf x_0^\\top \\mathbf Q \\mathbf x_0}_{\\mathrm{constant}}\n\\end{aligned}\n subject to \n\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\bm x_3\\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix} = \\underbrace{\\begin{bmatrix}\\mathbf 0 & & & &\\\\\\mathbf A & \\mathbf 0 & & &\\\\ &\\mathbf A &\\mathbf 0 & & \\\\ & & &\\ddots & \\\\& & &\\mathbf A & \\mathbf 0 \\end{bmatrix}}_{\\overline{\\mathbf A}}\n\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\bm x_3\\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf B & & & & \\\\ & \\mathbf B & & & \\\\& &\\mathbf B & \\\\ & & &\\ddots \\\\ & & & & \\mathbf B \\end{bmatrix}}_{\\overline{\\mathbf B}}\\begin{bmatrix} \\bm u_0 \\\\ \\bm u_1 \\\\ \\bm u_2\\\\\\vdots \\\\ \\bm u_{N-1} \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf A\\\\\\mathbf 0\\\\\\mathbf 0\\\\\\vdots\\\\\\mathbf 0\\end{bmatrix}}_{\\overline{\\mathbf A}_0}\\mathbf x_0,    \n\nin which we have already substituted the particular \\mathbf x_0 for the variable \\bm x_0. Consequently, the last term in the cost function can be discarded because it is constant.\nThe terms with the \\bar{\\bm x} vector can be combined and we get \n\\begin{bmatrix} \\mathbf 0 \\\\ \\mathbf 0 \\\\ \\mathbf 0\\\\ \\vdots \\\\ \\mathbf 0 \\end{bmatrix} = \\underbrace{\\begin{bmatrix}-\\mathbf I & & & &\\\\\\mathbf A & -\\mathbf I & & &\\\\ &\\mathbf A &-\\mathbf I & & \\\\ & & &\\ddots & \\\\& & &\\mathbf A & -\\mathbf I \\end{bmatrix}}_{\\overline{\\mathbf A} - \\mathbf I}\n\\begin{bmatrix} \\mathbf x_1 \\\\ \\mathbf x_2 \\\\ \\mathbf x_3\\\\ \\vdots \\\\ \\mathbf x_N \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf B & & & & \\\\ & \\mathbf B & & & \\\\& &\\mathbf B & \\\\ & & &\\ddots \\\\ & & & & \\mathbf B \\end{bmatrix}}_{\\overline{\\mathbf B}}\\begin{bmatrix} \\mathbf u_0 \\\\ \\mathbf u_1 \\\\ \\mathbf u_2\\\\\\vdots \\\\ \\mathbf u_{N-1} \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf A\\\\\\mathbf 0\\\\\\mathbf 0\\\\\\vdots\\\\\\mathbf 0\\end{bmatrix}}_{\\overline{\\mathbf A}_0}\\mathbf x_0.\n\\tag{1}\nUpon stacking the two “long” vectors into \\bar{\\bm z} we reformulate the optimization problem as \n\\operatorname*{minimize}_{\\widetilde{\\mathbf z}\\in\\mathbb{R}^{2N}}\\quad \\frac{1}{2}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}^\\top &\\bar{\\bm u}^\\top\\end{bmatrix}}_{\\bar{\\bm z}^\\top} \\underbrace{\\begin{bmatrix}\\overline{\\mathbf Q} & \\\\ & \\overline{\\mathbf R} \\end{bmatrix}}_{\\widetilde{\\mathbf Q}}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}\\\\\\bar{\\bm u}\\end{bmatrix}}_{\\bar{\\bm z}}\n subject to \n\\mathbf 0 = \\underbrace{\\begin{bmatrix}(\\overline{\\mathbf A}-\\mathbf I) & \\overline{\\mathbf B}\\end{bmatrix}}_{\\widetilde{\\mathbf A}}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}\\\\\\bar{\\bm u}\\end{bmatrix}}_{\\bar{\\bm z}} + \\underbrace{\\overline{\\mathbf A}_0 \\mathbf x_0}_{\\tilde{\\mathbf b}}.\n\nTo summarize, we have reformulated the optimal control problem as a linearly constrained quadratic program \n\\boxed{\n\\begin{aligned}\n\\underset{\\bar{\\bm z}\\in\\mathbb{R}^{2N}}{\\text{minimize}} &\\quad \\frac{1}{2}\\bar{\\bm z}^\\top \\widetilde{\\mathbf Q} \\bar{\\bm z}\\\\\n\\text{subject to} &\\quad \\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\bm b} = \\mathbf 0.\n\\end{aligned}}\n\nThis constrained optimization problem can still be solved without invoking a numerical solver for solving quadratic programs (QP). We do it by introducing a vector \\boldsymbol\\lambda of Lagrange multipliers to form the Lagrangian function \n\\mathcal{L}(\\bar{\\bm z}, \\boldsymbol \\lambda) = \\frac{1}{2}\\bar{\\bm z}^\\top \\widetilde{\\mathbf Q} \\bar{\\bm z} + \\boldsymbol\\lambda^\\top(\\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\mathbf b}),\n for which the gradients with respect to \\bar{\\bm z} and \\boldsymbol\\lambda are \n\\begin{aligned}\n\\nabla_{\\tilde{\\bm{z}}} \\mathcal{L}(\\bar{\\bm z}, \\boldsymbol\\lambda) &= \\widetilde{\\mathbf Q}\\bar{\\bm z} + \\tilde{\\mathbf A}^\\top\\boldsymbol\\lambda,\\\\\n\\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L}(\\tilde{\\bm x}, \\boldsymbol\\lambda) &=\\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\mathbf b}.\n\\end{aligned}\n\nRequiring that the overall gradient vanishes leads to the following KKT set of linear equations \n\\begin{bmatrix}\n  \\widetilde{\\mathbf Q} & \\widetilde{\\mathbf A}^\\top\\\\ \\widetilde{\\mathbf A} & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bar{\\bm z}\\\\\\boldsymbol\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf 0\\\\ -\\tilde{\\mathbf b}\n\\end{bmatrix}.\n\nSolving this could be accomplished by using some general solver for linear systems or by using some more tailored solver for symmetric indefinite systems (based on LDL factorization).\n\nExample 1 (Reformulating the unconstrained LQR problem as a system of linear equations – simultaneous approach)  \n\n\nShow the code\nusing BlockArrays\nusing LinearAlgebra\nusing LinearSolve\nusing QDLDL\nusing SparseArrays\n\nfunction direct_dlqr_simultaneous(A,B,x₀,Q,R,S,N)\n    n = size(A)[1]\n    m = size(B)[2]\n    Q̄ = BlockArray(spzeros(N*n,N*n),repeat([n],N),repeat([n],N))\n    for i=1:(N-1)\n        Q̄[Block(i,i)] = Q\n    end\n    Q̄[Block(N,N)] = S\n    R̄ = BlockArray(spzeros(N*m,N*m),repeat([m],N),repeat([m],N))\n    for i=1:N\n        R̄[Block(i,i)] = R\n    end\n    Q̃ = blockdiag(sparse(Q̄),sparse(R̄))              # The matrix defining the quadratic cost.\n    B̄ = BlockArray(spzeros(N*n,N*m),repeat([n],N),repeat([m],N))\n    for i=1:N\n        B̄[Block(i,i)] = B\n    end\n    Ā = BlockArray(sparse(-1.0*I,n*N,n*N),repeat([n],N),repeat([n],N))\n    for i=2:N\n        Ā[Block(i,(i-1))] = A\n    end\n    Ã = sparse([Ā B̄])                               # The matrix defining the linear (affine) equation.\n    Ā₀ = spzeros(n*N,n)\n    Ā₀[1:n,1:n] = A\n    b̃ = Ā₀*sparse(x₀)                               # The constant offset for the linear (affine) equation.\n    K = [Q̃ Ã'; Ã spzeros(size(Ã,1),size(Ã,1))]      # Sparse KKT matrix.\n    k = [spzeros(size(Q̃,1)); -b̃]                    # Right hand side of the KKT system\n    prob = LinearProblem(K,k)                       # The KKT system as a linear problem.\n    z̃λ = LinearSolve.solve(prob)                    # Solving the KKT system. Ready for trying various solvers.\n    xopt = reshape(z̃λ[1:(n*N)],(n,:))\n    uopt = reshape(z̃λ[(n*N+1):(n+m)*N],(m,:))\n    return xopt,uopt\nend\n\nn = 2               # Number of state variables.\nm = 1               # Number of (control) input variables. \nA = rand(n,n)       # State matrix.\nB = rand(n,m)       # Input coupling matrix.\nx₀ = [1.0, 3.0]     # Initial state.\n\nN = 10              # Time horizon.\n\ns = [1.0, 2.0]      \nq = [1.0, 2.0]\nr = [1.0]\n\nS = diagm(0=&gt;s)     # Matrix defining the terminal state cost.\nQ = diagm(0=&gt;q)     # Matrix defining the running state dost.\nR = diagm(0=&gt;r)     # Matrix defining the cost of control.\n\nxopt,uopt = direct_dlqr_simultaneous(A,B,x₀,Q,R,S,N)\n\nusing Plots\np1 = plot(0:(N-1),uopt',marker=:diamond,label=\"u\",linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"u\")\n\np2 = plot(0:N,hcat(x₀,xopt)',marker=:diamond,label=[\"x₁\" \"x₂\"],linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"x\")\n\nplot(p1,p2,layout=(2,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding constraints on controls and states\nWhen solving a realistic optimal control problem, we may want to impose inequality constraints on \\bm u_k due to saturation of actuators. We may also want to add constraints on \\bm x_k as well, which may reflect some performance specifications. In both cases, we would arrive at the full KKT conditions, and rather than trying to solve these, we resort to some finetuned numerical solver for quadratic programming (QP) instead.\n\nExample 2 (Simultaneous approach to the LQR problem with constraints on control – higher-level implementation using optimization modelling language JuMP) While developing the model all the way down to the individual matrices and vectors gives an insight into the structure of the problem (we learnt that in absence of constraints it amounts to solving an indefinite system of linear equations), here we show how the use of an optimization modelling language can make the process of building the model a lot more convenient. We use JuMP for this purpose, but things would be similar with, say, cvxpy in Python or Yalmip in Matlab.\n\n\nShow the code\nusing LinearAlgebra\nusing JuMP\nusing OSQP\n\nn = 2               # Number of state variables.\nm = 1               # Number of (control) input variables. \nA = rand(n,n)       # State matrix.\nB = rand(n,m)       # Input coupling matrix.\nx₀ = [1.0, 3.0]     # Initial state.\n\nN = 10              # Time horizon.\n\ns = [1.0, 2.0]      \nq = [1.0, 2.0]\nr = [1.0]\n\nS = diagm(0=&gt;s)     # Matrix defining the terminal state cost.\nQ = diagm(0=&gt;q)     # Matrix defining the running state dost.\nR = diagm(0=&gt;r)     # Matrix defining the cost of control.\n\numin = -1.0\numax = 1.0\n\nocp = Model(OSQP.Optimizer)\nset_silent(ocp)\n\n@variable(ocp, umin &lt;= u[1:N] &lt;= umax)\n@variable(ocp, x[1:n,1:N+1])\n\nfor i in 1:N\n    @constraint(ocp, x[:,i+1] == A*x[:,i] + B*u[i])\nend\n\nfix(x[1,1], x₀[1])\nfix(x[2,1], x₀[2])\n\n@objective(ocp, Min, 1/2*dot(x[:,N],S,x[:,N]) + 1/2*sum(dot(x[:,i],Q,x[:,i]) + dot(u[i],R,u[i]) for i in 1:N-1))\n\noptimize!(ocp)\nuopt = value.(u)\nxopt = value.(x)\n\nusing Plots\n\np1 = plot(0:(N-1),uopt,marker=:diamond,label=\"u\",linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"u\")\n\np2 = plot(0:N,xopt',marker=:diamond,label=[\"x₁\" \"x₂\"],linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"x\")\n\nplot(p1,p2,layout=(2,1))",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Finite-horizon LQR as a QP"
    ]
  },
  {
    "objectID": "discr_dir_LQR.html#sequential-dense-formulation",
    "href": "discr_dir_LQR.html#sequential-dense-formulation",
    "title": "Finite-horizon LQR as a QP",
    "section": "Sequential (dense) formulation",
    "text": "Sequential (dense) formulation\nWe can express \\bar{\\bm x} as a function of \\bar{\\bm u} and \\mathbf x_0. This can be done in a straightforward way using (Eq. 1), namely, \n\\bar{\\bm x} = (\\mathbf I-\\overline{\\mathbf A})^{-1}\\overline{\\mathbf B} \\bm u + (\\mathbf I-\\overline{\\mathbf A})^{-1} \\overline{\\mathbf A}_0 \\mathbf x_0.\n\nHowever, instead of solving the sets of equations, we can do this substitution in a more insightful way. Write down the state equation for several discrete times \n\\begin{aligned}\n\\bm x_1 &= \\mathbf A\\mathbf x_0 + \\mathbf B\\bm u_0\\\\\n\\bm x_2 &= \\mathbf A\\mathbf x_0 + \\mathbf B\\bm u_0\\\\\n     &= \\mathbf A(\\mathbf A\\mathbf x_0 + \\mathbf B\\bm u_0)+ \\mathbf B\\bm u_0\\\\\n     &= \\mathbf A^2\\mathbf x_0 + \\mathbf A\\mathbf B\\bm u_0 + \\mathbf B\\bm u_0\\\\\n     &\\vdots\\\\\n\\bm x_k &= \\mathbf A^k\\mathbf x_0 + \\mathbf A^{k-1}\\mathbf B\\bm u_0 +\\mathbf A^{k-2}\\mathbf B\\bm u_1 +\\ldots \\mathbf B\\bm u_{k-1}.\n\\end{aligned}\n\nRewriting into matrix-vector form (and extending the time k up to the final time N) \n\\begin{bmatrix}\n\\bm x_1\\\\\\bm x_2\\\\\\vdots\\\\\\bm x_N\n\\end{bmatrix}\n=\n\\underbrace{\n\\begin{bmatrix}\n  \\mathbf B & & & \\\\\n  \\mathbf A\\mathbf B & \\mathbf B & & \\\\\n  \\vdots & & \\ddots &\\\\\n  \\mathbf A^{N-1}\\mathbf B & \\mathbf A^{N-2}\\mathbf B & & \\mathbf B\n\\end{bmatrix}}_{\\widehat{\\mathbf C}}\n  \\begin{bmatrix}\n\\bm u_0\\\\\\bm u_1\\\\\\vdots\\\\\\bm u_{N-1}\n\\end{bmatrix}\n+\n\\underbrace{\n  \\begin{bmatrix}\n\\mathbf A\\\\\\mathbf A^2\\\\\\vdots\\\\\\mathbf A^N\n\\end{bmatrix}}_{\\widehat{\\mathbf A}}\\mathbf x_0.\n\nFor convenience, let’s rewrite the compact relation between \\bar{\\bm x} and \\bar{\\bm u} and \\mathbf x_0 \n\\bar{\\bm x} = \\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0.\n\\tag{2}\nWe can now substitute this into the original cost, which then becomes independent of \\bar{\\bm x}, which we reflect formally by using a new name \\tilde J \n\\begin{aligned}\n\\tilde J(\\bar{\\bm u};\\mathbf x_0) &= \\frac{1}{2}(\\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0)^\\top\\overline{\\mathbf Q} (\\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0) + \\frac{1}{2}\\bar{\\bm u}^\\top\\overline{\\mathbf R} \\bar{\\bm u} + \\frac{1}{2}\\mathbf x_0^\\top\\mathbf Q\\mathbf x_0\\\\\n&= \\frac{1}{2}\\bar{\\bm u}^\\top\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\mathbf x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\frac{1}{2} \\mathbf x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf A} \\mathbf x_0 + \\frac{1}{2}\\bar{\\bm u}^\\top\\overline{\\mathbf R} \\bar{\\bm u} + \\frac{1}{2}\\mathbf x_0^\\top\\mathbf Q\\mathbf x_0\\\\\n&= \\frac{1}{2}\\bar{\\bm u}^\\top(\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} + \\overline{\\mathbf R})\\bar{\\bm u} + \\mathbf x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\frac{1}{2} \\mathbf x_0^\\top(\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf A} + \\mathbf Q)\\mathbf x_0.\n\\end{aligned}\n\nThe last term (the one independent of \\bar{\\bm u}) does not have an impact on the optimal \\bar{\\bm u} and therefore it can be discarded, but such minor modification perhaps does not justify a new name for the cost function. We write it as \n\\tilde J(\\bar{\\bm u};\\mathbf x_0) = \\frac{1}{2}\\bar{\\bm u}^\\top\\underbrace{(\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} + \\overline{\\mathbf R})}_{\\mathbf H}\\bar{\\bm u} +  \\mathbf x_0^\\top\\underbrace{\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C}}_{\\mathbf F^\\top} \\bar{\\bm u}.\n\nThis cost is a function of \\bar{\\bm u}, the initial state \\mathbf x_0 is regarded as a fixed parameter. Its gradient is \n\\nabla \\tilde J = \\mathbf H\\bar{\\bm u}+\\mathbf F\\mathbf x_0.\n\nSetting it to zero leads to the following linear system of equations \n\\mathbf H\\bar{\\bm u}=-\\mathbf F\\mathbf x_0\n that needs to be solved for \\bar{\\bm u}. Formally, we write the solution as \n\\bar{\\bm u} = -\\mathbf H^{-1} \\mathbf F \\mathbf x_0.\n\n\nExample 3 (Reformulating the unconstrained LQR problem as a system of linear equations – sequential approach)  \n\n\nShow the code\nfunction direct_dlqr_sequential(A,B,x₀,Q,R,S,N)\n    n = size(A)[1]\n    m = size(B)[2]\n    Q̄ = BlockArray(spzeros(N*n,N*n),repeat([n],N),repeat([n],N))\n    for i=1:(N-1)\n        Q̄[Block(i,i)] = Q\n    end\n    Q̄[Block(N,N)] = S\n    R̄ = BlockArray(spzeros(N*m,N*m),repeat([m],N),repeat([m],N))\n    for i=1:N\n        R̄[Block(i,i)] = R\n    end\n    Ĉ = BlockArray(spzeros(N*n,N*m),repeat([n],N),repeat([m],N))\n    Â = BlockArray(spzeros(N*n,n),repeat([n],N),[n])\n    for i=1:N\n        for j = 1:i\n            Ĉ[Block(i,j)] = A^(i-j)*B\n            Â[Block(i,1)] = A^i\n        end\n    end\n    H = Ĉ'*Q̄*Ĉ + R̄\n    H = Array(H)\n    F = Ĉ'*Q̄*Â\n    F = Array(F)\n\n    uopt = -H\\(F*x₀)\n    xopt = Ĉ*uopt + Â*x₀\n    xopt = reshape(xopt,(2,:))\n    return xopt,uopt\nend\n\nn = 2               # Number of state variables.\nm = 1               # Number of (control) input variables. \nA = rand(n,n)       # State matrix.\nB = rand(n,m)       # Input coupling matrix.\nx₀ = [1.0, 3.0]     # Initial state.\n\nN = 10              # Time horizon.\n\ns = [1.0, 2.0]      \nq = [1.0, 2.0]\nr = [1.0]\n\nS = diagm(0=&gt;s)     # Matrix defining the terminal state cost.\nQ = diagm(0=&gt;q)     # Matrix defining the running state dost.\nR = diagm(0=&gt;r)     # Matrix defining the cost of control.\n\nxopt,uopt = direct_dlqr_sequential(A,B,x₀,Q,R,S,N)\n\nusing Plots\np1 = plot(0:(N-1),uopt,marker=:diamond,label=\"u\",linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"u\")\n\np2 = plot(0:N,hcat(x₀,xopt)',marker=:diamond,label=[\"x₁\" \"x₂\"],linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"x\")\n\nplot(p1,p2,layout=(2,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding the constraints on controls\nAdding constraints on \\bar{\\bm u} is straightforward. It is just that instead of a linear system we will have a linear system with additional inequality constraints. Let’s get one \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u}} & \\quad \\frac{1}{2}\\bar{\\bm u}^T \\mathbf H \\bar{\\bm u} + \\mathbf x_0^T\\mathbf F^T \\bar{\\bm u}\\\\\n\\text{subject to} &\\quad \\bar{\\bm u} \\leq \\bar{\\mathbf u}^\\mathrm{max}\\\\\n               &\\quad \\bar{\\bm u} \\geq \\bar{\\mathbf u}^\\mathrm{min},\n\\end{aligned}\n which we can rewrite more explicitly (in the matrix-vector format) as \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u}} & \\quad \\frac{1}{2}\\bar{\\bm u}^T \\mathbf H \\bar{\\bm u} + \\mathbf x_0^T\\mathbf F^T \\bar{\\bm u}\\\\\n\\text{subject to} & \\begin{bmatrix}\n                  \\mathbf{I}  &    &         &    \\\\\n                    & \\mathbf{I}  &         &    \\\\\n                    &    & \\ddots  &    \\\\\n                    &    &         &  \\mathbf{I} \\\\\n                    -\\mathbf{I}   &   &     &    \\\\\n                    & -\\mathbf{I} &         &    \\\\\n                    &    & \\ddots  &    \\\\\n                    &    &         &  -\\mathbf{I}\n                 \\end{bmatrix}\n                 \\begin{bmatrix}\n                  \\bm u_0 \\\\ \\bm u_1 \\\\ \\vdots \\\\ \\bm u_{N-1}\n                 \\end{bmatrix}\n                 \\leq\n                 \\begin{bmatrix}\n                  \\mathbf u^\\mathbf{max} \\\\ \\mathbf u^\\mathrm{max} \\\\ \\vdots \\\\ \\mathbf u^\\mathrm{max}\\\\ -\\mathbf u^\\mathrm{min} \\\\ -\\mathbf u^\\mathrm{min} \\\\ \\vdots \\\\ -\\mathbf u^\\mathrm{min}\n                 \\end{bmatrix}.\n\\end{aligned}\n\n\nExample 4 (Reformulating the LQR problem with constraints on control as a quadratic program – sequential approach)  \n\n\nShow the code\nusing LinearAlgebra\nusing BlockArrays\nusing SparseArrays\nusing JuMP\nusing OSQP\n\nfunction direct_dlqr_sequential(A,B,x₀,Q,R,S,N,(umin,umax))\n    n = size(A)[1]\n    m = size(B)[2]\n    Q̄ = BlockArray(spzeros(N*n,N*n),repeat([n],N),repeat([n],N))\n    for i=1:(N-1)\n        Q̄[Block(i,i)] = Q\n    end\n    Q̄[Block(N,N)] = S\n    R̄ = BlockArray(spzeros(N*m,N*m),repeat([m],N),repeat([m],N))\n    for i=1:N\n        R̄[Block(i,i)] = R\n    end\n    Ĉ = BlockArray(spzeros(N*n,N*m),repeat([n],N),repeat([m],N))\n    Â = BlockArray(spzeros(N*n,n),repeat([n],N),[n])\n    for i=1:N\n        for j = 1:i\n            Ĉ[Block(i,j)] = A^(i-j)*B\n            Â[Block(i,1)] = A^i\n        end\n    end\n    H = Ĉ'*Q̄*Ĉ + R̄\n    H = Array(H)\n    F = Ĉ'*Q̄*Â\n    F = Array(F)\n    prob = Model()\n    @variable(prob, u[1:N*m])\n    @objective(prob, Min, 1/2*dot(u,H,u) + dot(F*x₀,u))\n    @constraint(prob, u .&gt;= umin)\n    @constraint(prob, u .&lt;= umax)\n    set_silent(prob)\n    set_optimizer(prob, OSQP.Optimizer)\n    optimize!(prob)\n    uopt = value.(u)\n    xopt = Ĉ*uopt + Â*x₀\n    xopt = reshape(xopt,(2,:))\n#=     u = Variable(N*m)\n    problem = minimize(1/2*quadform(u,H) + dot(F*x₀,u))\n    problem.constraints = [u &gt;= umin, u &lt;= umax]\n    Convex.solve!(problem, SCS.Optimizer; silent = true)\n    xopt = Ĉ*u.value + Â*x₀\n    xopt = reshape(xopt,(2,:))\n    uopt = u.value =#\n    return xopt,uopt\nend\n\nn = 2               # Number of state variables.\nm = 1               # Number of (control) input variables. \nA = rand(n,n)       # State matrix.\nB = rand(n,m)       # Input coupling matrix.\nx₀ = [1.0, 3.0]     # Initial state.\n\nN = 10              # Time horizon.\n\ns = [1.0, 2.0]      \nq = [1.0, 2.0]\nr = [1.0]\n\nS = diagm(0=&gt;s)     # Matrix defining the terminal state cost.\nQ = diagm(0=&gt;q)     # Matrix defining the running state dost.\nR = diagm(0=&gt;r)     # Matrix defining the cost of control.\n\numin = -1.0\numax = 1.0\n\nxopt,uopt = direct_dlqr_sequential(A,B,x₀,Q,R,S,N,(umin,umax))\n\nusing Plots\np1 = plot(0:(N-1),uopt,marker=:diamond,label=\"u\",linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"u\")\n\np2 = plot(0:N,hcat(x₀,xopt)',marker=:diamond,label=[\"x₁\" \"x₂\"],linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"x\")\n\nplot(p1,p2,layout=(2,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding the constraints on states\nWe might feel a little bit uneasy about loosing an immediate access to \\bar{\\bm x}. But the game is not lost. We just need to express \\bar{\\bm x} as a function of \\bar{\\bm u} and \\mathbf x_0 and impose the constraint on the result. But such expression is already available, see (Eq. 2). Therefore, we can formulate the constraint, say, an upper bound on the state vector \n\\bm x_k \\leq \\mathbf x_\\mathrm{max}\n as \n\\bar{\\mathbf x}^\\mathrm{min} \\leq \\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0 \\leq \\bar{\\mathbf x}^\\mathrm{max},\n where the the bars in \\bar{\\mathbf x}^\\mathrm{min} and \\bar{\\mathbf x}^\\mathrm{max} obviously indicates that these vectors were obtained by stacking the corresponding vectors for all times k=1,\\ldots,N.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Finite-horizon LQR as a QP"
    ]
  },
  {
    "objectID": "opt_theory_hw.html",
    "href": "opt_theory_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Solve the problem of distributing a limited power to N electric vehicle chargers throughout the next K hours. Specifically, you are supposed to find an hourly-sampled optimal plan for each car that minimizes the total cost of the charging.\nYou are given a time-dependent maximum energy a[k] available for charging (in kWh), a time-dependent cost of the energy c[k] (in €/kWh), maximum allowed charging energy per hour for the ith car m_i (in kWh), total requested energy r_i for the ith car (also in kWh), and the departure (discrete) time d_i for each car.\nAll the cars are connected to chargers and can start charging from time 1. Departure time is the time when the energy charged to the ith car has reached at least r_i and charging of the car must stop.\nLet’s emphasize: the index k is a time index running from 1 to K, and the index i specifies the corresponding car, i.e. i\\in\\{1,2,\\ldots, N\\}\nFormulate this task as an optimization problem, identify the class of this optimization problem (LP, QP or NLP) and solve it by completing the following Julia script and heeding the following instructions.\n\nModel the optimization problem either trough JuMP or Convex.\nSolve it using one of the available solvers: HiGHS, SCS, Ipopt.\nUpload only a single file named hw.jl as your solution.\n\n\nusing JuMP # or Convex\n\nusing HiGHS, SCS, Ipopt # Available solvers\n\n\"\"\"\n    find_optimal_charging_plan(\n        a::Vector{Float64},\n        c::Vector{Float64},\n        m::Vector{Float64},\n        r::Vector{Float64},\n        d::Vector{Int64}\n    )\n\nComputes an optimal charging schedule for `N` electric vehicles over `K` hours.\n\n# Arguments\n- `a`: A `K`-element vector specifying the maximum available charging energy per hour (kWh).\n- `c`: A `K`-element vector representing the cost of charging per hour (€/kWh).\n- `m`: An `N`-element vector with the maximum allowed charging energy for each vehicle (kWh).\n- `r`: An `N`-element vector specifying the total energy required by each vehicle (kWh).\n- `d`: An `N`-element vector indicating the departure time (hour) of each vehicle.\n\n# Returns\nA tuple containing:\n- An `N × K` matrix representing the optimal charging schedule (kWh allocated per vehicle per hour).\n- The optimal total charging cost (€).\n- A symbol indicating the type of optimization problem solved (`:LP`, `:QP`, or `:NLP`).\n\"\"\"\nfunction find_optimal_charging_plan(\n    a::Vector{Float64},\n    c::Vector{Float64},\n    m::Vector{Float64},\n    r::Vector{Float64},\n    d::Vector{Int64}\n)\n\n    K = length(a) # Timespan (hours)\n    N = length(m) # Number of vehicles\n\n    # TODO model and solve the problem\n\n    return zeros(N, K), 0.0, :NLP # or :LP or :QP\n\nend\n\nThe data you can use to test your solution is given in the following tables.\n\nData for the three vehicles: maximum energy per hour, total requested energy, and the departure time\n\n\n\n\n\n\n\n\nCar\nm_i (kWh)\nr_i (kWh)\nd_i\n\n\n\n\n1\n6\n15\n3\n\n\n2\n6\n25\n7\n\n\n3\n4\n30\n10\n\n\n\n\nEvolution in time of the maximum available energy and the cost of the energy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\na[k]\n11.6\n11.9\n10.6\n8.8\n8.0\n8.8\n10.6\n11.9\n11.6\n10.0\n\n\nc[k]\n0.58\n0.72\n0.92\n0.68\n0.54\n0.78\n0.64\n0.57\n0.74\n0.74",
    "crumbs": [
      "1. Optimization – theory",
      "Homework"
    ]
  },
  {
    "objectID": "opt_theory_hw.html#electric-vehicle-charging",
    "href": "opt_theory_hw.html#electric-vehicle-charging",
    "title": "Homework",
    "section": "",
    "text": "Solve the problem of distributing a limited power to N electric vehicle chargers throughout the next K hours. Specifically, you are supposed to find an hourly-sampled optimal plan for each car that minimizes the total cost of the charging.\nYou are given a time-dependent maximum energy a[k] available for charging (in kWh), a time-dependent cost of the energy c[k] (in €/kWh), maximum allowed charging energy per hour for the ith car m_i (in kWh), total requested energy r_i for the ith car (also in kWh), and the departure (discrete) time d_i for each car.\nAll the cars are connected to chargers and can start charging from time 1. Departure time is the time when the energy charged to the ith car has reached at least r_i and charging of the car must stop.\nLet’s emphasize: the index k is a time index running from 1 to K, and the index i specifies the corresponding car, i.e. i\\in\\{1,2,\\ldots, N\\}\nFormulate this task as an optimization problem, identify the class of this optimization problem (LP, QP or NLP) and solve it by completing the following Julia script and heeding the following instructions.\n\nModel the optimization problem either trough JuMP or Convex.\nSolve it using one of the available solvers: HiGHS, SCS, Ipopt.\nUpload only a single file named hw.jl as your solution.\n\n\nusing JuMP # or Convex\n\nusing HiGHS, SCS, Ipopt # Available solvers\n\n\"\"\"\n    find_optimal_charging_plan(\n        a::Vector{Float64},\n        c::Vector{Float64},\n        m::Vector{Float64},\n        r::Vector{Float64},\n        d::Vector{Int64}\n    )\n\nComputes an optimal charging schedule for `N` electric vehicles over `K` hours.\n\n# Arguments\n- `a`: A `K`-element vector specifying the maximum available charging energy per hour (kWh).\n- `c`: A `K`-element vector representing the cost of charging per hour (€/kWh).\n- `m`: An `N`-element vector with the maximum allowed charging energy for each vehicle (kWh).\n- `r`: An `N`-element vector specifying the total energy required by each vehicle (kWh).\n- `d`: An `N`-element vector indicating the departure time (hour) of each vehicle.\n\n# Returns\nA tuple containing:\n- An `N × K` matrix representing the optimal charging schedule (kWh allocated per vehicle per hour).\n- The optimal total charging cost (€).\n- A symbol indicating the type of optimization problem solved (`:LP`, `:QP`, or `:NLP`).\n\"\"\"\nfunction find_optimal_charging_plan(\n    a::Vector{Float64},\n    c::Vector{Float64},\n    m::Vector{Float64},\n    r::Vector{Float64},\n    d::Vector{Int64}\n)\n\n    K = length(a) # Timespan (hours)\n    N = length(m) # Number of vehicles\n\n    # TODO model and solve the problem\n\n    return zeros(N, K), 0.0, :NLP # or :LP or :QP\n\nend\n\nThe data you can use to test your solution is given in the following tables.\n\nData for the three vehicles: maximum energy per hour, total requested energy, and the departure time\n\n\n\n\n\n\n\n\nCar\nm_i (kWh)\nr_i (kWh)\nd_i\n\n\n\n\n1\n6\n15\n3\n\n\n2\n6\n25\n7\n\n\n3\n4\n30\n10\n\n\n\n\nEvolution in time of the maximum available energy and the cost of the energy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\na[k]\n11.6\n11.9\n10.6\n8.8\n8.0\n8.8\n10.6\n11.9\n11.6\n10.0\n\n\nc[k]\n0.58\n0.72\n0.92\n0.68\n0.54\n0.78\n0.64\n0.57\n0.74\n0.74",
    "crumbs": [
      "1. Optimization – theory",
      "Homework"
    ]
  },
  {
    "objectID": "cont_numerical_software.html",
    "href": "cont_numerical_software.html",
    "title": "Software",
    "section": "",
    "text": "The methods studied in this chapter are already quite mature and well described. Software implementations exist. Here we enumerate some of them.\nacados Implemented in C but interfaces exist for Matlab and Python FOSS [1]\nGPOPS-II Matlab\nrockit In Python, built on top of CasADi, interface to Matlab.\n\n\n\n\n Back to topReferences\n\n[1] R. Verschueren et al., “Acados—a modular open-source framework for fast embedded optimal control,” Mathematical Programming Computation, vol. 14, no. 1, pp. 147–183, Mar. 2022, doi: 10.1007/s12532-021-00208-8.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Software"
    ]
  },
  {
    "objectID": "discr_indir_LQR_fin_horizon.html",
    "href": "discr_indir_LQR_fin_horizon.html",
    "title": "Discrete-time LQR on a finite horizon",
    "section": "",
    "text": "We consider a linear time-invariant (LTI) system described by the state equation \n\\bm x_{k+1} = \\mathbf A \\bm x_{k} + \\mathbf B \\bm u_k, \\qquad \\bm x_0 = \\mathbf x_0,\n and our goal is to find a (vector) control sequence \\bm u_0, \\bm u_{1},\\ldots, \\bm u_{N-1} that minimizes \nJ_0^N = \\frac{1}{2}\\bm x_N^\\top\\mathbf S_N\\bm x_N + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[\\bm x_k^\\top \\mathbf Q \\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right],\n where the quadratic cost function is parameterized the matrices that must be symmetric and at least positive semidefinite, otherwise the corresponding quadratic terms will not play a good role of penalizing the (weighted) distance from zero.\nWe will see in a moment that the matrix \\mathbf R must comply with an even stricter condition – it must be positive definite. To summarize the assumptions about the matrices, we require \n\\mathbf S_N\\succeq 0, \\mathbf Q\\succeq 0, \\mathbf R\\succ 0.\nThe Hamiltonian for our problem is \n\\boxed{\nH(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}) = \\frac{1}{2}\\left(\\bm x_k^\\top \\mathbf Q\\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right) + \\boldsymbol \\lambda_{k+1}^\\top\\left(\\mathbf A\\bm x_k+\\mathbf B\\bm u_k\\right).\n}\nIn the following derivations we use the shorthand notation H_k for H(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}).\nSubstituting into the general necessary conditions derived in the previous section we obtain \n\\begin{aligned}\n\\mathbf x_{k+1} &= \\nabla_{\\boldsymbol \\lambda_{k+1}}H_k=\\mathbf A\\bm x_k+\\mathbf B\\bm u_k,\\\\\n\\boldsymbol\\lambda_k &= \\nabla_{\\mathbf x_{k}}H_k=\\mathbf Q\\bm x_k+\\mathbf A^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\mathbf 0 &= \\nabla_{\\mathbf u_{k}}H_k = \\mathbf R\\bm u_k + \\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n0 &= (\\mathbf S_N \\bm x_N - \\boldsymbol \\lambda_N)^\\top\\; \\text{d} \\bm x_N,\\\\\n\\bm x_0 &= \\mathbf x_0.\n\\end{aligned}\nThe last two equations represent the boundary conditions. Note that here we have already fixed the initial state. If this is not appropriate in a particular scenario, go back and adjust the boundary equation accordingly.\nThe third equation above – the stationarity equation – can be used to extract the optimal control \n\\bm u_k = -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1}.\nThe need for nonsingularity of \\mathbf R is now obvious. Upon substituting the recipe for the optimal \\bm u_k into the state and the co-state equations, two recursive (or recurrent or just discrete-time) equations result \n\\begin{bmatrix}\n\\mathbf x_{k+1}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf A & -\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q & \\mathbf A^\\top\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bm x_k \\\\ \\boldsymbol\\lambda_{k+1}\n\\end{bmatrix}.\nThis is a two-point boundary value problem (TP-BVP). The problem is of order 2n, where n is the dimension of the state space. In order to solve it we need 2n boundary values: n boundary values are provided by \\bm x_i = \\mathbf x_0, and n boundary values are given by the other boundary condition, from which \\boldsymbol\\lambda_N must be extracted. Most of our subsequent discussion will revolve around this task.\nAn idea might come into our mind: provided \\mathbf A is nonsingular, we can left-multiply the above equation by the inverse of \\mathbf A to obtain \n\\begin{bmatrix}\n\\mathbf x_{k}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf A^{-1} & \\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q\\mathbf A^{-1} & \\mathbf A^\\top+\\mathbf Q\\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf x_{k+1} \\\\ \\boldsymbol\\lambda_{k+1}\n\\end{bmatrix}\n\\tag{1}\nThis helped at least to have both variable evolving in the same direction in time (both backward) but we do not know \\boldsymbol\\lambda_N anyway. Nonetheless, do not forget this result. We are going to invoke it later.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on a finite horizon"
    ]
  },
  {
    "objectID": "discr_indir_LQR_fin_horizon.html#fixed-final-state-and-finite-time-horizon",
    "href": "discr_indir_LQR_fin_horizon.html#fixed-final-state-and-finite-time-horizon",
    "title": "Discrete-time LQR on a finite horizon",
    "section": "Fixed final state and finite time horizon",
    "text": "Fixed final state and finite time horizon\nBack to the nonzero control case. First we are going to investigate the scenario when the final requested state is given by \\mathbf x^\\text{ref}. The optimal control problem turns into \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x_0, \\bm{x}_{1},\\ldots,\\bm{x}_{N},\\bm{u}_{0},\\ldots,\\bm{u}_{N-1}} &\\; \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[\\bm x_k^T \\mathbf Q \\bm x_k+\\bm u_k^T \\mathbf R\\bm u_k\\right]\\\\\n\\text{s.t. } & \\; \\mathbf x_{k+1} = \\mathbf A \\mathbf x_{k} + \\mathbf B \\bm u_k,\\\\\n&\\; \\bm x_0 = \\mathbf x_0,\\\\\n&\\; \\bm x_N = \\mathbf x^\\text{ref},\\\\\n&\\; \\mathbf Q\\geq 0, \\mathbf R&gt;0.\n\\end{aligned}\n\n\nNote also that the term penalizing the final state is removed from the cost because it is always fixed. After eliminating the controls using the stationarity equation \n\\bm u_k = -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\n and replacing the general boundary condition at the final time by \\bm x_N = \\mathbf x^\\text{ref}, the two-point boundary value problem specializes to \n\\begin{aligned}\n\\mathbf x_{k+1} &=\\mathbf A\\bm x_k-\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\boldsymbol\\lambda_k &= \\mathbf Q\\bm x_k+\\mathbf A^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\bm x_0 &= \\mathbf x_0,\\\\\n\\bm x_N &= \\mathbf x^\\text{ref}.\n\\end{aligned}\n\nThis problem is clearly an instance of a two-point boundary value problem (TP-BVP) as the state vector is specified at both ends of the time interval. The costate is left unspecified, but it is fine because only 2n boundary conditions are needed. While BVP are generally difficult to solve, our problem at hand adds one more layer of complexity. For the state variable its evolution forward in time is specified by the state equation, while for the co-state variable the evolution backward in time is prescribed by the co-state equation.\n\n\\begin{bmatrix}\n\\mathbf x_{k+1}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf A & -\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q & \\mathbf A^\\top\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bm x_k \\\\ \\boldsymbol\\lambda_{k+1}.\n\\end{bmatrix}\n\nThere is not much we can do with these equations in this form. However, in case of a nonsingular matrix \\mathbf A, we can invoke the discrete-time Hamiltonian system (Eq. 1), in which we reorganized the equations so that both state and co-state variables evolve backwards. For convenience we give it here again \n\\begin{bmatrix}\n\\mathbf x_{k}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\\underbrace{\n\\begin{bmatrix}\n\\mathbf A^{-1} & \\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q\\mathbf A^{-1} & \\mathbf A^\\top+\\mathbf Q\\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\n\\end{bmatrix}}_{\\mathbf H}\n\\begin{bmatrix}\n\\mathbf x_{k+1} \\\\ \\boldsymbol\\lambda_{k+1}.\n\\end{bmatrix}\n\nThis can be used to relate the state and costate at the initial and final times of the interval \n\\begin{bmatrix}\n\\mathbf x_{0}\\\\\\boldsymbol\\lambda_0\n\\end{bmatrix}\n=\\underbrace{\n\\begin{bmatrix}\n\\mathbf A^{-1} & \\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q\\mathbf A^{-1} & \\mathbf A^\\top+\\mathbf Q\\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\n\\end{bmatrix}^N}_{\\mathbf M\\coloneqq \\mathbf H^N}\n\\begin{bmatrix}\n\\mathbf x_{N} \\\\ \\boldsymbol\\lambda_{N}\n\\end{bmatrix}.\n\nFrom the first equation we can get \\boldsymbol \\lambda_N. First, let’s rewrite it here \n\\mathbf M_{12}\\boldsymbol \\lambda_N = \\bm x_0-\\mathbf M_{11}\\bm x_N,\n from which (after substituting for the known initial and final states) \n\\boldsymbol \\lambda_N = \\mathbf M_{12}^{-1}(\\mathbf r_0-\\mathbf M_{11}\\mathbf r_N).\n\nHaving the final state and the final co-state, \\bm x_N and \\boldsymbol \\lambda_N, respectively, we can solve the Hamiltonian system backward to get the states and co-states on the whole time interval [0,N-1].\n\nSpecial case: minimum-energy control (\\mathbf Q = \\mathbf 0)\nWe can get some more insight into the problem if we further restrict the class of problems we can treat. Namely, we will assume \n\\mathbf Q = \\mathbf 0.\n\nThis is a significant restriction, nonetheless the resulting problem is still practically reasonable. And we do not need to assume that \\mathbf A is nonsingular. The cost function is then \nJ = \\sum_{k=0}^N \\mathbf u^\\top_k\\;\\bm u_k = \\sum_{k=0}^N \\|\\mathbf u\\|_2^2,   \n which is why the problem is called the minimum-energy control problem. Rewriting the state and co-state equations with the new restriction \\mathbf Q=\\mathbf 0 we get \n\\begin{aligned}\n\\bm x_{k+1} &= \\mathbf A\\bm x_k - \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1}\\\\\n\\boldsymbol \\lambda_k &= \\mathbf A^\\top\\boldsymbol\\lambda_{k+1}.\n\\end{aligned}\n\nIt is obvious why we wanted to enforce the \\mathbf Q=\\mathbf 0 restriction — the co-state equation is now completely decoupled from the state equation and can be solved independently \n\\boldsymbol \\lambda_k = (\\mathbf A^\\top)^{N-k}\\boldsymbol \\lambda_N.\n\nNow substitute this solution of the co-state equation into the state equation \n\\bm x_{k+1} = \\mathbf A\\bm x_k - \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-k-1}\\boldsymbol \\lambda_N.\n\nFinding a solution to the state equation is now straightforward — the second summand on the right is considered as a an “input”. The solution is then \n\\bm x_{k} = \\mathbf A^k\\bm x_0 - \\sum_{i=0}^{k-1}\\mathbf A^{k-1-i}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-i-1}\\boldsymbol \\lambda_N.\n\nThe last step reveals the motivation for all the previous steps — we can now express the state at the final time, and by doing that we introduce some known quantity into the problem \n\\bm x_{N} = \\mathbf x^\\text{ref}= \\mathbf A^N\\bm x_0 - \\underbrace{\\sum_{i=0}^{N-1}\\mathbf A^{N-1-i}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-i-1}}_{G_{0,N,R}}\\boldsymbol \\lambda_N.\n\nThis enables us to calculate \\boldsymbol \\lambda_N directly as a solution to a linear equation. To make the notation simpler, denote the sum in the expression above by \\mathbf G_{0,N,R} (we will discuss this particular object in a while) \n\\boldsymbol \\lambda_N = -\\mathbf G^{-1}_{0,N,R}\\; (\\mathbf x^\\text{ref}-\\mathbf A^N\\bm x_0).\n\nThe rest is quite straightforward as the optimal control depends (through the stationarity equation) on the co-state \n\\boxed{\n\\bm u_k = \\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-k-1}\\mathbf G^{-1}_{0,N,R}\\; (\\mathbf x^\\text{ref}-\\mathbf A^N\\bm x_0).\n}\n\nThis is the desired formula for computation of the optimal control.\nA few observations can be made\n\nThe control is proportional to the difference (\\mathbf x^\\text{ref}-\\mathbf A^N\\bm x_0). The intuitive interpretation is that the further the requested final state is from the state into which the system would finally evolve without any control, the higher the control is needed.\n\nThe control is proportional to the inverse of a matrix \\mathbf G_{0,N,R} which is called weighted reachability Gramian. The standard result from the theory of linear dynamic systems is that nonsingularity of a reachability Gramian is equivalent to reachability of the system. More on this below.\n\n\nWeighted reachability Gramian\nRecall (perhaps from your linear systems course) that there is a matrix called discrete-time reachability Gramian defined as \n\\mathbf G = \\sum_{k=0}^{\\infty} \\mathbf A^{k}\\mathbf B\\mathbf B^\\top(\\mathbf A^\\top)^k\n and the nonsingularity of this matrix serves as a test of reachability for stable discrete-time linear systems.\nHow does this classical object relate to the object \\mathbf G_{0,N,R} introduced in the previous paragraph? First consider the restriction of the summation from the infinite interval [0,\\infty] to [0,N-1]. In other words, we analyze the matrix \n\\mathbf G_{0,N} = \\sum_{k=0}^{N-1} \\mathbf A^{N-1-k}\\mathbf B\\mathbf B^\\top(\\mathbf A^\\top)^{N-1-k}.\n\nRecall that Caley-Hamilton theorem tells us that every higher power of an N\\times N matrix can be expressed as a linear combination of powers of 0 through N-1. In other words, using higher order powers of A than N-1 cannot increase the rank of the matrix.\nFinally, provided \\mathbf R is nonsingular (hence \\mathbf R^{-1} is nonsingular as well), the rank of the Gramian is not changed after introducing the weight\n\n\\mathbf G_{0,N,R} = \\sum_{k=0}^{N-1} \\mathbf A^{N-1-k}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-1-k}.\n\nThe weighted Gramian defined on a finite discrete-time horizon is invertible if and only if the (stable) system is reachable. This conclusion is quite natural: if an optimal control is to be found, first it must be guaranteed that any control can be found which brings the system from an arbitrary initial state into an arbitrary final state on a finite time interval — the very definition of reachability.\nTo summarize the whole fixed-final state case, the optimal control can be computed numerically by solving a TP-BVP. For the minimum-problem even a formula exists and there is no need for a numerical optimization solver. But the outcome is always just a sequence of controls. In this regard, the new (indirect) approach did not offer much more that what the direct approach did. Although the new insight is rewarding, it is paid for by the inability to handle constraints on the control or state variables.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on a finite horizon"
    ]
  },
  {
    "objectID": "discr_indir_LQR_fin_horizon.html#free-final-state-and-finite-time-horizon",
    "href": "discr_indir_LQR_fin_horizon.html#free-final-state-and-finite-time-horizon",
    "title": "Discrete-time LQR on a finite horizon",
    "section": "Free final state and finite time horizon",
    "text": "Free final state and finite time horizon\nThe previous discussion revolved around the task of bringing the system to a given final state exactly. What if we relax this strict requirement and instead just request that the system be eventually brought to the close vicinity of the requested state? How close — this could be affected by the terminal state penalty in the cost function.\n\nThe only change with respect to the previous development is just in the boundary condition — the one at the final time. Now the final state \\bm x_N can also be used as a parameter for our optimization. Hence \\text{d}\\bm x_N\\neq 0 and the other term in the product must vanish. We write down again the full necessary conditions including the new boundary conditions \n\\begin{aligned}\n\\bm x_{k+1} &=\\mathbf A\\bm x_k-\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\boldsymbol\\lambda_k &= \\mathbf Q\\bm x_k+\\mathbf A^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\bm u_k &= -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\mathbf S_N \\bm x_N &= \\boldsymbol \\lambda_N,\\\\\n\\bm x_0 &= \\mathbf x_0.\n\\end{aligned}\n\nWe find ourselves in a pretty much similar trouble as before. The final-time boundary condition refers to the variables whose values we do not know. The solution is provided by the insightful guess, namely, why not trying to extend the linear relationship between the state and the co-state at the final time to all preceding discrete times? That is, we assume \n\\mathbf S_k \\bm x_k = \\boldsymbol \\lambda_k.\n\\tag{2}\nAt first, we can have no idea if it works. But let’s try it and see what happens. Substitute (Eq. 2) into the state and co-state equations. We start with the state equation \n\\bm x_{k+1} =\\mathbf A\\bm x_k-\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\bm x_{k+1}.\n\nSolving for \\bm x_{k+1} yields \n\\bm x_{k+1} =(\\mathbf I+\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1})^{-1}\\mathbf A\\bm x_k.\n\nNow perform the same substitution into the co-state equation \n\\mathbf S_k \\bm x_k = \\mathbf Q\\bm x_k+\\mathbf A^\\top\\mathbf S_{k+1}\\bm x_{k+1},\n and substitute for \\bm x_{k+1} from the state equation into the previous equation to get \n\\mathbf S_k \\bm x_k = \\mathbf Q\\bm x_k+\\mathbf A^\\top\\mathbf S_{k+1}(\\mathbf I+\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1})^{-1}\\mathbf A\\bm x_k.\n\nSince this equation must hold for an arbitrary \\bm x_k, we get an equation in the matrices \\mathbf S_k \n\\boxed{\n\\mathbf S_k = \\mathbf Q+\\mathbf A^\\top\\mathbf S_{k+1}(\\mathbf I+\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1})^{-1}\\mathbf A.\n}\n\nThis is a superfamous equation and is called difference (or discrete-time) Riccati equation. When initialized with \\mathbf S_N, it generates the sequence of matrices \\mathbf S_{N-1}, \\mathbf S_{N-2}, \\mathbf S_{N-3},\\ldots Indeed, a noteworthy feature of this sequence is that it is initialized at the final time and the equation prescribes how the sequence evolves backwards.\nOnce we have generated a sufficiently long sequence (down to \\mathbf S_{1}), the optimal control sequence \\bm u_0, \\bm u_1, \\ldots, \\bm u_{N-1} is then computed using the stationary equation \n\\bm u_k = -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1}=-\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\bm x_{k+1}.\n\nThis suggests that the optimal control is generated using the state but the current scheme is noncausal because the control at a given time depends on the state at the next time. But turning this into a causal one is easy — just substitute the state equation for \\bm x_{k+1} and get \n\\bm u_k =-\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}(\\mathbf A\\bm x_{k}+\\mathbf B\\bm u_{k}).\n\nSolving this equation for \\bm u_k gives \n\\bm u_k = -\\underbrace{(\\mathbf I + \\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf B)^{-1}\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf A}_{\\mathbf K_k}\\mathbf x_{k}.\n\nMission accomplished. This is our desired control. A striking observation is that although we made no specifications as for the controller structure, the optimal control strategy turned out a feedback one! Let’s write it down explicitly \n\\boxed{\n\\bm u_k = -\\mathbf K_k \\bm x_{k}.\n}\n\n\n\n\n\n\n\nLQ-optimal control on a finite time horizon with a free final state is a feedback control\n\n\n\nThe importance of this result can hardly be overstated – the optimal control comes in the form of a proportional state-feedback control law.\n\n\nThe feedback gain is time-varying and deserves a name after its inventor — Kalman gain. Incorporating the knowledge that \\mathbf R is nonsingular, a minor simplification of the lengthy expression can be made \n\\mathbf K_k = (\\mathbf R + \\mathbf B^\\top\\mathbf S_{k+1}\\mathbf B)^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf A.\n\\tag{3}\nBefore we move on, let us elaborate a bit more on the difference Riccati equation. Invoking a popular (but hard to reliably memorize) rule for inversion of a sum of two matrices called matrix inversion lemma, which reads \n(\\mathbf A_{11}^{-1}+\\mathbf A_{12}\\mathbf A_{22}\\mathbf A_{21})^{-1} =\\mathbf A_{11}-\\mathbf A_{11}\\mathbf A_{12}(\\mathbf A_{21}\\mathbf A_{11}\\mathbf A_{12}+\\mathbf A_{22}^{-1})^{-1}\\mathbf A_{21}\\mathbf A_{11},\n the Riccati equation can be rewritten (after multiplying the brackets out) as \n\\boxed{\n\\mathbf S_k = \\mathbf Q + \\mathbf A^\\top\\mathbf S_{k+1}\\mathbf A - \\mathbf A^\\top\\mathbf S_{k+1}\\mathbf B( \\mathbf B^\\top\\mathbf S_{k+1}\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf A,\n}\n which we will regard as an alternative form of difference Riccati equation.\nObserving that the steps of the computation of the Kalman gain \\mathbf K_k reappear in the computation of the solution of the Riccati equation, a more efficient arrangement of the computation in every iteration step is \n\\boxed{\n\\begin{aligned}\n\\mathbf K_k &= \\left(\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B+\\mathbf R\\right)^{-1}\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A\\\\\n\\mathbf S_k &= \\mathbf A^\\top \\mathbf S_{k+1}(\\mathbf A-\\mathbf B\\mathbf K_k) + \\mathbf Q.\n\\end{aligned}\n}\n\nFinally, yet another equivalent version of Riccati equation is known as Joseph stabilized form of Riccati equation \n\\boxed{\n\\mathbf S_k = (\\mathbf A-\\mathbf B\\mathbf K_k)^\\top \\mathbf S_{k+1}(\\mathbf A-\\mathbf B\\mathbf K_k) + \\mathbf K_k^\\top \\mathbf R\\mathbf K_k + \\mathbf Q.\n}\n\\tag{4}\nShowing the equivalence can be an exercise. Hint: from Eq. 3, we can write \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A=\\left(\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B+\\mathbf R\\right) \\mathbf K_k.\n\nSecond order sufficient conditions\nSo far we only found a solution that satisfies the first-order necessary equation but we have been warned at the introductory lessons to optimization that such solution need not necessarily constitute an optimum (minimum in our case). In order to check this, the second derivative (Hessian, curvature matrix) must be found and checked for positive definiteness. Our strategy will be to find the value of the optimal cost first and then we will identify its second derivative with respect to \\bm u_k.\nThe trick to find the value of the optimal cost is from [1] and it is rather technical and it may be hard to learn a general lesson from it. Nonetheless we will need the result. Therefore we swiftly go through the procedure without pretending that we are building a general competence. The trick is based on the observation that \n\\frac{1}{2}\\sum_{k=0}^{N-1}(\\mathbf x^\\top _{k+1}\\mathbf S_{k+1} \\mathbf x_{k+1} - \\mathbf x^\\top _{k}\\mathbf S_{k} \\mathbf x_{k}) = \\frac{1}{2}\\mathbf x^\\top _{N}\\mathbf S_{N} \\mathbf x_{N} - \\frac{1}{2}\\mathbf x^\\top _{0}\\mathbf S_{0} \\mathbf x_{0}.\n\nNow consider our optimization criterion and add zero to it. The value of the cost function does not change. Weird procedure, right? Observing that zero can also be expressed as the right hand side minus the left hand side in the above equation, we get \nJ_0 = \\frac{1}{2}\\bm x_0^\\top\\mathbf S_0\\bm x_0 + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[\\mathbf x^\\top _{k+1}\\mathbf S_{k+1} \\mathbf x_{k+1}+\\bm x_k^\\top (\\mathbf Q - \\mathbf S_k) \\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right].\n\nSubstituting the state equation, the cost function transforms to \n\\begin{aligned}\nJ_0 &= \\frac{1}{2}\\bm x_0^\\top\\mathbf S_0\\bm x_0 + \\frac{1}{2}\\sum_{k=0}^{N-1}[\\mathbf x^\\top _{k}(\\mathbf A^\\top \\mathbf S_{k+1}\\mathbf A + \\mathbf Q - \\mathbf S_k) \\mathbf x_{k}+\\bm x_k^\\top \\mathbf A^\\top \\mathbf S_{k+1}\\mathbf B \\bm u_k\\\\\n&\\qquad\\qquad\\qquad\\qquad+\\bm u_k^\\top \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A \\bm x_k+\\bm u_k^\\top (\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B + \\mathbf R)\\bm u_k].\n\\end{aligned}\n\nSubstituting for \\mathbf S_k from the Riccati equation gives \n\\begin{aligned}\nJ_0 &= \\frac{1}{2}\\bm x_0^\\top\\mathbf S_0\\bm x_0 + \\frac{1}{2}\\sum_{k=0}^{N-1}[\\mathbf x^\\top _{k}(\\mathbf A^\\top \\mathbf S_{k+1}\\mathbf B( \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A) \\mathbf x_{k}+\\bm x_k^\\top \\mathbf A^\\top \\mathbf S_{k+1}\\mathbf B \\bm u_k\\\\\n&\\qquad\\qquad\\qquad\\qquad+\\bm u_k^\\top \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A \\bm x_k+\\bm u_k^\\top (\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B + \\mathbf R)\\bm u_k].\n\\end{aligned}\n\nThe time-varying Hessian with respect to the control \\bm u_k is \n\\nabla_{\\bm u_k}^2 J_0 = \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B + \\mathbf R.\n\nProvided that \\mathbf R\\succ 0, it can be seen that it is always guaranteed that \\nabla_{\\bm u_k}^2 J_0\\succ 0. To prove this it must be shown that \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B\\succeq 0. As usual, let us make things more intuitive by switching to the scalar case. The previous expression simplifies to b^2s_{k+1}. No matter what the value of b is, the square is always nonnegative. It remains to show that s_{k+1}\\geq0 (and in the matrix case \\mathbf S_{k+1}\\succeq 0). This can be seen from the prescription for \\mathbf S_{k} given by the Riccati equation using similar arguments for proving positive semidefiniteness of compound expressions.\nTo conclude, the solution to the first-order necessary conditions represented by the Riccati equation is always a minimizing solution.\nWe can work a bit more with the value of the optimal cost. Substituting the optimal control we can see (after some careful two-line work) that \nJ_0^\\star = \\frac{1}{2}\\bm x_0^\\top  \\mathbf S_0 \\bm x_0.\n\nThe same conclusion can be obtained for any time instant k inside the interval [0,N] \n\\boxed{\nJ_k^\\star = \\frac{1}{2}\\bm x_k^\\top  \\mathbf S_k \\bm x_k.\n}\n\nThis is a result that we have already seen in the no-control case: the optimal cost can be obtained as a quadratic function of the initial state using a matrix obtained as a solution to some iteration. We will use this result in the future derivations.\n\n\nNumerical example with a scalar and first-order system\nAs usual, some practical insight can be developed by analyzing the things when restricted to the scalar case. For this, consider a first order system described by the first-order state equation \nx_{k+1} = ax_k + bu_k\n and the optimization criterion in the form \nJ_0 = \\frac{1}{2}s_N x_N^2 + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[ q x_k^2+r u_k^2\\right ].\n\nThe scalar Riccati equation simplifies to \ns_k = a^2s_{k+1} - \\frac{a^2b^2s_{k+1}^2}{b^2s_{k+1}+r} + q\n or \ns_k = \\frac{a^2rs_{k+1}}{b^2s_{k+1}+r} + q.\n\nJulia code and its outputs follow.\n\n\nShow the code\nfunction dre(a,b,q,r,sN,N)\n    s = Vector{Float64}(undef,N+1)          # the S[1] will then not be needed (even defined) but the indices will fit\n    k = Vector{Float64}(undef,N)\n    s[end] = sN\n    for i=N:-1:1\n        k[i]=(a*b*s[i+1])/(r + s[i+1]*b^2);\n        s[i]= a*s[i+1]*(a-b*k[i]) + q;\n    end\n    return s,k\nend\n\na = 1.05;\nb = 0.01;\nq = 100;\nr = 1;\nx0 = 10;\nsN = 100;\nN = 20;\n\ns,k = dre(a,b,q,r,sN,N);\n\nusing Plots\n\np1 = plot(0:1:N,s,xlabel=\"i\",ylabel=\"RE solution\",label=\"s\",markershape=:circ,markersize=1,linetype=:steppost)\np2 = plot(0:1:N-1,k,xlabel=\"i\",ylabel=\"State-feedback gain\",label=\"k\",markershape=:circ,markersize=1,linetype=:steppost,xlims=xlims(p1))\n\nx = Vector{Float64}(undef,N+1)\nu = Vector{Float64}(undef,N)\n\nx[1]=x0;\n\nfor i=1:N\n    u[i] = -k[i]*x[i];\n    x[i+1] = a*x[i] + b*u[i];\nend\n\np3 = plot(0:1:N,x,xlabel=\"i\",ylabel=\"State\",label=\"x\",markershape=:circ,markersize=1,linetype=:steppost)\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObviously the final state is not particularly close to zero, which is the desired final value. However, increasing the s_N term we can bring the system arbitrarily close, as the next simulation confirms.\n\n\nShow the code\nsN = 10000;\nN = 20;\n\ns,k = dre(a,b,q,r,sN,N);\n\np1 = plot(0:1:N,s,xlabel=\"i\",ylabel=\"RE solution\",label=\"s\",markershape=:circ,markersize=1,linetype=:steppost)\np2 = plot(0:1:N-1,k,xlabel=\"i\",ylabel=\"State-feedback gain\",label=\"k\",markershape=:circ,markersize=1,linetype=:steppost,xlims=xlims(p1))\n\nx = Vector{Float64}(undef,N+1)\nu = Vector{Float64}(undef,N)\n\nx[1]=x0;\n\nfor i=1:N\n    u[i] = -k[i]*x[i];\n    x[i+1] = a*x[i] + b*u[i];\nend\n\np3 = plot(0:1:N,x,xlabel=\"i\",ylabel=\"State\",label=\"x\",markershape=:circ,markersize=1,linetype=:steppost)\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we explore what changes if we make the time horizon longer.\n\n\nShow the code\nN = 100;\n\ns,k = dre(a,b,q,r,sN,N);\n\np1 = plot(0:1:N,s,xlabel=\"i\",ylabel=\"RE solution\",label=\"s\",markershape=:circ,markersize=1,linetype=:steppost)\np2 = plot(0:1:N-1,k,xlabel=\"i\",ylabel=\"State-feedback gain\",label=\"k\",markershape=:circ,markersize=1,linetype=:steppost,xlims=xlims(p1))\n\nx = Vector{Float64}(undef,N+1)\nu = Vector{Float64}(undef,N)\n\nx[1]=x0;\n\nfor i=1:N\n    u[i] = -k[i]*x[i];\n    x[i+1] = a*x[i] + b*u[i];\nend\n\np3 = plot(0:1:N,x,xlabel=\"i\",ylabel=\"State\",label=\"x\",markershape=:circ,markersize=1,linetype=:steppost)\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe last outputs suggests that both s_N and K_k stay constant for most of the time interval and they only change dramatically towards the end of the control interval.\nThe observation in the example poses a question of how much is lost after replacing the optimal control represented by the sequence \\mathbf K_k by some constant value \\mathbf K. A natural candidate is the steady-state value that \\mathbf K_k has as the beginning of the control interval, that is at k=0 in our case.\nObviously, on a finite time horizon there is not much to be investigated, the constant feedback gain is just suboptimal (even if the suboptimality can be negligible). Things will turn out fairly interesting as the time horizon stretches to infinity, that is, N\\rightarrow \\infty.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on a finite horizon"
    ]
  },
  {
    "objectID": "reduction_order_references.html",
    "href": "reduction_order_references.html",
    "title": "References",
    "section": "",
    "text": "The primary reference for our overview of methods for model and controller order reduction is Chapter 11 in [1]. For a more detailed introduction, there a few dedicated monographs such as [2] and [3]. A short extract from the latter is in [4].\nThe latter also excels in that it also admits that the topic of reduction of order of mathematical models formatted as state equations is not only relevant for the control systems community but from a number of other engineering and scientific communities as well. After all, mathematical models are not only for model-based control design but for simulation, optimization, and other purposes. For example, the in [5] they are motivated by fast simulation of VLSI circuits.\n\n\n\n\n Back to topReferences\n\n[1] S. Skogestad and I. Postlethwaite, Multivariable Feedback Control: Analysis and Design, 2nd ed. Wiley, 2005. Available: https://folk.ntnu.no/skoge/book/\n\n\n[2] G. Obinata and B. D. O. Anderson, Model Reduction for Control System Design. New York: Springer, 2000.\n\n\n[3] A. C. Antoulas, Approximation of Large-Scale Dynamical Systems. Philadelphia: Society for Industrial and Applied Mathematics, 2005.\n\n\n[4] A. C. Antoulas and D. C. Sorensen, “Approximation of large-scale dynamical systems: An Overview,” Rice University, Houston, Texas, Technical Report, Feb. 2001. Accessed: May 21, 2024. [Online]. Available: https://hdl.handle.net/1911/101964\n\n\n[5] S. Tan and L. He, Advanced Model Order Reduction Techniques in VLSI Design. Cambridge: Cambridge University Press, 2007.",
    "crumbs": [
      "14. Model and controller order reduction",
      "References"
    ]
  },
  {
    "objectID": "discr_indir_DARE.html",
    "href": "discr_indir_DARE.html",
    "title": "Discrete-time algebraic Riccati equation (DARE)",
    "section": "",
    "text": "We have learnt previously that the following matrix equation \n\\bm X=\\mathbf A^\\top\\left[\\bm X-\\bm X\\mathbf B(\\mathbf B^\\top\\bm X\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top\\bm X\\right]\\mathbf A+\\mathbf Q\n or, equivalently, \\boxed{\n\\mathbf A^\\top\\bm X\\mathbf A - \\bm X +\\mathbf Q - \\mathbf A^\\top\\bm X\\mathbf B(\\mathbf B^\\top\\bm X\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top\\bm X\\mathbf A = \\mathbf 0}\n\ncalled discrete-time algebraic Riccati equation (DARE) is instrumental in solving the infinite time horizon LQR problem. The equation must be solved for the matrix \\bm X in order to compute the state feedback gain.\nThe key assumptions are that \\mathbf Q \\succeq 0, \\mathbf R \\succ 0, the pair (\\mathbf A, \\mathbf B) is stabilizable, and the pair (\\mathbf A, \\sqrt{\\mathbf Q}) is detectable. The solution to the DARE is unique, symmetric and positive semidefinite. If, furthermore, the pair (\\mathbf A, \\mathbf Q) is observable, the solution is positive definite.\n\n\n\n\n\n\nThe matrix variable \\bm X\n\n\n\nNote that here we have replaced the previous name for matrix variable \\bm S_\\infty by the new \\bm X to emphasize that it is the unknown here.\n\n\nThere are several approaches to solving the DARE, and the most reliable and accurate once have already been implemented in major computational packages (see the section on software). Some overviews and detailed explanations are provided in [1], [2], and [3]. Here we only sketch one of them, that displays an important property.\nRecall the linear system that we have developed from the two-point boundary value problem under the assumption of invertability of the matrix \\mathbf A:\n\n\\begin{bmatrix}\n\\mathbf x_{k}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\n\\underbrace{\\begin{bmatrix}\n\\mathbf A^{-1} & \\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q\\mathbf A^{-1} & \\mathbf A^\\top+\\mathbf Q\\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\n\\end{bmatrix}}_{\\mathbf H}\n\\begin{bmatrix}\n\\mathbf x_{k+1} \\\\ \\boldsymbol\\lambda_{k+1}\n\\end{bmatrix}\n\nThe matrix H has a very special property. Defining the auxilliary matrix \\mathbf J \n\\mathbf J = \\begin{bmatrix}\\mathbf 0 & \\mathbf I\\\\ -\\mathbf I & \\mathbf 0\\end{bmatrix},\n the matrix \\mathbf H can be shown to satisfy the following: \n\\mathbf H^\\top\\mathbf J\\mathbf H = \\mathbf J.\n\nSuch matrices are called symplectic and they have several special properties. First, note that \n\\mathbf H^\\top\\mathbf J = \\mathbf J\\mathbf H^{-1},\n from which it follows that \n\\mathbf J^{-1}\\mathbf H^\\top\\mathbf J = \\mathbf H^{-1}.\n\nUsing the fact that \\mathbf J^{-1} = -\\mathbf J, we get the promised special property – a particularly simple way to compute the inverse of \\mathbf H: \n\\mathbf H^{-1} = -\\mathbf J\\mathbf H^\\top\\mathbf J.\n\nWe can use this to obtain the inverse of our particular \\mathbf H matrix:\n\n\\mathbf H^{-1}\n=\n\\begin{bmatrix}\n\\mathbf A + \\mathbf B \\mathbf R^{-1} \\mathbf B^\\top \\mathbf A^{-\\top}\\mathbf Q & \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\mathbf A^{-\\top} \\\\\n\\mathbf A^{-\\top} \\mathbf Q & \\mathbf A^{-\\top},\n\\end{bmatrix}\n where we used the shortand notation \\mathbf A^{-\\top} for (\\mathbf A^{-1})^\\top.\nNow, it can be shown that if \\lambda is an eigenvalue of \\mathbf H, so is 1/\\lambda. We are not going to prove it here, we refer an interested reader to [4, p. 81].\n\nExample 1 (Eigenvalues of a symplectic matrix)  \n\n\nShow the code\nusing LinearAlgebra\n\nA = [0.225384  0.166015   0.60408\n    0.920342  0.0644107  0.354692\n    0.483302  0.536062   0.718341]\nB = [0.587251  0.29765\n    0.305953  0.616242  \n    0.400612  0.201951]\n\nq = [1.0, 2.0, 3.0]\nr = [1.0, 2.0]\n\nQ = diagm(0=&gt;q)\nR = diagm(0=&gt;r)\n\nH = [inv(A) A\\B/R*B'; Q/A A'+Q/A*B/R*B']\n\nλ, V = eigen(H)\n\nusing Plots\nscatter(λ, aspect_ratios = 1, label=\"\")\nθ = range(0, 2π, length=100)\nx = cos.(θ)\ny = sin.(θ)\nplot!(x, y, label=\"\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Eigenvalues of the symplectic (discrete-time Hamiltonian) matrix \\mathbf H are symmetric with respect to the unit circle\n\n\n\n\n\nSimilarly, only without providing a proof we state here that if the eigenvectors of \\mathbf H corresponding to the unstable eigenvalue (|\\lambda|&gt;1) are denoted as \\mathbf v_1, \\ldots, \\mathbf v_n, and are stacked together to form a matrix \\mathbf V^\\mathrm{unstable} = \\begin{bmatrix}\\mathbf v_1 & \\ldots & \\mathbf v_n \\end{bmatrix}, and we name the two vertical n\\times n blocks in the matrix \\mathbf V^\\mathrm{unstable} = \\begin{bmatrix} \\mathbf X_1\\\\\\mathbf X_2\\end{bmatrix}, the solution to the DARE can be expressed as \n\\bm X = \\mathbf X_2\\mathbf X_1^{-1}.\n\nThis method of solving the DARE is referred to as the eigenvector method and is regarded as a subset of a broader family of invariant subspace methods. It is not particularly reliable when the matrix \\mathbf H has eigenvalues close to the unit circle. And it also required invertibility of the matrix \\mathbf A in the first place. Still, we wanted to present it here because it displays a fundamental phenomenon often encountered in optimal control – the need to split some eigenvalues, eigenvectors, subspaces into stable and unstable ones.\n\nExample 2 (Solving the DARE using the eigenvector method)  \n\n\nShow the code\nindices_unstable = findall(x -&gt; abs(x) &gt; 1, λ)\nVᵘ = V[:,indices_unstable]\nX₁ = Vᵘ[1:3,:]\nX₂ = Vᵘ[4:6,:]\n\nX = X₂/X₁\nX = real(X) # We get rid of the negligible (≈ 10⁻¹⁶) imaginary parts.\n\n\n3×3 Matrix{Float64}:\n 2.5197    0.499383  0.914329\n 0.499383  2.65859   0.835092\n 0.914329  0.835092  4.30357\n\n\nWe can compare against the solution provided by the specialized package MatrixEquations.jl:\n\n\nShow the code\nusing MatrixEquations\nX_ared, CLSEIG, F = ared(A,B,R,Q);\nX_ared\n\n\n3×3 Matrix{Float64}:\n 2.5197    0.499383  0.914329\n 0.499383  2.65859   0.835092\n 0.914329  0.835092  4.30357\n\n\n\nAs a demonstration of this stable-unstable decomposition phenomenon, we show that the LQR state feedback places the closed loop poles into the stable eigenvalues of the symplectic \\mathbf H matrix. Again we provide no proof here, but an interested reader can find it in [4, Sec. 2.5].\n\nExample 3  \n\n\nShow the code\nusing ControlSystems\n\nK = lqr(Discrete, A,B,Q,R)\np = eigvals(A-B*K)      # closed-loop poles\n\nscatter(λ, aspect_ratios = 1, label=\"Eigenvalues of H\")\nscatter!(p, aspect_ratios = 1, label=\"LQR closed-loop poles\")\n\nθ = range(0, 2π, length=100)\nx = cos.(θ)\ny = sin.(θ)\nplot!(x, y, label=\"\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Eigenvalues of the symplectic (discrete-time Hamiltonian) matrix \\mathbf H and the poles of the state feedback loop closed by the LQR controller\n\n\n\n\n\nThe property demonstrated through the example suggests a naive method for designing an LQR controller – form the discrete-time Hamiltonina matrix \\mathbf H, compute its eigenvalues, select the stable ones, and then use some pole-placement method to design a state feedback gains to places the closed-loop poles into the selected stable eigenvalues. The method is not recommendable, but it provides yet another insight into the LQR problem.\n\n\n\n\n Back to topReferences\n\n[1] V. Sima, Algorithms for Linear-Quadratic Optimization. New York: Chapman and Hall/CRC, 1996.\n\n\n[2] D. A. Bini, B. Iannazzo, and B. Meini, Numerical Solution of Algebraic Riccati Equations. Philadelphia: SIAM-Society for Industrial and Applied Mathematics, 2012.\n\n\n[3] B. Datta, Numerical Methods for Linear Control Systems. Amsterdam; Boston: Academic Press, 2003.\n\n\n[4] F. L. Lewis, D. Vrabie, and V. L. Syrmo, Optimal Control, 3rd ed. John Wiley & Sons, 2012. Accessed: Mar. 09, 2022. [Online]. Available: https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time algebraic Riccati equation (DARE)"
    ]
  },
  {
    "objectID": "opt_theory_references.html",
    "href": "opt_theory_references.html",
    "title": "References",
    "section": "",
    "text": "If a single reference book on nonlinear optimization is to be recommended, be it [1] that sits on your book shelf.\nIf one or two more can still fit, [2], [3] are classical comprehensive references on nonlinear programming (the latter covers linear programming too).\nWhile all the three books are only available for purchase, there is a wealth of resources that are freely available online such as the notes [4] accompanying a course on optimal control, which do a decent job of introduction to a nonlinear programming, and beautifully typeset modern textbooks [5] and [6], the former based on Julia language. Yet another high-quality textbook that is freely available online is [7].\nWhen restricting to convex optimization, the bible of this field [8] is also freely available online. It is a must-have for everyone interested in optimization. Yet another advanced and treatment of convex optimization is [9], which is also freely available online.\nMaybe a bit unexpected resources on theory are materials accompanying some optimization software. Partilarly recommendable is [10], it is very useful even if you do not indend to use their software. In particular, their introduction to conic optimization is very well written and easy to follow.\n\n\n\n\n Back to topReferences\n\n[1] J. Nocedal and S. Wright, Numerical Optimization, 2nd ed. in Springer Series in Operations Research and Financial Engineering. New York: Springer, 2006. Available: https://doi.org/10.1007/978-0-387-40065-5\n\n\n[2] D. Bertsekas, Nonlinear Programming, 3rd ed. Belmont, Mass: Athena Scientific, 2016. Available: http://www.athenasc.com/nonlinbook.html\n\n\n[3] D. G. Luenberger and Y. Ye, Linear and Nonlinear Programming, 5th ed. in International Series in Operations Research & Management Science, no. 228. Cham, Switzerland: Springer, 2021. Available: https://doi.org/10.1007/978-3-030-85450-8\n\n\n[4] S. Gros and M. Diehl, “Numerical Optimal Control (Draft).” Systems Control; Optimization Laboratory IMTEK, Faculty of Engineering, University of Freiburg, Apr. 2022. Available: https://www.syscop.de/files/2020ss/NOC/book-NOCSE.pdf\n\n\n[5] M. J. Kochenderfer and T. A. Wheeler, Algorithms for Optimization. The MIT Press, 2019. Accessed: Dec. 29, 2020. [Online]. Available: https://algorithmsbook.com/optimization/\n\n\n[6] J. R. R. A. Martins and A. Ning, Engineering Design Optimization. Cambridge ; New York, NY: Cambridge University Press, 2022. Available: https://mdobook.github.io/\n\n\n[7] M. Bierlaire, Optimization: Principles and Algorithms, 2nd ed. Lausanne: EPFL Press, 2018. Available: https://transp-or.epfl.ch/books/optimization/html/OptimizationPrinciplesAlgorithms2018.pdf\n\n\n[8] S. Boyd and L. Vandenberghe, Convex Optimization, Seventh printing with corrections 2009. Cambridge, UK: Cambridge University Press, 2004. Available: https://web.stanford.edu/~boyd/cvxbook/\n\n\n[9] A. Ben-Tal and A. Nemirovski, “Lectures on Modern Convex Optimization - 2020/2021/2022/2023 Analysis, Algorithms, Engineering Applications,” Technion & Georgia Institute of Technology, 2023. Available: https://www2.isye.gatech.edu/~nemirovs/LMCOLN2023Spring.pdf\n\n\n[10] “MOSEK Modeling Cookbook.” Mosek ApS, Sep. 2024. Available: https://docs.mosek.com/MOSEKModelingCookbook-a4paper.pdf",
    "crumbs": [
      "1. Optimization – theory",
      "References"
    ]
  },
  {
    "objectID": "cont_dp_HJB.html",
    "href": "cont_dp_HJB.html",
    "title": "Dynamic programming for continuous-time optimal control",
    "section": "",
    "text": "In the previous sections we investigated both direct and indirect approaches to the optimal control problem. Similarly as in the discrete-time case, complementing the two approaches is the dynamic programming. Indeed, the key Bellmans’s idea, which we previously formulated in discrete time, can be extended to continuous time as well.\nWe consider the continuous-time system \n\\dot{\\bm{x}} = \\mathbf f(\\bm{x},\\bm{u},t)\n with the cost function \nJ(\\bm x(t_\\mathrm{i}), \\bm u(\\cdot), t_\\mathrm{i}) = \\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}L(\\bm x(t),\\bm u(t),t)\\, \\mathrm d t.\nThe final time can be fixed to a particular value t_\\mathrm{f}, in which case the state at the final time \\bm x(t_\\mathrm{f}) is either free (unspecified but penalized through \\phi(\\bm x(t_\\mathrm{f}))), or it is fixed (specified and not penalized, that is, \\bm x(t_\\mathrm{f}) = \\mathbf x^\\mathrm{ref}).\nThe final time can also be free (regarded as an optimization variable itself), in which case general constraints on the state at the final time can be expressed as \n\\psi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f})=0\n or possibly even using an inequality, which we will not consider here.\nThe final time can also be considered infinity, that is, t_\\mathrm{f}=\\infty, but we will handle this situation later separately.",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Dynamic programming for continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_dp_HJB.html#hamilton-jacobi-bellman-hjb-equation",
    "href": "cont_dp_HJB.html#hamilton-jacobi-bellman-hjb-equation",
    "title": "Dynamic programming for continuous-time optimal control",
    "section": "Hamilton-Jacobi-Bellman (HJB) equation",
    "text": "Hamilton-Jacobi-Bellman (HJB) equation\nWe now consider an arbitrary time t and split the (remaining) time interval [t,t_\\mathrm{f}] into two parts [t,t+\\Delta t] and [t+\\Delta t,t_\\mathrm{f}] , and structure the cost function accordingly \nJ(\\bm x(t),\\bm u(\\cdot),t) = \\int_{t}^{t+\\Delta t} L(\\bm x,\\bm u,\\tau)\\,\\mathrm{d}\\tau + \\underbrace{\\int_{t+\\Delta t}^{t_\\mathrm{f}} L(\\bm x,\\bm u,\\tau)\\,\\mathrm{d}\\tau + \\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f})}_{J(\\bm x(t+\\Delta t), \\bm u(t+\\Delta t), t+\\Delta t)}.\n\nBellman’s principle of optimality gives \nJ^\\star(\\bm x(t),t) = \\min_{\\bm u(\\tau),\\;t\\leq\\tau\\leq t+\\Delta t} \\left[\\int_{t}^{t+\\Delta t} L(\\bm x,\\bm u,\\tau)\\,\\mathrm{d}\\tau + J^\\star(\\bm x+\\Delta \\bm x, t+\\Delta t)\\right].\n\nWe now perform Taylor series expansion of J^\\star(\\bm x+\\Delta \\bm x, t+\\Delta t) about (\\bm x,t) \nJ^\\star(\\bm x,t) = \\min_{\\bm u(\\tau),\\;t\\leq\\tau\\leq t+\\Delta t} \\left[L\\Delta t + J^\\star(\\bm x,t) + (\\nabla_{\\bm x} J^\\star)^\\top \\Delta \\bm x + \\frac{\\partial J^\\star}{\\partial t}\\Delta t + \\mathcal{O}((\\Delta t)^2)\\right].\n\nUsing \n\\Delta \\bm x = \\bm f(\\bm x,\\bm u,t)\\Delta t\n and noting that J^\\star and J_t^\\star are independent of \\bm u(\\tau),\\;t\\leq\\tau\\leq t+\\Delta t, we get \n\\cancel{J^\\star (\\bm x,t)} = \\cancel{J^\\star (\\bm x,t)} + \\frac{\\partial J^\\star }{\\partial t}\\Delta t + \\min_{\\bm u(\\tau),\\;t\\leq\\tau\\leq t+\\Delta t}\\left[L\\Delta t + (\\nabla_{\\bm x} J^\\star )^\\top f\\Delta t\\right].\n\nAssuming \\Delta t\\rightarrow 0 leads to the celebrated Hamilton-Jacobi-Bellman (HJB) equation \\boxed{\n-\\frac{\\partial {\\color{blue}J^\\star (\\bm x(t),t)}}{\\partial t} = \\min_{\\bm u(t)}\\left[L(\\bm x(t),\\bm u(t),t)+(\\nabla_{\\bm x} {\\color{blue} J^\\star (\\bm x(t),t)})^\\top \\bm f(\\bm x(t),\\bm u(t),t)\\right].}\n\nThis is obviously a partial differential equation (PDE) for the optimal cost function J^\\star(\\bm x,t).\n\nBoundary conditions for the HJB equation\nSince the HJB equation is a differential equation, initial/boundary value(s) must be specified to determine a unique solution. In particular, since the equation is first-order with respect to both time and state, specifying the value of the optimal cost function at the final state and the final time is enough.\nFor a fixed-final-time, free-final-state, the optimal cost at the final time is \nJ^\\star (\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) = \\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}).\n\nFor a fixed-final-time, fixed-final-state, since the component of the cost function corresponding to the terminal state is zero, the optimal cost at the final time is zero as well \nJ^\\star (\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) = 0.\n\nWith the general final-state constraints introduced above, the boundary value condition reads \nJ^\\star (\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) = \\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}),\\qquad \\text{on the hypersurface } \\psi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) = 0.\n\n\n\nOptimal control using the optimal cost (-to-go) function\nAssume now that the solution J^\\star (\\bm x(t),t) to the HJB equation is available. We can then find the optimal control by the minimization \\boxed\n{\\bm u^\\star(t) = \\arg\\min_{\\bm u(t)}\\left[L(\\bm x(t),\\bm u(t),t)+(\\nabla_{\\bm x} J^\\star (\\bm x(t),t))^\\top \\bm f(\\bm x(t),\\bm u(t),t)\\right].}\n\nFor convenience, the minimized function is often labelled as \nQ(\\bm x(t),\\bm u(t),t) = L(\\bm x(t),\\bm u(t),t)+(\\nabla_{\\bm x} J^\\star (\\bm x(t),t))^\\top \\bm f(\\bm x(t),\\bm u(t),t)\n and called just Q-function. The optimal control is then \n\\bm u^\\star(t) = \\arg\\min_{\\bm u(t)} Q(\\bm x(t),\\bm u(t),t).",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Dynamic programming for continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_dp_HJB.html#hjb-equation-formulated-using-a-hamiltonian",
    "href": "cont_dp_HJB.html#hjb-equation-formulated-using-a-hamiltonian",
    "title": "Dynamic programming for continuous-time optimal control",
    "section": "HJB equation formulated using a Hamiltonian",
    "text": "HJB equation formulated using a Hamiltonian\nRecall the definition of Hamiltonian H(\\bm x,\\bm u,\\bm \\lambda,t) = L(\\bm x,\\bm u,t) + \\boldsymbol{\\lambda}^\\top \\mathbf f(\\bm x,\\bm u,t). The HJB equation can also be written as \\boxed\n{-\\frac{\\partial J^\\star (\\bm x(t),t)}{\\partial t} = \\min_{\\bm u(t)}H(\\bm x(t),\\bm u(t),\\nabla_{\\bm x} J^\\star (\\bm x(t),t),t).}\n\nWhat we have just derived is one of the most profound results in optimal control – Hamiltonian must be minimized by the optimal control. We will exploit it next as a tool for deriving some theoretical results.",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Dynamic programming for continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_dp_HJB.html#hjb-equation-vs-pontryagins-principle-of-maximum-minimum",
    "href": "cont_dp_HJB.html#hjb-equation-vs-pontryagins-principle-of-maximum-minimum",
    "title": "Dynamic programming for continuous-time optimal control",
    "section": "HJB equation vs Pontryagin’s principle of maximum (minimum)",
    "text": "HJB equation vs Pontryagin’s principle of maximum (minimum)\nRecall also that we have already encountered a similar results that made statements about the necessary maximization (or minimization) of the Hamiltonian with respect to the control – the celebrated Pontryagin’s principle of maximum (or minimum). Are these two related? Equivalent?",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Dynamic programming for continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_dp_HJB.html#hjb-equation-for-an-infinite-time-horizon",
    "href": "cont_dp_HJB.html#hjb-equation-for-an-infinite-time-horizon",
    "title": "Dynamic programming for continuous-time optimal control",
    "section": "HJB equation for an infinite time horizon",
    "text": "HJB equation for an infinite time horizon\nWhen both the system and the cost function are time-invariant, and the final time is infinite, that is, t_\\mathrm{f}=\\infty, the optimal cost function J^\\star() must necessarily be independent of time, that is, it’s partial derivative with respect to time is zero, that is, \\frac{\\partial J^\\star (\\bm x(t),t)}{\\partial t} = 0. The HJB equation then simplifies to\n\\boxed{\n0 = \\min_{\\bm u(t)}\\left[L(\\bm x(t),\\bm u(t))+(\\nabla_{\\bm x} {J^\\star (\\bm x(t),t)})^\\top \\bm f(\\bm x(t),\\bm u(t))\\right],}\n or, using a Hamiltonian \\boxed\n{0 = \\min_{\\bm u(t)}H(\\bm x(t),\\bm u(t),\\nabla_{\\bm x} J^\\star (\\bm x(t))).}",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Dynamic programming for continuous-time optimal control"
    ]
  },
  {
    "objectID": "discr_indir_LQR_tracking.html",
    "href": "discr_indir_LQR_tracking.html",
    "title": "Reference tracking using LQR",
    "section": "",
    "text": "#TODO\n\n\n\n Back to top",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Reference tracking using LQR"
    ]
  },
  {
    "objectID": "discr_indir_goals.html",
    "href": "discr_indir_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Give the first-order necessary conditions of optimality for a general optimal control problem for a nonlinear discrete-time system over a finite horizon. Namely, give the general two-point boundary value problem, highlighting the state equation, the co-state equation and a stationarity equation. Do not forget to include general boundary conditions.\nGive the first-order necessary conditions of optimality for a linear and time invariant (LTI) discrete-time system and a quadratic cost function over a finite horizon. Namely, give them in the format displaying the state equation, co-state equation and stationarity equation. Show and discuss also two types of boundary conditions.\nGive a qualitative characterization of the solution to the fixed final state LQ-optimal control problem over a finite horizon, that is, you do not have to give formulas but you should be able to state among the highlights that the control is open-loop and that reachability of the system is a necessary condition.\nGive a qualitative characterization of the solution to the free final state LQ-optimal control problem over a finite horizon, that is, you do not have to give formulas but you should be able to state among the highlights that the control is closed-loop, namely, a time-varying linear state feedback and that the feedback gains can be computed by solving a difference Riccati equation.\nDiscuss how solution to the free final state LQ problem changes as the horizon is extended to infinity. Emphasize that the optimal solution is given by a constant linear state feedback whose gains are computed by solving a discrete-time algebraic Riccati equation (DARE). What are the conditions under which a stabilizing solution is guaranteed to exist? What are the conditions under which it is guaranteed that there is a unique stabilizing solution of DARE?",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "discr_indir_goals.html#knowledge-remember-and-understand",
    "href": "discr_indir_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Give the first-order necessary conditions of optimality for a general optimal control problem for a nonlinear discrete-time system over a finite horizon. Namely, give the general two-point boundary value problem, highlighting the state equation, the co-state equation and a stationarity equation. Do not forget to include general boundary conditions.\nGive the first-order necessary conditions of optimality for a linear and time invariant (LTI) discrete-time system and a quadratic cost function over a finite horizon. Namely, give them in the format displaying the state equation, co-state equation and stationarity equation. Show and discuss also two types of boundary conditions.\nGive a qualitative characterization of the solution to the fixed final state LQ-optimal control problem over a finite horizon, that is, you do not have to give formulas but you should be able to state among the highlights that the control is open-loop and that reachability of the system is a necessary condition.\nGive a qualitative characterization of the solution to the free final state LQ-optimal control problem over a finite horizon, that is, you do not have to give formulas but you should be able to state among the highlights that the control is closed-loop, namely, a time-varying linear state feedback and that the feedback gains can be computed by solving a difference Riccati equation.\nDiscuss how solution to the free final state LQ problem changes as the horizon is extended to infinity. Emphasize that the optimal solution is given by a constant linear state feedback whose gains are computed by solving a discrete-time algebraic Riccati equation (DARE). What are the conditions under which a stabilizing solution is guaranteed to exist? What are the conditions under which it is guaranteed that there is a unique stabilizing solution of DARE?",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "discr_indir_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "discr_indir_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nDesign an LQ-optimal state feedback controller for a discrete-time linear system both for a finite and an infinite horizon, both for regulation and for tracking.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "rocond_mu_synthesis.html",
    "href": "rocond_mu_synthesis.html",
    "title": "Mu synthesis",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "Mu synthesis"
    ]
  },
  {
    "objectID": "cont_dp_hw.html",
    "href": "cont_dp_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Homework"
    ]
  },
  {
    "objectID": "opt_algo_references.html",
    "href": "opt_algo_references.html",
    "title": "References",
    "section": "",
    "text": "Pretty much identical to the literature recommended in the previous section on optimization theory.\nSome practical aspects are discussed in Guidelines for Numerical Issues for Gurobi Optimizer.\n\n\n\n Back to top",
    "crumbs": [
      "2. Optimization – algorithms",
      "References"
    ]
  },
  {
    "objectID": "discr_dir_mpc_stability.html",
    "href": "discr_dir_mpc_stability.html",
    "title": "Stability of MPC",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "Stability of MPC"
    ]
  },
  {
    "objectID": "opt_theory_problems.html",
    "href": "opt_theory_problems.html",
    "title": "Optimization problems",
    "section": "",
    "text": "We formulate a general optimization problem (also a mathematical program) as \n\\begin{aligned}\n\\operatorname*{minimize} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\bm x \\in \\mathcal{X},\n\\end{aligned}\n where f is a scalar function, \\bm x can be a scalar, a vector, a matrix or perhaps even a variable of yet another type, and \\mathcal{X} is a set of values that \\bm x can take, also called the feasible set.\n\n\n\n\n\n\nNote\n\n\n\nThe term “program” here has nothing to do with a computer program. Instead, it was used by the US military during the WWII to refer to plans or schedules in training and logistics.\n\n\nIf maximization of the objective function f() is desired, we can simply multiply the objective function by -1 and minimize the resulting function.\nTypically there are two types of constraints that can be imposed on the optimization variable \\bm x:\n\nexplicit characterization of the set such as \\bm x \\in \\mathbb{R}^n or \\bm x \\in \\mathbb{Z}^n, possibly even a direct enumeration such as \\bm x \\in \\{0,1\\}^n in the case of binary variables,\nimplicit characterization of the set using equations and inequalities such as g_i(\\bm x) \\leq 0 and h_i(\\bm x) = 0 for i = 1, \\ldots, m and j = 1, \\ldots, p.\n\nAn example of a more structured and yet sufficiently general optimization problems over several real and integer variables is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^{n_x}, \\, \\bm y \\in \\mathbb{Z}^{n_y}} \\quad & f(\\bm x, \\bm y) \\\\\n\\text{subject to} \\quad & g_i(\\bm x, \\bm y) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_j(\\bm x, \\bm y) = 0, \\quad j = 1, \\ldots, p.\n\\end{aligned}\n\nIndeed, for named sets such as \\mathbb R or \\mathbb Z, it is common to place the set constraints directly underneath the word “minimize”. But it is just one convention, and these constraints could be listed in the “subject to” section as well.\n\n\n\n\n\n\nInteger optimization not included in this course\n\n\n\nIn this course we are only going to consider optimization problems with real-valued variables. This decision does not suggest that optimization with integer variables is less relevant for optimal control, quite the opposite! It is just that the theory and algorithms for integer or mixed integer optimization are based on different principles than those for real variables. And they can hardly fit into a single course. Good news for the interested students is that a graduate course named Combinatorial algorithms (B3B35KOA) covering integer optimization in detail is offered by our Cybernetics and Robotics study program at CTU FEE. Additionally, applications of integer optimization to optimal control are partly discussed in the course on Hybrid systems (B3B39HYS).\n\n\nThe form of an optimization problem that we are going to use in our course most often than not is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & g_i(\\bm x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_i(\\bm x) = 0, \\quad i = 1, \\ldots, p,\n\\end{aligned}\n which can also be written using vector-valued functions (reflected in the use of the bold face for their names) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#optimization-problem-formulation",
    "href": "opt_theory_problems.html#optimization-problem-formulation",
    "title": "Optimization problems",
    "section": "",
    "text": "We formulate a general optimization problem (also a mathematical program) as \n\\begin{aligned}\n\\operatorname*{minimize} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\bm x \\in \\mathcal{X},\n\\end{aligned}\n where f is a scalar function, \\bm x can be a scalar, a vector, a matrix or perhaps even a variable of yet another type, and \\mathcal{X} is a set of values that \\bm x can take, also called the feasible set.\n\n\n\n\n\n\nNote\n\n\n\nThe term “program” here has nothing to do with a computer program. Instead, it was used by the US military during the WWII to refer to plans or schedules in training and logistics.\n\n\nIf maximization of the objective function f() is desired, we can simply multiply the objective function by -1 and minimize the resulting function.\nTypically there are two types of constraints that can be imposed on the optimization variable \\bm x:\n\nexplicit characterization of the set such as \\bm x \\in \\mathbb{R}^n or \\bm x \\in \\mathbb{Z}^n, possibly even a direct enumeration such as \\bm x \\in \\{0,1\\}^n in the case of binary variables,\nimplicit characterization of the set using equations and inequalities such as g_i(\\bm x) \\leq 0 and h_i(\\bm x) = 0 for i = 1, \\ldots, m and j = 1, \\ldots, p.\n\nAn example of a more structured and yet sufficiently general optimization problems over several real and integer variables is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^{n_x}, \\, \\bm y \\in \\mathbb{Z}^{n_y}} \\quad & f(\\bm x, \\bm y) \\\\\n\\text{subject to} \\quad & g_i(\\bm x, \\bm y) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_j(\\bm x, \\bm y) = 0, \\quad j = 1, \\ldots, p.\n\\end{aligned}\n\nIndeed, for named sets such as \\mathbb R or \\mathbb Z, it is common to place the set constraints directly underneath the word “minimize”. But it is just one convention, and these constraints could be listed in the “subject to” section as well.\n\n\n\n\n\n\nInteger optimization not included in this course\n\n\n\nIn this course we are only going to consider optimization problems with real-valued variables. This decision does not suggest that optimization with integer variables is less relevant for optimal control, quite the opposite! It is just that the theory and algorithms for integer or mixed integer optimization are based on different principles than those for real variables. And they can hardly fit into a single course. Good news for the interested students is that a graduate course named Combinatorial algorithms (B3B35KOA) covering integer optimization in detail is offered by our Cybernetics and Robotics study program at CTU FEE. Additionally, applications of integer optimization to optimal control are partly discussed in the course on Hybrid systems (B3B39HYS).\n\n\nThe form of an optimization problem that we are going to use in our course most often than not is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & g_i(\\bm x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_i(\\bm x) = 0, \\quad i = 1, \\ldots, p,\n\\end{aligned}\n which can also be written using vector-valued functions (reflected in the use of the bold face for their names) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#properties-of-optimization-problems",
    "href": "opt_theory_problems.html#properties-of-optimization-problems",
    "title": "Optimization problems",
    "section": "Properties of optimization problems",
    "text": "Properties of optimization problems\nIt is now the presence/absence and the properties of individual components in the optimization problem defined above that characterize classes of optimization problems. In particular, we can identify the following properties:\n\nUnconstrained vs constrained\n\nPractically relevant problems are almost always constrained. But still there are good reasons to study unconstrained problems too, as many theoretical results and algorithms for constrained problems are based on transformations to unconstrained problems.\n\nLinear vs nonlinear\n\nBy linear problems we mean problems where the objective function and all the functions defining the constraints are linear (or actually affine) functions of the optimization variable \\bm x. Such problems constitute the simplest class of optimization problems, are very well understood, and there are efficient algorithms for solving them. In contrast, nonlinear problems are typically more difficult to solve (but see the discussion of the role of convexity below).\n\nSmooth vs nonsmooth\n\nEfficient algorithms for optimization over real variables benefit heavily from knowledge of the derivatives of the objective and constraint functions. If the functions are differentiable (aka smooth), we say that the whole optimization problem is smooth. Nonsmooth problems are typically more difficult to analyze and solve (but again, see the discussion of the role of convexity below).\n\nConvex vs nonconvex\n\nIf the objective function and the feasible set are convex (the latter holds when the functions defining the inequality constraints are convex and the functions defining the equality constrains are affine), the whole optimization problem is convex. Convex optimization problems are very well understood and there are efficient algorithms for solving them. In contrast, nonconvex problems are typically more difficult to solve. It is not becomming a common knowledge that convexity is a lot more important property than linearity and smoothness when it comes to solving optimization problems efficiently.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#classes-of-optimization-problems",
    "href": "opt_theory_problems.html#classes-of-optimization-problems",
    "title": "Optimization problems",
    "section": "Classes of optimization problems",
    "text": "Classes of optimization problems\nBased on the properties discussed above, we can identify the following distinct classes of optimization problems:\n\nLinear program (LP)\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A_\\mathrm{ineq}\\bm x \\leq \\mathbf b_\\mathrm{ineq},\\\\\n& \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq}.\n\\end{aligned}\n\nAn LP is obviously linear, hence it is also smooth and convex.\nSome theoretical results and numerical algorithms require a linear program in a specific form, called the standard form: \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A\\bm x = \\mathbf b,\\\\\n& \\bm x \\geq \\mathbf 0,\n\\end{aligned}\n where the inequality \\bm x \\geq \\mathbf 0 is understood componentwise, that is, x_i \\geq 0 for all i = 1, \\ldots, n.\n\n\nQuadratic program (QP)\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\bm x^\\top \\mathbf Q \\bm x +  \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A_\\mathrm{ineq}\\bm x \\leq \\mathbf b_\\mathrm{ineq},\\\\\n& \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq}.\n\\end{aligned}\n\nEven though the QP is nonlinear, it is smooth and if the matrix \\mathbf Q is positive semidefinite, it is convex. Its analysis and numerical solution are not much more difficult than those of an LP problem.\n\nQuadratically constrained quadratic program (QCQP)\nIt is worth emphasizing that for the standard QP the constraints are still given by a system of linear equations and inequalities. Sometimes we can encounter problems in which not only the cost function but also the functions defining the constraints are quadratic as in \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\bm x^\\top \\mathbf Q \\bm x +  \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\bm x^\\top \\mathbf A_i\\bm x + \\mathbf b_i \\bm x + c_i \\leq \\mathbf 0, \\quad i=1, \\ldots, m.\n\\end{aligned}\n\nA QCQP problem is convex if and only if the the constraints define a convex feasible set, which is the case when all the matrices \\mathbf A_i are positive semidefinite.\n\n\n\nConic program (CP)\nFirst, what is a cone? It is a set such that if something is in the cone, then a multiple of it by a nonnegative number is still in the set. We are going to restrict ourselves to regular cones, which are are pointed, closed and convex. An example of such regular cone in a plane is in Fig. 1 below.\n\n\n\n\n\n\nFigure 1: Regular (pointed, convex, closed) cone in a plane\n\n\n\nNow, what is the role of cones in optimization? Reformulation of nonlinear optimization problems using cones constitutes a systematic way to identify what these (conic) optimization problems have in common with linear programs, for which powerful theory and efficient algorithms exist.\nNote that an LP in the standard form can be written as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathbb{R}_+^n,\n\\end{aligned}\n where \\mathbb R_+^n is a positive orthant. Now, the positive orthant is a convex cone! We can then see the LP as an instance of a general conic optimization problem (conic program)\n\n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathcal{K},\n\\end{aligned}\n where \\mathcal{K} is a cone in \\mathbb R^n.\n\n\n\n\n\n\nInequality interpreted as belonging to a cone\n\n\n\nA fundamental idea unrolled here: the inequality \\bm x\\geq 0 can be interpreted as \\bm x belonging to a componentwise nonegative cone, that is \\bm x \\in \\mathbb R_+^n. What if some other cone \\mathcal K is considered? What would be the interpretation of the inequality then?\n\n\nSometimes in order to emphasize that the inequality is induced by the cone \\mathcal K, we write it as \\geq_\\mathcal{K}. Another convention – the one we actually adopt here – is to use another symbol for the inequality \\succeq to distinguish it from the componentwise meaning, assuming that the cone is understood from the context. We then interpret conic inequalities such as \n\\mathbf A_\\mathrm{ineq}\\bm x \\succeq \\mathbf b_\\mathrm{ineq}\n in the sense that \n\\mathbf A_\\mathrm{ineq}\\bm x - \\mathbf b_\\mathrm{ineq} \\in \\mathcal{K}.\n\nIt is high time to explore some concrete cones (other than the positive orthant). We consider just two, but there are a few more, see the literature.\n\nSecond-order cone program (SOCP)\nThe most immediate cone in \\mathbb R^n is the second-order cone, also called the Lorentz cone or even the ice cream cone. We explain it in \\mathbb R^3 for the ease of visualization, but generalization to \\mathbb R^n is straightforward. The second-order cone in \\mathbb R^3 is defined as \n\\mathcal{K}_\\mathrm{SOC}^3 = \\left\\{ \\bm x \\in \\mathbb R^3 \\mid \\sqrt{x_1^2 + x_2^2} \\leq x_3 \\right\\}.\n\nand is visualized in Fig. 2 below.\n\n\n\n\n\n\n\nFigure 2: A second-order cone in 3D\n\n\n\n\nWhich of the three axes plays the role of the axis of symmetry for the cone must be agreed beforehand. Singling this direction out, the SOC in \\mathbb R^n can also be formulated as \n\\mathcal{K}_\\mathrm{SOC}^n = \\left\\{ (\\bm x, t) \\in \\mathbb R^{n-1} \\times \\mathbb R \\mid \\|\\bm x\\|_2 \\leq t \\right\\}.\n\nA second-order conic program in standard form is then \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathcal{K}_\\mathrm{SOC}^n,\n\\end{aligned}\n\nwhich can be written explicitly as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad x_1^2 + \\cdots + x_{n-1}^2 - x_n^2 \\leq 0.\n\\end{aligned}\n\nA second-order conic program can also come in non-standard form such as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A_\\mathrm{ineq}\\bm x \\succeq  \\mathbf b_\\mathrm{ineq}.\n\\end{aligned}\n\nAssuming the data is structured as \n\\begin{bmatrix}\n\\mathbf A\\\\\n\\mathbf r^\\top\n\\end{bmatrix}\n\\bm x \\succeq\n\\begin{bmatrix}\n\\mathbf b\\\\\nq\n\\end{bmatrix},\n the inequality can be rewritten as \n\\begin{bmatrix}\n\\mathbf A\\\\\n\\mathbf r^\\top\n\\end{bmatrix}\n\\bm x -\n\\begin{bmatrix}\n\\mathbf b\\\\\nq\n\\end{bmatrix} \\in \\mathcal{K}_\\mathrm{SOC}^n,\n which finally gives \n\\|\\mathbf A \\bm x - \\mathbf b\\|_2 \\leq \\mathbf r^\\top \\bm x + q.\n\nTo summarize, another form of a second-order cone program (SOCP) is\n\n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq},\\\\\n&\\quad \\|\\mathbf A \\bm x - \\mathbf b\\|_2 \\leq \\mathbf r^\\top \\bm x + q.\n\\end{aligned}\n\nWe can see that the SOCP contains both linear and quadratic constraints, hence it generalizes LP and QP, including convex QCQP. To see the latter, expand the square of \\|\\mathbf A \\bm x - \\mathbf b\\|_2 into (\\bm x^\\top \\mathbf A^\\top  - \\mathbf b^\\top)(\\mathbf A \\bm x - \\mathbf b) = \\bm x^\\top \\mathbf A^\\top \\mathbf A \\bm x + \\ldots\n\n\nSemidefinite program (SDP)\nAnother cone of great importance the control theory is the cone of positive semidefinite matrices. It is commonly denoted as \\mathcal S_+^n and is defined as \n\\mathcal S_+^n = \\left\\{ \\bm X \\in \\mathbb R^{n \\times n} \\mid \\bm X = \\bm X^\\top, \\, \\bm z^\\top \\bm X \\bm z \\geq 0\\; \\forall \\bm z\\in \\mathbb R^n \\right\\},\n and with this cone the inequality \\mathbf X \\succeq 0 is a common way to express that \\mathbf X is positive semidefinite.\nUnlike the previous classes of optimization problems, this one is formulated with matrix variables instead of vector ones. But nothing prevents us from collecting the components of a symmetric matrix into a vector and proceed with vectors as usual, if needed:\n\n\\bm X = \\begin{bmatrix} x_1 & x_2 & x_3 \\\\ x_2 & x_4 & x_5\\\\ x_3 & x_5 & x_6 \\end{bmatrix},\n\\quad\n\\bm x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_6 \\end{bmatrix}.\n\nAn optimization problem with matrix variables constrained to be in the cone of semidefinite matrices (or their vector representations) is called a semidefinite program (SDP). As usual, we start with the standard form, in which the cost function is linear and the optimization is subject to an affine constraint and a conic constraint. In the following, in place of the inner products of two vectors \\mathbf c^\\top x we are going to use inner products of matrices defined as \n\\langle \\mathbf C, \\bm X\\rangle = \\operatorname{Tr} \\mathbf C \\bm X,\n where \\operatorname{Tr} is a trace of a matrix defined as the sum of the diagonal elements.\nThe SDP program in the standard form is then \n\\begin{aligned}\n\\operatorname{minimize}_{\\bm X} &\\quad \\operatorname{Tr} \\mathbf C \\bm X\\\\\n\\text{subject to} &\\quad \\operatorname{Tr} \\mathbf A_i \\bm X = \\mathbf b_i, \\quad i=1, \\ldots, m,\\\\\n&\\quad \\bm X \\in \\mathcal S_+^n,\n\\end{aligned}\n where the latter constraint is more often than not written as \\bm X \\succeq 0, understanding from the context that the cone of positive definite matrices is assumed.\n\n\nOther conic programs\nWe are not going to cover them here, but we only enumerate a few other cones useful in optimization: rotated second-order cone, exponential cone, power cone, … A concise overview is in [1]\n\n\n\nGeometric program (GP)\n#TODO: although of little immediate use in our course.\n\n\nNonlinear program (NLP)\nFor completeness we include here once again the general nonlinear programming problem:\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}\n\nSmoothness of the problem can easily be determined based on the differentiability of the functions. Convexity can also be determined by inspecting the functions, but this is not necessarily easy. One way to check convexity of a function is to view it as a composition of simple functions and exploit the knowledge about convexity of these simple functions. See [2, Sec. 3.2]",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "roban_structured.html#robust-performance-with-a-structured-uncertainty",
    "href": "roban_structured.html#robust-performance-with-a-structured-uncertainty",
    "title": "Robustness analysis for structured uncertainty",
    "section": "Robust performance with a structured uncertainty",
    "text": "Robust performance with a structured uncertainty",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for structured uncertainty"
    ]
  },
  {
    "objectID": "cont_indir_software.html",
    "href": "cont_indir_software.html",
    "title": "Software for continuous-time LQR and CARE",
    "section": "",
    "text": "ControlSystems.jl\n\nlqr\nare – actually uses MatrixEquations.jl.\n\nMatrixEquations.jl\n\narec",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Software for continuous-time LQR and CARE"
    ]
  },
  {
    "objectID": "cont_indir_software.html#julia",
    "href": "cont_indir_software.html#julia",
    "title": "Software for continuous-time LQR and CARE",
    "section": "",
    "text": "ControlSystems.jl\n\nlqr\nare – actually uses MatrixEquations.jl.\n\nMatrixEquations.jl\n\narec",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Software for continuous-time LQR and CARE"
    ]
  },
  {
    "objectID": "cont_indir_software.html#matlab",
    "href": "cont_indir_software.html#matlab",
    "title": "Software for continuous-time LQR and CARE",
    "section": "MATLAB",
    "text": "MATLAB\n\nControl System Toolbox\n\nicare (care deprecated)\nlqr",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Software for continuous-time LQR and CARE"
    ]
  },
  {
    "objectID": "cont_indir_software.html#python",
    "href": "cont_indir_software.html#python",
    "title": "Software for continuous-time LQR and CARE",
    "section": "Python",
    "text": "Python\n\nPython Control\n\ncare\nlqr",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Software for continuous-time LQR and CARE"
    ]
  },
  {
    "objectID": "ext_stochastic_LQR.html",
    "href": "ext_stochastic_LQR.html",
    "title": "LQR for stochastic systems",
    "section": "",
    "text": "We now consider an extension to the standard LQR problem, namely, the plant is subject to random disturbances and the initial state is also random. The question for which we seek an answer is: how does this affect the design of the optimal LQR controller? The question surely makes sense in both discrete- and continuous-time cases, but here we only focus on continuous-time systems.\nWe consider the following model \n\\dot{\\bm{x}}(t) = \\mathbf{A}\\bm{x}(t) + \\mathbf{B}\\bm{u}(t) + {\\color{red}\\mathbf{B}_w\\bm{w}(t)},\n in which the initial state is a normally distributed zero-mean random vector of prescribed covariance, that is,\n\n\\begin{aligned}\n\\mathbb E\\left\\{\\bm{x}(0)\\right\\} &= \\mathbf 0,\\\\\n\\mathbb E\\left\\{\\bm{x}(0)\\bm{x}^\\top(0)\\right\\} &= \\mathbf P_{x}(0),\n\\end{aligned}\n and the random disturbance w(\\cdot) is (modelled as) a white noise of spectral density \\mathbf S_{w} \n\\begin{aligned}\n\\mathbb E\\left\\{\\bm{w}(t)\\right\\} &= \\mathbf 0,\\\\\n\\mathbb E\\left\\{\\bm{w}(t)\\bm{w}^\\top(t+\\tau)\\right\\} &= \\mathbf S_w\\delta(\\tau),\n\\end{aligned}\n where \\delta(\\cdot) is a Dirac delta “function”.\nBefore we proceed to analyzing the impact of these assumptions on the solution, we must discuss the very problem statement. Note that since the initial state and the disturbance are random, the cost function is random too. It then makes sense to modify the cost function to use the expected value as in \nJ = \\frac{1}{2}{\\color{red}\\mathbb{E}}\\left\\{\\bm{x}^\\top (t_\\mathrm{f})\\mathbf{S}\\bm{x}(t_\\mathrm{f}) + \\int_{0}^{t_\\mathrm{f}}\\left[\\bm{x}^\\top(t)\\mathbf{Q}\\bm{x}(t) + \\bm{u}^\\top (t)\\mathbf{R}\\bm{u}(t)\\right]dt \\right\\}\nThis was straightforward. But what if t_\\mathrm{f} = \\infty? The problem is that\nJ = \\frac{1}{2}\\mathbb{E}\\left\\{\\int_{0}^{\\infty}\\left[\\bm{x}^\\top(t)\\mathbf{Q}\\bm{x}(t) + \\bm{u}^\\top (t)\\mathbf{R}\\bm{u}(t)\\right]dt \\right\\}\n does not generally converge (neither x nor u settle).\nBut we can scale the cost by \\frac{2}{t_\\mathrm{f}} \nJ' = \\frac{1}{2}\\mathbb{E}\\left\\{\\lim_{t_\\mathrm{f}\\rightarrow \\infty} \\frac{2}{t_\\mathrm{f}}\\int_{0}^{t_\\mathrm{f}}\\left[\\bm{x}^\\top(t)\\mathbf{Q}\\bm{x}(t) + \\bm{u}^\\top (t)\\mathbf{R}\\bm{u}(t)\\right]dt \\right\\},\n in which we used the prime to emphasize that the cost function is modified/scaled.\nFor a growing t_\\mathrm{f}, the modified cost function converges to a finite value corresponding to the steady state. With some abuse of notation (by understanding that \\bm x(\\infty) and \\bm u(\\infty) correspond to the steady-state values of the state and control vector), we can write the cost function as \\boxed{\nJ'= \\mathbb{E}\\left\\{\\bm{x}^\\top(\\infty)\\mathbf{Q}\\bm{x}(\\infty) + \\bm{u}^\\top (\\infty)\\mathbf{R}\\bm{u}(\\infty)\\right\\}}\nNow that we have adjusted the LQR problem statement to the stochastic case, we could proceed to the solution. But we skip the derivation (it can be found elsewhere) and provide the results instead.\nThe optimal control must necessarily come in the form of a feedback controller because of the random initial states and disturbances (recall that in the deterministic case we optimized over u(\\cdot) and it only turned out as the outcome of some theoretical derivations that the optimal control is provided by a linear state feedback).\nAnd can be shown that if a Gaussian white noise disturbances is assumed, the optimal controller is a linear state feedback.\nThe solution is found via algebraic Riccati equation exactly as in the deterministic case.\nThe optimal controller is independent of the initial state covariance matrix and the disturbance spectral density matrix. This can be understood intuitively as follows: in the deterministic case, the optimal controller does not depend on the initial state, and apparently nothing changes about this if the initial state is characterized as a random vector. The impact of the disturbance can be seen as if it was resetting the system to a new initial state at each time instant. The optimal controller is then independent of the disturbance spectral density matrix.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQR for stochastic systems"
    ]
  },
  {
    "objectID": "ext_stochastic_LQR.html#alternative-input-output-interpretation-of-stochastic-lqr",
    "href": "ext_stochastic_LQR.html#alternative-input-output-interpretation-of-stochastic-lqr",
    "title": "LQR for stochastic systems",
    "section": "Alternative input-output interpretation of stochastic LQR",
    "text": "Alternative input-output interpretation of stochastic LQR\nWe now formulate the stochastic LQR problem as an input-output problem. Consider the block diagram in Fig. 3.\n\n\n\n\n\n\nFigure 3: Stochastic LQR problem formulated as an interconnection of a (state) feedback controller and an artificial (also called generalized) plant\n\n\n\nThe grey block in Fig. 3 is our first instance of the so-called generalized plant. It has two inputs:\n\nthe white noise input \\bm w(t),\nthe control input \\bm u(t).\n\nand two outputs:\n\nthe regulated output \\bm z(t), here composed of two parts \\bm z_1(t) and \\bm z_2(t),\nthe measured output \\bm y(t).\n\nThe optimal control problem is then formulated as \n\\operatorname*{minimize}_{\\mathbf K} \\mathbb E\\left\\{\\begin{bmatrix}\\bm  z_1^\\top(\\infty)&\\bm  z_2^\\top(\\infty)\\end{bmatrix}\\begin{bmatrix}\\bm  z_1(\\infty)\\\\ \\bm  z_2(\\infty)\\end{bmatrix}\\right\\}.\n\nIf this is not immediately visible, just use the fact that \\bm z_1 = \\sqrt{\\mathbf R} \\bm u and \\bm z_2 = \\sqrt{\\mathbf Q} \\bm x, and then \\bm z_1^\\top \\bm z_1 = \\bm u^\\top \\mathbf R \\bm u and \\bm z_2^\\top \\bm z_2 = \\bm x^\\top \\mathbf Q \\bm x.\nThe motivation for this reformulation will become clear in a moment.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQR for stochastic systems"
    ]
  },
  {
    "objectID": "ext_stochastic_LQR.html#mathcalh_2-system-norm",
    "href": "ext_stochastic_LQR.html#mathcalh_2-system-norm",
    "title": "LQR for stochastic systems",
    "section": "\\mathcal{H}_2 system norm",
    "text": "\\mathcal{H}_2 system norm\nFor a stable and strictly proper LTI system modelled by the (matrix) transfer function \\mathbf G, the \\mathcal{H}_2 norm is defined as \\boxed{\n\\|\\mathbf G\\|_2 = \\sqrt{\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\text{Tr}\\left[\\mathbf G^\\ast(j\\omega)\\mathbf G(j\\omega)\\right]\\text{d}\\omega},\n}\n where \\text{Tr} is a trace of a matrix, that is, a sum of its diagonal elements.\nUsing Parseval’s theorem (relating inner products in time and frequency domains), we can rewrite the \\mathcal{H}_2 norm as \n\\boxed{\n\\|\\mathbf G\\|_2 = \\sqrt{\\int_{0}^\\infty \\text{Tr}\\left[\\mathbf g^\\top(t)\\mathbf g(t)\\right]\\text{d}t},}\n where \\mathbf g(t) is the impulse response of the system with the transfer function \\mathbf G(s). Clearly, it is equal to the \\mathcal{L}_2 norm of the impulse response.\nAnd why did we bother to introduce the \\mathcal{H}_2 norm?",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQR for stochastic systems"
    ]
  },
  {
    "objectID": "ext_stochastic_LQR.html#mathcalh_2-norm-as-a-gain-of-a-system-subject-to-a-stationary-white-noise-input",
    "href": "ext_stochastic_LQR.html#mathcalh_2-norm-as-a-gain-of-a-system-subject-to-a-stationary-white-noise-input",
    "title": "LQR for stochastic systems",
    "section": "\\mathcal{H}_2 norm as a gain of a system subject to a stationary white noise input",
    "text": "\\mathcal{H}_2 norm as a gain of a system subject to a stationary white noise input\nWhen presenting the following result, we restrict ourselves just to a single white noise input with the spectral density S_w. Equivalently, we can also consider a vector white noise with the spectral density matrix parameterized by a single parameter as in \\mathbf S_w = S_w \\mathbf I. The following result is useful:\n\\boxed{\n\\mathbb E\\left\\{\\bm y^\\top(\\infty)\\bm y(\\infty)\\right\\}  = \\|\\mathbf G\\|_2^2 \\;S_w}.\n\nFor convenience, we also rewrite it using a square root\n\\boxed{\n\\sqrt{\\mathbb E\\left\\{\\bm y^\\top(\\infty)\\bm y(\\infty)\\right\\}}  = \\|\\mathbf G\\|_2 \\;\\sqrt{S_w}}.\n\nIt shows that the \\mathcal H_2 system norm determines the root mean square (RMS) value of the steady-state output of the system subject to a white noise input. If the white noise input is of nonunit spectral density, the result is scaled by the square root of the spectral density of the input.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQR for stochastic systems"
    ]
  },
  {
    "objectID": "ext_stochastic_LQR.html#lqr-viewed-as-mathcalh_2-optimal-control",
    "href": "ext_stochastic_LQR.html#lqr-viewed-as-mathcalh_2-optimal-control",
    "title": "LQR for stochastic systems",
    "section": "LQR viewed as \\mathcal{H}_2-optimal control",
    "text": "LQR viewed as \\mathcal{H}_2-optimal control\nAssembling the pieces together, we can now see that the reformulated LQR problem can be viewed as a special case of the \\mathcal{H}_2 optimal control problem, for which the goal is to find a controller that minimizes the \\mathcal{H}_2 norm of the closed-loop system.\n\n\n\n\n\n\nFigure 4: Stochastic LQR as \\mathcal{H}_2 optimal control problem (the controller is restricted to a matrix of gains)\n\n\n\nThis is certainly interesting, but once again – what is the point in all this? So far we have only reformulated our good old LQR problem in a new framework, but when it comes to solving the problem, we already know that the optimal solution can be found by solving the algebraic Riccati equation. We also know that the unique optimal controller is just a linear (proportional) state feedback controller.\nHere comes the crucial fact: for a generalized plant (satisfying some minor technical conditions), not only the one in Fig. 3 corresponding to the LQR problem, there exists optimization solvers capable of finding a stabilizing feedback controller that minimizes the \\mathcal H_2 norm of the closed-loop transfer function; see the section on software. Having been informed about this availability of solvers, how about modifying the generalized plant by changing the existing or adding some new blocks? Perhaps this could cover some new types of optimal control problem other than the LQR problem. For example, instead of the identity matrix \\mathbf I in Fig. 3, we can use a general output matrix \\mathbf C, which would allow us to use only a subset of the state variables for feedback control. Or we could add some first- or second-order filters right after the input \\bm w, so that we can color the white noise input (to turn its flat spectral density into something frequency-dependent).\nIndeed, this line of reasoning leads to a new broad family of optimal control problems, which is called \\mathcal{H}_2 optimal control, diagrammed in Fig. 5. We emphasize that the controller is not restricted to a matrix of gains, but can also contain some dynamics, and as such can be modelled as a (matrix) transfer function.\n\n\n\n\n\n\nFigure 5: General \\mathcal{H}_2 optimal control problem (the controller can also contain some dynamics)\n\n\n\nAlthough useful on its own, we get perhaps even better benefit from it by learning about the possibility to formulate an optimal control problem as a minimization of some system norm. We will encounter another system norm – the \\mathcal{H}_\\infty norm – in the next chapter, which will open up a whole new world of optimal control problems.\nBut before we do that, we are going to discuss another extension of the LQR problem – the popular LQG problem, which can also be formulated within the unified framework of \\mathcal{H}_2 optimal control.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQR for stochastic systems"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html",
    "href": "opt_theory_constrained.html",
    "title": "Theory for constrained optimization",
    "section": "",
    "text": "We consider the following optimization problem with equality constraints \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\n\\end{aligned}\n where \\mathbf h(\\bm x) \\in \\mathbb R^m defines a set of m equations \n\\begin{aligned}\nh_1(\\bm x) &= 0\\\\\nh_2(\\bm x) &= 0\\\\\n\\vdots\\\\\nh_m(\\bm x) &= 0.\n\\end{aligned}\n\nAugmenting the original cost function f with the constraint functions h_i scaled by Lagrange variables \\lambda_i gives the Lagrangian function \n\\mathcal{L}(\\bm x,\\boldsymbol\\lambda) \\coloneqq f(\\bm x) + \\sum_{i=1}^m \\lambda_i h_i(\\bm x) = f(\\bm x) + \\boldsymbol \\lambda^\\top \\mathbf h(\\bm x).\n\n\n\n\n\nThe first-order necessary condition of optimality is \n\\nabla \\mathcal{L}(\\bm x,\\boldsymbol\\lambda) = \\mathbf 0,\n which amounts to two (generally vector) equations \n\\boxed{\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nDefining a matrix \\nabla \\mathbf h(\\bm x) \\in \\mathbb R^{n\\times m} as horizontally stacked gradients of the constraint functions \n\\nabla \\mathbf h(\\bm x) \\coloneqq \\begin{bmatrix}\n                                 \\nabla h_1(\\bm x) && \\nabla h_2(\\bm x) && \\ldots && \\nabla h_m(\\bm x)\n                            \\end{bmatrix},\n in fact, a transpose of the Jacobian matrix, the necessary condition can be rewritten in a vector form as \\boxed\n{\\begin{aligned}\n\\nabla f(\\bm x) + \\nabla \\mathbf h(\\bm x)\\boldsymbol \\lambda &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nBeware of the nonregularity issue! The (\\nabla \\mathbf h(\\bm x))^\\mathrm T is regular at a given \\bm x (the \\bm x is a regular point) if it has a full column rank. Rank-deficiency reveals a defect in formulation.\n\nExample 1 (Equality-constrained quadratic program) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} &\\quad \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x}\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x + \\mathbf b = \\mathbf 0.\n\\end{aligned}\n\nThe first-order necessary condition of optimality is\n\n\\begin{bmatrix}\n  \\mathbf Q & \\mathbf A^\\top\\\\\\mathbf A & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\bm x \\\\ \\boldsymbol \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  -\\mathbf r\\\\\\mathbf b\n\\end{bmatrix}.\n\n\n\n\n\n\nUsing the unconstrained Hessian \\nabla^2_{\\mathbf{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda) is too conservative. Instead, use projected Hessian\n\n\\mathbf{Z}^\\mathrm{T}\\;\\nabla^2_{\\bm{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda)\\;\\mathbf Z &gt; 0,\n where \\mathbf Z is an (orthonormal) basis of the nullspace of the Jacobian (\\nabla \\mathbf h(\\bm x))^\\top.",
    "crumbs": [
      "1. Optimization – theory",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#equality-constraints",
    "href": "opt_theory_constrained.html#equality-constraints",
    "title": "Theory for constrained optimization",
    "section": "",
    "text": "We consider the following optimization problem with equality constraints \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\n\\end{aligned}\n where \\mathbf h(\\bm x) \\in \\mathbb R^m defines a set of m equations \n\\begin{aligned}\nh_1(\\bm x) &= 0\\\\\nh_2(\\bm x) &= 0\\\\\n\\vdots\\\\\nh_m(\\bm x) &= 0.\n\\end{aligned}\n\nAugmenting the original cost function f with the constraint functions h_i scaled by Lagrange variables \\lambda_i gives the Lagrangian function \n\\mathcal{L}(\\bm x,\\boldsymbol\\lambda) \\coloneqq f(\\bm x) + \\sum_{i=1}^m \\lambda_i h_i(\\bm x) = f(\\bm x) + \\boldsymbol \\lambda^\\top \\mathbf h(\\bm x).\n\n\n\n\n\nThe first-order necessary condition of optimality is \n\\nabla \\mathcal{L}(\\bm x,\\boldsymbol\\lambda) = \\mathbf 0,\n which amounts to two (generally vector) equations \n\\boxed{\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nDefining a matrix \\nabla \\mathbf h(\\bm x) \\in \\mathbb R^{n\\times m} as horizontally stacked gradients of the constraint functions \n\\nabla \\mathbf h(\\bm x) \\coloneqq \\begin{bmatrix}\n                                 \\nabla h_1(\\bm x) && \\nabla h_2(\\bm x) && \\ldots && \\nabla h_m(\\bm x)\n                            \\end{bmatrix},\n in fact, a transpose of the Jacobian matrix, the necessary condition can be rewritten in a vector form as \\boxed\n{\\begin{aligned}\n\\nabla f(\\bm x) + \\nabla \\mathbf h(\\bm x)\\boldsymbol \\lambda &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nBeware of the nonregularity issue! The (\\nabla \\mathbf h(\\bm x))^\\mathrm T is regular at a given \\bm x (the \\bm x is a regular point) if it has a full column rank. Rank-deficiency reveals a defect in formulation.\n\nExample 1 (Equality-constrained quadratic program) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} &\\quad \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x}\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x + \\mathbf b = \\mathbf 0.\n\\end{aligned}\n\nThe first-order necessary condition of optimality is\n\n\\begin{bmatrix}\n  \\mathbf Q & \\mathbf A^\\top\\\\\\mathbf A & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\bm x \\\\ \\boldsymbol \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  -\\mathbf r\\\\\\mathbf b\n\\end{bmatrix}.\n\n\n\n\n\n\nUsing the unconstrained Hessian \\nabla^2_{\\mathbf{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda) is too conservative. Instead, use projected Hessian\n\n\\mathbf{Z}^\\mathrm{T}\\;\\nabla^2_{\\bm{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda)\\;\\mathbf Z &gt; 0,\n where \\mathbf Z is an (orthonormal) basis of the nullspace of the Jacobian (\\nabla \\mathbf h(\\bm x))^\\top.",
    "crumbs": [
      "1. Optimization – theory",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#inequality-constraints",
    "href": "opt_theory_constrained.html#inequality-constraints",
    "title": "Theory for constrained optimization",
    "section": "Inequality constraints",
    "text": "Inequality constraints\n \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf g(\\bm x) \\leq \\mathbf 0,\n\\end{aligned}\n where \\mathbf g(\\bm x) \\in \\mathbb R^p defines a set of p inequalities.\n\nFirst-order necessary condition of optimality\nKarush-Kuhn-Tucker (KKT) conditions of optimality are then composed of these four (sets of) conditions \n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^p \\mu_i \\nabla g_i(\\bm x) &= \\mathbf 0,\\\\\n\\mathbf{g}(\\bm{x}) &\\leq \\mathbf 0,\\\\\n\\mu_i g_i(\\bm x) &= 0,\\quad i = 1,2,\\ldots, m\\\\\n\\mu_i &\\geq 0,\\quad   i = 1,2,\\ldots, m.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#equality-and-inequality-constraints",
    "href": "opt_theory_constrained.html#equality-and-inequality-constraints",
    "title": "Theory for constrained optimization",
    "section": "Equality and inequality constraints",
    "text": "Equality and inequality constraints\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\\\\\n                    &\\quad \\mathbf g(\\bm x) \\leq \\mathbf 0.\n\\end{aligned}\n\n\nFirst-order necessary condition of optimality\nThe KKT conditions\n\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) + \\sum_{i=1}^p \\mu_i \\nabla g_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\mathbf{x}) &= \\mathbf 0\\\\\n\\mathbf{g}(\\mathbf{x}) &\\leq \\mathbf 0\\\\\n\\mu_i g_i(\\bm x) &= 0,\\quad i = 1,\\ldots, m\\\\\n\\mu_i &\\geq 0,\\quad   i = 1,\\ldots, m.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#duality",
    "href": "opt_theory_constrained.html#duality",
    "title": "Theory for constrained optimization",
    "section": "Duality",
    "text": "Duality\nDuality theory offers another view of the original optimization problem by bringing in another but related one.\nCorresponding to the general optimization problem \n  \\begin{aligned}\n  \\operatorname*{minimize}\\;&f(\\bm x)\\\\\n  \\text{subject to}\\; & \\mathbf g(\\bm x)\\leq \\mathbf 0\\\\\n  & \\mathbf h(\\bm x) = \\mathbf 0,\n  \\end{aligned}\n\nwe form the Lagrangian function \\mathcal L(\\bm x,\\bm \\lambda,\\bm \\mu) = f(\\bm x) + \\bm \\lambda^\\top \\mathbf h(\\bm x) + \\bm \\mu^\\top \\mathbf g(\\bm x)\nFor any (fixed) values of (\\bm \\lambda,\\bm \\mu) such that \\bm \\mu\\geq 0, we define the Lagrange dual function through the following unconstrained optimization problem \nq(\\bm\\lambda,\\bm\\mu) = \\inf_{\\bm x}\\mathcal L(\\bm x,\\bm \\lambda,\\bm \\mu).\n\nObviously, it is alway possible to pick a feasible solution \\bm x, in which case the value of the Lagrangian and the original function coincide, and so the result of this minimization is no worse (larger) than the minimum for the original optimization problem. It can thus serve as a lower bound q(\\bm \\lambda,\\bm \\mu) \\leq f(\\bm x^\\star).\nThis result is called weak duality. A natural idea is to find the values of \\bm \\lambda and \\bm \\mu such that this lower bound is tightest, that is, \n\\begin{aligned}\n  \\operatorname*{maximize}_{\\bm\\lambda, \\bm\\mu}\\; & q(\\bm\\lambda,\\bm\\mu)\\\\\n  \\text{subject to}\\;& \\bm\\mu \\geq \\mathbf 0.\n\\end{aligned}\n\nUnder some circumstances the result can be tight, which leads to strong duality, which means that the minimum of the original (primal) problem and the maximum of the dual problem coincide. \nq(\\bm \\lambda^\\star,\\bm \\mu^\\star) = f(\\bm x^\\star).\n\nThis related dual optimization problem can have some advantages for development of both theory and algorithms.",
    "crumbs": [
      "1. Optimization – theory",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "discr_indir_general.html",
    "href": "discr_indir_general.html",
    "title": "General nonlinear discrete-time optimal control as a two-point boundary value problem",
    "section": "",
    "text": "While in the previous chapter we formulated an optimal control problem (OCP) directly as a mathematical programming (general NLP or even QP) problem over the control (and possibly state) trajectories, in this chapter we introduce an alternative – indirect – approach. The essence of the approach is that we formulate first-order necessary conditions of optimality for the OCP in the form of equations, and then solve these. Although less straightforward to extend with additional constraints than the direct approach, the indirect approach also exhibits some advantages. In particular, in some cases (such as a quadratic cost and a linear system) it yields a feedback control law and not just a control trajectory.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "discr_indir_general.html#optimization-constrains-given-only-by-the-state-equations",
    "href": "discr_indir_general.html#optimization-constrains-given-only-by-the-state-equations",
    "title": "General nonlinear discrete-time optimal control as a two-point boundary value problem",
    "section": "Optimization constrains given only by the state equations",
    "text": "Optimization constrains given only by the state equations\nAs in the chapter on the direct approach, here we also start by considering a general nonlinear and possibly time-varying discrete-time dynamical system characterized by the state vector \\bm x_k\\in\\mathbb R^n whose evolution in discrete time k is uniquely determined by the state equation \n\\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\n accompanied by the initial state (vector) \\bm x_i\\in\\mathbb R^n and a sequence of control inputs \\bm u_i, \\bm u_{i+1}, \\ldots, \\bm u_{k-1}, where the control variable can also be a vector, that is, \\bm u_k \\in \\mathbb R^m.\nThese state equations will constitute the only constraints of the optimization problem. Unlike in the direct approach, here in our introductory treatment we do not impose any inequality constraints such as bounds on the control inputs, because the theory to be presented is not able to handle them.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "discr_indir_general.html#general-additive-cost-function",
    "href": "discr_indir_general.html#general-additive-cost-function",
    "title": "General nonlinear discrete-time optimal control as a two-point boundary value problem",
    "section": "General additive cost function",
    "text": "General additive cost function\nFor the above described dynamical system we want to find a control sequence \\bm u_k that minimizes a suitable optimization criterion over a finite horizon k\\in[i,N]. Namely, we will look for a control that minimizes a criterion of the following kind \nJ_i^N(\\underbrace{\\bm x_{i+1}, \\bm x_{i+2}, \\ldots, \\bm x_{N}}_{\\bar{\\bm x}}, \\underbrace{\\bm u_{i}, \\ldots, \\bm u_{N-1}}_{\\bar{\\bm u}};\\bm x_i) = \\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k).\n\\tag{1}\n\n\n\n\n\n\nNote\n\n\n\nRegarding the notation J_i^N(\\cdot) for the cost, if the initial and final times are understood from the context, they do not have to be displayed. But we will soon need to indicate the initial time explicitly in our derivations.\n\n\nThe property of the presented cost function that will turn out crucial in our subsequent work is that is additive over the time horizon. Although this restricts the class of cost functions a bit, it is still general enough to encompass a wide range of problems, such as minimizing the total (financial) cost to be paid, the total energy to be used, the total distance to be travelled, the cumulative error to be minimized, etc.\nHere is a list of a few popular cost functions.\n\nMinimum-time (or time-optimal) problem\n\nSetting \\phi=1 and L_k=1 gives J=N-i, that is, the length of the time horizon, the duration of control. Altough in this course we do not introduce concepts and tools for optimization over integer variables, in this simple case of just a single integer variable even a simple search over the length of control interval will be computationally tractable. Furthermore, as we will see in one of the next chapters once we switch from discrete-time to continuous-time systems, this time-optimal control design problem will turn out tractable using the tools presented in this course.\n\nMinimum-fuel problem\n\nSetting \\phi=0 and L_k=|u_k|, which gives J=\\sum_{k=i}^{N-1}|u_k|.\n\nMinimum-energy problem\n\nSetting \\phi=0 and L_k=\\frac{1}{2} u_k^2, which gives J=\\frac{1}{2} \\sum_{k=i}^{N-1} u_k^2. It is fair to admit that this sum of squared inputs cannot always be interpretted as the energy – for instance, what if the control input is a degree of openning of a valve? Sum of angles over time can hardly be interpreted as energy. Instead, it should be interpretted in the mathematical way as the (squared) norm, that is, a “size” of the input. Note that the same objection can be given to the previous case of a minimum-fuel problem.\n\nMixed quadratic problem (also LQ-optimal control problem)\n\nSetting \\phi=\\frac{1}{2}s_N x_N^2 and L_k=\\frac{1}{2} (qx_k^2+ru_k^2),\\, q,r\\geq 0, which gives J=\\frac{1}{2}s_Nx_N^2+\\frac{1}{2} \\sum_{k=i}^{N-1} (r x_k^2+q u_k^2). Or in the case of vector state and control variables J=\\frac{1}{2}\\bm x_N^\\top \\mathbf S_N \\bm x_N+\\frac{1}{2} \\sum_{k=i}^{N-1} (\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k), \\, \\mathbf Q, \\mathbf R \\succeq 0. This type of an optimization cost is particularly popular. Both for the mathematical reasons (we all now appreciate the nice properties of quadratic functions) and for practical engineering reasons as it allows us to capture a trade-off between the control performance (penalty on \\bm x_k) and control effort (penalty on \\bm u_k). Note also that the state at the terminal time N is penalized separately just in order to allow another trade-off between the transient and terminal behavior. The cost function can also be modified to penalize deviation of the state from some nonzero desired (aka reference) state trajectory, that is J=\\frac{1}{2}(\\bm x_N - \\bm x_N^\\text{ref})^\\top \\mathbf S_N (\\bm x_N - \\bm x_N^\\text{ref}) +\\frac{1}{2} \\sum_{k=i}^{N-1} \\left((\\bm x_k - \\bm x_k^\\text{ref})^\\top \\mathbf Q (\\bm x_k - \\bm x_k^\\text{ref}) + \\bm u_k^\\top \\mathbf R \\bm u_k\\right).\n\n\nNote that in none of these cost function did we include \\bm u_{N} as an optimization variables as it has no influence over the interval [i,N].\nNeedless to emphasize, as in some other applications maximizing may seem more appropriate (such as maximizing the yield, bandwidth or robustness), we can always reformulate the maximization into minimization. Therefore in our course we always formulate the optimal control problems as minimization problems.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "discr_indir_general.html#derivation-of-the-first-order-necessary-conditions-of-optimality",
    "href": "discr_indir_general.html#derivation-of-the-first-order-necessary-conditions-of-optimality",
    "title": "General nonlinear discrete-time optimal control as a two-point boundary value problem",
    "section": "Derivation of the first-order necessary conditions of optimality",
    "text": "Derivation of the first-order necessary conditions of optimality\nHaving formulated a finite-dimensional constrained nonlinear optimization problem, we avoid the temptation to call an NLP solver to solve it numerically, and proceed instead with our own analysis of the problem. Let’s see how far we can get. By introducing Lagrange multipliers {\\color{blue}\\bm\\lambda_k}, we turn the constrained problem into an unconstrained one. The new cost function (we use the prime to distinguish it from the original cost) is \n\\begin{aligned}\n& {J'}_i^N(\\bm x_i, \\ldots, \\bm x_N, \\bm u_i, \\ldots, \\bm u_{N-1},{\\color{blue}\\bm \\lambda_i, \\ldots, \\bm \\lambda_{N-1}}) \\\\\n&\\qquad\\qquad\\qquad = \\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1}\\left[L_{k}(\\bm x_k,\\bm u_k)+\\bm {\\color{blue}\\lambda^\\top_{k}}\\;\\left[\\mathbf f_k(\\bm x_k,\\bm u_k)-\\bm x_{k+1}\\right]\\right].\n\\end{aligned}\n\nFrom now on, in principle, we do not need any guidance here, do we? We are given an unconstrained optimization problem and its solution is just a few moments away. In particular, stationary point(s) must be found (and then we are going to argue if these qualify as minimizers or not). This calls for differentiating the above expression with respect to all the variables and setting these derivatives equal to zeros.\nAlthough the principles are clear, some hindsight might be shared here if compact formulas are to be found. First such advice is to rename the variable(s) {\\color{blue}\\boldsymbol \\lambda_k} to {\\color{red}\\boldsymbol \\lambda_{k+1}} \n\\begin{aligned}\n& {J'}_i^N(\\bm x_i, \\ldots, \\bm x_N, \\bm u_i, \\ldots, \\bm u_{N-1},{\\color{red}\\bm \\lambda_{i+1}, \\ldots, \\bm \\lambda_{N}}) \\\\\n& \\qquad\\qquad\\qquad = \\phi(N,\\bm x_N) + \\sum_{k=i}^{N-1}\\left[L_{k}(\\bm x_k,\\bm u_k)+\\boldsymbol {\\color{red}\\boldsymbol \\lambda^\\top_{k+1}}\\; \\left[\\mathbf f_k(\\bm x_k,\\bm u_k)-\\mathbf x_{k+1}\\right]\\right].\n\\end{aligned}\n\nThis is really just a notational decision but thanks to it our resulting formulas will enjoy some symmetry.\n\n\n\n\n\n\nNote\n\n\n\nMaybe it would be more didactic to leave you to go on without this advice on notation, and only then to nudge you to figure out this remedy by yourself. But admittedly this is not the kind of competence that we aim at in this course. Let’s spend time with more rewarding things.\n\n\nAnother notational advice – but this one is more systematic and fundamental — is to make the above expression a bit shorter by introducing a new variable defined as \\boxed{H_k(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1}) = L_{k}(\\bm x_k,\\bm u_k)+\\boldsymbol \\lambda_{k+1}^\\top \\; \\mathbf f_k(\\bm x_k,\\bm u_k).}\n\nWe will call this new function Hamiltonian. Indeed, the choice of this name is motivated by the analogy with the equally named concept used in physics and theoretical mechanics, but we will only make more references to this analogy later in the course once we transition to continuous-time systems modelled by differential equations.\nIntroducing the Hamiltonian reformulates the cost function (and we omit the explicit dependence on all its input arguments) as \n{J'}_i^N = \\phi(N,\\bm x_N) + \\sum_{k=i}^{N-1}\\left[H_{k}(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1})-\\boldsymbol\\lambda^\\top_{k+1}\\;\\mathbf x_{k+1}\\right].\n\nThe final polishing of the expression before starting to compute the derivatives consists in bringing together the terms that contain related variables: the state \\bm x_N at the final time, the state \\bm x_i at the initial time, and the states, controls and Lagrange multipliers in the transient period\n\n{J'}_i^N = \\underbrace{\\phi(N,\\bm x_N) -\\boldsymbol\\lambda^\\top_{N}\\;\\mathbf x_{N}}_\\text{at terminal time} + \\underbrace{H_i(\\bm x_i,\\mathbf u_i,\\boldsymbol\\lambda_{i+1})}_\\text{at initial time} + \\sum_{k=i+1}^{N-1}\\left[H_{k}(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1})-\\boldsymbol\\lambda^\\top_{k}\\;\\mathbf x_{k}\\right].\n\nAlthough this step was not necessary, it will make things a bit more convenient once we start looking for the derivatives. And the time for it has just come.\nRecall now the recommended procedure for finding derivatives of functions of vectors: find the differential instead, and identify the derivative within the result. The gradient is then (by convention) obtained as the transpose of the derivative. Following this derivative-identification procedure, we anticipate the differential of the augmented cost function in the following form \n\\begin{split}\n\\text{d}{J'}_i^N &= (\\qquad)^\\top \\; \\text{d}\\bm x_N + (\\qquad)^\\top \\; \\text{d}\\bm x_i \\\\&+ \\sum_{k=i+1}^{N-1}(\\qquad)^\\top \\; \\text{d}\\bm x_k + \\sum_{k=i}^{N-1}(\\qquad)^\\top \\; \\text{d}\\bm u_k + \\sum_{k=i+1}^{N}(\\qquad)^\\top \\; \\text{d}\\boldsymbol \\lambda_k.\n\\end{split}\n\nIdentifying the gradients amounts to filling in the empty brackets. It straightforward if tedious (in particular the lower and upper bounds on the summation indices must be carefuly checked). The solution is \n\\begin{split}\n\\text{d}{J'}_i^N &= \\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\top \\; \\text{d}\\bm x_N + \\left(\\nabla_{\\bm x_i}H_i\\right)^\\top \\; \\text{d}\\bm x_i \\\\&+ \\sum_{k=i+1}^{N-1}\\left(\\nabla_{\\bm x_k}H_k-\\boldsymbol\\lambda_k\\right)^\\top \\; \\text{d}\\bm x_k + \\sum_{k=i}^{N-1}\\left(\\nabla_{\\bm u_k}H_k\\right)^\\top \\; \\text{d}\\bm u_k + \\sum_{k=i+1}^{N}\\left(\\nabla_{\\boldsymbol \\lambda_k}H_{k-1}-\\bm x_k\\right)^\\top \\; \\text{d}\\boldsymbol \\lambda_k.\n\\end{split}\n\nThe ultimate goal of this derivation was to find stationary points for the augmented cost function, that is, to find conditions under which \\text{d}{J'}_i^N=0. In typical optimization problems, the optimization is conducted with respect to all the participating variables, which means that the corresponding differentials may be arbitrary and the only way to guarantee that the total differential of J_i' is zeros is to make the associated gradients (the contents of the brackets) equal to zero. There are two exceptions to this rule in our case, though:\n\nThe state at the initial time is typically fixed and not available for optimization. Then \\text{d}\\bm x_i=0 and the corresponding necessary condition is replaced by the statement that \\bm x_i is equal to some particular value, say, \\bm x_i = \\mathbf x^\\text{init}. We have already discussed this before. In fact, in these situations we might even prefer to reflect it by the notation J_i^N(\\ldots;\\bm x_i), which emphasizes that \\bm x_i is a parameter and not a variable. But in the solution below we do allow for the possibility that \\bm x_i is a variable too (hence \\text{d}\\bm x_i\\neq 0) for completeness.\nThe state at the final time may also be given/fixed, in which case the corresponding condition is replaced by the statement that \\bm x_N is equal to some particular value, say, \\bm x_N = \\mathbf x^\\text{ref}. But if it is not the case, then the final state is also subject to optimization and the corresponding necessary condition of optimality is obtained by setting the content of the corresponding brackets to zero.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "discr_indir_general.html#necessary-conditions-of-optimality-as-two-point-boundary-value-problem-tp-bvp",
    "href": "discr_indir_general.html#necessary-conditions-of-optimality-as-two-point-boundary-value-problem-tp-bvp",
    "title": "General nonlinear discrete-time optimal control as a two-point boundary value problem",
    "section": "Necessary conditions of optimality as two-point boundary value problem (TP-BVP)",
    "text": "Necessary conditions of optimality as two-point boundary value problem (TP-BVP)\nThe ultimate form of the first-order necessary conditions of optimality, which incorporates the special cases discussed above, is given by these equations \n\\boxed{\n\\begin{aligned}\n\\mathbf x_{k+1} &= \\nabla_{\\boldsymbol\\lambda_{k+1}}H_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1},\\\\\n\\boldsymbol\\lambda_k &= \\nabla_{\\bm x_k}H_k, \\;\\;\\; \\color{gray}{k=i+1,\\ldots, N-1}\\\\\n0 &=  \\nabla_{\\bm u_k}H_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1}\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\top \\mathrm{d}\\bm x_N},\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_i}H_i\\right)^\\top \\mathrm{d}\\bm x_i},\n\\end{aligned}\n}\n or more explicitly \n\\boxed{\n\\begin{aligned}\n\\mathbf x_{k+1} &= \\mathbf f_k(\\bm x_k,\\bm u_k), \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1},\\\\\n\\boldsymbol\\lambda_k &= \\nabla_{\\bm x_k}\\mathbf f_k\\;\\;   \\boldsymbol\\lambda_{\\mathbf k+1}+\\nabla_{\\bm x_k}L_k, \\;\\;\\; \\color{gray}{k=i+1,\\ldots, N-1}\\\\\n0 &=  \\nabla_{\\bm u_k}\\mathbf f_k\\;\\; \\boldsymbol\\lambda_{k+1}+\\nabla_{u_k}L_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1}\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\top \\mathrm{d}\\bm x_N},\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_i}H_i\\right)^\\top \\mathrm{d}\\bm x_i}.\n\\end{aligned}\n}\n\nRecall that since \\mathbf f is a vector function, \\nabla \\mathbf f is not just a gradient but rather a matrix whose columns are gradients of the individual components of the vector \\mathbf f — it is a transpose of Jacobian.\n\n\n\n\n\n\nNote\n\n\n\nThe first three necessary conditions above can be made completely “symmetric” by running the second one from k=i because the \\boldsymbol\\lambda_i introduced this way does not influence the rest of the problem and we could easily live with one useless variable.\n\n\nWe have just derived the (necessary) conditions of optimality in the form of five sets of (vector) equations:\n\nThe first two are recurrent (or discrete-time) equations, which means that they introduce coupling between the variables evaluated at consecutive times. In fact, the former is just the standard state equation that gives the state at one time as a function of the state (and the control) at the previous time. The latter gives a prescription for the variable \\bm \\lambda_k as a function of (among others) the same variable evaluated at the next (!) time, that is, \\bm \\lambda_{k+1}. Although from the optimization perspective these variables play the role of Lagrange multipliers, we call them co-state variables in optimal control theory because of the way they relate to the state equations. The corresponding vector equation is called a co-state equation.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is a crucial property of the co-state equation that the evolution of the co-state variable is dictated backward in time by the equation.\n\n\n\nThe third set of equations are just algebraic equations that relate the control inputs to the state and co-state variables. Sometimes it is called a stationarity equation.\nThe last two are just single (vector) equations related to the end and the beginning of the time horizon. They are both stated in the general enough form that allows the corresponding states to be treated as either fixed or subject to optimization. In particular, if the final state is to be treated as free (subject to optimization), that is, \\mathrm{d}\\bm x_N can be atritrary and the only way the corresponding equation can be satisfied is \\nabla_{\\bm x_N}\\phi=\\lambda_N. If, on the other hand, the final state is to be treated as fixed, the the corresponding equation is just replaced by \\bm x_N = \\mathbf x^\\text{ref}. Similarly for the initial state. But as we have hinted a few times, most often than not the initial state will be regarded as fixed and not subject to optimization, in which case the corresponding equation is replaced by \\bm x_i = \\mathbf x^\\text{init}.\n\nTo summarize, the equations that give the necessary conditions of optimality for a general nonlinear discrete-time optimal control problem form a discrete-time two-point boundary value problem (TP-BVP). Values of some variables are specified at the initial time, values of some (maybe the same or some other) variables are defined at the final time. The equations prescribe the evolution of some variables forward in time while for some other variables the evolution backward in time is dictated.\n\n\n\n\n\n\nNote\n\n\n\nThis is in contrast with the initial value problem (IVP) for state equations, for which we only specify the state at one end of the time horizon — the initial state — and then the state equation disctates the evolution of the (state) variable forward in time.\n\n\nBoundary value problems are more difficult to solve than the initial value problems. Typically we can only solve them numerically, in which case it is appropriate to ask if anything has been gained by this indirect procedure compared with the direct one. After all, we did not even incorporate the inequality constraints in the problem, which was a piece of case in the direct approach. But we will see that in some special cases the TP-BVP they can be solved analytically and the outcome is particularly useful and would never have been discovered, if only the direct approach had been followed. We elaborate on this in the next section.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "cont_indir_CARE.html",
    "href": "cont_indir_CARE.html",
    "title": "Continuous-time Riccati equation",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "ext_LQG.html",
    "href": "ext_LQG.html",
    "title": "LQG control",
    "section": "",
    "text": "So far we have assumed that all the state variables are measured, that is, the knowledge of the current state available to the feedback controller. Oftentimes this is not the case, and only a subset of (possibly even scaled or more generally transformed) state variables is measured. This encoded in the output equation that needs to be considered together with he state equation\n\\begin{aligned}\n\\dot{\\bm{x}} &= \\mathbf A\\bm x + \\mathbf B\\bm u,\\\\\n      \\bm y &= \\mathbf C\\bm x (+ \\mathbf D\\bm u).\n\\end{aligned}",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQG control"
    ]
  },
  {
    "objectID": "ext_LQG.html#static-proportional-output-feedback-control",
    "href": "ext_LQG.html#static-proportional-output-feedback-control",
    "title": "LQG control",
    "section": "Static (proportional) output feedback control",
    "text": "Static (proportional) output feedback control\nWe can now formulate the optimal control problem with the same cost function as in the LQR case, but we require that the optimal controller comes in the form of the optimal output feedback \n\\bm u(t) = - \\mathbf K\\bm y(t)\n instead of the state feedback one. It turns out, however, that this is a challenging problem, mainly because the underlying optimization is nonconvex. Some results are available in the literature (for example in [1, Ch. 8]), but here we will take another route.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQG control"
    ]
  },
  {
    "objectID": "ext_LQG.html#lqg-optimal-control",
    "href": "ext_LQG.html#lqg-optimal-control",
    "title": "LQG control",
    "section": "LQG optimal control",
    "text": "LQG optimal control\nInstead of restricting the output feedback controller to be proportional (just an array of gains), we can accept that it has its own dynamics. But this adds quite some complexity to the problem. Nonetheless, a smart way to deal with this is to decompose the dynamic output feedback control problem into two subproblems:\n\nthe first one is to estimate the state variables from the measured output,\nand the second one is to compute the optimal control law based on the estimated state variables.\n\nThe latter is just the state feedback control problem that we have just solved within the LQR framework. We have shown that when it comes to the numerical computation of the gains, there is no difference between the deterministic and stochastic versions of the problem.\nThe former is known as state estimation or state observation, and powerful methods exist. In the deterministic case we can use Luenberger’s observer, in the stochastic case we can use a Kalman filter. Combination of the two is known as Linear Quadratic Gaussian (LQG) control.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQG control"
    ]
  },
  {
    "objectID": "ext_LQG.html#optimal-estimation-of-states",
    "href": "ext_LQG.html#optimal-estimation-of-states",
    "title": "LQG control",
    "section": "Optimal estimation of states",
    "text": "Optimal estimation of states\nAlthough we have no ambitions to cover the state estimation methods in detail, because a dedicated course BE3M35OFD exists within our study program, we do provide some basics here so that the structure of the resulting output feedback controller is visible.\nWe continue our discussion (somewhat arbitrarily) in the continuous time domain. In the deterministic case we consider a linear model of the plant \n\\dot{\\bm x} = \\mathbf A\\bm x + \\mathbf B\\bm u,\\quad \\bm y = \\mathbf C\\bm x,\n in which we assume that the direct feedthrough term \\mathbf D is zero.\nWe set the observer (estimator) to be of the form\n\n\\dot{\\hat{\\bm{x}}} = \\mathbf A\\hat{\\bm{x}} + \\mathbf B\\bm u + \\underbrace{\\mathbf L(\\bm y-\\mathbf C\\hat{\\bm{x}})}_{\\text{correction}},\n which clearly contains a model of the plant, and some correction term that is proportional to the difference between the measured output and the estimated output. The matrix \\mathbf L is known as the observer gain(s).\nUpon restructuring the right hand side, we can write the observer in the form \\boxed{\n\\dot{\\hat{\\mathbf{x}}}= \\underbrace{\\left(\\mathbf A-\\mathbf L\\mathbf C\\right)}_{\\mathbf{A}_o}\\hat{\\mathbf{x}} + \\mathbf B\\mathbf u + \\mathbf L\\mathbf y},\n which emphasizes that the observer has two inputs – the control \\bm u and the measured output \\bm y, and also that by the choice of the observer gain \\mathbf L we can influence the dynamics of the observer. The design of the observer gain \\mathbf L can be done using pole placement techniques used in the linear state feedback design: transposing (\\mathbf A-\\mathbf L\\mathbf C) gives (\\mathbf A^\\top-\\mathbf C^\\top \\mathbf L^\\top), which has the same structure (\\mathbf A-\\mathbf B\\mathbf K) encountered in the pole placement problem.\nHere we aim for a method based on optimization. We can formulate such optimal estimation problem within the stochastic setting. The plant is \n\\begin{aligned}\n\\dot{\\bm{x}}(t) &= \\mathbf A\\bm x(t) + \\mathbf B \\bm u(t) + \\mathbf B_w {\\color{red}\\bm w(t)}\\\\\n\\bm y(t) &= \\mathbf C \\bm x(t) + {\\color{red}\\bm v(t)}\n\\end{aligned}\n where \\bm w(t) and \\bm v(t) are white noises with spectral densities \\mathbf S_w and \\mathbf S_v, respectively.\nOptimal estimator design is dual to the state feedback design. If an infinite time horizon is considered, the ARE needs to be solved for \\mathbf P_e(\\infty) \n\\mathbf 0 = \\mathbf A\\mathbf P_e(\\infty) + \\mathbf P_e(\\infty)\\mathbf A^\\top + \\mathbf B_w\\mathbf S_w\\mathbf B_w^\\top - \\mathbf P_e(\\infty)\\mathbf C^\\top \\mathbf S_v^{-1}\\mathbf C\\mathbf P_e(\\infty),\n where \n\\mathbf P_e(t) = \\mathbf{E}\\left\\{[\\mathbf x(t)-\\hat{\\mathbf{x}}(t)][\\mathbf{x}(t)-\\hat{\\mathbf{x}}(t)]^\\top \\right\\}.\n\nThe cost function to be minimized is the “size” of \\mathbf P_e(\\infty): \n\\operatorname{Tr}\\mathbf P_e(\\infty) = \\mathbf{E}\\left\\{[\\mathbf x(\\infty)-\\hat{\\mathbf{x}}(\\infty)]^\\top[\\mathbf{x}(\\infty)-\\hat{\\mathbf{x}}(\\infty)] \\right\\},\n that is, the mean square error (MSE) of the state estimate.\nThe optimal observer gain is given by \n\\mathbf L = \\mathbf P_e(\\infty)\\mathbf C^\\top \\mathbf S_v^{-1}.\n\n\nExample 1 (Kalman filter) Estimate the range and radial velocity of an aircraft from noisy radar measurements. The model is \n\\begin{bmatrix}\n\\dot r(t) \\\\ \\ddot r(t)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 & 1\\\\ 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nr(t) \\\\ \\dot r(t)\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n0 \\\\ 1\n\\end{bmatrix}\nw(t),\n where r(t) is the actual range of the aircraft, and w(t) is a random process that models the unknown phenomena affecting the acceleration of the aircraft. The range measurements are given \ny(t) = \\begin{bmatrix}\n1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nr(t) \\\\ \\dot r(t)\n\\end{bmatrix}\n+ v(t),\n where v(t) is a measurement noise.\nThe initial state is \n\\begin{bmatrix}\nr(0) \\\\ \\dot r(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n10 000\\, \\text{m}\\\\ -150\\, \\text{m/s}\n\\end{bmatrix}\n\nThe two random processes are assumed to be uncorreleated white noises with the following spectral densities \nS_w = 4\\,\\frac{\\text{m}^2}{\\text{s}^4 \\text{Hz}},\\quad S_v = 10^4\\,\\frac{\\text{m}^2}{\\text{Hz}}.\n\nThe covariance of the initial estimation error is \n\\mathbf P_e(0) = \\begin{bmatrix}10^6 \\text{m}^2 & 0\\\\ 0 & 4\\times 10^5 \\text{m}^2/\\text{s}^2\\end{bmatrix}\n\n#TODO",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQG control"
    ]
  },
  {
    "objectID": "ext_LQG.html#lqg-optimal-control-combined-kalman-filtering-and-lqr-state-feedback",
    "href": "ext_LQG.html#lqg-optimal-control-combined-kalman-filtering-and-lqr-state-feedback",
    "title": "LQG control",
    "section": "LQG optimal control – combined Kalman filtering and LQR state feedback",
    "text": "LQG optimal control – combined Kalman filtering and LQR state feedback\nNow that we also know how to estimate the state, we are all set. The combination of the state estimator (Kalman filter) and the proportional state feedback controller is shown in Fig. 1.\n\n\n\n\n\n\nFigure 1: LQG controller in an output feedback loop\n\n\n\nThe computation of the two components is done separately, each consisting of solving their own (continuous-time) ARE, that is, two AREs altogether.\nNote that while we use low-level solvers in the code below, dedicated high-level functions are available not only for computing the LQR regulator and Kalman filter, but also the resulting LQG controller, see the section on software.\n\nExample 2 (Satellite tracking antenna with noisy measurements of angle)  \n\n\nShow the code\nusing ControlSystems: ss, lqr\nusing DifferentialEquations\nusing LinearAlgebra # For identity matrix I\nusing MatrixEquations: arec\nusing Plots\nusing SparseArrays\nusing Random\nRandom.seed!(1234) # For reproducibility\n\n# Model of the plant\nA = [0.0 1.0; 0.0 -0.1] \nB = [0.0, 0.001] \nBw = [0.0, 0.001] \nC = [1.0 0.0] \nG = ss(A,[B Bw],C,0)\n\n# Model of the stochastic disturbance and measurement noise\nSw = 5000.0   # Spectral density (N^2 m^2 / Hz)\nSv = 1.0      # Noise spectral density (deg^2/Hz)\n\n# Design of the LQR controller\nq1 = 180.0 \nQ1 = [q1 0.0; 0.0 0.0]\nR1 = 1.0\nK = lqr(G[:,1],Q1,R1)     # LQR controller gain\n\n# Design of the Kalman filter\n#L = kalman(G[:,2],Sw,Sv) # Beware that they assume the plant in the form dx/dt = Ax + Bu + w, that is, Bw=I.\nP, cleigvals = arec(A', C'/Sv*C, Bw*Sw*Bw')\nL = P*C'/Sv               # Kalman filter gain\n\n# Initial state for the closed-loop system, that is, both the plant state and the observer error state\nx₀ = [2.0, -1.0]\nx̂₀ = [0.0, 0.0]\ne₀ = x₀ - x̂₀\nxe₀ = vcat(x₀, e₀)\n\n# Simulation of the response of the closed-loop system to random disturbance and noise. \n# The problem must be formulated as a stochastic differential equation (SDE) problem.\n# See https://docs.sciml.ai/DiffEqDocs/stable/types/sde_types/.\n\nfunction f!(dxe, xe, p, t) \n   dxe .= [A-B*K B*K; zeros(Float64,2,2) A-L*C]*xe  \nend\n\nfunction g!(dxe, xe, p, t)\n   dxe .= [Bw*sqrt(Sw) zeros(Float64,2,1); Bw*sqrt(Sw) -L*sqrt(Sv)]\nend\n\nN = zeros(4, 2)     # This encodes the random process structure in the SDE, \nN[2, 1] = 1         # see https://docs.sciml.ai/DiffEqDocs/stable/tutorials/sde_example/#Example-4:-Systems-of-SDEs-with-Non-Diagonal-Noise\nN[4, 1] = 1\nN[3, 2] = 1\nN[4, 2] = 1\nN = sparse(N)       \n\n# Simulation parameters\ndt = 1 // 2^(4)\ntspan = (0.0, 100.0)\n\n# Creating and solving the SDE problem\nprob = SDEProblem(f!, g!, xe₀, tspan, noise_rate_prototype = N)\nsol = solve(prob, SRA1(), dt = dt)      # Not all SDE solvers can be used here, non-diagonal solvers needed.\n\n# Computging the control input\nu = [-K K]*sol[1:4,:]\n\n# Plotting the results\np1 = plot(sol.t,sol[1:2,:]',lw=2, lab=[\"Angle (deg)\" \"Angular rate (deg/s)\"], xlabel=\"Time [s]\", ylabel=\"State\", title=\"\")\np2 = plot(sol.t, u', lw=2, xlabel=\"Time [s]\", ylabel=\"Control\", title=\"\",label=\"Torque (Nm)\")\nplot(p1, p2, layout=(2,1), size=(800,600))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Simulation of a response a LQG-controlled satellite tracking antenna a nonzero initial state, and a random disturbing torque modelled by a white noise process. The measurement of the angle is corrupted by a measurement noise modelled by a white noise process as well.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQG control"
    ]
  },
  {
    "objectID": "ext_LQG.html#stability-margins-of-lqg",
    "href": "ext_LQG.html#stability-margins-of-lqg",
    "title": "LQG control",
    "section": "Stability margins of LQG",
    "text": "Stability margins of LQG\nWe have seen previously that the LQR enjoys remarkable guarantees on the gain and phase margins of the closed-loop system. Sadly, the LQG does not exhibit such property. A concise summary of the situation was given by John Doyle in 1978 in his famous paper [2]:\n\n\n\n\n\n\nFigure 3: Screenshot of “the shortest abstract in any IEEE journal ever”\n\n\n\n\nExample 3 (GM and PM of LQG can be arbitrarily small) This is the example from [2]. \n\\begin{aligned}\n    \\dot{\\bm{x}}(t) &= \\begin{bmatrix}1 & 1\\\\0 & 1\\end{bmatrix}\\bm x(t)+\\begin{bmatrix}0\\\\1\\end{bmatrix}u(t)\n                        +\\begin{bmatrix}1\\\\1\\end{bmatrix}w(t),\\\\\n                   y(t) &= \\begin{bmatrix}1 & 0\\end{bmatrix}\\bm x(t)+v(t)\\\\\n               \\mathbf Q&=\\begin{bmatrix}10&10\\\\10&10\\end{bmatrix},\\quad R=1,\\\\\n                     S_w&=10,\\quad S_v=1.\n\\end{aligned}\n\n\n\nShow the code\nusing ControlSystems: ss, lqr, kalman, marginplot, nyquistplot\nusing DifferentialEquations\nusing LinearAlgebra # For identity matrix I\nusing MatrixEquations: arec\nusing Plots\n\nA = [1.0 1.0; 0.0 1.0] \nB = [0.0, 1.0] \nBw = [1.0, 1.0] \nBv = [0.0, 0.0] \nC = [1.0 0.0] \nD = 0.0 \nDw = 0.0 \nDv = 1.0\n\nG = ss(A,[B Bw Bv],C,[D Dw Dv])\n\nq1 = 10.0 \nQ1 = q1*[1 1; 1 1]\nR1 = 1.0\nK = lqr(G[:,1],Q1,R1)\n\nQ2 = 10\nR2 = 1\n#L = kalman(G[:,1:2],Q2,R2)\nP, cleigvals = arec(A', C'/Sv*C, Bw*Sw*Bw')\nL = P*C'/Sv               # Kalman filter gain\n\n\nR_lqg = ss(A-L*C-B*K,L,K,0)     # LQG controller\nL_lqg = R_lqg*G[:,1]            # Open-loop transfer function with the LQG controller\n\nmarginplot(L_lqg,lw=2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Bode plot of the closed-loop system with LQG controller that demonstrates mediocre gain and phase margins\n\n\n\n\nAnd the Nyquist plot of the closed-loop system is\n\n\nShow the code\nnyquistplot(L_lqg,title=\"\",xlabel=\"Real\",ylabel=\"Imaginary\",label=\"\",lw=2,aspect_ratio=1)\n\n\n\n┌ Warning: Keyword argument hover not supported with Plots.GRBackend().  Choose from: annotationcolor, annotationfontfamily, annotationfontsize, annotationhalign, annotationrotation, annotations, annotationvalign, arrow, aspect_ratio, axis, background_color, background_color_inside, background_color_outside, background_color_subplot, bar_width, bins, bottom_margin, camera, clims, color_palette, colorbar, colorbar_entry, colorbar_scale, colorbar_title, colorbar_titlefont, colorbar_titlefontcolor, colorbar_titlefontrotation, colorbar_titlefontsize, connections, contour_labels, discrete_values, fill, fill_z, fillalpha, fillcolor, fillrange, fillstyle, flip, fontfamily, fontfamily_subplot, foreground_color, foreground_color_axis, foreground_color_border, foreground_color_grid, foreground_color_subplot, foreground_color_text, formatter, framestyle, grid, gridalpha, gridlinewidth, gridstyle, group, guide, guidefont, guidefontcolor, guidefontfamily, guidefonthalign, guidefontrotation, guidefontsize, guidefontvalign, html_output_format, inset_subplots, label, layout, left_margin, legend_background_color, legend_column, legend_font, legend_font_color, legend_font_family, legend_font_halign, legend_font_pointsize, legend_font_rotation, legend_font_valign, legend_foreground_color, legend_position, legend_title, legend_title_font_color, legend_title_font_family, legend_title_font_pointsize, legend_title_font_rotation, legend_title_font_valigm, levels, lims, line, line_z, linealpha, linecolor, linestyle, linewidth, link, margin, marker_z, markeralpha, markercolor, markershape, markersize, markerstrokealpha, markerstrokecolor, markerstrokewidth, minorgrid, minorgridalpha, minorgridlinewidth, minorgridstyle, minorticks, mirror, normalize, orientation, overwrite_figure, permute, plot_title, plot_titlefontcolor, plot_titlefontfamily, plot_titlefontrotation, plot_titlefontsize, plot_titlelocation, plot_titlevspan, polar, primary, projection, quiver, ribbon, right_margin, rotation, scale, series_annotations, seriesalpha, seriescolor, seriestype, show, show_empty_bins, showaxis, size, smooth, subplot, subplot_index, thickness_scaling, tick_direction, tickfontcolor, tickfontfamily, tickfonthalign, tickfontrotation, tickfontsize, tickfontvalign, ticks, title, titlefontcolor, titlefontfamily, titlefonthalign, titlefontrotation, titlefontsize, titlefontvalign, top_margin, unitformat, weights, widen, window_title, x, xdiscrete_values, xerror, xflip, xforeground_color_axis, xforeground_color_border, xforeground_color_grid, xforeground_color_text, xformatter, xgrid, xgridalpha, xgridlinewidth, xgridstyle, xguide, xguidefontcolor, xguidefontfamily, xguidefonthalign, xguidefontrotation, xguidefontsize, xguidefontvalign, xlims, xlink, xminorgrid, xminorgridalpha, xminorgridlinewidth, xminorgridstyle, xminorticks, xmirror, xrotation, xscale, xshowaxis, xtick_direction, xtickfontcolor, xtickfontfamily, xtickfonthalign, xtickfontrotation, xtickfontsize, xtickfontvalign, xticks, xunitformat, xwiden, y, ydiscrete_values, yerror, yflip, yforeground_color_axis, yforeground_color_border, yforeground_color_grid, yforeground_color_text, yformatter, ygrid, ygridalpha, ygridlinewidth, ygridstyle, yguide, yguidefontcolor, yguidefontfamily, yguidefonthalign, yguidefontrotation, yguidefontsize, yguidefontvalign, ylims, ylink, yminorgrid, yminorgridalpha, yminorgridlinewidth, yminorgridstyle, yminorticks, ymirror, yrotation, yscale, yshowaxis, ytick_direction, ytickfontcolor, ytickfontfamily, ytickfonthalign, ytickfontrotation, ytickfontsize, ytickfontvalign, yticks, yunitformat, ywiden, z, z_order, zdiscrete_values, zerror, zflip, zforeground_color_axis, zforeground_color_border, zforeground_color_grid, zforeground_color_text, zformatter, zgrid, zgridalpha, zgridlinewidth, zgridstyle, zguide, zguidefontcolor, zguidefontfamily, zguidefonthalign, zguidefontrotation, zguidefontsize, zguidefontvalign, zlims, zlink, zminorgrid, zminorgridalpha, zminorgridlinewidth, zminorgridstyle, zminorticks, zmirror, zrotation, zscale, zshowaxis, ztick_direction, ztickfontcolor, ztickfontfamily, ztickfonthalign, ztickfontrotation, ztickfontsize, ztickfontvalign, zticks, zunitformat, zwiden\n└ @ Plots ~/.julia/packages/Plots/MR7sb/src/args.jl:1557\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Nyquist plot of the closed-loop system with LQG controller that demonstrates mediocre gain and phase margins\n\n\n\n\n\nWe have just seen by means of an example that the gain and phase margins of the closed-loop system with LQG controller can be arbitrarily small. After learning the nearly magical property of the LQR, it may be disappointing to see that LQG does not share it. This lead to developments of new methods that would guarantee the stability margins, or robustness in a broader sense. We start with a minor modification of the LQG controller, which is known as a Loop Transfer Recovery (LTR) controller.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQG control"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "B(E)3M35ORR – Optimal and Robust Control",
    "section": "",
    "text": "This website constitutes the (work-in-progress) lecture notes for the graduate course Optimal and Robust Control (B3M35ORR, BE3M35ORR) taught within Cybernetics and Robotics graduate program at Faculty of Electrical Engineering, Czech Technical University in Prague, Czechia.\nEach chapter corresponds to a single weekly block (covered by a lecture, a seminar/exercise, and some homework), hence 14 chapters in total.\nAlthough most of the material needed for the course is here, occassionally we may refer the officially enrolled students to the course page within the FEL Moodle system for some other material.\n\n\n\n Back to top"
  },
  {
    "objectID": "rocond_H_infinity_control.html",
    "href": "rocond_H_infinity_control.html",
    "title": "Hinfinity-optimal control",
    "section": "",
    "text": "Here we formulate the general problem of \\mathcal{H}_\\infty-optimal control. There are two motivations for this. First, it gives the general framework within which we can formulate and solve the mixed-sensitivity problem defined in the frequency domain that we discused previously. Second, it allows us to consider exploit the time-domain (or signal) interpretation of the \\mathcal{H}_\\infty norm of a system to formulate a new class of problems that can be solved with these optimization tools. For the latter, recall that \n\\|\\mathbf G\\|_{\\infty} = \\sup_{u\\in\\mathcal{L}_{2}\\backslash \\emptyset}\\frac{\\|\\bm y\\|_2}{\\|\\bm u\\|_2},\n in which we allow for vector input and output signals, hence MIMO systems, from the very beginning.\nNow, for particular control requirements, we build the generalized plant \\mathbf P such that after forming the feedback interconnection with the controller \\mathbf K as in Fig. 1\n\n\n\n\n\n\nFigure 1: Lower LFT of the generalized plant and the controller\n\n\n\nit makes sense require the stabilizing controller to minimize the amplification of the exogenous inputs (disturbances, references, noises) into the regulated outputs. We want to make the regulated outputs as insensitive as possible to the exogenous inputs and to quantify the sizes of the inputs and outputs, we use the \\mathcal L_2 norm.\nBut then what we have is really the standard \\mathcal{H}_\\infty optimization problem \\boxed\n{\\operatorname*{minimize}_{\\mathbf K \\text{ stabilizing}}\\|\\mathcal{F}_{\\mathrm l}(\\mathbf P,\\mathbf K)\\|_{\\infty}.}\n\nNumerical solvers exist in various software environments.\n\nMixed-sensitivity problem reformulated as the standard \\mathcal{H}_\\infty optimization problem\nWe now show how the mixed-sensitivity problem discussed previously can be reformulated within as the standard \\mathcal{H}_\\infty optimization problem. We consider the full mixed-sensitivity problem for a SISO plant\n\n\\operatorname*{minimize}_{K \\text{ stabilizing}}  \n\\left\\|\n\\begin{bmatrix}\nW_1S\\\\W_2KS\\\\W_3T\n\\end{bmatrix}\n\\right\\|_{\\infty},\n which obviously considers a closed-loop system with one input and three outputs. With only one exogenous input, we must choose its role. Say, the only exogenous input is the reference signal. The closed-loop system for which the norm is minimized is in the following block diagram Fig. 2.\n\n\n\n\n\n\nFigure 2: Mixed-sensitivity problem interpreted as the standard \\mathcal{H}_\\infty optimization problem\n\n\n\nThe matrix transfer function for generalized plant \\mathbf P has two inputs and four outputs and it can then be written as \n\\mathbf P = \\left[\\begin{array}{c|c}\nW_1 & -W_1G\\\\\n0 & W_2\\\\\n0 & W_3G\\\\\n\\hline\n1 & -G\n\\end{array}\\right].\n\nA state space realization of this plant \\mathbf P is then used as the input argument to the solver for the \\mathcal{H}_\\infty optimization problem. In fact, we must also tell the solver how the inputs and outputs are structured. In this case, the solver must know that of the two inputs, only the second one can be used by the controller, and of the four outputs, only the fourth one is measured.\n\n\nSignal-based \\mathcal{H}_\\infty-optimal control problem\nBeing able to solve the \\mathcal{H}_\\infty optimization problem, indeed we do not have to restrict ourselves to the generalized plants \\mathbf P that correspond to the mixed-sensitivity problem. We can consider any plant \\mathbf P, for which the problem makes sense. For example, if we want to consider not only references but also disturbances, and possibly even noises, there is no way to formulate this within the mixed-sensitivity framework. But we can still formulate this as the standard \\mathcal{H}_\\infty optimal control problem.\n\n\nWhat is behind the \\mathcal{H}_\\infty solver?\n\n\nStructure of the \\mathcal{H}_\\infty-optimal controller\n\n\n\n\n Back to top",
    "crumbs": [
      "12. Robust control",
      "Hinfinity-optimal control"
    ]
  },
  {
    "objectID": "opt_theory_goals.html",
    "href": "opt_theory_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Give a rigorous definition of a local minimum (and maximum) and explain how it differs from a global minimum (maximum).\nDefine a convex function, convex set and convex optimization problem and explain the impact the convexity has on the optimization.\nState the Weierstrass (extreme value) theorem on the existence of a minimum (maximum).\nExplain the concepts of big O() and little o() within the framework of truncated Taylor series.\nGive first-order necessary conditions of optimality for a scalar function of a scalar argument, and define critical (or stationary) point. Extend these to the vector argument case. Formulate them both using a Fréchet and Gateaux derivatives. Specialize the result to quadratic functions.\nGive second-order sufficient conditions of optimality for a scalar function of a scalar argument. How can we distinguish between a minimum, maximum, and an inflection point? Extend these to the vector case. Define Hessian and show how it can be used to classify the critical points into minimum, maximum, saddle point, and singularity point. Specialize the results to quadratic functions.\nGive first-order necessary condition of optimality for an equality-constrained optimization problem using Lagrange multipliers. Specialize the results to quadratic cost functions and linear constraints.\nCharacterize the regular point (for a given set of equality constraints). Give an example of equality constraints lacking regularity.\nGive second-order sufficient conditions of optimality for an equality-constrained optimization problem using the concept of a projected Hessian.\nState and explain the Karush-Kuhn-Tucker (KKT) conditions for inequality-constrained optimization problems.",
    "crumbs": [
      "1. Optimization – theory",
      "Learning goals"
    ]
  },
  {
    "objectID": "opt_theory_goals.html#knowledge-remember-and-understand",
    "href": "opt_theory_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Give a rigorous definition of a local minimum (and maximum) and explain how it differs from a global minimum (maximum).\nDefine a convex function, convex set and convex optimization problem and explain the impact the convexity has on the optimization.\nState the Weierstrass (extreme value) theorem on the existence of a minimum (maximum).\nExplain the concepts of big O() and little o() within the framework of truncated Taylor series.\nGive first-order necessary conditions of optimality for a scalar function of a scalar argument, and define critical (or stationary) point. Extend these to the vector argument case. Formulate them both using a Fréchet and Gateaux derivatives. Specialize the result to quadratic functions.\nGive second-order sufficient conditions of optimality for a scalar function of a scalar argument. How can we distinguish between a minimum, maximum, and an inflection point? Extend these to the vector case. Define Hessian and show how it can be used to classify the critical points into minimum, maximum, saddle point, and singularity point. Specialize the results to quadratic functions.\nGive first-order necessary condition of optimality for an equality-constrained optimization problem using Lagrange multipliers. Specialize the results to quadratic cost functions and linear constraints.\nCharacterize the regular point (for a given set of equality constraints). Give an example of equality constraints lacking regularity.\nGive second-order sufficient conditions of optimality for an equality-constrained optimization problem using the concept of a projected Hessian.\nState and explain the Karush-Kuhn-Tucker (KKT) conditions for inequality-constrained optimization problems.",
    "crumbs": [
      "1. Optimization – theory",
      "Learning goals"
    ]
  },
  {
    "objectID": "opt_theory_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "opt_theory_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nFormulate a provided problem as an instance of mathematical optimization: identify the cost function, the constraints, decide if the problems fits into one of the (numerous) families of optimization problems such as linear program, quadratic program (with linear constraints, with quadratic constraints), (general) nonlinear program, …\nSolve a provided linear and/or quadratic programming problem using a solver of your choice.",
    "crumbs": [
      "1. Optimization – theory",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_indir_via_calculus_of_variations.html",
    "href": "cont_indir_via_calculus_of_variations.html",
    "title": "Indirect approach to optimal control of a nonlinear system on a finite time horizon",
    "section": "",
    "text": "Now we finally seem to be ready for solving our optimal control problems stated at the beginning of the lecture/chapter. Equipped with the solution to the fixed-ends basic problem of calculus of variation, we start with the finite-horizon fixed-final-state version of the optimal control problem. We will extend the result for a free final state in due course.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "cont_indir_via_calculus_of_variations.html#continuous-time-optimal-control-problem",
    "href": "cont_indir_via_calculus_of_variations.html#continuous-time-optimal-control-problem",
    "title": "Indirect approach to optimal control of a nonlinear system on a finite time horizon",
    "section": "Continuous-time optimal control problem",
    "text": "Continuous-time optimal control problem\nThe problem to be solved is\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x(\\cdot),\\bm u(\\cdot)} &\\quad \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}L(\\bm x(t),\\bm u(t),t)\\text{d}t\\\\\n\\text{subject to} &\\quad \\dot{\\bm x}(t)= \\mathbf f(\\bm x(t),\\bm u(t),t),\\\\\n&\\quad \\bm x(t_\\mathrm{i}) = \\mathbf x_\\mathrm{i},\\\\\n&\\quad \\bm x(t_\\mathrm{f}) = \\mathbf x_\\mathrm{f}.\n\\end{aligned}\n\nThere is, of course, no term in the cost function penalizing the state at the final time since it is requested that the system is brought to some prespecified state.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "cont_indir_via_calculus_of_variations.html#first-order-necessary-conditions-of-optimality",
    "href": "cont_indir_via_calculus_of_variations.html#first-order-necessary-conditions-of-optimality",
    "title": "Indirect approach to optimal control of a nonlinear system on a finite time horizon",
    "section": "First-order necessary conditions of optimality",
    "text": "First-order necessary conditions of optimality\nThe augmented cost function and the augmented Lagrangian are \nJ^\\mathrm{aug}(t,\\bm x(\\cdot),\\dot{\\bm x}(\\cdot),\\bm u(\\cdot),\\bm\\lambda(\\cdot)) = \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}\\left[\\underbrace{ L(\\bm x,\\bm u,t)+\\bm\\lambda^\\top\\left( \\dot {\\bm x}-\\mathbf f(\\bm x,\\bm u,\\mathbf t)\\right)}_{L^\\text{aug}}\\right ]\\text{d}t.\n\\tag{1}\nNote also that compared to the original unconstrained calculus of variations setting, here we made a notational shift from x to t as the independent variable, and from y to the triplet (\\bm x,\\bm u,\\bm \\lambda) as the triplet of dependent variables (possibly vector ones). Finally, note that the only derivative appearing in the augmented Lagrangian is \\dot{\\bm x}.\n\nBoundary value problem (BVP) as the necessary conditions of optimality\nApplying the Euler-Lagrange equation to this augmented Lagrangian, we obtain three equations (or three systems of equations), one for each dependent variable:\n\n\\begin{aligned}\nL_{\\bm{x}}^\\text{aug} &= \\frac{\\text{d}}{\\text{d}t} L^\\text{aug}_{\\dot{\\bm{x}}},\\\\\nL_{\\bm{u}}^\\text{aug} &= 0,\\\\\nL_{\\bm \\lambda}^\\text{aug} &= 0.\n\\end{aligned}\n\\tag{2}\nThese can be expanded in terms of the unconstrained Lagrangian. First, we assume scalar functions for notational simplicity: \n\\begin{aligned}\n\\frac{\\partial L}{\\partial x} - \\lambda \\frac{\\partial f}{\\partial x}&= \\dot{\\lambda},\\\\\n\\frac{\\partial L}{\\partial u} - \\lambda \\frac{\\partial f}{\\partial u} &= 0,\\\\\n\\dot {x} - f(x,u,t) &= 0.\n\\end{aligned}\n\nIn the vector case (wherein \\bm x, hence \\mathbf f(), and/or \\bm u are vectors): \n\\begin{aligned}\n\\nabla_{\\bm{x}} L - \\sum_{i=1}^n \\lambda_i \\nabla_{\\bm{x}} f_i(\\bm x, \\bm u) &= \\dot{\\bm{\\lambda}},\\\\\n\\nabla_{\\bm{u}} L - \\sum_{i=1}^n \\lambda_i \\nabla_{\\bm{u}} f_i(\\bm x, \\bm u) &= \\mathbf 0,\\\\\n\\dot{\\bm{x}} - \\mathbf{f}(\\bm x,\\bm u,t) &= \\mathbf 0.\n\\end{aligned}\n\nWe can also write the same result in the compact vector form. Recall that we agreed in this course to regard gradients as column vectors and that \\nabla \\mathbf f for a vector function \\mathbf{f} is a matrix whose columns are gradients \\nabla_\\mathbf{x} f_i of the individual elements of the vector function. We can then write the first order conditions compactly as \n\\begin{aligned}\n\\nabla_\\mathbf{x}L - \\nabla_\\mathbf{x} \\mathbf{f} \\; \\bm\\lambda &= \\dot{\\bm\\lambda},\\\\\n\\nabla_\\mathbf{u}L - \\nabla_\\mathbf{u} \\mathbf{f} \\; \\bm\\lambda &= \\mathbf 0,\\\\\n\\dot {\\mathbf{x}} - \\mathbf{f}(\\bm x,\\bm u,t) &= \\mathbf 0.\n\\end{aligned}\n\nAfter reordering the equations and shuffling the terms within the equations, we get \\boxed{\n\\begin{aligned}\n\\dot {\\mathbf{x}}(t) &= \\mathbf{f}(\\bm x(t),\\bm u(t),t),\\\\\n\\dot{\\bm\\lambda}(t) &= \\nabla_\\mathbf{x}L(\\bm x(t),\\bm u(t),t) - \\nabla_\\mathbf{x} \\mathbf{f}(\\bm x(t),\\bm u(t),t) \\, \\bm\\lambda(t),\\\\\n\\mathbf 0 &= \\nabla_\\mathbf{u}L(\\bm x(t),\\bm u(t),t) - \\nabla_\\mathbf{u} \\mathbf{f}(\\bm x(t),\\bm u(t),t) \\, \\bm\\lambda(t).\n\\end{aligned}}\n\\tag{3}\nThese three (systems of) equations give the necessary conditions of optimality that we were looking for. We can immediately recognize the first one – the original state equation describing how the state vector variable \\bm{x} evolves in time.\nThe other two equations are new, though. The second one is called costate equation because the variable \\bm\\lambda, originally introduced as a Lagrange multiplier, now evolves also according to a first-order differential equation; we call it a costate variable.\nThe last equation is called equation of stationarity. Unlike the previous two, it is not a differential equation, it is just a nonlinear equation. With the exception of some singular cases that we are going to mentions soon, it can be be solved for the control vector \\bm u as a function of the state \\bm x and the costate \\bm \\lambda, in which case \\bm u can be eliminated from the two differential equations and we end up with differential equations just in \\bm{x} and \\bm\\lambda.\n\n\nBoundary conditions\nRecall that for differential equations we always need a sufficient number of boundary conditions to determine the solution uniquely. In particular, for a state vector of dimension n, the costate is also of dimension n, hence we need in total 2n boundary conditions. In our current setup these are given by the n specified values of the state vector at the beginning and n values at the end: \\boxed{\n\\begin{aligned}\n&\\quad \\bm x(t_\\mathrm{i}) = \\mathbf x_\\mathrm{i},\\\\\n&\\quad \\bm x(t_\\mathrm{f}) = \\mathbf x_\\mathrm{f}.\n\\end{aligned}}\n\\tag{4}\nOnly after these equations are added to the above DAE systems, we have a full set of necessary conditions of optimality.\nThis class of problems is called two-point boundary value problem (BVP) and generally it can only be solved numerically (dedicated solvers exist, see the section on software).\n\n\nFirst-order necessary conditions of optimality using the Hamiltonian function\nWe now introduce an auxilliary function called Hamiltonian, which not only makes the conditions of optimality even more compact, but also – and even more importantly – it strengthens the link with the physics-motivated calculus of variations. Recall that the Hamiltonian function in the calculus of variations is defined as H(x,y,y') = py'-L. In the optimal control setting, since we consider constraints in the form of the state equation, we need to use the augmented Lagrangian, which we defined as\n\nL^\\mathrm{aug}(\\bm x,\\bm u,\\bm \\lambda,t) = L(\\bm x,\\bm u,t)+\\bm \\lambda^\\top\\left( \\dot {\\bm x}-\\mathbf f(\\bm x,\\bm u,\\mathbf t)\\right).\n\nWe now define the optimal control related Hamiltonian in the same way – product of the auxillary variable and the state variable minus the (augmented) Lagrangian: \n\\begin{aligned}\nH(\\bm x,\\bm u,\\dot{\\bm{x}},\\bm\\lambda,t) &= \\bm\\lambda^\\top \\, \\dot{\\bm x} - L^\\text{aug},\\nonumber\\\\\n&= \\bm\\lambda^\\top \\,\\dot{\\mathbf{x}}-L(\\bm x,\\bm u,t) - \\bm\\lambda^\\top \\, \\dot{\\bm x}+\\bm\\lambda^\\top \\, \\mathbf f(\\bm x,\\bm u, t),\\nonumber\\\\\n&=\\bm\\lambda^\\top \\, \\mathbf f(\\bm x,\\bm u, t)-L(\\bm x,\\bm u,t).\n\\end{aligned}\n\nRealizing that through the state equation \\dot{\\bm x}=\\mathbf f(\\bm x,\\bm u, t) the derivative \\dot{\\bm x} at a given state x (and time t) is uniquelly determined by u, we can consider the control-related Hamiltonian as a function of t, \\mathbf x, \\mathbf u and \\bm\\lambda (and discard the \\dot{\\bm x} argument): \\boxed{\nH(\\bm x,\\bm u,\\bm\\lambda,t) = \\bm\\lambda^\\top \\mathbf f(\\bm x,\\bm u,t) - L(\\bm x,\\bm u,t).}\n\\tag{5}\nThe necessary conditions of optimality in Eq. 3 can be rewritten in a more compact form: \\boxed{\n\\begin{aligned}\n\\dot {\\mathbf{x}} &= \\nabla_{\\bm \\lambda} H(\\bm x,\\bm u,\\bm \\lambda, t),\\\\\n\\dot{\\bm\\lambda} &= - \\nabla_{\\bm x} H(\\bm x,\\bm u,\\bm \\lambda,t),\\\\\n\\mathbf 0 &= \\nabla_\\mathbf{u}H(\\bm x,\\bm u,\\bm \\lambda,t).\n\\end{aligned}}\n\\tag{6}\nOf course, we must not forget to add the boundary conditions Eq. 4 to have a full set of necessary conditions of optimality.\n\n\n\n\n\n\nTwo different conventions for defining the Hamiltonian in optimal control\n\n\n\nOur choice of the Hamiltonian function was determined by our somewhat arbitrary choice of formulating the constraint function (that is, the function that should be equal to zero) as \\dot {\\bm x}-\\mathbf f(\\bm x,\\bm u,\\mathbf t) when defining the augmented Lagrangian L^\\mathrm{aug}(\\bm x,\\bm u,\\bm\\lambda,t) = L(\\bm x,\\bm u,t)+\\bm\\lambda^\\top\\left( \\dot {\\bm x}-\\mathbf f(\\bm x,\\bm u,\\mathbf t)\\right). If only we (equally arbitrarily) had chosen to define the constraint function as \\mathbf f(\\bm x,\\bm u,\\mathbf t)-\\dot {\\bm x}, we would have ended up with a different augmented Lagrangian, for which the more appropriate definition of the Hamiltonian function would be H(\\bm x,\\bm u,\\bm\\lambda,t) = L(\\bm x,\\bm u,t)+ \\bm\\lambda^\\top \\mathbf f(\\bm x,\\bm u,t). More on the implications of this in the dedicated section.\n\n\n\nExample 1 (Pendulum swingup problem approached as an optimal control problem and solved as a two-point boundary value problem) We consider a pendulum on a horizontally moving cart. The only control input is the reference acceleration of the cart – it is assumed that a feedback controller is already implemented that tracks this reference acceleration. The resulting model of dynamics is \n\\ddot \\theta =  \\frac{\\mathrm g}{l}\\sin\\theta + \\frac{1}{l}\\cos\\theta \\;u,\n where u is the reference acceleration of the cart, l is the length of the pendulum, and g is the gravitational constant, \\theta(t) is the angular deviation from the upright vertical, positive in the counterclockwise direction.\nThe pendulum is initially at rest in the downward vertical position, that is, \\bm x(0) = \\begin{bmatrix}\\pi\\\\ 0\\end{bmatrix}. The desired final state is \\bm x(t_\\mathrm{f}) = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}, that is, the pendulum should be upright and at rest. The final time t_\\mathrm{f} is fixed, we must specify it before solving the optimal control problem (in the next chapter we will learn how to consider it an optimization variable too).\n\n\nShow the code\nusing DifferentialEquations\nfunction pendulum_swingup_bvp()\n    g = 9.81                                                        # Gravitational acceleration.\n    l = 1.0                                                         # Length of the pendulum.\n    q = [1.0, 1.0]                                                  # State weights.\n    r = 100.0                                                       # Control weight.  \n    #f(x,u) = [x[2]; g/l*sin(x[1]) + 1/l*cos(x[1])*u]\n    #L(x,u) = 1/2(q[1]*x[1]^2 + q[2]*x[2]^2 + u^2)\n    #H(x,u,λ) = L(x,u) + λ[1]*f(x,u)[1] + λ[2]*f(x,u)[2]\n    tinit = 0.0                                                     # Initial time.\n    tfinal = 1.0                                                    # Final time.\n    xinit = [pi, 0.0]                                               # Initial state.\n    xfinal = [0.0, 0.0]                                             # Final state.\n    function statecostateeq!(dw, w, p, t)\n        x = w[1:2]                                                  # State vector [θ, ω].\n        λ = w[3:4]                                                  # Costate vector.\n        u = -1/(r*l)*cos(x[1])*λ[2]                                 # Optimal control from the stationarity equation.\n        dw[1] = x[2]                                                # State equation 1.\n        dw[2] = g/l*sin(x[1]) + 1/l*cos(x[1])*u                     # State equation 2. Damping: - b/(m*l^2)*x[2]\n        dw[3] = -q[1]*x[1] -( g/l*cos(x[1]) - 1/l*sin(x[1])*u)*λ[2] # Costate equation 1.\n        dw[4] = -q[2]*x[2] - λ[1]                                   # Costate equation 2.\n    end\n    function bc!(res, w, p,t)\n        res[1:2] = w(tinit)[1:2] - xinit\n        res[3:4] = w(tfinal)[1:2] - xfinal   \n    end\n    w0 = [xinit[1], xinit[2], 0.0, 0.0]                             # Guess at the initial state and costate.\n    tspan = (tinit, tfinal)                                         # Initial and final time.\n    bvprob = BVProblem(statecostateeq!, bc!, w0, tspan)\n    sol = solve(bvprob, MIRK4(), dt=0.05)                           # Solve the BVP.\n    u = -1/(r*l)*cos.(sol[1,:]).*sol[4,:]\n    return sol.t, sol[1:2,:], sol[3:4,:], u\nend\nt, x, λ, u = pendulum_swingup_bvp()\nusing Plots\np1 = plot(t, x', ylabel=\"States\", label=[\"θ\" \"ω\"], lw=2)\np2 = plot(t, λ', ylabel=\"Costates\", label=[\"λ₁\" \"λ₂\"], lw=2)\np3 = plot(t, u, ylabel=\"Control\", label=\"u\", lw=2)\nplot(p1, p2, p3, layout=(3,1), size=(600, 600))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Optimal trajectories of state, costate and control for the pendulum swingup problem.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "intro.html#what-is-optimal-control",
    "href": "intro.html#what-is-optimal-control",
    "title": "What is optimal and robust control?",
    "section": "What is optimal control?",
    "text": "What is optimal control?\nWhat does the adjective “optimal” stand for? According to the authoritative Oxford English Dictionary (OED), the word is defined as: best, most favourable, especially under a particular set of circumstance.\nOptimal control is then simply… the best control.\n\n\n\n\n\n\nCaution\n\n\n\nSince the word optimal already denotes the best, there is no point in saying “more optimal” or “the most optimal” (in Czech: “optimálnější” or “nejoptimálnější”). People using these terms simply do not know what they are talking about.",
    "crumbs": [
      "0. Introduction",
      "What is optimal and robust control?"
    ]
  },
  {
    "objectID": "intro.html#cost-function",
    "href": "intro.html#cost-function",
    "title": "What is optimal and robust control?",
    "section": "Cost function",
    "text": "Cost function\nOptimality of a (control) design can only be meaningfully considered if some quantitative measure of the quality of the design is specified. In control engineering it is typically called cost function, or simply cost. It is either provided by the customer, or chosen by the control engineer so that it reflects the customer’s requirements. An optimal controller then minimizes this cost. Alternatively, instead of a cost to be minimized, some kind of value to be maximized can be considered. But is a lot more common in control engineering to minimize a cost.\nAmong the classical examples of a cost function are the following:\n\nrise time, overshoot, settling time, …\nclassical integral criteria:\n\nintegral of the square of the error (ISE): J = \\int_0^\\infty e^2(t) \\mathrm{d}t, (or J = \\int_0^\\infty \\bm e^\\top\\bm e \\,\\mathrm{d}t in the vector case),\nintegral of the absolute error (IAE): J = \\int_0^\\infty |e(t)| \\mathrm{d}t,\nintegral of the absolute error weighted by time (ITAE): J = \\int_0^\\infty t|e(t)| \\mathrm{d}t,\n…\n\n\nThese are all quantifying in one way or another the performance, that is, how small the regulation error is, or how fast it gets small. But oftentimes the control effort must also be kept small. This necessity of a trade-off between performance and control effort is standard in optimal control theory. One way to handle it is through the celebrated LQR cost\n\nquadratic cost function: J = \\int_0^\\infty (\\bm x^\\top \\mathbf Q \\bm x + \\bm u^\\top \\mathbf R \\bm u) \\mathrm{d}t, where \\bm x and \\bm u are the (possibly vector) state and control variables, respectively, and \\mathbf Q and \\mathbf R are positive semidefinite weighting matrices that serve as the “tuning knobs” through which we express the trade-off.\n\nFinally, an alternative – and nicely unifying – approach to expressing cost functions is through the use of system norms.\n\nsystem norms: \\mathcal H_2 system norm, \\mathcal H_\\infty system norm, \\ell_1 system norm, …\n\nThe time to define these will only come later in the course, but at this moment we just mention it here.",
    "crumbs": [
      "0. Introduction",
      "What is optimal and robust control?"
    ]
  },
  {
    "objectID": "intro.html#why-do-we-need-optimal-control",
    "href": "intro.html#why-do-we-need-optimal-control",
    "title": "What is optimal and robust control?",
    "section": "Why do we need optimal control?",
    "text": "Why do we need optimal control?\nThere are three slightly different motivations for using the methods of optimal control, all equally useful:\n\nThe best performance is really needed. This is the most obvious opportunity. It occurs when, for example, the time for a robot to finish the assembly process must be as short as possible, or the fuel consumed by a vehicle to reach the final destination should be as low as possible.\nKnowledge of the best achievable performance is needed. Frequently, the technology chosen for the project imposes stringent coinstraints on the class of a controller that can be implemented. Say, just a PI or PID controller is allowed. Since the methods of optimal control typically yield controllers of higher complexity (higher-order, or nonlinear, or employing online optimization), an optimal controller would not be implementable with the chosen technology. However, even in these situations it may be useful to have an optimal controller as a baseline for comparison. If we know the minimum possible cost needed to accomplish the control task, we can compare it with the cost incurred by the current (setting of the) controller. Is the difference acceptable? If not, we can perhaps try to convice the customer to allow a more complex controller. If yes, we know that there is no need to keep on tuning the PID controller manually.\nAny reasonable controller would suffice, but it is difficult to find. Even if optimality of the control design is not a strict requirement, and it is only required to make the system “just work”, say, just to stable, decently fast, and free of large oscillations, even these modest requirements may turn out rather challenging in many situations. In particular, if the system has several inputs and outputs, the more so if it is unstable, or non-minimum phase, or nonlinear, or when the system is subject to disturbances or uncertainty. The methods of optimal control provide a systematic way to design a controller in these situations.\n\n\nWhat is robust control?\nRobust control system, as we understand it in this course, is capable of maintaining its performance even though the controlled system is subject to changes in its parameters, structure, or operating conditions.\n\n\nRobust control vs adaptive control\nIt is commonly understood that a robust controller is a controller with a fixed structure and fixed values of its parameters. As such it does not adapt to the aformentioned changes of the controlled system. This is in contrast with another important branch of control engineering, adaptive control.\n\n\nRobustness just as one aspect of a control system\nRobustness of a control system is certainly an important practical property, and as such it has been discussed in a number of publications and courses under the name of “robust control”. But we emphasize it here that we really view robustness as just another, however important, aspect of any control system. There is no point in seriously considering a nonrobust controller. Recall that even those classical concepts such as gain and phase margins, GM and PM, respectively, that you are familiar with from the introductory courses on automatic control, do characterize robustness. In our course, we will be investigating robustness in a systematic way, see the next paragraph.\n\n\nRobustness as an optimization criterion\nThe reason why we address this particular aspect of a control system in our course is that robustness can be naturally incorporated into the design of a controller using the methods of optimal control. Namely, the frameworks \\mathcal H_\\infty-optimal control and \\mu-synthesis are the most prominent examples of such optimization-based robust control design. While it used to be possible to design, say, a PID controller while keeping an eye on the gain and phase margins, we aim at attaining robustness as an outcome of optimization procedures.",
    "crumbs": [
      "0. Introduction",
      "What is optimal and robust control?"
    ]
  },
  {
    "objectID": "dynamic_programming_LQR.html",
    "href": "dynamic_programming_LQR.html",
    "title": "Solving LQ regulation (LQR) via dynamic programming",
    "section": "",
    "text": "In the previous section we have used dynamic programming as a numerical algorithm for solving a general discrete-time optimal control problem. We now show how to use dynamic programming to solve the discrete-time LQR problem. We consider a linear discrete-time system modelled by \n\\bm x_{k+1} = \\mathbf A\\bm x_k + \\mathbf B\\bm u_k,\n for which we want to minimize the quadratic cost given by \nJ_0(\\bm x_0, \\bm u_0, \\bm u_1, \\ldots, \\bm u_{N-1}) = \\frac{1}{2}\\bm x_N^\\top \\mathbf S_N \\bm x_N + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left(\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k\\right),\n with \\mathbf S_N\\succeq 0, \\mathbf Q\\succeq 0, \\mathbf R\\succ 0, as usual.\nWe now invoke the principle of optimality, that is, we start at the end of the time interval and find the optimal cost \nJ_N^\\star(\\bm x_N) = \\frac{1}{2}\\bm x_N^\\top \\mathbf S_N \\bm x_N.\n\nActually, we have nothing to minimize here, we just evaluate the cost. We then proceed backwards in time, that is, we decrease the time to k=N-1. Here we do have something to minimize: \nJ^\\star_{N-1}(\\bm x_{N-1}) = \\min_{\\bm u_{N-1}\\in\\mathbb R^m} \\left[\\underbrace{L(\\bm x_{N-1},\\bm u_{N-1}) + J^\\star_{N}(\\bm x_{N})}_{Q_{N-1}^\\star(\\bm x_k, \\bm u_k)} \\right],\n in which we recall our previously introduced notation for the function to be minimized as Q_{N-1}^\\star, also called a Q-function.\n\n\n\n\n\n\nWarning\n\n\n\nThe notational clash between the function Q^\\star and the weighting matrix \\mathbf Q is unfortunate, but the use of both symbols is so much established in this area that we will have to live with it. We have to rely on the font and the symbol of \\star to distinguish between the two.\n\n\nWe now expand the expression for the Q-function, with the obvious motivation to solve the optimization with respect to \\bm u_{N-1} analytically:\n\n\\begin{aligned}\nQ_{N-1}^\\star(\\bm x_{N-1},\\bm u_{N-1}) &= \\frac{1}{2} \\left(\\bm x_{N-1}^\\top \\mathbf Q \\bm x_{N-1} + \\bm u_{N-1}^\\top \\mathbf R \\bm u_{N-1} \\right) + J^\\star_{N}(\\bm x_{N}) \\\\\n&= \\frac{1}{2} \\left(\\bm x_{N-1}^\\top \\mathbf Q \\bm x_{N-1} + \\bm u_{N-1}^\\top \\mathbf R \\bm u_{N-1} \\right) + \\frac{1}{2}\\mathbf x_N^\\top \\mathbf S_N \\mathbf x_N\\\\\n&= \\frac{1}{2} \\left( \\bm x_{N-1}^\\top \\mathbf Q \\bm x_{N-1} + \\bm u_{N-1}^\\top \\mathbf R \\bm u_{N-1} + \\mathbf x_N^\\top \\mathbf S_N \\mathbf x_N \\right)\\\\\n&= \\frac{1}{2} \\left[ \\bm x_{N-1}^\\top \\mathbf Q \\bm x_{N-1} + \\bm u_{N-1}^\\top \\mathbf R \\bm u_{N-1} + (\\bm x_{N-1}^\\top \\mathbf A^\\top + \\bm u_{N-1}^\\top \\mathbf B^\\top) \\mathbf S_N (\\mathbf A\\bm x_{N-1} + \\mathbf B\\bm u_{N-1}) \\right]\\\\\n&= \\frac{1}{2} \\left[\\bm x_{N-1}^\\top (\\mathbf Q  + \\mathbf A^\\top\\mathbf S_N \\mathbf A)\\bm x_{N-1} + 2\\mathbf x^\\top_{N-1}\\mathbf A ^\\top \\mathbf S_N \\mathbf B  \\bm u_{N-1} + \\mathbf u^\\top_{N-1}(\\mathbf R + \\mathbf B^\\top \\mathbf S_n \\mathbf B)\\bm u_{N-1} \\right].\n\\end{aligned}\n\nSince we assumed no constraint on \\bm u_{N-1}, finding the minimum of Q_{N-1}^\\star(\\bm x_{N-1},\\bm u_{N-1}) with respect to \\bm u_{N-1} (while regarding \\bm x_{N-1} fixed) is as easy as setting its gradient to zero \n\\nabla_{\\bm u_{N-1}} Q_{N-1}^\\star(\\bm x_{N-1},\\bm u_{N-1}) = (\\mathbf R + \\mathbf B^\\top \\mathbf S_N \\mathbf B)\\bm u_{N-1} + \\mathbf B^\\top \\mathbf S_N\\mathbf A\\bm x_{N-1} = \\mathbf 0,\n which leads to \n\\bm u_{N-1}^\\star = -\\underbrace{(\\mathbf B^\\top \\mathbf S_N\\mathbf B + \\mathbf R)^{-1}\\mathbf B^\\top \\mathbf S_N \\mathbf A}_{\\mathbf K_{N-1}} \\bm x_{N-1},\n which amounts to solving a system of linear equations. We can also recognize the Kalman gain matrix \\mathbf K_{N-1}, which we derived using the indirect approach in the previous chapter.\nThe optimal cost J^\\star_{N-1} can be obtained by substituting \\bm u_{N-1}^\\star into Q_{N-1}^\\star \nJ_{N-1}^\\star = \\frac{1}{2}\\bm x_{N-1}^\\top \\underbrace{\\left[(\\mathbf A-\\mathbf B\\mathbf K_{N-1})^\\top \\mathbf S_N(\\mathbf A-\\mathbf B\\mathbf K_{N-1}) + \\mathbf K_{N-1}^\\top \\mathbf R \\mathbf K_{N-1} + \\mathbf Q\\right]}_{\\mathbf S_{N-1}} \\bm x_{N-1}.\n\nNote that the optimal cost J^\\star_{N-1} is also a quadratic function of the state as is the cost J^\\star_{N}. We denote the matrix that defines this quadratic function as \\mathbf S_{N-1}. We do this in anticipation of continuation of this procedure to k = N-2, N-3, \\ldots, which will give \\mathbf S_{N-2}, \\mathbf S_{N-3}, \\ldots. The rest of the story is quite predictable, isn’t it? Applying the Bellman’s principle of optimality we (re)discovered the discrete-time Riccati equation in the Joseph stabilized form \\boxed{\n\\mathbf S_k = (\\mathbf A-\\mathbf B\\mathbf K_{k})^\\top \\mathbf S_{k+1}(\\mathbf A-\\mathbf B\\mathbf K_{k}) + \\mathbf K_{k}^\\top \\mathbf R \\mathbf K_{k} + \\mathbf Q,}\n together with the prescription for the state feedback (Kalman) gain \\boxed{\n\\mathbf K_{k} = (\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B + \\mathbf R)^{-1}\\mathbf B^\\top \\mathbf S_{k+1} \\mathbf A,}\n and with the expression for the optimal cost as a (quadratic) function of the initial state \n\\boxed{\nJ_k^\\star(\\bm x_{k}) = \\frac{1}{2}\\bm x_k^\\top \\mathbf S_k \\bm x_k.}\n\n\n\n\n Back to top",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "LQR via DP"
    ]
  },
  {
    "objectID": "rocond_references.html",
    "href": "rocond_references.html",
    "title": "References",
    "section": "",
    "text": "The literature for \\mathcal H_\\infty control is essentially identical to the one we gave in the previous chapter on analysis of robustness. In particular, we stick to our primary textbook [1], in which the material is discussed in the forty-page Chapter 9.\nWhile discussing the analysis of robustness in the previous chapter, we omited discussion of structured uncertainties using the structured singular values (SSU, mu, \\mu). Similarly here we did not delve into the extension of that framework towards control synthesis. Purely because of time constraints. But should you find some time, have a look at section 8.12, which discusses the methodology called \\mu synthesis.\n\n\n\n\n Back to topReferences\n\n[1] S. Skogestad and I. Postlethwaite, Multivariable Feedback Control: Analysis and Design, 2nd ed. Wiley, 2005. Available: https://folk.ntnu.no/skoge/book/",
    "crumbs": [
      "12. Robust control",
      "References"
    ]
  },
  {
    "objectID": "roban_uncertainty.html",
    "href": "roban_uncertainty.html",
    "title": "Uncertainty (in) modelling",
    "section": "",
    "text": "Through this chapter we are stepping into the domain of robust control. We need to define a few keywords first.\nWhile these two terms are used in many other fields, here we are tailoring them to the discipline of control systems, in particular their model-based design.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Uncertainty (in) modelling"
    ]
  },
  {
    "objectID": "roban_uncertainty.html#origins-of-uncertainty-in-models",
    "href": "roban_uncertainty.html#origins-of-uncertainty-in-models",
    "title": "Uncertainty (in) modelling",
    "section": "Origins of uncertainty in models?",
    "text": "Origins of uncertainty in models?\n\nPhysical parameters are not known exactly (say, they are known to be within ±10% or ±3σ interval around the nominal value).\nEven if the physical parameters are initially known with a high accuracy, they can evolve in time, unmeasured.\nThere may be variations among the individual products of the same kind.\nIf a nonlinear system is planned to be operated around a given operating point, it can be linearized at that operating point, which gives a nominal linear model. If the system is then operated at a significantly different operating point, the corresponding linear model differs from the nominal one.\nOur understanding of the underlying physics (or chemistry or biology or …) is imperfect, hence our model is imperfect too. In fact, our understanding can even be incorrect, in which case the model contains some discrepancies too. The imperfections of the model are typically observed at higher frequencies, while at low frequencies or at least at the model of the steady-state response the accuracy is typically better (referring to the frequency-domain modeling such as transfer functions).\nEven if we are able to eventually capture full dynamics of the system in a model, we may opt not to do so. We may want to keep the model simple, even if less accurate, because time invested into modelling is not for free, of course.\nEven if we can get a high-fidelity model with a reasonable effort, we may still prefer using a simpler (and less accurate) model for a controller design. The reason is that very often the complexity of the model used for model-based control design is reflected by the complexity of the controller; and high-complexity controllers are not particularly appreciated in industry.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Uncertainty (in) modelling"
    ]
  },
  {
    "objectID": "roban_uncertainty.html#models-of-uncertainty-uncertainty-in-models",
    "href": "roban_uncertainty.html#models-of-uncertainty-uncertainty-in-models",
    "title": "Uncertainty (in) modelling",
    "section": "Models of uncertainty (uncertainty in models)",
    "text": "Models of uncertainty (uncertainty in models)\nThere are several approaches to modelling (characterizing) of the uncertainty. They all aim – in one way or another – to expressing that instead of only a single (nominal) system, a whole family of systems needs to be considered. Depending on the mathematical frameworks used for characterization of such a family, there are two major classes of approaches.\n\nWorst-case models of uncertainty\nProbabilistic models of uncertainty\n\nThe former assumes sets of systems with no additional information about the structure of such sets. The latter imposes some probability structure on the set of systems – in other words, although in principle any member of the set possible, some may be more probable than the others. In this course we are focusing on the former, which is also the mainstream in the robust control literature, but note that the latter we already encountered while considering control for systems exposed to random disturbances, namely the LQG control. A possible viewpoint is that the random disturbance represents the uncertainty in the system.\nAnother classification of models of uncertainty is according to the actual aspect of the (model of the) system that is uncertain. We distinguish these two\n\nParametric uncertainty\nFrequency-dependent (aka dynamical) uncertainty\n\nUnstructured uncertainty\nStructured uncertainty\n\n\n\nParametric uncertainty\nThis is obviously straightforward to state: some real (physical) parameters are uncertain. The conceptually simplest way to characterize such uncertain parameters is by considering intervals instead of just single (nominal) values.\n\nExample 1 (A pendulum on a cart) Within the state equation \n\\dot{\\bm x}(t) =\n\\begin{bmatrix}\n0 & 1 & 0 & 0\\\\\n0 & 0 & \\frac{\\textcolor{red}{m_\\mathrm{l}}}{m_\\mathrm{c}} g & 0\\\\\n0 & 0 & 0 & 1\\\\\n0 & 0 & -\\frac{(\\textcolor{red}{m_\\mathrm{l}}+m_\\mathrm{c})g}{m_\\mathrm{c}\\textcolor{red}{l}} & 0\n\\end{bmatrix}\n\\bm x(t)\n+\n\\begin{bmatrix}\n0\\\\\n\\frac{1}{m_\\mathrm{c}}\\\\\n0\\\\\n-\\frac{1}{m_\\mathrm{c}\\textcolor{red}{l}}\n\\end{bmatrix}\nu(t),\n two parameters are known to have their values within some intervals: \n\\begin{aligned}\n{\\color{red} m_\\mathrm{l}} & \\in [m_\\mathrm{l}^{-},m_\\mathrm{l}^{+}],\\\\\n{\\color{red} l} & \\in [l^{-}, l^{+}].\n\\end{aligned}\n\n\n\n\nUnstructured frequency-dependent uncertainty\nThere may be some phenomena exhibited by the system that cannot be explained by perturbation of physical parameters of the model. Possibly some lightly damped modes should be included in the model, possinly some fast pole or non-minimum phase zero, possibly some time delay here and there, in which case it is not only some enumerated physical parameters, but even the order of the system that is uncertain. In other words, the system contains uncertain dynamics. In the linear case, all this can be expressed by regarding the magnitude and phase responses uncertain without the need to map these to actual physical parameters.\n\n\n\n\n\n\nFigure 1: A subsystem is uncertain\n\n\n\nA popular model for an uncertain subsystem is that of a transfer function \\Delta(s), about which we know only that it is stable and that its magnitude is bounded by 1 \\boxed\n{\\sup_{\\omega}|\\Delta(j\\omega)|\\leq 1,\\;\\;\\Delta \\;\\text{stable}. }\n\nTypically the uncertainty is higher at higher frequencies. This can be expressed by using some weighting function w(\\omega). For later theoretical and computational purposes we approximate the real weighting function using a magnitude of a low-order rational stable transfer function W(s). That is, |W(j\\omega)|\\approx w(\\omega) for \\omega \\in \\mathbb R, that is for s=j\\omega on the imaginary axis.\nThe ultimate transfer function model of the uncertainty is then\n\\boxed{\nW(s)\\;\\Delta(s),\\quad \\max_{\\omega}|\\Delta(j\\omega)|\\leq 1,\\;\\;\\Delta\\; \\text{stable}. }\n\n\n\n\n\n\n\nRational function W(s) distorts the phase but we do not care because \\Delta(s) does it anyway\n\n\n\nIf you start worrying that by considering a rational function W(s) we inevitably distort the phase too (by \\angle W(j\\omega)), you can relax, because \\Delta(s) does it anyway. The only responsibility of W(s) is to model the magnitude well.\n\n\n\n\\mathcal H_\\infty norm of an LTI system\nA useful concept for characterizing the magnitude of a transfer function is the \\mathcal H_\\infty norm. There are two (equivalent) definitions. One in the frequency domain, and the other in the time domain. We start with the former.\n\nH-infinity norm of an LTI system interpreted in frequency domain\n\nDefinition 4 (\\mathcal H_\\infty norm of a SISO LTI system) For a stable LTI system modelled by the transfer function G with a single input and single output, the \\mathcal H_\\infty norm is defined as \n\\|G\\|_{\\infty} = \\sup_{\\omega\\in\\mathbb{R}}|G(j\\omega)|.\n\n\n\n\n\n\n\n\nWhy supremum and not maximum?\n\n\n\nSupremum is used in the definition because it is not guaranteed that the peak value of the magnitude frequency response is attained at a finite frequency. Consider, for example, the first-order system G(s) = \\frac{s}{Ts+1}. The peak gain of 1/T is not attained at any finite frequency.\n\n\nHaving just defined the \\mathcal H_\\infty norm, the uncertainty model can be expressed compactly as \\boxed{\nW(s)\\;\\Delta(s),\\quad \\|\\Delta(j\\omega)\\|\\leq 1. }\n\n\n\n\n\n\n\n\\mathcal H_\\infty as a space of functions\n\n\n\n\\mathcal H_\\infty denotes a normed vector space of functions that are analytic in the closed extended right half plane (of the complex plane). In parlance of control systems, \\mathcal H_\\infty is the space of proper and stable transfer functions. Poles on the imaginary axis are not allowed. The functions do not need to be rational, but very often we do restrict ourselves to rational functions, in which case we typically write such space as \\mathcal{RH}_\\infty.\n\n\nWe now extend the concept of the \\mathcal H_\\infty norm to MIMO systems. The extension is perhaps not quite intuitive – certainly it is not computed as the maximum of the norms of individual transfer functions, which is what newcomers might guess.\n\nDefinition 5 (\\mathcal H_\\infty norm of a MIMO LTI system) For a stable LTI system modelled by a matrix transfer function \\mathbf G with multiple inputs and/or multiple outputs, the \\mathcal H_\\infty norm is defined as \n\\|\\mathbf G\\|_{\\infty} = \\sup_{\\omega\\in\\mathbb{R}}\\bar{\\sigma}(\\mathbf{G}(j\\omega))\n where \\bar\\sigma is the largest singular value.\n\nHere we include a short recap of singular values and singular value decomposition (SVD) of a matrix. Consider a matrix \\mathbf M, possibly a rectangular one. It can be decomposed as a product of three matrices \n\\mathbf M = \\mathbf U\n\\underbrace{\n\\begin{bmatrix}\n\\sigma_1 & & & &\\\\\n  & \\sigma_2 & & &\\\\\n  & &\\sigma_3 & &\\\\\n\\\\\n  & & & & \\sigma_n\\\\\n\\end{bmatrix}\n}_{\\boldsymbol\\Sigma}\n\\mathbf V^{*}.\n\nThe two square matrices \\mathbf V and \\mathbf U are unitary, that is, \n\\mathbf V\\mathbf V^*=\\mathbf I=\\mathbf V^*\\mathbf V\n and \n\\mathbf U\\mathbf U^*=\\mathbf I=\\mathbf U^*\\mathbf U.\n\nThe nonnegative diagonal entries \\sigma_i \\in \\mathbb R_+, \\forall i of the (possibly rectangular) matrix \\Sigma are called singular values. Commonly they are ordered in a nonincreasing order, that is \n\\sigma_1\\geq \\sigma_2\\geq \\sigma_3\\geq \\ldots \\geq \\sigma_n.\n\nIt is also a common notation to denote the largest singular value as \\bar \\sigma, that is, \\bar \\sigma \\coloneqq \\sigma_1.\n\n\n\\mathcal{H}_{\\infty} norm of an LTI system interpreted in time domain\nWe can also view the dynamical system with inputs and outputs as an operator G mapping from some chosen space of functions (signals) to another space of functions (signals). A popular model for these function/signal spaces are the spaces of square-integrable functions, denoted as \\mathcal{L}_2, and sometimes interpreted as bounded-energy signals \nG:\\;\\mathcal{L}_2\\rightarrow \\mathcal{L}_2.\n\nIt is a powerful fact that the induced operator norm is equal to the \\mathcal{H}_{\\infty} norm of the corresponding transfer function \\boxed{\n\\|G\\|_{\\infty} = \\sup_{u(\\cdot)\\in\\mathcal{L}_2\\setminus 0}\\frac{\\|y(\\cdot)\\|_2}{\\|u(\\cdot)\\|_2}}.\n\nRecall the definition of the \\mathcal{L}_2 norm of a signal u(\\cdot) (here we assume that the signal starts at time 0) \n\\|u(\\cdot)\\|_2 = \\sqrt{\\int_{0}^{+\\infty}|u(t)|^2dt}.\n\nIn some cases the \\mathcal{L}_2 norm of a signal u(\\cdot) can be interpreted as (proportional to) the energy (when u(\\cdot) is voltage, current, velocity, force, …). The system norm can then be interpreted as the worst-case energy gain of the system.\nIf the input or output signals are vector valued, we extend the definition to \\boxed{\n\\|\\mathbf G(s)\\|_{\\infty} = \\sup_{\\bm u(\\cdot)\\in\\mathcal{L}_2\\setminus 0}\\frac{\\|\\bm y(\\cdot)\\|_2}{\\|\\bm u(\\cdot)\\|_2}},\n in which the absolute value must be replaced by some norm, typically the Euclidean norm is used \n\\|\\bm u(\\cdot)\\|_2 = \\sqrt{\\int_{0}^{+\\infty}\\|u(t)\\|_2^2dt}.\n\n\n\n\n\n\n\nScaling necessary for MIMO systems\n\n\n\nScaling of the model is necessary to get any useful info from MIMO models! See [1, Sec. 1.4], pages 5-8, which is contained in the online available part.\n\n\n\n\n\nHow does the uncertainty enter the model of the system?\n\nAdditive uncertainty\nThe transfer function of an uncertain system can be written as a sum of a nominal system and an uncertainty \nG(s) = \\underbrace{G_0(s)}_{\\text{nominal model}}+\\underbrace{W(s)\\Delta(s)}_{\\text{additive uncertainty}}.\n\nThe block diagram interpretation is in Fig. 2.\n\n\n\n\n\n\nFigure 2: Additive uncertainty\n\n\n\nThe magnitude frequency response of the weighting filter W(s) then serves as an upper bound on the absolute error in the magnitude frequency responses \n|G(j\\omega)-G_0(j\\omega)|&lt;|W(j\\omega)|\\quad \\forall \\omega\\in\\mathbb R.\n\n\n\nMultiplicative uncertainty\n\nG(s) = (1+W(s)\\Delta(s))\\,G_0(s).\n\nThe block diagram interpretation is in\n\n\n\n\n\n\nFigure 3: Multiplicative uncertainty\n\n\n\n\n\n\n\n\n\nFor SISO transfer functions no need to bother about the order of terms in the products\n\n\n\nSice we are considering SISO transfer functions, the order of terms in the products is not important. We will have to be more alert to the ordering of terms when we move to MIMO systems. We then have to distinguish between the input and output multiplicative uncertainties.\n\n\nThe magnitude frequency response of the weighting filter W(s) then serves as an upper bound on the relative error in the magnitude frequency responses \\boxed\n{\\frac{|G(j\\omega)-G_0(j\\omega)|}{|G_0(j\\omega)|}&lt;|W(j\\omega)|\\quad \\forall \\omega\\in\\mathbb R.}\n\n\nExample 2 (Uncertain first-order delayed system) We consider a first-order system with a delay described by \nG(s) = \\frac{k}{T s+1}e^{-\\theta s}, \\qquad 2\\leq k,\\tau,\\theta,\\leq 3.\n\nWe now need to choose the nominal model G_0(s) and then the uncertainty weighting filter W(s). The nominal model corresponds to the nominal values of the parameters, therefore we must choose these. There is no single correct way to do this. Perhaps the most intuitive way is to choose the nominal values as the average of the bounds. But we can also choose the nominal values in a way that makes the nominal system simple. For example, for this system with a delay, we can even choose the nominal value of the delay as zero, which makes the nominal system a first-order system without delay, hence simple enough for application of some basic linear control system design methods. Of course, the price to pay is that the resulting model of an uncertain system, which is actually a family (a set) of systems, contains even models that were not prescribed.\n\n\nShow the code\nusing ControlSystems\nusing Plots\n\nfunction uncertain_first_order_delayed()\n    kmin = 2; kmax = 3; \n    θmin = 2; θmax = 3; \n    Tmin = 2; Tmax = 3;\n\n    k₀ = (kmin+kmax)/2; \n    θ₀ = (θmin+θmax)/2;\n    θ₀ = 0 \n    T₀ = (Tmin+Tmax)/2;\n\n    G₀ = tf(k₀,[T₀, 1])*delay(θ₀)  \n\n    ω = exp10.(range(-2, 2, length=50))\n    G₀ω = freqresp(G₀,ω)\n    G₀ω = vec(G₀ω)\n\n    EEω_db = [];\n    for k in range(kmin,kmax,length=10)\n        for θ in range(θmin,θmax,length=10)\n            for T in range(Tmin,Tmax,length=10)\n                G = tf(k,[T, 1])*delay(θ)  \n                Gω = freqresp(G,ω)\n                Gω = vec(Gω)            \n                Eω = abs.(Gω-G₀ω)./abs.(G₀ω)\n                Eω_db = 20 * log10.(Eω)\n                push!(EEω_db,Eω_db)\n            end\n        end\n    end\n\n    plot(ω,EEω_db,xscale=:log10,label=\"\",ylims=(-40,20))\n    xlabel!(\"Frequency [rad/s]\")\n    ylabel!(\"Relative error [dB]\")\nend\nuncertain_first_order_delayed()\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Magnitude of the relative error for a grid of parameters as a function of frequency\n\n\n\n\nNow we need to find some upper bound on the relative error. Simplicity is a virtue here too, hence we are looking for a rational filter of very low order, say 1 or 2. Speaking of the first-order filter, one useful way to format it is \n\\boxed{\nW(s) = \\frac{\\tau s+r_0}{(\\tau/r_{\\infty})s+1},}\n where r_0 is uncertainty at steady state, 1/\\tau is the frequency, where the relative uncertainty reaches 100%, r_{\\infty} is relative uncertainty at high frequencies, often r_{\\infty}\\geq 2.\nFor our example, the parameters of the filter are in the code below and the frequency response follows.\n\n\nShow the code\nusing ControlSystems\nusing Plots\n\nfunction uncertain_first_order_delayed_with_weights()\n    kmin = 2; kmax = 3; \n    θmin = 2; θmax = 3; \n    Tmin = 2; Tmax = 3;\n\n    k₀ = (kmin+kmax)/2; \n    θ₀ = (θmin+θmax)/2;\n    θ₀ = 0 \n    T₀ = (Tmin+Tmax)/2;\n\n    G₀ = tf(k₀,[T₀, 1])*delay(θ₀)  \n\n    ω = exp10.(range(-2, 2, length=50))\n    G₀ω = freqresp(G₀,ω)\n    G₀ω = vec(G₀ω)\n\n    EEω_db = [];\n    for k in range(kmin,kmax,length=10)\n        for θ in range(θmin,θmax,length=10)\n            for T in range(Tmin,Tmax,length=10)\n                G = tf(k,[T, 1])*delay(θ)  \n                Gω = freqresp(G,ω)\n                Gω = vec(Gω)            \n                Eω = abs.(Gω-G₀ω)./abs.(G₀ω)\n                Eω_db = 20 * log10.(Eω)\n                push!(EEω_db,Eω_db)\n            end\n        end\n    end\n\n    plot(ω,EEω_db,xscale=:log10,label=\"\",ylims=(-40,20))\n    xlabel!(\"Frequency [rad/s]\")\n    ylabel!(\"Relative error [dB]\")\n\n    τ = 1/0.25\n    r₀ = 0.2\n    r∞ = 10^(8/20)\n    W = tf([τ, r₀],[τ/r∞, 1])\n    magW, phaseW = bode(W,ω)\n    plot!(ω,20*log10.(vec(magW)),xscale=:log10,lw=3,color=:red,label=\"W\")\n\n    # W2 = W*tf([1, 1.6, 1],[1, 1.4, 1]); \n    # magW2, phaseW2 = bode(W2,ω)\n    # plot!(ω,20*log10.(vec(magW2)),xscale=:log10,lw=3,color=:blue,label=\"W₂\")\nend\nuncertain_first_order_delayed_with_weights()\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: (Approximate) upper bound on the magnitude of the relative error(s)\n\n\n\n\nObviously the filter does not capture the family of systems perfectly. It is now up to the control engineer to decide if this is a problem. If yes, if the control design should be really robust against all uncertainties in the considered set, some more complex (higher-order) filter is needed to described the uncertainty more accurately. The source code shows (in commented lines) one particular candidate, but in general the whole problem boils down to designing a stable filter with a prescribed magnitude frequency response.\n\n\n\nInverse additive uncertainty, inverse multiplicative uncertainty\nWe can also consider the configuration in which the uncertainty enters in a feedback manner as in Fig. 6 and Fig. 7.\n\n\n\n\n\n\nFigure 6: Inverse additive uncertainty\n\n\n\n\n\n\n\n\n\nFigure 7: Inverse multiplicative uncertainty\n\n\n\nThrough these feedback configurations we can also model unstable uncertain dynamics even if W and \\Delta are stable.\n\n\nLinear fractional transformation (LFT)\nFinally, we are going to introduce a framework within which all the above described configurations can be expressed in a unified way. The framework is called linear fractional transformation (LFT). For a matrix \\mathbf P sized (n_1+n_2)\\times(m_1+m_2) and divided into blocks like \n\\mathbf P=\n\\begin{bmatrix}\n\\mathbf P_{11} & \\mathbf P_{12}\\\\\n\\mathbf P_{21} & \\mathbf P_{22}\n\\end{bmatrix},\n and a matrix \\mathbf K sized m_2\\times n_2, the lower LFT of \\mathbf P with respect to \\mathbf K is \n\\boxed{\n\\mathcal{F}_\\mathbf{l}(\\mathbf P,\\mathbf K) = \\mathbf P_{11}+\\mathbf P_{12}\\mathbf K(\\mathbf I-\\mathbf P_{22}\\mathbf K)^{-1}\\mathbf P_{21}}.\n\nIf the matrices \\mathbf P and \\mathbf K are input-ouutput models (matrix transfer functions) of the plant and the controller, respectively, the LFT can be viewed as their interconnection in which not all plant inputs are used as control inputs and not all plant outputs are measured, as depicted in Fig. 8.\n\n\n\n\n\n\nFigure 8: Lower LFT of \\mathbf P with respect to \\mathbf K\n\n\n\nSimilarly, for a matrix \\mathbf N sized (n_1+n_2)\\times(m_1+m_2) and a matrix \\boldsymbol\\Delta sized m_1\\times n_1, the upper LFT of \\mathbf N with respect to \\mathbf K is \n\\boxed{\n\\mathcal{F}_\\mathbf{u}(\\mathbf N,\\boldsymbol\\Delta) = \\mathbf N_{22}+\\mathbf N_{21}\\boldsymbol\\Delta(\\mathbf I-\\mathbf N_{11}\\boldsymbol\\Delta)^{-1}\\mathbf N_{12}}.\n\nIt can be viewed as a feedback interconnection of the nominal plant \\mathbf N and the uncertainty block \\boldsymbol\\Delta, as depicted in Fig. 9.\n\n\n\n\n\n\nFigure 9: Upper LFT of \\mathbf N with respect to \\boldsymbol \\Delta\n\n\n\nHere we already anticipated a MIMO uncertainty block. One motivation is explained in the next section on structured uncertainties, another one appears once we start formulating robust performance within the same analytical framework as robust stability.\n\n\n\n\n\n\nWhich is lower and which is upper is a matter of convention, but a useful one\n\n\n\nOur usage of the lower LFT for a feedback interconnection of a (generalized) plant and a controller and the upper LFT for a feedback interconnection of a nominal system and and uncertainty is completely arbitrary. We could easily use the lower LFT for the uncertainty. But it is a convenient convention to adhere to. The more so that it allows for the combination of both as in the diagram Fig. 10 below, which corresponds to composition of the two LFTs.\n\n\n\n\n\n\nFigure 10: Combination of the lower and upper LFT\n\n\n\n\n\n\nExample 3 (Input multiplicative uncertainty formulated as an upper LFT) We advertized the LFT as a unifying modelling tool. Here comes an example. We consider the (input) multiplicative uncertainty and reformulate it as and upper LFT.\n\n\n\n\n\n\nFigure 11: Input multiplicative uncertainty formulated as an upper LFT\n\n\n\nWe can read from Fig. 11 that the generalized plant \\mathbf P is modelled by the matrix transfer function \n\\mathbf P =\n\\begin{bmatrix}\n0 & W\\\\\nG_0 & G_0\n\\end{bmatrix}.\n\n\n\n\n\n\nStructured frequency-domain uncertainty\nNot just a single \\Delta(s) but several \\Delta_i(s), i=1,\\ldots,n are considered. Some of them scalar-valued, some of them matrix-valued.\nIn the upper LFT, all the individual \\Delta_is are collected into a single overall \\boldsymbol \\Delta, which then exhibits some structure. Typically it is block-diagonal as in \n\\boldsymbol\\Delta =\n\\begin{bmatrix}\n\\Delta_1& 0 & \\ldots & 0\\\\\n0 & \\Delta_2 & \\ldots & 0\\\\\n\\vdots\\\\\n0 & 0 & \\ldots & \\boldsymbol\\Delta_n\n\\end{bmatrix},\n with each block (including the MIMO blocks) satisfying the usual condition \n\\|\\Delta_i\\|_{\\infty}\\leq 1, \\; i=1,\\ldots, n.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Uncertainty (in) modelling"
    ]
  },
  {
    "objectID": "roban_references.html",
    "href": "roban_references.html",
    "title": "References",
    "section": "",
    "text": "Even when restricted to control systems, the concept of robustness is quite broad and can be approached from many different angles. In our course we are restricting the focus to the approaches formulated in frequency domain. The main reference for this part of the course is the book [1]. The concepts and techniques introduced in our lecture are covered in Chapters 7 and 8 (up to 8.5) of the book.\nWhat we typically do not cover in the lecture, but only due to time constraints, is the topic of structured uncertainties and their analysis using structured singular value (SSV, 𝜇). These are treated in the section 8.6 through 8.11 of the book. It is warmly recommended to have a look at it.\nAlthough the book is not freely available online (only the first three chapters are downloadable on the authors’ web page, but even these are useful), it is available in a decent number of copies in the university library.\n\n\n\n\n\n\nGet the second edition of Skogestad’s book\n\n\n\nIn case you are interested in getting the book in one way or another (perhaps even by purchasing it), make sure you get the second edition published 2005. The book contains some useful snippets of Matlab code and the first edition relies on some ancient version of Matlab toolboxes, which makes it useless these days.\n\n\nThe topic of modeling uncertainty in frequency domain using weighting filters plugged into additive or multiplicative structures is fairly classical now and as such can be found in numerous textbooks on robust control such as [2], [3], [4], [5]. Although these are fine texts, frankly speaking they offer nearly no guidance for applying the highly advanced concepts to practical problems – they mostly focus on building up the theoretical framework. In this regard, Skogestad’s book is truly unique.\nThere are some newer texts that appear to be a bit more on the application side such as [6], [7] or [8], but I am not familiar them, to be honest.\n\n\n\n\n Back to topReferences\n\n[1] S. Skogestad and I. Postlethwaite, Multivariable Feedback Control: Analysis and Design, 2nd ed. Wiley, 2005. Available: https://folk.ntnu.no/skoge/book/\n\n\n[2] J. C. Doyle, B. A. Francis, and A. R. Tannenbaum, Feedback Control Theory, Reprint of the 1990 edition by Macmillan Publishing. Dover Publications, 2009. Available: https://www.control.utoronto.ca/people/profs/francis/dft.pdf\n\n\n[3] K. Zhou, J. C. Doyle, and K. Glover, Robust and Optimal Control, 1st ed. Prentice Hall, 1995.\n\n\n[4] G. E. Dullerud and F. Paganini, A Course in Robust Control Theory: A Convex Approach. in Texts in Applied Mathematics. New York: Springer-Verlag, 2000. doi: 10.1007/978-1-4757-3290-0.\n\n\n[5] R. S. Sánchez-Peña and M. Sznaier, Robust Systems Theory and Applications, 1st ed. Wiley-Interscience, 1998.\n\n\n[6] E. Lavretsky and K. Wise, Robust and Adaptive Control: With Aerospace Applications, 2nd ed. in Advanced Textbooks in Control and Signal Processing (C&SP). Cham: Springer, 2024. Available: https://doi.org/10.1007/978-3-031-38314-4\n\n\n[7] R. K. Yedavalli, Robust Control of Uncertain Dynamic Systems: A Linear State Space Approach. New York: Springer, 2014. Available: https://doi.org/10.1007/978-1-4614-9132-3\n\n\n[8] D.-W. Gu, P. H. Petkov, and M. M. Konstantinov, Robust Control Design with MATLAB, 2nd ed. in Advanced Textbooks in Control and Signal Processing. New York: Springer, 2013. Available: https://doi.org/10.1007/978-1-4471-4682-7",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "References"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html",
    "href": "opt_theory_modellers.html",
    "title": "Optimization modelling languages",
    "section": "",
    "text": "Realistically complex optimization problems cannot be solved with just a pen and a paper – computer programs (often called optimization solvers) are needed to solve them. And now comes the challenge: as various solvers for even the same class of problems differ in the algorithms they implement, so do their interfaces – every solver expects the inputs (the data defining the optimization problem) in a specific format. This makes it difficult to switch between solvers, as the problem data has to be reformatted every time.\n\nExample 1 (Data formatting for different solvers) Consider the following optimization problem: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} \\leq \\begin{bmatrix} 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix} \\bm x \\leq  \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7\\end{bmatrix}\n\\end{aligned}\n\nThere are dozens of solvers that can be used to solve this problem. Here we demonstrate a usage of these two: OSQP and COSMO.jl. And we are going to call the solvers in Julia (using the the wrappers OSQP.jl for the former). First, we start with OSQP (in fact, this is their example):\n\n\nShow the code\nusing OSQP\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nproblem_OSQP = OSQP.Model()\nOSQP.setup!(problem_OSQP; P=P, q=q, A=A, l=l, u=u, alpha=1, verbose=false)\n\n# Solve the optimization problem and show the results\nresults_OSQP = OSQP.solve!(problem_OSQP)\nresults_OSQP.x\n\n\n2-element Vector{Float64}:\n 0.300000191114193\n 0.6999998090276102\n\n\nNow we do the same with COSMO. First, we must take into account that COSMO cannot accept two-sided inequalities, so we have to reformulate the problem so that the constraints are only in the form of \\mathbf A\\bm x + b \\geq \\bm 0: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix} -1 & -1\\\\ -1 & 0\\\\ 0 & -1\\\\ 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix}\\bm x + \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7 \\\\ -1 \\\\ 0 \\\\ 0\\end{bmatrix} \\geq  \\mathbf 0.\n\\end{aligned}\n\n\n\nShow the code\nusing COSMO\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nAa = [-A; A]\nba = [u; -l]\n\nproblem_COSMO = COSMO.Model()\nconstraint = COSMO.Constraint(Aa, ba, COSMO.Nonnegatives)\nsettings = COSMO.Settings(verbose=false)\nassemble!(problem_COSMO, P, q, constraint, settings = settings)\n\n# Solve the optimization problem and show the results\nresults_COSMO = COSMO.optimize!(problem_COSMO)\nresults_COSMO.x\n\n\n2-element Vector{Float64}:\n 0.30000000000000143\n 0.6999999999999995\n\n\nAlthough the two solvers are solving the same problem, the data has to be formatted differently for each of them (and the difference in syntax is not negligible either).\nWhat if we could formulate the same problem without considering the pecualiarities of each solver? It turns out that it is possible. In Julia we can use JuMP.jl:\n\n\nShow the code\nusing JuMP\nusing SparseArrays\nusing OSQP, COSMO\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nmodel_JuMP = Model()\n@variable(model_JuMP, x[1:2])\n@objective(model_JuMP, Min, 0.5*x'*P*x + q'*x)\n@constraint(model_JuMP, A*x .&lt;= u)\n@constraint(model_JuMP, A*x .&gt;= l)\n\n# Solve the optimization problem using OSQP and show the results\nset_silent(model_JuMP)\nset_optimizer(model_JuMP, OSQP.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_OSQP = value.(x)\n\n# Now solve the problem using COSMO and show the results\nset_optimizer(model_JuMP, COSMO.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_COSMO = value.(x)\n\n\n2-element Vector{Float64}:\n 0.2999999999999993\n 0.7000000000000002\n\n\n\nNotice how the optimization problem is defined just once in the last code and then different solvers can be chosen to solve it. The code represents an instance of a so-called optimization modelling language (OML), or actually its major class called algebraic modelling language (AML).\nThe key motivation for using an OML/AML is to separate the process of formulating the problem from the process of solving it (using a particular solver). Furthermore, such solver-independent problem description (called optimization model) better mimics the way we formulate these problems using a pen and a paper, making it (perhaps) a bit more convenient to write our own and read someone else’s models.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#why-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#why-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "",
    "text": "Realistically complex optimization problems cannot be solved with just a pen and a paper – computer programs (often called optimization solvers) are needed to solve them. And now comes the challenge: as various solvers for even the same class of problems differ in the algorithms they implement, so do their interfaces – every solver expects the inputs (the data defining the optimization problem) in a specific format. This makes it difficult to switch between solvers, as the problem data has to be reformatted every time.\n\nExample 1 (Data formatting for different solvers) Consider the following optimization problem: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} \\leq \\begin{bmatrix} 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix} \\bm x \\leq  \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7\\end{bmatrix}\n\\end{aligned}\n\nThere are dozens of solvers that can be used to solve this problem. Here we demonstrate a usage of these two: OSQP and COSMO.jl. And we are going to call the solvers in Julia (using the the wrappers OSQP.jl for the former). First, we start with OSQP (in fact, this is their example):\n\n\nShow the code\nusing OSQP\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nproblem_OSQP = OSQP.Model()\nOSQP.setup!(problem_OSQP; P=P, q=q, A=A, l=l, u=u, alpha=1, verbose=false)\n\n# Solve the optimization problem and show the results\nresults_OSQP = OSQP.solve!(problem_OSQP)\nresults_OSQP.x\n\n\n2-element Vector{Float64}:\n 0.300000191114193\n 0.6999998090276102\n\n\nNow we do the same with COSMO. First, we must take into account that COSMO cannot accept two-sided inequalities, so we have to reformulate the problem so that the constraints are only in the form of \\mathbf A\\bm x + b \\geq \\bm 0: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix} -1 & -1\\\\ -1 & 0\\\\ 0 & -1\\\\ 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix}\\bm x + \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7 \\\\ -1 \\\\ 0 \\\\ 0\\end{bmatrix} \\geq  \\mathbf 0.\n\\end{aligned}\n\n\n\nShow the code\nusing COSMO\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nAa = [-A; A]\nba = [u; -l]\n\nproblem_COSMO = COSMO.Model()\nconstraint = COSMO.Constraint(Aa, ba, COSMO.Nonnegatives)\nsettings = COSMO.Settings(verbose=false)\nassemble!(problem_COSMO, P, q, constraint, settings = settings)\n\n# Solve the optimization problem and show the results\nresults_COSMO = COSMO.optimize!(problem_COSMO)\nresults_COSMO.x\n\n\n2-element Vector{Float64}:\n 0.30000000000000143\n 0.6999999999999995\n\n\nAlthough the two solvers are solving the same problem, the data has to be formatted differently for each of them (and the difference in syntax is not negligible either).\nWhat if we could formulate the same problem without considering the pecualiarities of each solver? It turns out that it is possible. In Julia we can use JuMP.jl:\n\n\nShow the code\nusing JuMP\nusing SparseArrays\nusing OSQP, COSMO\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nmodel_JuMP = Model()\n@variable(model_JuMP, x[1:2])\n@objective(model_JuMP, Min, 0.5*x'*P*x + q'*x)\n@constraint(model_JuMP, A*x .&lt;= u)\n@constraint(model_JuMP, A*x .&gt;= l)\n\n# Solve the optimization problem using OSQP and show the results\nset_silent(model_JuMP)\nset_optimizer(model_JuMP, OSQP.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_OSQP = value.(x)\n\n# Now solve the problem using COSMO and show the results\nset_optimizer(model_JuMP, COSMO.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_COSMO = value.(x)\n\n\n2-element Vector{Float64}:\n 0.2999999999999993\n 0.7000000000000002\n\n\n\nNotice how the optimization problem is defined just once in the last code and then different solvers can be chosen to solve it. The code represents an instance of a so-called optimization modelling language (OML), or actually its major class called algebraic modelling language (AML).\nThe key motivation for using an OML/AML is to separate the process of formulating the problem from the process of solving it (using a particular solver). Furthermore, such solver-independent problem description (called optimization model) better mimics the way we formulate these problems using a pen and a paper, making it (perhaps) a bit more convenient to write our own and read someone else’s models.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#why-not-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#why-not-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "Why not optimization modelling languages?",
    "text": "Why not optimization modelling languages?\nAs a matter of fact, some optimization experts even keep avoiding OML/AML altogether. For example, if a company pays for a (not really cheap) license of Gurobi Optimizer – a powerful optimization library for (MI)LP/QP/QCQP –, it may be the case that for a particular very large-scale optimization problem their optimization specialist will have hard time to find a third-party solver of comparable performance. If then its Python API makes definition of optimization problems convenient too (see the code below), maybe there is little regret that such problem definitions cannot be reused with a third-party solver. The more so that since it is tailored to Gurobi solver, it will offer control over the finest details.\nimport gurobipy as gp\nimport numpy as np\n\n# Define the data for the model\nP = np.array([[4.0, 1.0], [1.0, 2.0]])\nq = np.array([1.0, 1.0])\nA = np.array([[1.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\nl = np.array([1.0, 0.0, 0.0])\nu = np.array([1.0, 0.7, 0.7])\n\n# Create a new model\nm = gp.Model(\"qp\")\n\n# Create a vector variable\nx = m.addMVar((2,))\n\n# Set the objective\nobj = 1/2*(x@P@x + q@x)\nm.setObjective(obj)\n\n# Add the constraints\nm.addConstr(A@x &gt;= l, \"c1\")\nm.addConstr(A@x &lt;= u, \"c2\")\n\n# Run the solver\nm.optimize()\n\n# Print the results\nfor v in m.getVars():\n    print(f\"{v.VarName} {v.X:g}\")\n\nprint(f\"Obj: {m.ObjVal:g}\")\nSimilar and yet different is the story of the IBM ILOG CPLEX, another top-notch solvers addressing the same problems as Gurobi. They do have their own modeling language called Optimization Modelling Language (OPL), but it is also only interfacing with their solver(s). We can only guess that their motivation for developing their own optimization modelling language was that at the time of its developments (in 1990s) Python was still in pre-2.0 stage and formulating optimization problems in programming languages like C/C++ or Fortran was nowhere close to being convenient. Gurobi, in turn, started in 2008, when Python was already a popular language.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#language-independent-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#language-independent-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "Language-independent optimization modelling languages",
    "text": "Language-independent optimization modelling languages\nOptimization/algebraic modelling languages were originally developed outside programming languages, essentially as standalone tools. Examples are AMPL, GAMS, and, say, GLPK/GMPL (MathProg). We listed these main names here since they can be bumped across (they are still actively developed), but we are not going to discuss them in our course any further. The reason is that there are now alternatives that are implemented as packages/toolboxes in programming languages such as Julia, Matlab, and Python, which offer a more fluent workflow – a user can use the same programming language to acquire the data, preprocess them, formulate the optimization problem, configure and call a solver, and finally do some postprocessing including a visualization and whatever reporting, all without leaving the language of their choice.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-julia",
    "href": "opt_theory_modellers.html#optimization-modelling-in-julia",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Julia",
    "text": "Optimization modelling in Julia\nMy obvious (personal) bias towards Julia programming language is partly due to the terrific support for optimization modelling in Julia:\n\nJuMP.jl not only constitutes one of the flagship packages of the Julia ecosystem but it is on par with the state of the art optimization modelling languages. Furthermore, being a free and open source software, it enjoys a vibrant community of developers and users. They even meet annually at JuMP-dev conference (in 2023 in Boston, MA).\nConvex.jl is an implementation of the concept of Disciplined Convex Programming (DCP) in Julia (below we also list its implementations in Matlab and Python). Even though it is now registered as a part of the JuMP.jl project, it is still a separate concept. Interesting, convenient, but it seems to be in a maintanence mode now.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-matlab",
    "href": "opt_theory_modellers.html#optimization-modelling-in-matlab",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Matlab",
    "text": "Optimization modelling in Matlab\nPopularity of Matlab as a language and an ecosystem for control-related computations is undeniable. Therefore, let’s have a look at what is available for modelling optimization problems in Matlab:\n\nOptimization Toolbox for Matlab is one of the commercial toolboxes produced by Matlab and Simulink creators. Since the R2017b release the toolbox supports Problem-based optimization workflow (besides the more traditional Solver-based optimization workflow supported since the beginning), which can be regarded as a kind of an optimization/algebraic modelling language, albeit restricted to their own solvers.\nYalmip started as Yet Another LMI Parser quite some time ago (which reveals its control theoretic roots), but these days it serves as fairly complete algebraic modelling language (within Matlab), interfacing to perhaps any optimization solver, both commercial and free&open-source. It is free and open-source. Is is still actively developed and maintained and it abounds with tutorials and examples.\nCVX is a Matlab counterpart of Convex.jl (or the other way around, if you like, since it has been here longer). The name stipulates that it only allows convex optimization probles (unlike Yalmip) – it follows the Disciplined Convex Programming (DCP) paradigm. Unfortunately, the development seems to have stalled – the last update is from 2020.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-python",
    "href": "opt_theory_modellers.html#optimization-modelling-in-python",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Python",
    "text": "Optimization modelling in Python\nPython is a very popular language for scientific computing. Although it is arguable if it is actually suitable for implementation of numerical algoritms, when it comes to building optimization models, it does its job fairly well (and the numerical solvers it calls can be developed in different language). Several packages implementing OML/AML are available:\n\ncvxpy is yet another instantiation of Disciplined Convex Programming that we alredy mention when introducing Convex.jl and CVX. And it turns out that this one exhibits the greatest momentum. The team of developers seems to be have exceeded a critical mass, hence the tools seems like a safe bet already.\nPyomo is a popular open-source optimization modelling language within Python.\nAPMonitor and GEKKO are relatively young projects, primarily motivated by applications of machine learning and optimization in chemical process engineering.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_algo_goals.html",
    "href": "opt_algo_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Explain the main principle of descent direction methods for unconstrained optimization. In particular, give the descent direction condition.\nGive an overview of approaches for line search, that is, a one-dimensional optimization.\nExplain the steepest descent (aka gradient) method. Discuss its shortcomings.\nExplain conditioning of a matrix and what impact it has on convergence of steepest descent algorithm. Propose a modification of a steepest descent method that includes scaling of the original matrix such that conditioning is improved.\nExplain the Newton method for unconstrained minimization. Give also the its interpretation as a method for root finding. Discuss its shortcomings.\nDiscuss the issue of solving a set of linear equations (in matrix-vector form) as they appear in the Newton method. Which matrix factorization will be appropriate?\nExplain the key idea behind Quasi-Newton methods.\nExplain the key idea behind trust region methods for unconstrained optimization. What are the advantages with respect to descent direction methods?",
    "crumbs": [
      "2. Optimization – algorithms",
      "Learning goals"
    ]
  },
  {
    "objectID": "opt_algo_goals.html#knowledge-remember-and-understand",
    "href": "opt_algo_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Explain the main principle of descent direction methods for unconstrained optimization. In particular, give the descent direction condition.\nGive an overview of approaches for line search, that is, a one-dimensional optimization.\nExplain the steepest descent (aka gradient) method. Discuss its shortcomings.\nExplain conditioning of a matrix and what impact it has on convergence of steepest descent algorithm. Propose a modification of a steepest descent method that includes scaling of the original matrix such that conditioning is improved.\nExplain the Newton method for unconstrained minimization. Give also the its interpretation as a method for root finding. Discuss its shortcomings.\nDiscuss the issue of solving a set of linear equations (in matrix-vector form) as they appear in the Newton method. Which matrix factorization will be appropriate?\nExplain the key idea behind Quasi-Newton methods.\nExplain the key idea behind trust region methods for unconstrained optimization. What are the advantages with respect to descent direction methods?",
    "crumbs": [
      "2. Optimization – algorithms",
      "Learning goals"
    ]
  },
  {
    "objectID": "opt_algo_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "opt_algo_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nWrite a code implementing a Quasi-Newton method for minimization of a provided function.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_indir_goals_2.html",
    "href": "cont_indir_goals_2.html",
    "title": "Learning goals",
    "section": "",
    "text": "Discuss what kind of changes to the necessary conditions of optimality in the form of Euler-Lagrange equations must be made if the final time is relaxed. Consider two cases: first, the state at the final time is free, second, the state at the final time must be on some (hyper)surface.\nExplain the essence of Pontryagin’s principle of maximum. Give the full first-order necessary conditions of optimality (for optimal control) using Pontryagin’s principle (instead of the equation of stationarity).\nGive (a sketch of) the derivation of time-optimal control for a double integrator with bounds on the control signal – bang-bang control. Discuss the key properties of the resulting controller. Discuss also the practical issues related to implementation. What kind of heuristics could be used to compensate those issues?",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_indir_goals_2.html#knowledge-remember-and-understand",
    "href": "cont_indir_goals_2.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Discuss what kind of changes to the necessary conditions of optimality in the form of Euler-Lagrange equations must be made if the final time is relaxed. Consider two cases: first, the state at the final time is free, second, the state at the final time must be on some (hyper)surface.\nExplain the essence of Pontryagin’s principle of maximum. Give the full first-order necessary conditions of optimality (for optimal control) using Pontryagin’s principle (instead of the equation of stationarity).\nGive (a sketch of) the derivation of time-optimal control for a double integrator with bounds on the control signal – bang-bang control. Discuss the key properties of the resulting controller. Discuss also the practical issues related to implementation. What kind of heuristics could be used to compensate those issues?",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_indir_goals_2.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "cont_indir_goals_2.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nDesign a constrained optimal (possibly minimum-time) controller for a continuous-time systems by invoking Pontryagin’s principle of maximum.",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Learning goals"
    ]
  },
  {
    "objectID": "discr_dir_hw.html",
    "href": "discr_dir_hw.html",
    "title": "Homework",
    "section": "",
    "text": "In this homework, you will be implementing a tracking Model Predictive Controller (MPC) for a linear model in Julia. Your task is to take the Optimal Control Problem of the tracking MPC \n\\begin{align*}\n    \\underset{\\mathbf{u}_k}{\\text{minimize}} \\quad & \\frac{1}{2}\\sum_{k=t}^{t+N-1} (\\mathbf{y}_{k+1} - r_{k+1})^T\\mathbf{Q}(\\mathbf{y}_{k+1} - r_{k+1}) + \\Delta\\mathbf{u}_k^T\\mathbf{R}\\Delta\\mathbf{u}_k\\\\\n    \\text{subject to} \\quad & \\mathbf{x}_{k+1} = \\mathbf{A}\\mathbf{x}_k + \\mathbf{B}\\mathbf{u}_k, \\qquad k = t,\\dots,t+N-1,\\\\\n    & \\mathbf{y}_k = \\mathbf{C}\\mathbf{x}_k, \\qquad k = t+1,\\dots,t+N,\\\\\n    &\\mathbf{u}_\\mathrm{min} \\leq \\mathbf{u}_{k} \\leq \\mathbf{u}_\\mathrm{max}, \\qquad k =t,\\dots,t+N-1,\n\\end{align*}\n where \\Delta\\mathbf{u}_k = \\mathbf{u}_k - \\mathbf{u}_{k-1}, and reformulate it as a Quadratic Program (QP) of the form \n\\begin{array}{rl}\n\\underset{\\mathbf{z}}{\\text{minimize}} \\quad &  \\frac{1}{2}\\mathbf{z}^T \\mathbf{H} \\mathbf{z} + [\\mathbf{x}_t^T \\: \\mathbf{u}_{t-1}^T \\: \\mathbf{r}_{t+1,\\dots,t+N}^T]\\,\\mathbf{F}\\,\\mathbf{z} \\\\\n\\text{subject to} \\quad &\\mathbf{G}\\mathbf{z} \\leq \\mathbf{W} + \\mathbf{S}\\left[\\begin{array}{c}\\mathbf{x}_t\\\\ \\mathbf{u}_{t-1}\\end{array}\\right].\n\\end{array}\n where \\mathbf{z}=\\left[\\begin{array}{c}\\Delta \\mathbf{u}_t^\\mathrm{T} & \\ldots & \\Delta \\mathbf{u}_{t+N-1}^\\mathrm{T}\\end{array}\\right]^\\mathrm{T}, \\mathbf{x}_t is the current state value of the model, \\mathbf{u}_{t-1} is the most recently applied input and \\mathbf{r}_{t+1,\\dots,t+N} is the reference over the current prediction horizon. You should then implement the MPC controller using this QP formulation.\nWe recommend starting by writing down the QP formulation on paper and only then proceeding to implement the MPC construction in code. In case of doubts, we advise you to consult the lecture notes, especially the section on the Sequential (Dense) formulation of direct discrete-time optimal control problems, as well as the related MPC video lectures. Another source which may serve you well is slides from a doctoral course on MPC by Bemporad, which can be found here.\nThe MPC controller should be based on the template provided below. Your goal is to complete the implementation by filling in the missing parts, specifically\n\nComplete the setup_mpc function that constructs the matrices \\mathbf{H}, \\mathbf{F}, \\mathbf{S}, \\mathbf{W}, and \\mathbf{G} for the QP formulation.\nComplete the solve! function that does the single MPC step, that is, it solves the QP for the given initial state, input, and reference trajectory, and returns the optimal control input.\n\nYour solution should be contained in a single file named hw.jl, which you will upload to the BRUTE system. You should use the COSMO.jl package for solving the QP.\n\nusing LinearAlgebra, SparseArrays, COSMO\n\n# You might find these other packages useful \n# using ToeplitzMatrices, BlockArrays\n\nmutable struct MPCProblem{T &lt;: AbstractFloat}\n    model::COSMO.Workspace{T}\n    F::Matrix{T}\n    W::Vector{T}\n    S::Matrix{T}\nend\n\n\"\"\"\nSets up the necessary matrices for a tracking Model Predictive Controller (MPC).\n\nGiven a discrete-time linear system with state-space representation:\n    x(k+1) = A*x(k) + B*u(k)\n    y(k) = C*x(k)\n\nThis function constructs the required matrices to reformulate the MPC problem as a quadratic program (QP):\n\n    min_z  (1/2) * z' * H * z + [x_t', u_(t-1)', r_(t+1:t+N)'] * F * z\n    subject to: G * z ≤ W + S * [x_t, u_(t-1)]\n\nwhere:\n    - z = [Δu_t; ...; Δu_(t+N-1)] (control input changes over the horizon)\n    - x_t: current state\n    - u_(t-1): most recently applied input\n    - r_(t+1:t+N): reference trajectory\n\nArguments:\n    - A, B, C: System matrices defining dynamics and output equations\n    - Q: State tracking cost matrix\n    - R: Control input cost matrix\n    - N: Prediction horizon length\n    - u_min, u_max: Input constraints\n\nReturns:\n    A dictionary containing the constructed QP matrices:\n    - H: Quadratic cost matrix\n    - F: Linear cost term matrix\n    - S: Constraint matrix for state and past input\n    - W: Constraint bounds vector\n    - G: Inequality constraint matrix\n\"\"\"\nfunction setup_mpc(A::Matrix{T}, B::Matrix{T}, C::Matrix{T}, Q::Matrix{T}, R::Matrix{T}, N::Int, u_min::Vector{T}, u_max::Vector{T}) where T &lt;: AbstractFloat\n\n    nx = size(A, 1)\n    nu = size(B, 2)\n    ny = size(C, 1)\n\n\n    # TODO Construct the matrices H, F, S, W, and G\n    H = zeros(T, N * nu, N * nu)\n    F = zeros(T, nx + nu + N * ny, N * nu)\n    S = zeros(T, 2N * nu, nx + nu)\n    W = zeros(T, 2N * nu)\n    G = zeros(T, 2N * nu, N * nu) \n   \n\n    return H, F, S, W, G\nend\n\n\"\"\"\nConstructs a tracking Model Predictive Controller (MPC) problem.\n\nArguments:\n    - A, B, C: System matrices defining dynamics and output equations\n    - Q: State tracking cost matrix\n    - R: Control input cost matrix\n    - N: Prediction horizon length\n    - u_min, u_max: Input constraints\n\nReturns:\n    An instance of `MPCProblem` containing the COSMO model and the matrices F, W, and S.\n\"\"\"\nfunction MPCProblem(A::Matrix{T}, B::Matrix{T}, C::Matrix{T}, Q::Matrix{T}, R::Matrix{T}, N::Int, u_min::Vector{T}, u_max::Vector{T}) where T &lt;: AbstractFloat\n    H, F, S, W, G = setup_mpc(A, B, C, Q, R, N, u_min, u_max)\n\n    model = COSMO.Model{T}() # COSMO model\n\n    ## We use COSMO with the following QP formulation:\n    ## min 1/2 x' * P * x + q' * x\n    ## s.t. A_constr * x - b_constr ≥ 0\n\n    # Dummy variables - just for illustration and to provide correct stuff to COSMO\n    x₀ = zeros(T, size(A, 1))\n    r = zeros(T, N * size(C, 1))\n    u₀ = zeros(T, size(B, 2)) \n\n    q = [x₀; u₀; r]' * F \n\n    P = sparse(H)\n\n    A_constr = -G \n    b_constr = W + S * [x₀; u₀] \n    \n    constr = COSMO.Constraint(A_constr, b_constr, COSMO.Nonnegatives);\n\n    COSMO.assemble!(model, sparse(H), q, constr, settings = COSMO.Settings(verbose=true)) # Assemble the QP\n\n    return MPCProblem(model, F, W, S)\nend\n\n\"\"\"\nSolves the MPC problem for the given initial state, input, and reference trajectory.\n\nArguments:\n    - mpc: An instance of `MPCProblem` containing the COSMO model and the matrices F, W, and S.\n    - xₖ: The current state vector.\n    - uₖ₋₁: The most recently applied input vector.\n    - r: The reference trajectory vector.\n\nReturns:\n    The optimal control input vector at the current time step.\n\"\"\"\nfunction solve!(mpc::MPCProblem{T}, xₖ::Vector{T}, uₖ₋₁::Vector{T}, r::Matrix{T}) where T &lt;: AbstractFloat\n\n    # TODO implement the updates for the COSMO model (q and b_constr) \n    q = zeros(T, size(mpc.F, 2))\n    b_constr = zeros(T, size(mpc.S, 1))\n\n    COSMO.update!(mpc.model, q = q, b = b_constr)\n\n    result = COSMO.optimize!(mpc.model)\n\n    # TODO Extract the optimal control input and return it\n    Δu_opt = result.x\n \n    return zeros(T, size(uₖ₋₁, 1))\nend\n\nYou should test your implementation using the following example we prepared for you.\n\nusing ControlSystemsBase\nusing Plots\nAc = [-.0151 -60.5651 0 -32.174;\n     -.0001 -1.3411 .9929 0;\n     .00018 43.2541 -.86939 0;\n      0      0       1      0];\n\nBc = [-2.516 -13.136;\n     -.1689 -.2514;\n     -17.251 -1.5766;\n     0        0];\nCc = [0 1 0 0;\n     0 0 0 1];\nDc = [0 0;\n     0 0];\n\nsys=ss(Ac,Bc,Cc,Dc)\n\nTs = .05; # Sampling time\nmodel = c2d(sys,Ts)\n\nA = model.A\nB = model.B\nC = model.C\n\nN = 10 # Prediction horizon\nQ = diagm([10.0, 10.0]); # Tracking weight matrix\nR = Matrix(0.1I, 2, 2); # Input increment weight matrix\n\nu_max = 25.0*[1, 1];\nu_min = -25.0*[1, 1];\n\nmpc = MPCProblem(A, B, C, Q, R, N, u_min, u_max)\n\nTf = 75*Ts;\nt = 0:Ts:Tf; \n\nref = [2*ones(1, length(t)+N); 10*ones(1, length(t)+N)] # Reference trajectory\n\nref[:, Int(round(end/2)):end] ./= 2\n\nxs = zeros(4, length(t)+1)\nus = zeros(2, length(t))\nys = zeros(2, length(t))\n\nfor k = 2:length(t)\n    u = solve!(mpc, xs[:, k], us[:, k-1], ref[:, k:k+N-1])\n    xs[:, k+1] = A*xs[:, k] + B*u\n    ys[:, k] = C*xs[:, k]\n    us[:, k] = u\nend\n\n## Visualize the results\nusing Plots\np1= plot(t, ys[1, :], label=\"y₁\", linetype=:steppre, linewidth=2)\nplot!(t, ys[2, :], label=\"y₂\", linetype=:steppre, linewidth=2)\n\nplot!(t, ref[1, 1:end-N], linestyle=:dash, label=\"r₁\", linetype=:steppre, linewidth=2)\nplot!(t, ref[2, 1:end-N], linestyle=:dash, label=\"r₂\", linetype=:steppre, linewidth=2)\n\nxlabel!(\"Time [s]\")\nylabel!(\"Output\")\n\np2 = plot(t[1:end], us[1, :], label=\"u₁\", linetype=:steppre, linewidth=2)\nplot!(t[1:end], us[2, :], label=\"u₂\", linetype=:steppre, linewidth=2)\n\nplot!(t, u_max[1]*ones(length(t)), linestyle=:dash, label=\"u max\", linetype=:steppre, linewidth=2)\nplot!(t, u_min[1]*ones(length(t)), linestyle=:dash, label=\"u min\", linetype=:steppre, linewidth=2)\n\nxlabel!(\"Time [s]\")\nylabel!(\"Input\")\n\nplot(p1, p2, layout=(2,1), size=(800, 600))\n\n\n\n\n Back to top",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Homework"
    ]
  },
  {
    "objectID": "ext_LTR.html",
    "href": "ext_LTR.html",
    "title": "Loop transfer recovery (LTR)",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Loop transfer recovery (LTR)"
    ]
  },
  {
    "objectID": "dynamic_programming_DDP.html",
    "href": "dynamic_programming_DDP.html",
    "title": "Differential dynamic programming (DDP)",
    "section": "",
    "text": "We consider the standard discrete-time optimal control problem:\n\nWe are given a dynamical system modelled by \\bm x_{k+1} = \\mathbf f(\\bm x_k, \\bm u_k). For simplicity of notation we only consider a time invariant system, extension to the time-varying is straightforward.\nDue to time invariance we consider the time k=0 as the initial time. Specification of the initial state is thus \\bm x_0 = \\mathbf x_\\mathrm{init}.\nOn the time horizon [0,N] we now search for the optimal control trajectory (\\bm u_k)_{k=0}^{N-1} such that the cost function J_0\\left(\\bm x_0, (\\bm u_k)_{k=0}^{N-1}\\right) = \\phi(\\bm x_N) + \\sum_{k=0}^{N-1} L(\\bm x_k, \\bm u_k) is minimized.\nConstraints on the state at the final time can be added later, we do not consider them in this initial study.\n\nWe are already aware of the curse of dimensionality of the standard dynamic programming as it calls for evaluating the optimal cost (to go) J_k^\\star(\\bm x_k) at a grid of the state space and the time interval. Differential dynamic programming (DDP) is a way to relieve the computational burden by restricting the considered set of states to a neighborhood of some given state and control trajectories. It is only in this neighbourhood that an improving trajectories are search for. The key assumption for the method is that the optimal cost can be approximated by a quadratic function, for which such minimization is trivial.\n\n\n\n\n\n\nSimilarity to iterative algorithms for optimization such as Newton’s method\n\n\n\nWhen searching for the \\min_{\\bm x\\in\\mathbb R^n} f(\\bm x), Newton’s method starts with an initial guess of the solution \\bm x^0, assumes that in its neighbourhood the cost function f(\\bm x_0 + \\bm d) can be approximated by a quadratic function q(\\bm d) = f(\\bm x_0) + \\left(\\nabla f(\\bm x_0)\\right)^\\top \\bm d + \\frac{1}{2}\\bm d^\\top \\nabla^2 f(\\bm x_0) \\bm d, minimizes this quadratic function, which leads to the update of the guess \\bm x^1 = x^0 + \\underbrace{\\arg \\min_{\\bm d\\in\\mathbb R^n} q(\\bm d)}_{\\bm d_0}, that is, \\bm x^1 = \\bm x^0 + \\bm d^0, and repeats.\n\n\nWe start with some initial (guess of the optimal) control trajectory (\\bm u_0^0, \\bm u_1^0, \\ldots, \\bm u_{N-1}^0) \\eqqcolon (\\bm u_k^0)_{k=0}^{N-1}.\nThe system (initially at the fixed \\bm x_0) responds to this control trajectory with the state trajectory (\\bm x_1^0, \\bm x_2^0, \\ldots, \\bm x_{N}^0) \\eqqcolon (\\bm x_k^0)_{k=1}^{N}.\nCorrespondingly, at time k\\in[0,N-1] the cost to go from the state x_k^0 is \nJ_k^0(\\bm x_k^0) \\coloneqq J_k(\\bm x_k^0, (\\bm u_i^0)_{i=k}^{N-1}).\n\nRecall that if the optimal control trajectory (\\bm u_0^\\star, \\bm u_1^\\star, \\ldots, \\bm u_{N-1}^\\star) \\eqqcolon (\\bm u_k^\\star)_{k=0}^{N-1} is applied, the optimal state trajectory (\\bm x_1^\\star, \\bm x_2^\\star, \\ldots, \\bm x_{N}^\\star) \\eqqcolon (\\bm x_k^\\star)_{k=1}^{N} is a consequence. Furthermove, at every time k the cost to go is \nJ_k^\\star(\\bm x_k^\\star) \\eqqcolon J_k(\\bm x_k^\\star, (\\bm u_i^\\star)_{i=k}^{N-1}),\n and it is optimal, of course, by the very principle of dynamic programming.\n\n\n\n\n\n\nUpper index denotes the control used from the given state and time on\n\n\n\nIn the following we will heavily use the upper right index. For example, when writing J_k^\\heartsuit(\\bm x_k), we mean that the control sequence (\\bm u_k^\\heartsuit, \\bm u_{k+1}^\\heartsuit, \\ldots, \\bm u_{N-1}^\\heartsuit) is applied when the system is at state \\bm x_k at time k, that is\nJ_k^\\heartsuit(\\bm x_k) \\coloneqq J_k(\\bm x_k, (\\bm u_k^\\heartsuit, \\bm u_{k+1}^\\heartsuit, \\ldots, \\bm u_{N-1}^\\heartsuit)).\n\n\nWith the nominal control trajectory is applied, it obviously holds that \nJ_k^0(\\bm x_k^0) = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0).\n\\tag{1}\nAssume now a tiny perturbation \\delta \\bm x_k^0 from the state \\bm x_k^0 and \\delta \\bm u_k^0 from the control \\bm u_k^0 at the time k. The motivation for introduction of such perturbation is the same as we had in Newton’s method – trying to decrease the cost function a bit.\nThe previous equation also holds for the perturbed state and control, but note that since the control sequence is different, the optimal cost to go is different too. \nJ_k^1(\\underbrace{\\bm x_k^0+\\delta \\bm x_k^0}_{\\bm x_k^1}) = L(\\bm x_k^0+\\delta \\bm x_k^0,\\underbrace{\\bm u_k^0+\\delta \\bm u_k^0}_{\\bm u_k^1}) + J_{k+1}^1(\\underbrace{\\bm x_{k+1}^0+\\delta \\bm x_{k+1}^0}_{\\bm x_{k+1}^1}),\n\\tag{2} where \n\\delta \\bm x_{k+1}^0 = \\bm f(\\bm x_k^0+\\delta \\bm x_k^0,\\bm u_k^0+\\delta \\bm u_k^0) - \\bm x_{k+1}^0.\n\nWe now aim at approximating the cost to go by a quadratic function using the first three terms of Taylor’s series. We will need the first and second derivatives of the terms in the above equation. Let’s start with the first derivatives. When differentiating J_{k+1}^1(\\bm x_{k+1}^0) with respect to \\bm x_k^0, we need to invoke the chain rule as \\bm x_{k+1}^0 = \\bm f(\\bm x_k^0, \\bm u_k^0).\n\n\n\n\n\n\nChain rule for the first derivative of composed functions\n\n\n\nConsider the composed function h(\\bm x)\\coloneqq f(\\mathbf g(\\bm x)). Its first derivative is \n\\mathrm{D} h(\\bm x) = \\left.\\mathrm{D}g(\\bm y)\\right|_{\\bm y=\\mathbf f(\\bm x)}\\mathrm{D} \\mathbf f(\\bm x),\n where \\mathrm{D}g(\\bm y) is a row vector of derivatives, and \\mathrm{D} \\mathbf f(\\bm x) is a (Jacobian) matrix \n\\mathrm D\\mathbf f(\\bm x) =\n\\begin{bmatrix}\n\\frac{\\partial f_1(\\bm x)}{\\partial x_1} & \\frac{\\partial f_1(\\bm x)}{\\partial x_2} & \\ldots & \\frac{\\partial f_1(\\bm x)}{\\partial x_n}\\\\\n\\frac{\\partial f_2(\\bm x)}{\\partial x_1} & \\frac{\\partial f_2(\\bm x)}{\\partial x_2} & \\ldots & \\frac{\\partial f_2(\\bm x)}{\\partial x_n}\\\\\n\\vdots\\\\\n\\frac{\\partial f_m(\\bm x)}{\\partial x_1} & \\frac{\\partial f_m(\\bm x)}{\\partial x_2} & \\ldots & \\frac{\\partial f_m(\\bm x)}{\\partial x_n}\n\\end{bmatrix}.\n\nOr, if formatted as the gradient, which we prefer in our course, we can write \\boxed\n{\\nabla h(\\bm x) = \\nabla \\mathbf f(x) \\left.\\nabla g(\\bm y)\\right|_{\\bm y = \\mathbf f(\\bm x)},}\n where \n\\nabla \\mathbf f(x) = \\begin{bmatrix} \\nabla f_1(\\bm x) & \\nabla f_2(\\bm x) & \\ldots & \\nabla f_m(\\bm x)\\end{bmatrix}.\n\n\n\nApplying this to obtain derivatives of J_{k+1}^1\\left(\\bm f(\\bm x_k^0, \\bm u_k^0)\\right) with respect to \\bm x_k gives \n\\mathrm{D}_{\\bm x_{k}}J_{k+1}^1\\left(\\bm f(\\bm x_k^0, \\bm u_k^0)\\right) = \\left.\\mathrm{D}_{\\bm x_{k+1}}J_{k+1}^1\\left(\\bm x_{k+1}^0\\right)\\right|_{\\bm x_{k+1}^0 = \\mathbf f(\\bm x_k^0,\\bm u_k^0)} \\mathrm{D}_{\\bm x_k} \\bm f(\\bm x_k^0, \\bm u_k^0),\n or, using gradients \\boxed\n{\\nabla_{\\bm x_{k}}J_{k+1}^1\\left(\\bm f(\\bm x_k^0, \\bm u_k^0)\\right) = \\nabla_{\\bm x_k} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\left.\\nabla_{\\bm x_{k+1}}J_{k+1}^1\\left(\\bm x_{k+1}^0\\right)\\right|_{\\bm x_{k+1}^0 = \\mathbf f(\\bm x_k^0,\\bm u_k^0)}.}\n\nAnd similarly for the derivatives with respect to \\bm u_k.\n\n\n\n\n\n\nChain rule for the second derivative of composed functions\n\n\n\nWe consider (again) the composed function h(\\bm x)\\coloneqq g(\\mathbf f(\\bm x)). Referring to the previous box with the expressions for the first derivative, the second derivative must obviously invoke the rule for differentiation of a product. The Hessian is \n  \\mathrm H h(\\bm x) = \\left[\\mathrm{D}\\mathbf f(\\bm x)\\right]^\\top \\left.\\mathrm H g(\\bm y)\\right|_{\\bm y=\\mathbf f(\\bm x)}\\mathrm{D}\\mathbf f(\\bm x) + \\sum_{k=1}^m \\left.\\frac{\\partial g(\\bm y)}{\\partial y_k}\\right|_{\\bm y = \\mathbf f(\\bm x)} \\mathrm{H} f_k(\\bm x).\n\nThis is based on Magnus and Neudecker (2019), Theorem 6.8 on page 122. It is worth emphasizing that in the second term we form the Hessians (that is, matrices) for each component f_i(\\bm x) of the vector function \\mathbf f(\\bm x). This way we avoid the need for anything like a three-dimensional (tensor) version of Hessians and we can stick to matrices and vectors.\nSince in our course we prefer working with gradients, we present the result in the gradient form \\boxed\n{  \\nabla^2 h(\\bm x) = \\nabla \\mathbf f(\\bm x) \\left.\\nabla^2 g(\\bm y)\\right|_{\\bm y=\\mathbf f(\\bm x)}\\left[\\nabla\\mathbf f(\\bm x)\\right]^\\top + \\sum_{k=1}^m \\left.\\frac{\\partial g(\\bm y)}{\\partial y_k}\\right|_{\\bm y = \\mathbf f(\\bm x)} \\nabla^2 f_k(\\bm x).}\n\n\n\nApplying this to obtain the second derivatives of J_{k+1}^1\\left(\\bm f(\\bm x_k^0, \\bm u_k^0)\\right) with respect to \\bm x_k gives \n\\begin{aligned}\n\\nabla_{\\bm{xx}_k}^2 J_{k+1}^1\\left(\\bm f(\\bm x_k^0, \\bm u_k^0)\\right) &= \\nabla_{\\bm x_k} \\mathbf f(\\bm x_k^0, \\bm u_k^0) \\left.\\nabla_{\\bm{xx}_{k+1}}^2 J_{k+1}^1 (\\bm x_{k+1}^0) \\right|_{\\bm x_{k+1}^0 = \\mathbf f(\\bm x_k^0,\\bm u_k^0)}\\left[\\nabla_{\\bm x_k} \\mathbf f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top\\\\\n&+ \\sum_{i=1}^m \\left.\\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}}\\right|_{\\bm x_{k+1}^0 = \\mathbf f(\\bm x_k^0,\\bm u_k^0)} \\nabla_{\\bm{xx}_k}^2 f_i(\\bm x_k^0, \\bm u_k^0).\n\\end{aligned}\n\nWith this preparation, we can go for expanding the Equation 2. Strictly speaking, when keeping only the first three terms of the Taylor series on both sides, the quality no longer holds. But we will enforce it as a means of approximation.\n\n\\begin{aligned}\nJ_k^1(\\bm x_k^1) + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^1(\\bm x_{k+1}^0)\\\\\n+\\left[\\nabla_{\\bm x} L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right]^\\top \\,\\delta \\bm x_k^0 \\\\\n+\\left[\\nabla_{\\bm u} L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right]^\\top \\,\\delta \\bm u_k^0 \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\left[\\nabla_{\\bm x\\bm x}^2 L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{xx}}^2 f_i(\\bm x_k^0, \\bm u_k^0) \\right] \\,\\delta \\bm x_k^0\\\\\n+ \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\left[\\nabla_{\\bm u\\bm u}^2 L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{uu}}^2 f_i(\\bm x_k^0, \\bm u_k^0) \\right]  \\,\\delta \\bm u_k^0\\\\\n+ [\\delta \\bm x_k^0]^\\top \\left[\\nabla_{\\bm x\\bm u}^2 L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{xu}}^2 f_i(\\bm x_k^0, \\bm u_k^0) \\right] \\,\\delta \\bm u_k^0\n\\end{aligned}\n\nIn order to tame the complexity of the expressions a bit we introduce the auxiliary function – the discrete-time Hamiltonian \\boxed\n{H(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}) = L(\\bm x_k, \\bm u_k) + \\bm \\lambda_{k+1}^\\top \\bm f(\\bm x_k, \\bm u_k).}\n\nThe gradient of the Hamiltonian with respect to \\bm x_k is \\boxed\n{\\nabla_{\\bm x} H(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}) = \\nabla_{\\bm x} L(\\bm x_k, \\bm u_k) + \\nabla_{\\bm x} \\bm f(\\bm x_k, \\bm u_k) \\, \\bm \\lambda_{k+1}.}\n\nIf we evaluate \\bm \\lambda_{k+1} at \\nabla J_{k+1}^1(\\bm x_{k+1}^0), and the state and the control at \\bm x_k^0 and \\bm u_k^0, respectively, we get \n\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) = \\nabla_{\\bm x} L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla J_{k+1}^1(\\bm x_{k+1}^0),\n which can be used to simplify the second row in the above equation. Similarly, \n\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) = \\nabla_{\\bm u} L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla J_{k+1}^1(\\bm x_{k+1}^0),\n which can be used to simplify the third row in the above equation.\nThe matrix of second derivatives – the Hessian – of the Hamiltonian is \\boxed\n{\\nabla_{\\bm x\\bm x}^2 H(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}) = \\nabla_{\\bm x\\bm x}^2 L(\\bm x_k, \\bm u_k) + \\sum_{i=1}^m \\lambda_{k+1,i} \\nabla_{\\bm{xx}}^2 f_i(\\bm x_k^0, \\bm u_k^0).}\n\nEvaluating \\bm \\lambda_{k+1} at \\nabla J_{k+1}^1(\\bm x_{k+1}^0) and the state and the control at \\bm x_k^0 and \\bm u_k^0, respectively, we get \n\\nabla_{\\bm x \\bm x}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) = \\nabla_{\\bm x \\bm x}^2 L(\\bm x_k^0, \\bm u_k^0) + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{xx}}^2 f_i(\\bm x_k^0, \\bm u_k^0),\n and similarly \n\\nabla_{\\bm u \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) = \\nabla_{\\bm u \\bm u}^2 L(\\bm x_k^0, \\bm u_k^0) + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{uu}}^2 f_i(\\bm x_k^0, \\bm u_k^0),\n and \n\\nabla_{\\bm x \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) = \\nabla_{\\bm x \\bm u}^2 L(\\bm x_k^0, \\bm u_k^0) + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{xu}}^2 f_i(\\bm x_k^0, \\bm u_k^0).\n\nAll these can be used to simplify the above set of equations to \\boxed\n{\\begin{aligned}\nJ_k^1(\\bm x_k^0) + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^1(\\bm x_{k+1}^0)\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm x_k^0 \\\\\n+\\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm u_k^0 \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\left[\\nabla_{\\bm x \\bm x}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right] \\,\\delta \\bm x_k^0\\\\\n+ \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\left[\\nabla_{\\bm u \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right]  \\,\\delta \\bm u_k^0\\\\\n+ [\\delta \\bm x_k^0]^\\top \\left[\\nabla_{\\bm x \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right] \\,\\delta \\bm u_k^0\n\\end{aligned}}\n\nOne more modification is needed. When \\delta \\bm x_k = 0, \\bm x_k^0 = \\bm x_k^1, and yet J_k^0(\\bm x_k^0) \\neq J_k^1(\\bm x_k^1) in general, because the two assume different control from the time k onward. We denote the offset as \na_k = J_k^1(\\bm x_k^0) - J_k^0(\\bm x_k^0).\n\nBut then \nJ_k^1(\\bm x_k^0) = J_k^0(\\bm x_k^0) + a_k,\n which we substitute into the above boxed multiline equation. And while we are at it, let’s also label the long terms with shorter labels: \n\\begin{aligned}\nJ_k^{\\color{red}0}(\\bm x_k^0) + {\\color{red}a_k} + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^{\\color{red}0}(\\bm x_{k+1}^0) + {\\color{red}a_{k+1}}\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm x_k^0 \\\\\n+\\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm u_k^0 \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\underbrace{\\left[\\nabla_{\\bm x \\bm x}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right]}_{\\mathbf A_k} \\,\\delta \\bm x_k^0\\\\\n+ \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\underbrace{\\left[\\nabla_{\\bm u \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right]}_{\\mathbf C_k}  \\,\\delta \\bm u_k^0\\\\\n+ [\\delta \\bm x_k^0]^\\top \\underbrace{\\left[\\nabla_{\\bm x \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right]}_{\\mathbf B_k} \\,\\delta \\bm u_k^0\n\\end{aligned}\n\nThe fact that we do not know a_k and a_{k+1} does not have to worry us at this moment. We will figure it out later.\nFor convenience, we rewrite the above equation in the shortened form with the new symbols \\boxed\n{\\begin{aligned}\nJ_k^{0}(\\bm x_k^0) + a_k + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0) + a_{k+1}\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm x_k^0 + \\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm u_k^0 \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\mathbf A_k \\,\\delta \\bm x_k^0 + [\\delta \\bm x_k^0]^\\top \\mathbf B_k \\,\\delta \\bm u_k^0 + \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\mathbf C_k  \\,\\delta \\bm u_k^0.\n\\end{aligned}}\n\\tag{3}\nWe now apply the principle of optimality (the principle of dynamic programming) to the approximation of Equation 2 \nJ_k^1(\\bm x_k^1) = \\min_{\\delta u_k^0} \\left[L(\\bm x_k^0+\\delta \\bm x_k^0,\\bm u_k^0+\\delta \\bm u_k^0) + J_{k+1}^1(\\bm x_{k+1}^1)\\right]\n by the just derived quadratic model \n\\begin{aligned}\nJ_k^{0}(\\bm x_k^0) + a_k + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = {\\color{red}\\min_{\\delta \\bm u_k^0}}\\left[L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0) + a_{k+1}\\right.\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm x_k^0 + \\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm u_k^0 \\\\\n\\left.+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\mathbf A_k \\,\\delta \\bm x_k^0 + [\\delta \\bm x_k^0]^\\top \\mathbf B_k \\,\\delta \\bm u_k^0 + \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\mathbf C_k  \\,\\delta \\bm u_k^0\\right].\n\\end{aligned}\n\nSimplifying the minimized term on the right hand side by dropping the terms that do not depend on \\delta \\bm u_k^0 gives \n\\delta \\bm u_k^0 = \\arg \\min_{\\delta \\bm u_k^0} \\left[\\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm u_k^0 + [\\delta \\bm x_k^0]^\\top \\mathbf B_k \\,\\delta \\bm u_k^0 + \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\mathbf C_k  \\,\\delta \\bm u_k^0. \\right]\n\n\n\n\n\n\n\nWarning\n\n\n\nThe notation is a bit awkward. So far \\bm u_k^0 was understood as some perturbation, but now on the left hand side we consider a particular one – the one that is optimal. Shall we add some other superscript to distinguish these? Perhaps something like \\delta \\bm u_k^{0\\star}? We prefer not to clutter the notation and just declare that \\delta \\bm u_k^0 is the optimal control perturbation.\n\n\n\nNo constraints on the control\nIf no constraints are imposed on the control, the optimal control perturbation can be found analytically by setting the gradient of the above expression to zero, that is\n\n\\nabla_{\\bm u_k}H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\mathbf B_k^\\top \\delta \\bm x_k^0 + \\mathbf C_k  \\,\\delta \\bm u_k^0 = 0,\n from which it follows that the optimizing control perturbation is \n\\delta \\bm u_k^0 = \\underbrace{-\\mathbf C_k^{-1} \\mathbf B_k^\\top}_{\\color{blue}\\mathbf K_k} \\delta \\bm x_k^0 \\underbrace{- \\mathbf C_k^{-1}\\nabla_{\\bm u_k}H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)}_{\\color{blue}\\mathbf k_k},\n where \n\\delta \\bm x_k^0 = \\bm x_k^1 - \\bm x_k^0.\n\nThe major conclusion now is that the perturbation control is generated in a feedback manner using an affine control law (or policy) as \\boxed\n{\\delta \\bm u_k^0 = {\\color{blue}\\mathbf K_k} \\delta \\bm x_k^0 + {\\color{blue}\\mathbf k_k}.}\n\\tag{4}\nThe full control sequence \\left(\\bm u_k^1\\right)_{k=0}^{N-1} is then\n\n\\bm u_k^1 = \\bm u_k^0 + \\delta \\bm u_k^0.\n\nNow we substitute the affine feedback control law given by Equation 4 into Equation 3 for the optimal cost-to-go approximation. We get \n\\begin{aligned}\nJ_k^{0}(\\bm x_k^0) + a_k + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0) + a_{k+1}\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm x_k^0 + \\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,[\\mathbf K_k \\delta \\bm x_k^0 + \\mathbf k_k] \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\mathbf A_k \\,\\delta \\bm x_k^0 + [\\delta \\bm x_k^0]^\\top \\mathbf B_k \\,[\\mathbf K_k \\delta \\bm x_k^0 + \\mathbf k_k] + \\frac{1}{2}[\\mathbf K_k \\delta \\bm x_k^0 + \\mathbf k_k]^\\top \\mathbf C_k  \\,[\\mathbf K_k \\delta \\bm x_k^0 + \\mathbf k_k].\n\\end{aligned},\n\nwhich after combining the terms with the equal powers of \\delta \\bm x_k^0 transforms to \n\\begin{aligned}\nJ_k^{0}(\\bm x_k^0) + a_k + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 \\\\\n= L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0) + a_{k+1} + \\left[H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\mathbf k_k + \\frac{1}{2} \\mathbf k_k^\\top \\mathbf C_k \\,\\mathbf k_k\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\mathbf K_k^\\top \\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\mathbf k_k^\\top\\mathbf B_k^\\top + \\mathbf K_k^\\top \\mathbf C_k \\mathbf k_k\\right]^\\top \\,\\delta \\bm x_k^0 \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\left[\\mathbf A_k + 2\\mathbf B_k \\,\\mathbf K_k + \\mathbf K_k^\\top \\mathbf C_k \\mathbf K_k\\right]\\,\\delta \\bm x_k^0.\n\\end{aligned}.\n\nSetting equal the terms corresponding to the same powers of \\delta \\bm x_k^0 on both sides, while also recalling Equation 1, which we restate here \nJ_k^{0}(\\bm x_k^0) = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0),\n gives the following equations \\boxed\n{\\begin{aligned}\na_k &= a_{k+1} + \\left[H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\mathbf k_k + \\frac{1}{2} \\mathbf k_k^\\top \\mathbf C_k \\,\\mathbf k_k,\\\\\n\\nabla J_k^1(\\bm x_k^0) &= \\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\mathbf K_k^\\top \\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\mathbf k_k^\\top\\mathbf B_k^\\top + \\mathbf K_k^\\top \\mathbf C_k \\mathbf k_k,\\\\\n\\nabla^2 J_k^1(\\bm x_k^0) &= \\mathbf A_k + \\mathbf B_k\\,\\mathbf K_k + \\mathbf K_k^\\top \\mathbf B_k^\\top + \\mathbf K_k^\\top \\mathbf C_k \\mathbf K_k,\n\\end{aligned}}\n where in the last line we symmetrized by 2\\mathbf B_k\\,\\mathbf K_k = \\mathbf B_k\\,\\mathbf K_k + \\mathbf K_k^\\top \\mathbf B_k^\\top.\nThese are three backwards running difference equations. They are initialized with the boundary conditions at the final time \\boxed\n{\\begin{aligned}\na_N &= 0,\\\\\n\\nabla J_N^1(\\bm x_N^0) &= \\nabla \\phi(\\bm x_N^0), \\\\\n\\nabla^2 J_N^1(\\bm x_N^0) &= \\nabla^2 \\phi(\\bm x_N^0).\n\\end{aligned}}\n\n\n\n\n\n\n Back to topReferences\n\nMagnus, Jan R., and Heinz Neudecker. 2019. Matrix Differential Calculus with Applications in Statistics and Econometrics. 3rd ed. Wiley Series in Probability and Statistics. Hoboken, NJ: Wiley. https://onlinelibrary.wiley.com/doi/book/10.1002/9781119541219.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Differential dynamic programming (DDP)"
    ]
  },
  {
    "objectID": "cont_indir_Pontryagin.html",
    "href": "cont_indir_Pontryagin.html",
    "title": "Pontryagin’s maximum (or minimum) principle",
    "section": "",
    "text": "The techniques of calculus of variations introduced in the previous lecture/chapter significantly extended our toolset for solving optimal control problems – instead of optimizing over (finite) sequences of real numbers we are now able to optimize over functions (or continuous-time trajectories if you like). Nonetheless, the methods were also subject to severe restrictions. For example, the property that the optimal control maximizes the Hamiltonian were checked by testing the derivative of the Hamiltonian, which only makes sense if\nIt also appears that the classes of perturbations modelled by \\mathcal C^1_{[a,b]} spaces of smooth functions defined on an interval and endowed with 1-norm or 0-norm are not rich enough for practical applications. Just consider switched (on-off) control trajectories that differ in the times of switching.\nFor all these reasons, some more advanced mathematical machinery has been developed. Unfortunately, it calls for a different and a bit more advanced mathematics than what we used in the calculus of variations. Therefore we will only state the most important result – the powerful Pontryagin’s principle of maximum (PMP).\nAlthough PMP sort of supersedes some of the previous results (and you might even start complaining why on earth we spent time with calculus of variations), thanks to having been introduced to the calculus of variations-style of reasoning, we are now certainly well-equipped to digest at least the very statement of the powerful result by Pontryagin. A motivated (and courageous) reader will find a proof elsewhere (see the recommended literature).",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Pontryagin's maximum (or minimum) principle"
    ]
  },
  {
    "objectID": "cont_indir_Pontryagin.html#pontryagins-principle-of-maximum",
    "href": "cont_indir_Pontryagin.html#pontryagins-principle-of-maximum",
    "title": "Pontryagin’s maximum (or minimum) principle",
    "section": "Pontryagin’s principle of maximum",
    "text": "Pontryagin’s principle of maximum\nWe have already seen in the calculus of variations that the Hamiltonian when evaluated along the extremal, has the property that \n\\left.\\frac{\\partial H(x,y,y',p)}{\\partial y'}\\right|_{p = \\frac{\\partial L(x,y,y')}{\\partial y'}} = 0.\n\\tag{1}\nCombining this result with the second-order necessary condition of minimum\n\nL_{y'y'} \\geq 0,\n we concluded that Hamiltonian is not only stationary on the extremal, it is actually maximized on the extremal since \nH_{y'y'} = -L_{y'y'} \\leq 0.\n\nThis result can be written as \nH(x,y^\\star,(y^\\star)',p^\\star) \\geq  H(x,y^\\star,y',p^\\star).\n\nThis is a major observation and would probably never be obtained without viewing y' as a separate variable (see [1] for an insightful discussion).\nAfter the (now surely familiar) notational transition to the optimal control setting (t instead of x, \\bm x(\\cdot) instead of y(\\cdot), \\bm u(\\cdot) instead of y'(\\cdot), \\bm \\lambda(\\cdot) instead of p(\\cdot)), and invoking the definition of the Hamiltonian \nH(\\bm x ,\\bm u ,\\bm \\lambda) = \\bm \\lambda^\\top \\, \\mathbf f- L,\n we make an important observation that as \\bm u now plays the role of y' in calculus of variations, the optimal control analog of Eq. 1 is given by the equation of stationarity \n\\nabla_{\\bm u} H(\\bm x ,\\bm u ,\\bm \\lambda) = \\nabla_{\\bm u}\\mathbf f \\, \\bm \\lambda - \\nabla_{\\bm u} L = 0.\n\nCombined with the second-order necessary condition of minimum \n  \\nabla_{\\bm u \\bm u}L(\\bm x ,\\bm u) \\succeq 0,\n which reads that the Hessian of the integrand L with respect to \\bm u is positive semi-definite, we conclude that the Hamiltonian is not only stationary on the extremal, it is actually maximized on the extremal. If we now denote the set of all allowable controls as \\mathcal{U}, the result on maximization of Hamiltonian can then be written as \\boxed{\nH(\\bm x^\\star ,\\bm u^\\star ,\\boldsymbol\\lambda^\\star ) \\geq  H(\\bm x^\\star ,{\\color{blue}\\bm u} , \\boldsymbol\\lambda^\\star ),\\quad \\forall {\\color{blue}\\bm u} \\in\\mathcal{U},}\n\\tag{2} where \\bm x^\\star is the optimal state trajectory, \\bm u^\\star is the optimal control, and \\boldsymbol\\lambda^\\star is the costate.\nwhere \\bm u^\\star is the optimal control. or, equivalently (and emphasizing that it holds at every time t) as \\boxed{\n\\bm u^\\star(t)  = \\operatorname*{argmax}_{\\color{blue}\\bm u(t)  \\in\\mathcal{U}} H(\\bm x^\\star(t) ,{\\color{blue}\\bm u(t)}, \\boldsymbol\\lambda^\\star(t)).}\n\\tag{3}\nThe essence of the celebrated Pontryagin’s principle is that actually the maximization of the Hamiltonian that is the fundamental necessary condition of optimality. The fact that \n\\nabla_{\\bm u}H = 0\n is just a consequence in the situation when \\nabla_{\\bm u}H exists and the set of allowable controls u is not bounded.\nAnd that is it!\nTo summarize, the celebrated Pontryagin’s principle of maximum just replaces the equation of stationarity \\nabla_{\\bm u} H = \\bm 0 in the necessary conditions of optimality by Eq. 2 or Eq. 3. Let’s now summarize the necessary conditions of optimality in the form of a theorem for later reference\n\nTheorem 1 (Pontryagin’s principle of maximum) For a given optimal control problem, let \\bm u^\\star \\in \\mathcal{U} be an optimal control, then there is a variable called costate which together with the state variable satisfies the Hamilton canonical equations \n\\begin{aligned}\n\\dot{\\bm x} &= \\left.\\nabla_{\\bm \\lambda}H\\right|_{\\bm x = \\bm x^\\star, \\bm u = \\bm u^\\star, \\bm \\lambda = \\bm \\lambda^\\star},\\\\\n\\dot{\\bm \\lambda} &= \\left.-\\nabla_{\\bm{x}}H\\right|_{\\bm x = \\bm x^\\star, \\bm u = \\bm u^\\star, \\bm \\lambda = \\bm \\lambda^\\star},\\\\\nH(\\bm x^\\star ,\\bm u^\\star ,\\boldsymbol\\lambda^\\star, t) &\\geq  H(\\bm x^\\star ,{\\color{blue}\\bm u} , \\boldsymbol\\lambda^\\star, t), \\; {\\color{blue}\\bm u} \\in \\mathcal{U},\n\\end{aligned}\n where \nH(\\bm x ,\\bm u ,\\boldsymbol\\lambda, t) = \\boldsymbol\\lambda^\\top(t)\\, \\mathbf f(\\bm x,\\bm u, t) - L(\\bm x,\\bm u,t).\n\nMoreover, the corresponding boundary conditions must hold.\n\nIn other words, Pontryagin’s principle just replaces the equation of stationarity \\nabla_{\\bm u} H = \\bm 0 in the necessary conditions of optimality by Eq. 2 or Eq. 3.\nAlthough we opt to skip the proof of the theorem in our course (it is really rather advanced), we must certainly at least mention that it does not relly on the smoothness of the function as the calculus of variations did. Less regular function are allowed. In particular, the set of admissible control trajectories \\bm u(\\cdot) also contains piecewise continuous functions (technically speaking, the set contains measurable functions). This can also be used as at least a partial justification of our hand-wavy approach to introducing the calculus of variations.\n\n\n\n\n\n\nPontryagin’s principle of minimum\n\n\n\nIf we used the alternative definition of the Hamiltonian \nH(\\bm x ,\\bm u ,\\bm \\lambda, t) = L(\\bm x,\\bm u,t) + \\bm \\lambda^\\top \\, \\mathbf f(\\bm x,\\bm u,t),\n then the Hessian of the Hamiltonian with respect to \\bm u evaluated on the extremal would be positive semi-definite, and the necessary condition of optimality would be that the Hamiltonian is minimized on the extremal. We would then call the result Pontryagin’s principle of minimum. While our guess is that the this “minimum version” of Pontryagin’s principle prevails in the literature (perhaps to emphasize its striking similarity with the continuous-time version of dynamica programming – HJB equation), the originally published result uses maximum of Hamiltonian.\n\n\nWe could certainly rederive our previous results on LQ-optimal control with fixed and free final states. Nonetheless, this would be an overkill unless constraints are imposed on the controls. We consider such constrained LQR case below.\n\nExample 1 (Constrained LQR) We consider a system modelled by \\dot{\\bm x} = \\mathbf A \\bm x + \\mathbf b u, in which we restrict ourselves to scalar control inputs for convenience. The task is to find a control u on the fixed interval [0,t_\\mathrm{f}] such that the system is brought from a given initial state to a given final state, while the control satisfies \\mathrm{u}^{\\min}\\leq u\\leq \\mathrm{u}^{\\max}. The two-point boundary value problem is \n\\begin{aligned}\n\\dot{\\bm{x}} &= \\mathbf A\\bm x + \\mathbf b u,\\\\\n\\dot{\\bm{\\lambda}} &= \\mathbf Q\\bm x - \\mathbf A^\\top\\bm\\lambda,\\\\\nu^\\star &= \\operatorname*{arg max}_{u\\in[\\mathrm u^{\\min},\\mathrm u^{\\max}]}\\cancel{-\\frac{1}{2}\\bm x^\\top\\mathbf Q\\bm x} - \\frac{\\mathbf r}{2}u^2 + \\bm\\lambda^\\top (\\cancel{\\mathbf A \\bm x} + \\mathbf b u),\\\\\n\\bm x(0) &= \\mathbf x_0,\\\\\n\\bm x(t_{\\mathrm{f}}) &= \\mathbf x_{\\mathrm{f}}.\n\\end{aligned}\n\nIn the maximization above we could cancel the two terms that do not depend on u. It is just a maximization of a quadratic (in u) function with an interval constraint. We can write its solution as \n\\begin{aligned}\nu^\\star &= \\operatorname*{argmax}_{u\\in[\\mathrm u^{\\min},\\mathrm u^{\\max}]} \\left(\\bm\\lambda^\\top \\mathbf b u - \\frac{1}{2}\\mathrm{r} u^2\\right)\\\\\n&= \\operatorname{sat}_{\\mathrm u^{\\min}}^{\\mathrm u^{\\max}}\\frac{\\bm\\lambda^\\top \\mathbf b}{\\mathrm{r}},\n\\end{aligned}\n where the \\operatorname{sat}() has the usual meaning of a saturation function with the lower and upper saturation bounds.\nThe BVP problem that needs to be solved is then \n\\begin{aligned}\n\\dot{\\bm{x}} &= \\mathbf A\\bm x + \\mathbf b u,\\\\\n\\dot{\\bm{\\lambda}} &= \\mathbf Q\\bm x - \\mathbf A^\\top\\bm\\lambda,\\\\\nu^\\star &= \\operatorname{sat}_{u^{\\min}}^{u^{\\max}}\\frac{\\bm\\lambda^\\top \\mathbf b}{\\mathrm{r}},\\\\\n\\bm x(0) &= \\mathbf x_0,\\\\\n\\bm x(t_\\mathrm{f}) &= \\mathbf x_\\mathrm{f}.\n\\end{aligned}\n\n\n\nShow the code\nusing DifferentialEquations\nusing LinearAlgebra\nusing Random\nRandom.seed!(1234)\n\nfunction cont_indir_constrained_lqr_via_pontryagin_bvp()\n    n = 2                                   # Order of the system.\n    m = 1                                   # Number of inputs.\n    A = rand(n,n)\n    B = rand(n,m)\n\n    umin = -2.0                             # Lower bound on control.\n    umax = 2.0                              # Upper bound on control.\n\n    q = 1\n    r = 1\n    s = 1\n    Q = diagm(0=&gt;q*ones(n))                 # Weighting matrices for the quadratic cost function.\n    R = diagm(0=&gt;r*ones(m))\n    S₁ = diagm(0=&gt;s*ones(n))\n    t₀ = 0.0\n    t₁ = 10.0\n    xinit = [1.0, 2.0]                      # Initial state.\n    xfinal = [0.0, 0.0]                     # Final state.\n\n    function statecostateeq!(dw, w, p, t)\n        x = w[1:2]                          # State vector [θ, ω].\n        λ = w[3:4]                          # Costate vector.\n        u = min.(max.(R\\B'*λ,umin),umax)    # Optimal control (Pontryagin).\n        dw[1:2] .= A*x + B*u                # State equation.\n        dw[3:4] .= Q*x - A'*λ               # Co-state equation.\n    end\n    function bc!(res, w, p,t)               \n        res[1:2] .= w(t₀)[1:2] - xinit      # Initial state boundary condition.\n        res[3:4] .= w(t₁)[1:2] - xfinal     # Final state boundary condition.\n    end\n\n    w0 = [xinit[1], xinit[2], 0.0, 0.0]     # Use the initial state and guess at the initial costate.\n    \n    bvprob = BVProblem(statecostateeq!, bc!, w0, (t₀, t₁)) # Boundary value problem.\n    sol = solve(bvprob, MIRK4(), dt=0.1)    # Solve the BVP.\n    \n    x = hcat(sol.u...)[1:2,:]               # State trajectory.\n    λ = hcat(sol.u...)[3:4,:]               # Costate trajectory.\n    u = min.(max.(R\\B'*λ,umin),umax)        # Optimal control.\n    t = sol.t\n    return (x,λ,u,t)\nend\n\nx,λ,u,t = cont_indir_constrained_lqr_via_pontryagin_bvp()\n\nusing Plots\np1 = plot(t,x',ylabel=\"x\",label=\"\",lw=2)\np2 = plot(t,λ',ylabel=\"λ\",label=\"\",lw=2)\np3 = plot(t,u',ylabel=\"u\",label=\"\",lw=2,xlabel=\"t\")\n\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that this optimal solution of a constrained LQR problem is not equal to the “clamped” solution of the unconstrained LQR problem. From an implementation viewpoint, unlike in the unconstrained problem, here the solution does not come in the form of a state-feedback controller, but rather in the form of precomputed state and control trajectories.\n\nThe good news is that there are scenarios, in which the Pontryagin’s principle leads to feedback controllers. One of them is the minimum-time problem. The task is to bring the system to a given state in the shortest time possible. Apparently, with no bounds on controls, the time can be shrunk to zero (the control signal approaching the shape of a Dirac impulse). Therefore, bounds must to be imposed on the magnitudes of control signals in order to compute realistic outcomes. Pontryagin’s principle is used for this. Furthermore, we must also know how our necessary conditions change if we relax the final time, that is, if the final time becomes one of the optimization variables. This is what we are going to investigate next.",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Pontryagin's maximum (or minimum) principle"
    ]
  },
  {
    "objectID": "discr_dir_mpc_recursive_feasibility.html",
    "href": "discr_dir_mpc_recursive_feasibility.html",
    "title": "Recursive feasibility",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "Recursive feasibility"
    ]
  },
  {
    "objectID": "cont_indir_calculus_of_variations.html",
    "href": "cont_indir_calculus_of_variations.html",
    "title": "Calculus of variations",
    "section": "",
    "text": "Since in our quest for the optimal control we optimize over functions (trajectories), we can view our optimization as cast in an infinite-dimensional vector space. The mathematical discipline of calculus of variations provides concepts and tools for such optimization. The general task (in a scalar version) is to find\n\\min_{y(x)\\in\\mathcal C^1[a,b]} J(y(x)),\n where we relabelled the variables in the following sense: the optimization is performed over functions y(\\cdot), which are functions of the independent scalar variable x. The reason for this notational choice is that many of the results in calculus of variations were motivated by problems where the independent variable was length or position. This change of notation is reflected in Fig. 1, which shows a few members of the space in which we search for a minimizer.\nHaving specified the optimization variable, we need to talk about the cost function. It is now a function of a function. An established name for such functions is functional.\nIt is now important to reinvoke the very definition of local minimum that we introduced in the lecture on finite-dimensional optimization. The cost function has a local minimum at a given point if there exists some neighbourhood within which all the other points achieve equal or higher value. With our current notation, J attains a local minimum at y^\\star if \nJ(y^\\star) \\leq J(y)\n for all y in some neighbourhood of y^\\star. The neighbourhood is given as a set of all those y for which \n\\|y-y^\\star\\| \\leq \\epsilon.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Calculus of variations"
    ]
  },
  {
    "objectID": "cont_indir_calculus_of_variations.html#variation-variational-derivative-and-first-order-conditions-of-optimality",
    "href": "cont_indir_calculus_of_variations.html#variation-variational-derivative-and-first-order-conditions-of-optimality",
    "title": "Calculus of variations",
    "section": "Variation, variational derivative and first-order conditions of optimality",
    "text": "Variation, variational derivative and first-order conditions of optimality\nSimilarly as in the finite-dimensional optimization, we will build the necessary conditions of optimality by studying how the cost function changes if we perturb the independent variable a bit. Let’s denote the minimizing function as y^\\star. We denote the perturbed function as y and we form it as \ny(x) = y^\\star(x) + \\delta y(x),\n where \\delta y(\\cdot) is variation of function and it is a function itself. It plays the same role in calculus of variations as the term \\mathrm{d}x does in differential calculus.\nRecall that one aproach to deriving the first-order necessary conditions of optimality in the case of vector variables was based on fixing the direction first and then analyzing how the function evolves along this direction. Namely, we considered evolution of the cost function \nf(\\mathbf x^\\star + \\alpha\\, \\mathbf d)\n\\tag{1} for given \\mathbf x^\\star\\in\\mathbb{R}^n and \\mathbf d\\in\\mathbb{R}^n while varying \\alpha\\in\\mathbb{R}. This enabled us to convert the vector problem into a scalar one. We can follow this procedure while perturbing a function. Namely, we can build the variation of a function by writing it as \n\\delta y(x) = \\alpha \\eta(x),\n\\tag{2} where \\eta(x) is a given (but arbitrary) function in \\mathcal{C}^1 (playing the role of \\mathbf d in the finite-dimensional optimization) and \\alpha\\in\\mathbb R. This way we are about to convert optimization over functions to optimization over real numbers.\nBefore we proceed, let’s elaborate a bit more on the above expression. Let’s assume that the function y(x) in the neighbourhood of the minimizing function y^\\star(x) is actually parameterized by some real parameter \\alpha, and that for \\alpha=0 it becomes the minimizing function y^\\star(x). The Taylor expansion around \\alpha = 0 is \ny(x,\\alpha) = \\underbrace{y(x,0)}_{y^\\star(x)} + \\left.\\frac{\\partial y(x,\\alpha)}{\\partial \\alpha}\\right|_{\\alpha=0} \\alpha + \\mathcal{O}(\\alpha^2).\n\nThe second term on the right is then the variation \\delta y of the function y. We will write it down here for later reference \n\\delta y(x) = \\underbrace{\\left.\\frac{\\partial y(x,\\alpha)}{\\partial \\alpha}\\right|_{\\alpha=0}}_{\\eta(x)} \\alpha.\n\\tag{3}\nThis add an interpretation to Eq. 2. Note also that for a fixed x, the variation is just a differential with respect to \\alpha (recall that a differential is the first-order approximation to the increment).\nSo far, so good. We have now discussed in quite some detail the concept of variation of a function, that is, the concept that will be used to describe the perturbation of the input argument of a cost functional. But now we want to see if another analogy can be found with the differential calculus. Recall that the first-order necessary condition of optimality of a cost function f(x) of a scalar real argument x is that the differential of the cost function vanishes, that is, \n\\mathrm d f = 0.\n\nBut we also know that that the differential is defined as the first-order approximation to the increment in the input argument, that is \n\\mathrm d f = \\underbrace{f'}_{\\frac{\\mathrm d f}{\\mathrm d x} } \\mathrm d x=0,\n from which it follows that if the variable x is unconstrained, the first-order condition of optimality can be given as a condition on the derivative \nf'(x) = 0.\n\nIn case of a vector variables \\mathbf x\\in\\mathbb R^n, we rewrite the above condition on the differential as \n\\mathrm d f = (\\nabla f)^\\mathrm{T}\\, \\mathbf{d}\\mathbf{x} = 0,\n\\tag{4} from which it follows that \n\\nabla f = \\mathbf 0.\n\nHaving recapitulated these basic facts from differential calculus, we are now curious if we can do similar development within calculus of variations. Namely, we would like to express the variation of the cost functional using the variation of the function, thus mimicking Eq. 4. Note that the product in Eq. 4 is actually the inner product (some may even write it as \\mathrm d f = \\langle \\nabla f, \\mathbf{d}\\mathbf{x} \\rangle). And inner products are also defined in other vector spaces, not just Euclidean spaces of n-tuples. For continuous functions, they are defined using integrals instead of sums. Namely, we have \\boxed{\n\\delta J = \\int_a^b {\\color{red}\\frac{\\delta J}{\\delta y(x)}} \\, \\delta y(x)\\mathrm d x, }\n where the fraction in the above expression is called variational derivative.\n\n\n\n\n\n\nWarning\n\n\n\nThe whole fraction should be regarded just as one symbol. You should not really treat it as a true ratio (and cancel the denominator term with the other \\delta y term). This is the same type of a trap that you can encounter in differential calculus using Leibniz’s notation.\n\n\nNow, following Eq. 3, we may want to express the variation of J as \n\\delta J = \\left.\\frac{\\mathrm{d} J}{\\mathrm{d} \\alpha}\\right|_{\\alpha=0}\\alpha.\n\\tag{5}\nWe compute the derivative of the cost with respect to the real parameter \\alpha first (recall that both the y and the \\eta functions are considered as fixed here) \n\\left.\\frac{\\mathrm{d}}{\\mathrm{d} \\alpha}J(y(x)+\\alpha\\eta(x))\\right|_{\\alpha=0} = \\lim_{\\alpha\\rightarrow 0}\\frac{J(y(x)+\\alpha\\eta(x))-J(y(x))}{\\alpha},\n\\tag{6} and once we have it, we just multiply the result by \\alpha, and we have the desired variation of the cost.\nWe are not going to consider completely arbitrary cost function(al)s, instead we are only going to consider a useful subset. We start by considering some concrete examples. We will then extract some common features and characterize some general and yet narrow enough family of cost functionals.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Calculus of variations"
    ]
  },
  {
    "objectID": "cont_indir_calculus_of_variations.html#some-examples-of-calculus-of-variations",
    "href": "cont_indir_calculus_of_variations.html#some-examples-of-calculus-of-variations",
    "title": "Calculus of variations",
    "section": "Some examples of calculus of variations",
    "text": "Some examples of calculus of variations\n\nMinimum distance between two points\nConsider two points in the plane. The task is to find the curve that connects these two points and minimizes the total length. Without a loss of generality, we consider the two ends on the x-axis as in Fig. 3. Although the answer to this problem is trivial, the problem serves a good job of demonstrating the essence of calculus of variations.\n\n\n\n\n\n\nFigure 3: Minimum distance between two points\n\n\n\nThe total length of the curve is J(y) = \\int_a^b \\sqrt{(\\mathrm d x)^2+(\\mathrm d y)^2}= \\int_a^b \\sqrt{1+(y'(x))^2}\\text{d}x. The optimization problem is then \n\\operatorname*{minimize}_{y\\in\\mathcal{C}^1_{[a,b]}} \\int_a^b \\sqrt{1+(y'(x))^2}\\text{d}x.\n\n\n\nDido’s problem\nGiven a rope of length c, what is the maximum area this rope can circumscribe? Obviously, here we have a problem with an equality-type constraint \n\\begin{aligned}\n\\operatorname*{minimize}_{y\\in\\mathcal{C}^1_{[a,b]}} &\\quad \\int_a^b y(x)\\text{d}x,\\\\\n\\text{subject to} &\\quad \\int_a^b \\sqrt{1+(y'(x))^2}\\text{d}x = c.\n\\end{aligned}\n\n\n\nBrachistochrone problem\nAnother classical problem mentioned in every textbook on calculus of variations is the problem of brachistochrone, where the task is to find a shape of a thin wire with a bead sliding along it (with no friction) in the shortest time, see Fig. 4.\n\n\n\n\n\n\nFigure 4: Problem of brachistochrone\n\n\n\nYou may also like watching the “no-equation” video by VSauce:\n\nThe cost function is simply the total time, that is \nJ = \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} \\mathrm{d}t = t_\\mathrm{f}-t_\\mathrm{i}.\n\nIt does not fit into the framework that we currently use because time enters here as the independent variable. But there is an easy fix to this. We will express time as a ratio of the distance and velocity. In particular, \nJ = \\int_a^b \\frac{\\text{d}s}{v}.\n\nWe are already well familiar with the numerator but the velocity in the denominator needs to be determined too. We will use a physical argument here: when the bead is in the initial position, the velocity is zero and the height (as measured along the y axis) is zero. Therefore the total energy given as a sum of kinetic and potential energies \\mathcal{T}+\\mathcal{V} is zero. And since we assume no friction, the total energy remains constant along the whole trajectory, that is, \n\\frac{1}{2}mv^2 - mgy = 0,\n from which we can write \nv(x) = \\sqrt{2gy(x)}.\n\nWe can then write the expression for the total time as \nJ = \\int_a^b \\frac{\\text{d}s}{v}= \\int_a^b \\frac{\\sqrt{1+(y'(x))^2}}{\\sqrt{2gy(x)}}\\text{d}x.\n\nThe optimization problem is then \n\\operatorname*{minimize}_{y\\in\\mathcal{C}^1_{[a,b]}} \\int_a^b \\sqrt{\\frac{1+(y'(x))^2}{2gy(x)}}\\text{d}x.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Calculus of variations"
    ]
  },
  {
    "objectID": "cont_indir_calculus_of_variations.html#basic-problem-of-calculus-of-variations-with-fixed-ends",
    "href": "cont_indir_calculus_of_variations.html#basic-problem-of-calculus-of-variations-with-fixed-ends",
    "title": "Calculus of variations",
    "section": "Basic problem of calculus of variations with fixed ends",
    "text": "Basic problem of calculus of variations with fixed ends\nThe only motivation for including those few simple examples was to justify the following general problem. We will call this . We will keep considering \\mathcal{C}^1 functions of x defined on an interval [a,b] with the values at the beginning and end of the interval fixed \ny(a) = \\mathrm y_a,\\qquad y(a) = \\mathrm y_b,\n see Fig. 5 and the task is to find y^\\star\\in\\mathcal{C}^1_{[a,b]} minimizing the functional of the following type\n\nJ(y(\\cdot)) = \\int_a^b L(x,y(x),y'(x))\\text{d}x.\n\n\n\n\n\n\n\nFigure 5: Basic problem of calculus of variations with values of the function at both ends fixed\n\n\n\nThe basic problem of calculus of variations is then \\boxed{\n\\begin{aligned}\n\\operatorname*{minimize}_{y\\in\\mathcal{C}^1_{[a,b]}} &\\quad \\int_a^b L(x,y(x),y'(x))\\text{d}x\\\\\n\\text{subject to} &\\quad y(a) = \\mathrm y_a,\\\\\n&\\quad y(b) = \\mathrm y_b.\n\\end{aligned}}\n\nIt is possible to extend this basic problem formulation into something more complicated, for example by relaxing the ends and add constraints, but will only have a look at those extensions later. First, we solve the basic problem and see if we can use it within the optimal control framework.\nIn order to state the first-order necessary condition of optimality, we need to find the variation of the cost functional. But we already know that we can form it from the partial derivative of the cost functional with respect to some real parameter as in Eq. 6, that is,\n\n\\begin{aligned}\n\\frac{\\mathrm{d} J(y^\\star(x)+\\alpha\\eta(x))}{\\mathrm{d} \\alpha} &= \\frac{\\mathrm d}{\\mathrm d \\alpha}\\int_a^b[L(x,y^\\star+\\alpha\\eta,(y^\\star)'+\\alpha\\eta')]\\text{d}x,\\nonumber\\\\\n&= \\int_a^b \\frac{\\mathrm d}{\\mathrm d \\alpha}[L(x,y^\\star+\\alpha\\eta,(y^\\star)'+\\alpha\\eta')]\\text{d}x,\\nonumber\\\\\n&= \\int_a^b \\left[ \\frac{\\partial L(x,y,y')}{\\partial y}\\eta(x) + \\frac{\\partial L(x,y,y')}{\\partial y'}\\eta'(x)\\right]\\text{d}x\n\\end{aligned}\n\nNow, setting this equal to zero, we do not learn much because the arbitrary \\eta(\\cdot) and also its derivative appear in the conditions. It will be much better if we can modify this into something like \\int_a^b \\left[ (\\qquad )\\eta(x)\\right]\\text{d}x. Why should we aim at this form? The fundamental lemma of calculus of variations says that if the following holds for any \\eta\\in\\mathcal{C}^1_{[a,b]} vanishing at a and b \n\\int_a^b \\xi(x)\\eta(x)\\text{d}x = 0,\n then necessarily \\xi(x)\\equiv 0 identically on the whole interval [a,b]. The proof can be found elsewhere.\nHence we are motivated to bring the formula for the variation into the format where the derivative of \\eta is missing. This will be accomplished by applying the per-partes integration to the term \\int_a^b\\frac{\\partial L(x,y,y')}{\\partial y'}\\eta'(x)\\text{d}x: \n\\int_a^b\\frac{\\partial L(x,y,y')}{\\partial y'}\\eta'(x)\\text{d}x = \\left[\\frac{\\partial L(x,y,y')}{\\partial y'}\\eta(x)\\right]_{a}^b-\\int_a^b\\frac{\\text{d}}{\\text{d}x}\\frac{\\partial L(x,y,y')}{\\partial y'}\\eta(x)\\text{d}x.\n\nSubstituting back to our expression for the variation, we get \n\\frac{\\mathrm{d} J}{\\mathrm{d} \\alpha} = \\left[\\frac{\\partial L(x,y,y')}{\\partial y'}\\eta(x)\\right]_{a}^b + \\int_a^b \\left( \\frac{\\partial L(x,y,y')}{\\partial y}-\\frac{\\text{d}}{\\text{d}x}\\frac{\\partial L(x,y,y')}{\\partial y'}\\right)\\eta(x)\\text{d}x.\n\\tag{7}\nThe first term on the right is zero because we assumed at the very beginning that the function y(\\cdot) is fixed at both ends, hence the variation \\delta y(\\cdot) is zero at both ends, hence \\eta(a)=\\eta(b)=0. As a result, we have the following equation \\boxed{\n  \\frac{\\partial L(x,y,y')}{\\partial y}-\\frac{\\text{d}}{\\text{d}x}\\frac{\\partial L(x,y,y')}{\\partial y'} = 0}\n\\tag{8} or \\boxed{\n  \\frac{\\partial L(x,y,y')}{\\partial y}=\\frac{\\text{d}}{\\text{d}x}\\frac{\\partial L(x,y,y')}{\\partial y'}.}\n\\tag{9}\nThis is the famous Euler-Lagrange equation.\n\n\n\n\n\n\nEuler-Lagrange equation in the top ten\n\n\n\nMy biased opinion is that Euler-Lagrage equation is a result that deserves its position in the list of top ten quations in applied mathematics.\n\n\nSmooth function which satisfy the Euler-Lagrange equation are called extremals. They play the same role within calculus of variations as stationary points do within differential calculus, that is, they are just candidate functions for a minimizer. This is another way of saying that the Euler-Lagrange equation provides necessary conditions of optimality.\nFinally, we format this result within the variational framework. We can now invoke Eq. 10 and write it as \n\\delta J = \\int_a^b \\underbrace{\\left[\\frac{\\partial L(x,y,y')}{\\partial y}-\\frac{\\text{d}}{\\text{d}x}\\frac{\\partial L(x,y,y')}{\\partial y'}\\right]}_{\\frac{\\delta J}{\\delta y(x)}} \\, \\underbrace{\\delta y(x)}_{\\alpha \\eta(x)}\\mathrm d x,\n\\tag{10} from which we can see that the left hand side of the Euler-Lagrange equation gives us the variational derivative that we were looking for.\nYou may now wonder why on earth did we actually bother to introduce the new concept of a variation (of a function and of a functional)? We were able to derive the Euler-Lagrange equation just using a partial derivative with respect to \\alpha. Good point. In fact, the major motivation was to develop a framework that would resemble that of differential calculus as closely as possible. Knowing now the resulting format of the first-order necessary conditions of optimality, let’s now try to rederive it in the fully variational style. That is, we want to find the variation \\delta J of the functional J(y(x)) that is given by the integral \n\\delta J = \\delta\\int_a^b L(x,y,y')\\mathrm d x.\n\nThe variation now constitutes an operation pretty much mimicking the differential when it comes to dealing with composite functions, products of two function and other situations. Namely, for constant lower and upper bounds in the integral we can move the variation operation into the integral \n\\delta J = \\int_a^b \\delta L(x,y,y')\\mathrm d x\n and then, following the standard rules (shared with operation of differentiation), we get \n\\delta J = \\int_a^b \\left[ \\frac{\\partial L(x,y,y')}{\\partial x}\\delta x + \\frac{\\partial L(x,y,y')}{\\partial y}\\delta y + \\frac{\\partial L(x,y,y')}{\\partial y'}\\delta y'\\right]\\mathrm d x.\n\nNow, x is an independent variable, hence it does not vary and \\delta x = 0. Furthermore, the operations of variation and derivative with respect to x commute, therefore \\delta y', which is a shorthand notation for \\delta\\left(\\frac{\\mathrm d}{\\mathrm d x} y(x)\\right) can be rewritten as \n\\delta y'(x)= \\frac{\\mathrm d}{\\mathrm d x} (\\delta y(x))\n and we can write the variation of the cost function as \n\\delta J = \\int_a^b \\left[\\frac{\\partial L(x,y,y')}{\\partial y}\\delta y + \\frac{\\partial L(x,y,y')}{\\partial y'} (\\delta y(x))' \\right]\\mathrm d x.\n\nIdentically as in our previous development, we can get rid of the derivative of the variation using integration , which gives \\boxed{\n\\delta J = \\left[\\frac{\\partial L(x,y,y')}{\\partial y'}\\delta y(x)\\right]_{a}^b + \\int_a^b \\left( \\frac{\\partial L(x,y,y')}{\\partial y}-\\frac{\\text{d}}{\\text{d}x}\\frac{\\partial L(x,y,y')}{\\partial y'}\\right)\\delta y(x)\\text{d}x,}\n\\tag{11} which under the assumption of fixed both ends, that is, \\delta y(a) = \\delta y(b) = 0, gives the Euler-Lagrange equation. Elegant procedure, isn’t it?\nThis can perhaps be regarded as culmination of our attempts to develop calculus of variations as an analogy to differential calculus.\nTo make the notation a bit more compact, we will often write the partial derivative of L(x,y,y') with respect to y(x) as L_y. Similarly, the partial of the same function with respect to y'(x) as L_{y'}. Using this notation, we can immediately show that Euler-Lagrange equation is actually a second-order ordinary differential equation \\boxed{\nL_y - L_{y'x} - L_{y'y}y' - L_{y'y'}y'' = 0.}\n\\tag{12}\nIn order to see how we got this, first recall that L is a function of x, y(x) and y'(x). Let’s write it explicitly as L(x,y(x),y'(x)). Then, in the Euler-Lagrange equation we need to find the total derivative of L_{y'} with respect to x, that is, we need \\frac{\\mathrm d}{\\mathrm dx}L_{y'}, and we invoke the chain rule for this. Remember, the function L_{y'} is generally (!) a function of three arguments too: x, y and y'. Therefore, applying the chain rule we will get three terms:\n\n\\frac{\\mathrm d}{\\mathrm dx}L_{y'}(x,y,y') = \\underbrace{\\frac{\\partial L_{y'}}{\\partial x}}_{L_{y'x}} + \\underbrace{\\frac{\\partial L_{y'}}{\\partial y}}_{L_{y'y}} \\underbrace{\\frac{\\mathrm d y(x)}{\\mathrm d x}}_{y'} + \\underbrace{\\frac{\\partial L_{y'}}{\\partial y'}}_{L_{y'y'}} \\underbrace{\\frac{\\mathrm d y'(x)}{\\mathrm d x}}_{y''}.\n\nThat is it. Combine it with the other term L_y from the E.-L. equation and we are done.\nWe know that in order to specify a solution of a second-order ODE completely, we need to provide two values. Sometimes we specify them at the beginning of the interval, in which case we would give the value of the function and its derivative. This is the well-known initial value problem (IVP). On some other occasions, we specify the values at two different points on the interval. And this is our case here. In particular, here we have y(a)=\\mathrm y_a and y(b)=\\mathrm y_b, which turns the problem into so-called boundary value problem (BVP). Both analysis and (numerical) methods for finding a solution of boundary value problems are a way more difficult that for initial value problems. But there are dedicated solvers (see the section on software).\nNonetheless, before jumping into calling some numerical solvers, let’s get some insight for two special cases by analyzing the situations carefully. First, assume that L does not depend on y. We call this a “no y case”. Then the Euler-Lagrange equation simplifies to \n0 = \\frac{\\text{d}}{\\text{d}x}L_{y'},\n and, as a consequence, L_{y'} is constant, independent of x.\nThe second special case is the “no x case”. Then \nL_y  - L_{y'y}y' - L_{y'y'}y'' = 0.\n\nBy multiplying both sides by y' (and some one-line work), the equation turns into \n\\frac{\\text{d}}{\\text{d}x}( L_{y'}y' - L ) = 0.\n\nAs a consequence, L_{y'}y' - L is constant along the optimal curve.\nThe two new functions whose values are preserved along the extremal (under the respective conditions) are so special that they deserve their own symbols and names: \\boxed{\np(x) \\coloneqq L_{y'},}\n and the choice of the symbol p is intentional as this variable plays the role of momentum in physics (when the independent variable x is time and L is the difference between the kinetic and potential energies), and \\boxed{\nH(x,y,y',p) \\coloneqq py'-L}\n and the choice of the symbol H is intentional becase this variable plays the role of Hamiltonian in physics.\nLet us now see how y and p develop as functions of x, and we will use Hamiltonian for that purpose. First, it is immediate from the definition of Hamiltonian that \ny' = H_p.\n\nSimilarly, the derivative of momentum is \np' = \\frac{\\text{d}}{\\text{d}x} L_{y'} = L_y = -H_y,\n where the second equality comes from Euler-Lagrange equation and the third one comes from the definition of H. We format the two differential equations as one vector differential equation \\boxed{\n\\begin{bmatrix}\n  y' \\\\ p'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  H_p \\\\ -H_y\n\\end{bmatrix}}\n\nThis version of first-order necessary conditions is no less famous in physics and theoretical mechanics – Hamilton’s canonical equations – and some of our results on optimal control will come in this format.\n\nExample 2 Let us now see how the analysis of the special cases can be practically useful. We will only have a look at the minimum distance problem. The Lagrangian is \nL(y') = \\sqrt{1+(y')^2},\n which is clearly independent of y (and of x as well). Therefore \nL_{y'} = \\frac{1}{2}\\frac{2y'}{\\sqrt{1+(y')^2}}\n must be constant. The only way is to have y'(x) constant, that is, the graph of the function y(x) must be a line. Introducing the boundary conditions, it is now obvious that the solution is a line connecting the two points.\n\nOne important general property will be revealed if we differentiate H with respect to y' \nH_{y'}(x,y,y',p) = p-L_{y'}.\n\nIf p is chosen as L_{y'}, the derivative of Hamiltonian vanishes. In other words, when H is evaluated on the curve, it has a stationary point with respect to the third variable. To support this mental step of regarding the third (input) argument as independent from the rest, we write the Hamiltonian for the extremal with the third variable relaxed as H(x,y,z,p). The above result says that \\boxed{\n\\left.\\frac{\\partial H(x,y,z,p)}{\\partial z}\\right|_{z = y', \\, p = L_{y'}} = 0}.\n\nIn fact, as we will see shortly, Hamiltonian is not only stationary along the optimal trajectory but it also achieves the maximum value! This will actually turn out the crucial property of a Hamiltonian – it is maximized along the optimal trajectory with respect to y'. The fact that the derivative is zero is just a consequence in the special case when such derivative exists.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Calculus of variations"
    ]
  },
  {
    "objectID": "cont_indir_calculus_of_variations.html#sufficient-conditions-of-optimality-minimum",
    "href": "cont_indir_calculus_of_variations.html#sufficient-conditions-of-optimality-minimum",
    "title": "Calculus of variations",
    "section": "Sufficient conditions of optimality (minimum)",
    "text": "Sufficient conditions of optimality (minimum)\nWhat remains to be done before we come to applying the Euler-Lagrange equation to control problems is to discuss how we can learn if the extremal is actually minimizing the cost functional. Or maximizing it? Or what if it is just a saddle “point”? The mathematics needed to answer these questions is quite delicate, we will only sketch the direction of reasoning and for complete proofs refer to the literature.\nKnowing that the first variation vanishes for an extremal, higher order terms need to be investigated, starting with the term in the Taylor’s expansion corresponding to the squared variations. Similarly as in the finite-dimensional optimization, we first argue that for small enough \\alpha, the second-order term dominates all the higher order terms and then we study under which conditions is the second order term nonnegative (for the second-order necessary condition) or positive (for the second-order sufficient condition).\nThe answer for the necessity part, which relies heavily on the fact that we have decided to work with the \\|.\\|_1 norm, is that \nL_{y'y'} \\geq  0\n needs to be satisfied. This is called the Legendre necessary condition. We have certainly skipped a lot of nontrivial work that needs to be done to show this result. Check the literature for details if you are interested.\nThe sufficiency part is even more complicated. It turns out that merely sharpening the necessary Legendre condition into \nL_{y'y'} &gt;  0\n is not enough to guarantee the minimality. The additional constraint is quite convoluted even to be stated. It is called Jacobi condition and has something to do with absence of conjugate points on the interval of control. The only motivation for stating these terms here without actually providing any explanation is just to provide the keywords and search phrases for learning more elsewhere. Here we just state that the optimality is guaranteed if the inverval of x is not “too long”… This is certainly too vague, but the reason why we are so easy-going here is that once we switch to optimal control, we will have other – and more convenient – tools to guarantee the optimality.\nWe now move on to one crucial observation: \nH_{y'y'} = -L_{y'y'}.\n\nHence, if a given function y minimizes J, then L_{y'y'}\\geq 0 and \nH_{y'y'} \\leq 0,\n\\tag{13} which reads that Hamiltonian achieves a maximum when evaluated on the optimal curve. It can be restated as \\boxed{\nH(t,y,y',p) \\geq  H(t,y,z,p)}\n for all z\\in\\mathcal{C}^1 on the interval [a,b] and close to y'(t) (in the sense of 1-norm).\nThis is a key property and constitutes a prequel to the celebrated Pontryagin’s principle of maximum that we are going to study in the next chapter.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Calculus of variations"
    ]
  },
  {
    "objectID": "cont_indir_calculus_of_variations.html#constrained-problems-in-calculus-of-variations",
    "href": "cont_indir_calculus_of_variations.html#constrained-problems-in-calculus-of-variations",
    "title": "Calculus of variations",
    "section": "Constrained problems in calculus of variations",
    "text": "Constrained problems in calculus of variations\nNow that we have covered the first-order and second-order necessary conditions of optimality and at least touched the second-order sufficient conditions of optimality, we should start discussing constraints. Here we will only investigate equality type-constraints. The inequality-type constraints are beyond the reach of basic methods of calculus of variations. But as we will see, an offspring of calculus of variations – the already mentioned Pontryagin’s principle – will handle these easily, at least in some cases.\nThe constraints that we are going to encounter in optimal control come in the form of differential equations, and these constitute pointwise constraints \nF(x,y,y')=0.\n\nFor every value of the independent variable x, we have one constraint. Since x is real, we have a continuum of constraints. As a consequence, we will need an infinite number of Lagrange multipliers as well— in other words, the Lagrange multiplier will be a function of x too. The augmented cost function is \nJ^\\text{aug}(y(\\cdot)) = \\int_a^b L(x,y,y')\\text{d}x + \\int_a^b \\lambda(x)\\cdot F(x,y,y')\\text{d}x,\n where the symbol \"\\cdot`` is there to emphasize that in the case when both y and \\lambda are vector functions (hence F is a vector), the second integrand is obtained as an inner product. Rewriting the above expression for the augmented criterion of optimality as \nJ^\\text{aug}(y) = \\int_a^b \\left [L(x,y,y')+\\lambda(x)\\cdot F(x,y,y')\\right ]\\text{d}x\n suggests that we can introduce an augmented Lagrangian \nL^\\text{aug}(x,y,y',\\lambda) = L(x,y,y')+\\lambda(x)\\cdot F(x,y,y')\n and continue as we did in the unconstrained case. For completeness, let’s state here that in the case of vector functions, the augmented Lagrangian is given as \nL^\\text{aug}(x,y,y',\\lambda) = L(x,y,y')+\\lambda(x)^{\\top} F(x,y,y'),\n or, using the alternative notation for the inner product, \nL^\\text{aug}(x,y,y',\\lambda) = L(x,y,y')+\\langle \\lambda(x), F(x,y,y')\\rangle,\n\nA word of warning is needed here, though. Similarly as in the unconstrained case, it can happen that the constraints will be degenerate, in which case the Euler-Lagrange equation fails to be a necessary condition of optimality. We will not discuss this delicate issue here and rather direct the interested student to the literature.\nThis is roughly it. This is what we need from calculus of variations to get started with optimal control in continuous-time setting.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Calculus of variations"
    ]
  },
  {
    "objectID": "ext_software.html",
    "href": "ext_software.html",
    "title": "Software",
    "section": "",
    "text": "Matlab (Control Systems Toolbox):\n\nlqg, lqgreg, lqgtrack, kalman (and some more description in Linear-Quadratic-Gaussian (LQG) Design)\n\nJulia (RobustAndOptimalControl.jl):\n\nLQGProblem",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Software"
    ]
  },
  {
    "objectID": "ext_software.html#lqg-optimal-control",
    "href": "ext_software.html#lqg-optimal-control",
    "title": "Software",
    "section": "",
    "text": "Matlab (Control Systems Toolbox):\n\nlqg, lqgreg, lqgtrack, kalman (and some more description in Linear-Quadratic-Gaussian (LQG) Design)\n\nJulia (RobustAndOptimalControl.jl):\n\nLQGProblem",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Software"
    ]
  },
  {
    "objectID": "ext_software.html#mathcal-h_2-optimal-control",
    "href": "ext_software.html#mathcal-h_2-optimal-control",
    "title": "Software",
    "section": "\\mathcal H_2-optimal control",
    "text": "\\mathcal H_2-optimal control\nMatlab (Robust Control Toolbox):\n\nh2syn\n\nJulia (RobustAndOptimalControl.jl):\n\nh2synthesize",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Software"
    ]
  },
  {
    "objectID": "ext_software.html#loop-transfer-recovery-ltr",
    "href": "ext_software.html#loop-transfer-recovery-ltr",
    "title": "Software",
    "section": "Loop Transfer Recovery (LTR)",
    "text": "Loop Transfer Recovery (LTR)\nMatlab (Robust Control Toolbox):\n\nltrsyn",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Software"
    ]
  },
  {
    "objectID": "discr_dir_references.html",
    "href": "discr_dir_references.html",
    "title": "References",
    "section": "",
    "text": "The key concept of this chapter — model predictive control (MPC), aka receding horizon control (RHC) — has been described in a number of dedicated monographs and textbooks. Particularly recommendable are [1] and [2]. They are not only reasonably up-to-date, written by leaders in the field, but they are also available online for free.\nSome updates as well as additional tutorials are in [3], which seems to be available to CTU students through the institutional access.\nThere is no shortage of lecture notes and slides as well. Particularly recommendable are the course slides [4], and [5].\nExtensions towards nonlinear systems (nonlinear model predictive control, NMPC) are described in [6], which is also available to CTU students through the institutional access. Alternatively, concise introductions are [7] and [8, Ch. 15].\nSince MPC essentially boils down to solving optimization problems in real time on some industrial device, the topic of embedded optimization is important. An overview is given in [9]. Although some new solvers appeared since its publication, the practical considerations highlighted in the paper are still valid.\n\n\n\n\n Back to topReferences\n\n[1] J. B. Rawlings, D. Q. Mayne, and M. M. Diehl, Model Predictive Control: Theory, Computation, and Design, 2nd ed. Madison, Wisconsin: Nob Hill Publishing, LLC, 2017. Available: http://www.nobhillpublishing.com/mpc-paperback/index-mpc.html\n\n\n[2] F. Borrelli, A. Bemporad, and M. Morari, Predictive Control for Linear and Hybrid Systems. Cambridge, New York: Cambridge University Press, 2017. Available: http://cse.lab.imtlucca.it/~bemporad/publications/papers/BBMbook.pdf\n\n\n[3] S. V. Raković and W. S. Levine, Eds., Handbook of Model Predictive Control. in Control Engineering. Birkhäuser Basel, 2019. Accessed: Mar. 06, 2019. [Online]. Available: https://www.springer.com/us/book/9783319774886\n\n\n[4] A. Bemporad, “Model predictive control.” May 2021. Available: http://cse.lab.imtlucca.it/~bemporad/teaching/mpc/imt/1-linear_mpc.pdf\n\n\n[5] S. Boyd, “Model Predictive Control (EE364b - Convex Optimization II.).” Stanford University. Accessed: Feb. 25, 2019. [Online]. Available: https://stanford.edu/class/ee364b/lectures/mpc_slides.pdf\n\n\n[6] L. Grüne and J. Pannek, Nonlinear Model Predictive Control: Theory and Algorithms, 2nd ed. in Communications and Control Engineering. Cham: Springer, 2017. Available: https://doi.org/10.1007/978-3-319-46024-6\n\n\n[7] S. Gros, M. Zanon, R. Quirynen, A. Bemporad, and M. Diehl, “From linear to nonlinear MPC: Bridging the gap via the real-time iteration,” International Journal of Control, vol. 93, no. 1, pp. 62–80, Jan. 2020, doi: 10.1080/00207179.2016.1222553.\n\n\n[8] S. Gros and M. Diehl, “Numerical Optimal Control (draft).” KU Leuven, May 2020. Available: https://www.syscop.de/teaching/ss2017/numerical-optimal-control\n\n\n[9] H. J. Ferreau et al., “Embedded Optimization Methods for Industrial Automatic Control,” IFAC-PapersOnLine, vol. 50, no. 1, pp. 13194–13209, Jul. 2017, doi: 10.1016/j.ifacol.2017.08.1946.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "References"
    ]
  },
  {
    "objectID": "limitations_hw.html",
    "href": "limitations_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Homework"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html",
    "href": "discr_dir_mpc.html",
    "title": "Model predictive control (MPC)",
    "section": "",
    "text": "In the previous section we learnt how to compute an optimal control sequence on a finite time horizon using numerical methods for solving nonlinear programs (NLP), and quadratic programs (QP) in particular. There are two major deficiencies of such approach:\n\nThe control sequence was computed under the assumption that the mathematical model is perfectly accurate. As soon as the reality deviates from the model, either because of some unmodelled dynamics or because of the presence of (external) disturbances, the performance of the system will deteriorate. We need a way to turn the presented open-loop (also feedforward) control scheme into a feedback one.\nThe control sequence was computed for a finite time horizon. It is commonly required to consider an infinite time horizon, which is not possible with the presented approach based on solving finite-dimensional mathematical programs.\n\nThere are several ways to address these issues. Here we introduce one of them. It is known are Model Predictive Control (MPC), also Receding Horizon Control (RHC). Some more are presented in the next two sections (one based on indirect approach, another one based on dynamic programming).",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#deficiencies-of-precomputed-open-loop-optimal-control",
    "href": "discr_dir_mpc.html#deficiencies-of-precomputed-open-loop-optimal-control",
    "title": "Model predictive control (MPC)",
    "section": "",
    "text": "In the previous section we learnt how to compute an optimal control sequence on a finite time horizon using numerical methods for solving nonlinear programs (NLP), and quadratic programs (QP) in particular. There are two major deficiencies of such approach:\n\nThe control sequence was computed under the assumption that the mathematical model is perfectly accurate. As soon as the reality deviates from the model, either because of some unmodelled dynamics or because of the presence of (external) disturbances, the performance of the system will deteriorate. We need a way to turn the presented open-loop (also feedforward) control scheme into a feedback one.\nThe control sequence was computed for a finite time horizon. It is commonly required to consider an infinite time horizon, which is not possible with the presented approach based on solving finite-dimensional mathematical programs.\n\nThere are several ways to address these issues. Here we introduce one of them. It is known are Model Predictive Control (MPC), also Receding Horizon Control (RHC). Some more are presented in the next two sections (one based on indirect approach, another one based on dynamic programming).",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#model-predictive-control-mpc-as-a-way-to-turn-open-loop-control-into-feedback-control",
    "href": "discr_dir_mpc.html#model-predictive-control-mpc-as-a-way-to-turn-open-loop-control-into-feedback-control",
    "title": "Model predictive control (MPC)",
    "section": "Model predictive control (MPC) as a way to turn open-loop control into feedback control",
    "text": "Model predictive control (MPC) as a way to turn open-loop control into feedback control\nThe idea is to compute an optimal control sequence on a finite time horizon using the optimization framework presented in the previous section, but then apply only the first element of the computed control trajectory to the system, and proceed to repeating the whole procedure after shifting the time horizon forward by one time step. The name “model predictive control” expresses the fact that a model-based prediction is the key component of the controller. This is expressed in Fig. 1.\n\n\n\n\n\n\nFigure 1: Diagram describing a single MPC step. Recall that the information carried by the past input trajectories can compressed into the state of the system. A state observer providing an estimate of the state must then be a component of the whole MPC, but then needs not only the input but also the output trajectories.\n\n\n\nThe other name “receding horizon control” is equally descriptive, it emphasizes the phenomenon of the finite time horizon (interval, window) receding (shifting, moving, rolling) as time goes by.\n\n\n\n\n\n\nWe all do MPC in our everyday lives\n\n\n\nIt may take a few moments to comprehend the idea, but then it turns out perfectly natural. As a matter of fact, this is the way most of us control our lifes every day. We plan our actions on a finite time horizon, and to build this plan we use both our understanding (model) of the world and our knowledge of our current situation (state). We then execute the first action from our plan, observe the consequences of our action the and recent changes in the environment, and update our plan accordingly on a new (shifted) time horizon. We repeat this procedure over and over again. It is crucial that the prediction horizon must be long enough so that the full impact of our actions can be observed, but it must not be too long because the planning then becomes too complex and predictions unreliable.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#mpc-regulation",
    "href": "discr_dir_mpc.html#mpc-regulation",
    "title": "Model predictive control (MPC)",
    "section": "MPC regulation",
    "text": "MPC regulation\nWe first investigate the situation when the reference (the set-point, the required final state) is zero.\n\nWe restrict ourselves to a linear system and a quadratic cost function. We impose lower and upper bounds on the control, and optionally on the states as well. We consider a (long) time horizon N. \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_0,\\ldots, \\bm u_{N-1}, \\bm x_{0},\\ldots, \\bm x_N} &\\quad  \\frac{1}{2} \\bm x_N^\\top \\mathbf S \\bm x_N + \\frac{1}{2} \\sum_{k=0}^{N-1} \\left(\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k \\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{k+1} = \\mathbf A\\bm x_k + \\mathbf B\\bm u_k,\\quad k = 0, \\ldots, N-1, \\\\\n                    &\\quad \\bm x_0 = \\mathbf x_0,\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_k \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad (\\mathbf x^{\\min} \\leq \\bm x_k \\leq \\mathbf x^{\\max}).\n\\end{aligned}\n\nIf the time horizon is really long, we can approximate it by \\infty, in which case we omit the terminal state penalty from the overall cost function \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_0,\\bm u_1,\\ldots, \\bm x_{0}, \\bm x_1,\\ldots} &\\quad  \\frac{1}{2} \\sum_{k=0}^{\\infty} \\left(\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k \\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{k+1} = \\mathbf A\\bm x_k + \\mathbf B\\bm u_k,\\quad k = 0, 1, 2, \\ldots, \\\\\n                    &\\quad \\bm x_0 = \\mathbf x_0,\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_k \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad (\\mathbf x^{\\min} \\leq \\bm x_k \\leq \\mathbf x^{\\max}).\n\\end{aligned}\n\nNow, in a model predictive control (MPC) scheme, at the discrete time t, an LQR problem on a finite (and typically only modestly long time prediction horizon N_\\mathrm{p} (typically N_\\mathrm{p}\\approx 20 or 30 or so) is formulated and solved. This time window then moves (recedes, rolls) either forever or untill the the final time N is reached.\nSince the problem is parameterized by the “initial” time t and the state at this time, we need to reflect this in the notation. Namely, \\bm x_{t+k|t} is the predicted state at time t+k as predicted at time t using the information available at that time, that is, \\mathbf x_t (= \\bm x_{t|t}) and the computed control trajectory up to the time just one step before t+k. We can emphasize this dependence by writing it explicitly as \\bm x_{t+k|t}(\\bm x_t, \\bm u_{t}, \\bm u_{t+1}, \\ldots, \\bm u_{t+k-1}), but then the notation becomes rather convoluted and so we stick to the shorter one. We emphasize that it is really just a prediction. The true state at time t+k is denoted \\bm x_{t+k}. Similarly, \\bm u_{t+k|t} is the future control at t+k as computed at time t using the information available at that time, that is, \\bm x_t.\n\n\n\n\n\n\nAlternative notation for variables in MPC optimization problems\n\n\n\nIf you find the notation \\bm x_{t+k|t} clumsy, feel free to replace it with something simpler that does not explicitly mention the time at which the prediction/optimization is made but does not clash with the true state at the given time. Perhaps just using a different letter with a simple lower index such as \\bm z_k for the state and \\bm v_k for the control, while understanding k relative with respect to the current discrete time.\n\n\nThe optimization problem to be solved at every discrete- time t is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_{t|t},u_{t+1}\\ldots, \\bm u_{t+N_\\mathrm{p}-1}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}} &\\quad  \\frac{1}{2} \\bm x_{t+N_\\mathrm{p}|t}^\\top \\mathbf S \\bm x_{t+N_\\mathrm{p}|t} + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left(\\bm x_{t+k|t}^\\top \\mathbf Q \\bm x_{t+k|t} + \\bm u_{t+k|t}^\\top \\mathbf R \\bm u_{t+k|t} \\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t, \\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_{t+k|t} \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\bm x_{t+k|t} \\leq \\mathbf x^{\\max}.\n\\end{aligned}\n\nNote that by using the upright front in \\mathbf x_t we emphasize that the current state plays (measured or estimated) the role of a parameter and not an optimization variable within this optimization problem.\nPreviously we have learnt how to rewrite this finite-horizon optimal control problem as a QP problem, which can then be solved with a dedicated QP solver. However, it is worth analyzing the case without the inequality constraints. We know that we can formulate a system of linear equations and solvem, which we formally write at \n\\begin{bmatrix} \\bm u_{t|t} \\\\ \\bm u_{t+1|t} \\\\ \\vdots \\\\ \\bm u_{t+N-1|t} \\end{bmatrix}\n=\n\\mathbf{H}^{-1} \\mathbf{F} \\mathbf x_t,\n but since we only indend to apply the first element of the control trajectory, we can write \n\\bm u_{t|t}\n=\n\\underbrace{\\begin{bmatrix} \\mathbf I & \\mathbf 0 & \\mathbf 0 & \\ldots & \\mathbf 0 \\end{bmatrix}\n\\mathbf{H}^{-1} \\mathbf{F}}_{\\mathbf K_t} \\mathbf x_t,\n in which we can recognize the classical state feedback with the (time-varying) gain \\mathbf K_t. This is a very useful observation – the MPC strategy, when not considering inequality constraints (aka bounds) on the control or state variables, is just a time-varying state feedback. This observation will turn crucial in later chapters when we come back to MPC and analyze its stability.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#mpc-tracking",
    "href": "discr_dir_mpc.html#mpc-tracking",
    "title": "Model predictive control (MPC)",
    "section": "MPC tracking",
    "text": "MPC tracking\n\n(Nonzero) state reference tracking\nAn immediate extension of the regulation problem is that we replace the desired target (reference) state \\mathbf x^\\mathrm{ref} = \\mathbf 0 with some nonzero \\mathbf x^\\mathrm{ref} \\neq \\mathbf 0. The cost function then changes to \n\\begin{aligned}\nJ(\\ldots) &= \\frac{1}{2} (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf S \\, (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})\\\\\n& \\qquad\\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}}) + \\bm u_{t+k|t}^\\top \\mathbf R \\bm u_{t+k|t}\\right).\n\\end{aligned}\n\nIt seems that we are done, that the only change that had to be made was replacement of the predicted state by the prediction of tracking error in the cost function. But not so fast! Recall that unless the nonzero reference state qualifies as an equilibrium, the control needed to keep the system at the corresponding state is nonzero. Namely, for a general discrete-time linear system \\bm x_{k+1} = \\mathbf A \\bm x_k + \\mathbf B \\bm u_k, at an equilibrium (a steady state) \\mathbf x^\\mathrm{ss}, the following must be satisfied by definition \n\\bm x^\\mathrm{ss} = \\mathbf A \\bm x^\\mathrm{ss} + \\mathbf B \\bm u^\\mathrm{ss}.\n\nRewriting this, we get \n(\\mathbf A - \\mathbf I) \\bm x^\\mathrm{ss} + \\mathbf B \\bm u^\\mathrm{ss} = \\mathbf 0.\n\nFixing the value of the steady state vector \\bm x^\\mathrm{ss} to \\mathbf x^\\mathrm{ref}, we see that \\bm u^\\mathrm{ss} = \\mathbf 0 if (\\mathbf A - \\mathbf I) \\bm x^\\mathrm{ref}, that is, if the desired steady state \\bm x^\\mathrm{ref} is in the null space of (\\mathbf A - \\mathbf I). If the desired steady state does not satisfy the condition, which will generally be the case, the corresponding control is necessarily nonzero. It then makes no sense to penalize the control itself. Instead, its deviation from the compatible nonzero value should be penalized. As a consequence, the cost function should be modified to\n\n\\begin{aligned}\nJ(\\ldots) &= \\frac{1}{2} (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf S \\, (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})\\\\\n& \\qquad\\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}}) + (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})^\\top \\mathbf R (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})\\right),\n\\end{aligned}\n in which we regard {\\color{blue}\\mathbf x^\\mathrm{ref}} as an input to the optimal control problem, and {\\color{ss}\\bm u^\\mathrm{ss}} is a new unknown.\nThe optimization problem to be solved at every discrete- time t is then \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_{t|t},u_{t+1}\\ldots, \\bm u_{t+N_\\mathrm{p}-1}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}, {\\color{red}\\bm u^\\mathrm{ss}}} &\\quad  \\frac{1}{2} (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf S \\, (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})\\\\\n& \\qquad\\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}}) + (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})^\\top \\mathbf R (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})\\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t, \\\\\n                    &\\quad \\mathbf B {\\color{red}\\bm u^\\mathrm{ss}} = (\\mathbf I-\\mathbf A) {\\color{blue}\\mathbf x^\\mathrm{ref}},\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_{t+k|t} \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\bm x_{t+k|t} \\leq \\mathbf x^{\\max}.\n\\end{aligned}\n\nNote that compared to the regulation problem, here the optimization problem contains a new (vector) optimization variable \\bm u^\\mathrm{ss}, and a new system of linear equations.\n\n\nOutput reference tracking\nOftentimes we do not have reference values for all the state variables but only for the output variables. These are given by the output equation \n\\bm y_k = \\mathbf C \\bm x_k + \\mathbf D \\bm u_k.\n\nFor notational convenience we restrict ourselves to the case of no feedthrough, that is, \\mathbf D = \\mathbf 0.\nThe goal for the controller is then to make the difference between \\bm y_k = \\mathbf C \\bm x_k and \\mathbf y^\\mathrm{ref} (often named just \\mathbf r if it is clear from the context the reference value of which variables it represents) go to zero.\nSimilarly as in the case of (nonzero) reference state tracking, nonzero control \\bm u^\\mathrm{ss} must be expected in steady state.\nNote, however, that the steady state \\bm x^\\mathrm{ss} is not provided this time – we must relate it to the provided reference output. But it is straightforward:\n\n\\mathbf y^\\mathrm{ref} = \\mathbf C \\bm x^\\mathrm{ss}\\qquad\\qquad (\\text{or}\\; \\mathbf y^\\mathrm{ref} = \\mathbf C \\bm x^\\mathrm{ss} + \\mathbf D \\bm u^\\mathrm{ss}\\; \\text{in general}).\n\nNow we have all that is needed to formulate the MPC problem for output reference tracking:\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_{t|t},u_{t+1}\\ldots, \\bm u_{t+N_\\mathrm{p}-1}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}, {\\color{red}\\bm u^\\mathrm{ss}}, {\\color{blue}\\bm x^\\mathrm{ss}}} &\\quad  \\frac{1}{2} (\\mathbf C\\bm x_{t+N_\\mathrm{p}|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})^\\top \\mathbf S \\, (\\mathbf C\\bm x_{t+N_\\mathrm{p}|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})\\\\\n& \\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\mathbf C\\bm x_{t+k|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\mathbf C\\bm x_{t+k|t}-{\\color{green}\\mathbf y^\\mathrm{ref}}) + (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})^\\top \\mathbf R (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})\\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t, \\\\\n                    &\\quad (\\mathbf A-\\mathbf I) {\\color{blue}\\bm x^\\mathrm{ss}} + \\mathbf B {\\color{red}\\bm u^\\mathrm{ss}} = 0,\\\\\n                    &\\quad \\mathbf C {\\color{blue}\\bm x^\\mathrm{ss}} = {\\color{green}\\mathbf y^\\mathrm{ref}},\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_{t+k|t} \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\bm x_{t+k|t} \\leq \\mathbf x^{\\max}.\n\\end{aligned}\n\nThe tricky part is the terminal term in the cost function. Note that if we consider a nonzero \\mathbf D matrix, the cost function is \n\\begin{aligned}\nJ(\\ldots) &= \\xcancel{\\frac{1}{2} (\\bm y_{t+N_\\mathrm{p}|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})^\\top \\mathbf S \\, (\\bm y_{t+N_\\mathrm{p}|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})}\\\\\n& \\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\bm y_{t+k|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\bm y_{t+k|t}-{\\color{green}\\mathbf y^\\mathrm{ref}}) + (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})^\\top \\mathbf R (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})\\right),\n\\end{aligned}\n but the trouble is that we only consider \\bm u_{t+k|t} for k = 0, \\ldots, N_\\mathrm{p}-1, and therefore \\bm y_{t+N_\\mathrm{p}-1|t} is the last available predicted output – it does not provide \\bm y_{t+N_\\mathrm{p}|t}. The terminal penalty must then be omitted from the cost function, which we have already indicated in the formula by crossing it out.\n\n\nPenalizing the control increments\nThere is an alternative way how to handle the need to consider a nonzero control at steady state. We can penalize not the control itself but its change, its increment \n\\Delta \\bm u_{t+k|t} = \\bm u_{t+k|t} - \\bm u_{t+k-1|t}.\n\nThe rationale behind this is that once at steady state, the control does not change any longer.\nBut then if \\Delta \\bm u_{t+k|t} is the new optimization variable that replaces \\bm u_{t+k|t} in the cost function, we must still be able to express the control \\bm u_{t+k|t}\n\n\\bm u_{t+k|t} = \\bm u_{t+k-1|t} + \\Delta \\bm u_{t+k|t},\n for which we need to keep track of the control at the previous time step. This can be done systematically just by augmenting the state model with an auxillary state variable \\bm x_{t+k|t}^\\mathrm{u} \\coloneqq \\bm u_{t+k-1|t} \n\\begin{bmatrix} \\bm x_{t+k+1|t} \\\\ \\bm x_{t+k+1|t}^\\mathrm{u} \\end{bmatrix} = \\underbrace{\\begin{bmatrix} \\mathbf A & \\mathbf B \\\\ \\mathbf 0 & \\mathbf I \\end{bmatrix}}_{\\mathbf{\\widetilde{A}}} \\underbrace{\\begin{bmatrix} \\bm x_{t+k|t} \\\\ \\bm x_{t+k|t}^\\mathrm{u} \\end{bmatrix}}_{\\tilde{\\bm{x}}_{t+k|t}} + \\underbrace{\\begin{bmatrix} \\mathbf B \\\\ \\mathbf I \\end{bmatrix}}_{\\mathbf{\\widetilde{B}}} \\Delta \\bm u_{t+k|t}.\n\nWith this new augmented state vector, the output is \n\\bm y_{t+k|t} = \\underbrace{\\begin{bmatrix} \\mathbf C & \\mathbf 0 \\end{bmatrix}}_{\\mathbf{\\widetilde{C}}} \\begin{bmatrix} \\bm x_{t+k|t} \\\\ \\bm x_{t+k|t}^\\mathrm{u} \\end{bmatrix}.\n\nWith this new system we can now proceed to formulate the MPC tracking problem \n\\begin{aligned}\n\\operatorname*{minimize}_{\\Delta \\bm u_{t|t},\\Delta u_{t+1}\\ldots, \\Delta \\bm u_{t+N_\\mathrm{p}-1}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}} &\\quad  \\frac{1}{2} (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+N_\\mathrm{p}|t} - \\mathbf y^\\mathrm{ref})^\\top \\mathbf S \\, (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+N_\\mathrm{p}|t} - \\mathbf y^\\mathrm{ref}) \\\\\n&\\qquad \\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+k|t} - \\mathbf y^\\mathrm{ref})^\\top \\mathbf Q \\, (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+k|t} - \\mathbf y^\\mathrm{ref}) + \\Delta \\bm u_{t+k|t}^\\top \\mathbf R \\, \\Delta \\bm u_{t+k|t} \\right)\\\\\n\\text{subject to}   &\\quad \\tilde{\\bm{x}}_{t+k+1|t} = \\mathbf{\\widetilde{A}}\\tilde{\\bm{x}}_{t+k|t} + \\mathbf{\\widetilde{B}}\\Delta \\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\tilde{\\bm{x}}_{t|t} = \\begin{bmatrix}\\mathbf x_t\\\\ \\mathbf u_{t-1}\\end{bmatrix}, \\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\begin{bmatrix}\\bm I & \\mathbf 0 \\end{bmatrix} \\tilde{\\bm x}_{t+k|t} \\leq \\mathbf x^{\\max},\\quad k = 0, \\ldots, N_\\mathrm{p},\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\begin{bmatrix}\\bm 0 & \\mathbf I \\end{bmatrix} \\tilde{\\bm x}_{t+k|t} +  \\Delta \\bm u_{t+k|t}\\leq \\mathbf u^{\\max},\\quad k = 0, \\ldots, N_\\mathrm{p}-1.\n\\end{aligned}\n\nA bonus of this formulation is that we can also impose contraints on \\Delta \\bm u_{t+k|t}, which effectively implements rate constraints on the control variables. This is useful because sometimes we want to restrict how fast the control variable (say, valve opening) changes in order to save the actuators.\nNote that similarly as before, we cannot include the terminal cost if the output equation contains a feedthrough term, that is if \\mathbf D\\neq 0, in which case \\bm u_{t+N|t} would be needed, while it is not available in our optimization problem. This does not pose a singificant trouble purely from the viewpoint of expressing the control requirements by the cost function, but we only mention in passing that it does pose a problem when it comes to guaranteening the closed-loop stability. We postpone our discussion of closed-loop stability till later chapters, but here we only mention that the terminal penalty (on the prediction horizon), through which we penalize the deviation of the system from the desired state, is one of the mechanisms for achieving closed-loop stability. Within this \\Delta \\bm u framework we eliminated the need to compute the steady state \\bm x^{ss}, but if stability guarantees are needed, it must be reintroduced to the problem.\n#TODO Show the optimization problem including the computation of the steady state and the terminal penalty on the deviation from the steady state (even though performance-wise we are only interested in the output reference tracking).\n\n\nOutput reference tracking for general references (preview control)\nFinally, we consider not just a single value of the output vector \\mathbf y^\\mathrm{ref} towards which the system should ultimately be steered, but we consider an arbitrary reference trajectory {\\color{orange}\\mathbf y_k^\\mathrm{ref}}. The MPC problem changes (slightly) to \n\\begin{aligned}\n\\operatorname*{minimize}_{\\Delta \\bm u_{t|t},\\Delta u_{t+1}\\ldots, \\Delta \\bm u_{t+N_\\mathrm{p}-1}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}} &\\quad  \\frac{1}{2} (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+N_\\mathrm{p}|t} - {\\color{orange}\\mathbf y_{t+N_\\mathrm{p}}^\\mathrm{ref}})^\\top \\mathbf S \\, (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+N_\\mathrm{p}|t} - {\\color{orange}\\mathbf y_{t+N_\\mathrm{p}}^\\mathrm{ref}}) \\\\\n&\\qquad \\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+k|t} - {\\color{orange}\\mathbf y_{t+k}^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+k|t} - {\\color{orange}\\mathbf y_{t+k}^\\mathrm{ref}}) + \\Delta \\bm u_{t+k|t}^\\top \\mathbf R \\, \\Delta \\bm u_{t+k|t} \\right)\\\\\n\\text{subject to}   &\\quad \\tilde{\\bm{x}}_{t+k+1|t} = \\mathbf{\\widetilde{A}}\\tilde{\\bm{x}}_{t+k|t} + \\mathbf{\\widetilde{B}}\\Delta \\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\tilde{\\bm{x}}_{t|t} = \\begin{bmatrix}\\mathbf x_t\\\\ \\mathbf u_{t-1}\\end{bmatrix}, \\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\begin{bmatrix}\\bm I & \\mathbf 0 \\end{bmatrix} \\tilde{\\bm x}_{t+k|t} \\leq \\mathbf x^{\\max},\\quad k = 0, \\ldots, N_\\mathrm{p},\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\begin{bmatrix}\\bm 0 & \\mathbf I \\end{bmatrix} \\tilde{\\bm x}_{t+k|t} +  \\Delta \\bm u_{t+k|t}\\leq \\mathbf u^{\\max},\\quad k = 0, \\ldots, N_\\mathrm{p}-1.\n\\end{aligned}\n\nOne last time we repeat that if a general output equation with a feedthrough term is considered, the terminal term in the cost function should be omitted.\n#TODO",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#hard-constraints-vs-soft-constraints-on-state-variables",
    "href": "discr_dir_mpc.html#hard-constraints-vs-soft-constraints-on-state-variables",
    "title": "Model predictive control (MPC)",
    "section": "Hard constraints vs soft constraints on state variables",
    "text": "Hard constraints vs soft constraints on state variables\nWhile it is fairly natural to encode the lower and upper bounds on the state variables as inequality constraints in the optimal control problem, this approach comes with a caveat – the corresponding optimization problem can be infeasible. This is a major trouble if the optimization problem is solved online (in real time), which is the case of an MPC controller. The infeasibility of the optimization problem then amounts to the controller being unable to provide its output.\n\nFor example, we may require that the error of regulating the intervehicular gap by an adaptive cruise control (ACC) system is less then 1 m. At one moment, this requirement may turn out unsatisfiable, while, say, 1.1 m error could be achievable, which would cause no harm. And yet the controller would essentially give up and produce no command to the engine. A major trouble.\nAn alternative is to move the requirement from the constraints to the cost function as an extra term. This way, however, the original hard constraint turns into a soft one, by which we mean that we do not guarantee that the requirement is satisfied, but we discourage the optimization algorithm from breaking it by imposing a penalty proportional to how much the constraint is exceeded.\nWe sketch the scheme here. For the original problem formulation with the hard constraints on the output variables\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_1,\\ldots, \\bm u_{N-1}} &\\quad \\sum_k^N \\left[\\ldots \\right]\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N-1, \\\\\n                    &\\quad \\bm y_{t+k|t} = \\mathbf C\\bm x_{t+k|t} + \\mathbf D\\bm u_{t+k|t},\\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t,\\\\\n                    &\\quad \\ldots \\\\\n                    &\\quad \\mathbf y^{\\min} \\leq \\bm y_{t+k|t} \\leq \\mathbf y^{\\max}.\n\\end{aligned}\n\nwe propose the version with soft constraints \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_1,\\ldots, \\bm u_{N-1}, {\\color{red}\\epsilon}} &\\quad \\sum_k^N  \\left[\\ldots {+ \\color{red}\\gamma \\epsilon} \\right]\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N-1, \\\\\n                    &\\quad \\bm y_{t+k|t} = \\mathbf C\\bm x_{t+k|t} + \\mathbf D\\bm u_{t+k|t},\\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t,\\\\\n                    &\\quad \\ldots \\\\\n                    &\\quad \\mathbf y^{\\min} {\\color{red}- \\epsilon \\mathbf v} \\leq \\bm y_k \\leq \\mathbf y^{\\max} {\\color{red}+ \\epsilon \\mathbf v},\n\\end{aligned}\n where \\gamma &gt; 0 and \\mathbf v\\in\\mathbb R^p are fixed parameters and \\epsilon is the additional optimization variable.\n\n\n\n\n\n\nRequirements expressed through constraints or an extra term in the cost function\n\n\n\nWe have just encountered another instance of the classical dillema in optimization and optimal control that we have had introduced previously. Indeed, it is fairly fundamental and appears both in applications and in development of theory. Keep this degree of freeedom in mind on your optimization and optimal control journey.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#prediction-horizon-vs-control-horizon",
    "href": "discr_dir_mpc.html#prediction-horizon-vs-control-horizon",
    "title": "Model predictive control (MPC)",
    "section": "Prediction horizon vs control horizon",
    "text": "Prediction horizon vs control horizon\nOne of the key parameters of a model predictive control is the prediction horizon N_\\mathrm{p}. It must be long enough so that the key dynamics of the system has enough time to exhibit, and yet it must not be too long, because the computational load will be then too high. A rule of thumb (but certainly not a law) is N_\\mathrm{p}\\approx 20. There is one simple way to reduce the computational load – consider the control trajectory defined on a much shorted time horizon than the predicted state trajectory. Namely, we introduce the control horizon N_\\mathrm{c} &lt; N_\\mathrm{p} (typically N_\\mathrm{c} can be as small as 2 or 3 or so), and we only consider the control as optimizable on this short horizon. Of course, we must provide some values after this horizon as well (untill the end of the prediction horizon). The simplest strategy is to set it to the last value on the control horizon. The MPC problem then changes to \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_{t|t},u_{t+1|t}\\ldots, \\bm u_{t+{\\color{red}N_\\mathrm{c}}-1|t}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}} &\\quad  \\frac{1}{2} \\bm x_{t+N_\\mathrm{p}|t}^\\top \\mathbf S \\bm x_{t+N_\\mathrm{p}|t} + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\bm x_{t+k|t}^\\top \\mathbf Q \\bm x_{t+k|t} + + \\frac{1}{2} \\sum_{k=0}^{{\\color{red}N_\\mathrm{c}}-1} \\bm u_{t+k|t}^\\top \\mathbf R \\bm u_{t+k|t}\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t, \\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_{t+k|t} \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\bm x_{t+k|t} \\leq \\mathbf x^{\\max},\\\\\n                    &\\quad {\\color{red} \\bm u_{t+k|t} = \\bm u_{t+N_\\mathrm{c}-1|t}, \\quad k=N_\\mathrm{c}, N_\\mathrm{c}+1, \\ldots, N_\\mathrm{p}}.\n\\end{aligned}",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#move-blocking",
    "href": "discr_dir_mpc.html#move-blocking",
    "title": "Model predictive control (MPC)",
    "section": "Move blocking",
    "text": "Move blocking\nAnother strategy for reducing the number of control variables is known as move blocking. In this approach, the control inputs are held constant over several time steps (they are combined into blocks), effectively reducing the number of optimization variables. More on this in [1].",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#open-questions-for-us-at-this-moment",
    "href": "discr_dir_mpc.html#open-questions-for-us-at-this-moment",
    "title": "Model predictive control (MPC)",
    "section": "Open questions (for us at this moment)",
    "text": "Open questions (for us at this moment)\n\nCan the stability of a closed-loop system with an MPC controller be guaranteed? Even in the linear case we cannot just have a look at some poles to make a conclusion.\nWhat is the role of the terminal state cost in the MPC problem? And how shall we choose it? Apparently, if the original time horizon is finite but very long (say, 1000), with the prediction horizon set to 20, we can hardly argue that the corresponding term in the MPC cost function expresses our requirements on the behaviour of the system at time 1000.\n\nWe are going to come back to these after we investigate the other two approaches to discrete-time optimal control – the indirect approach and the dynamical programming.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "rocond_mixed_sensitivity.html",
    "href": "rocond_mixed_sensitivity.html",
    "title": "Mixed sensitivity design",
    "section": "",
    "text": "In the previous chapter we mentioned that there are several ways to capture uncertainty in the model and analyze robustness with respect to the uncertainty. We have chosen the worst-case approach based on the small gain theorem for analysis of robust stability, which in the case of linear systems has an intuitive frequency-domain interpretation.\nThis chosen framework has two benefits. First, having being formulated in frequency domain, it offers us to take advantage of the insight developed in introductory courses on automatic control, that typically invest quite some effort into developing frequency doman concepts such as magnitude and phase Bode plots, Nyquist plot, and sensitivity and complementary sensitivity functions. Generations of control engineers have contributed to the collective know-how carried by these classical concepts and techniques.\nSecond, by formulating the requirements on robust stability, nominal stability and robust performance as constraints on \\mathcal H_\\infty norms of some closed-loop systems, an immediate extension from analysis to automated synthesis (control design) is enabled by availability of numerical methods for \\mathcal H_\\infty optimization. This enhances the classical frequency-domain control design techniques in that while the classical methods require that we know what we want and we also know how to achieve it, the \\mathcal H_\\infty optimization based methods require that we only know what we want (and express our requirements in frequency domain). We don’t have to bother with how to achieve it because there are numerical solvers that will do the job for us.\nLet’s introduce the first instance of such methodology. We have learnt that the robust performance condition in presence of multiplicative uncertainty is formulated as a bound on the \\mathcal H_\\infty norm of the mixed sensitivity function \\begin{bmatrix}W_pS\\\\WT\\end{bmatrix}, namely \n\\left\\|\n\\begin{bmatrix}\nW_pS\\\\WT\n\\end{bmatrix}\n\\right\\|_{\\infty}\n&lt; \\frac{1}{\\sqrt{2}}.\n\nEvaluating this condition can be done in a straightforward way, either at a grid of frequencies (inefficient) or by invoking a method for computing the norm.\nBut the major god news of this chapter is that we can also turn this into an optimization problem \n\\operatorname*{minimize}_{K \\text{ stabilizing}}\n\\left\\|\n\\begin{bmatrix}\nW_pS\\\\WT\n\\end{bmatrix}\n\\right\\|_{\\infty}.\n\nIn words, we are looking for a controller K that guaranees stability of the closed-loop system and it also minimizes the \\mathcal H_\\infty norm of the mixed sensitivity function.\nSuch optimization solvers are indeed available.\n\n\n\n\n\n\nMixed sensitivity minimization as a special case of the general \\mathcal H_\\infty optimization\n\n\n\nIn anticipation of what is to come, we note here that the above minimization of the \\mathcal H_\\infty norm of the mixed sensitivity function is a special case of the more general \\mathcal H_\\infty optimization problem (minimization of the norm of a general closed-loop transfer function). Therefore, even if your software tools does not have a specific function for mixed sensitivity optimization, chances are that a solver for the general \\mathcal H_\\infty optimization function is available. And we will soon see how to reformulate the mixed sensitivity minimization as the general \\mathcal H_\\infty optimization problem.\n\n\nHaving derived the bound on the norm of the mixed sensitivity function (equal to 1/\\sqrt{2} in the SISO case), it may now be tempting to conclude that the only goal of the optimization is to find a controller that satisfies this bound. However, it turns out that the optimization has another useful property – it is called self-equalizing property. We are not going to prove it, we will be happy just to interpret it: it means that with the optimal controller the frequency response of the considered (weighted and possibly mixed) sensitivity function is flat (constant over all frequencies).\nIn order to understand the impact of this property, let us consider the problem of minimizing just \\|WT\\|_\\infty. We choose this problem even though practically it is not really useful to require just (robust) stability. For \\gamma = \\min_{K}\\|WT\\|_\\infty, the flatness of the frequency response |W(j\\omega)T(j\\omega)| means that the magnitude frequency response |T(j\\omega)| is proportional to 1/|W(j\\omega)|, that is,\n\n|T(j\\omega)| = \\frac{\\gamma}{|W(j\\omega)|},\\qquad \\gamma \\in \\mathbb R, \\gamma &gt; 0.\n\nThis gives another motivation for our \\mathcal{H}_\\infty optimization endeavor – through minimization we shape the closed-loop magnitude frequency responses. This automatic/automated loopshaping is the second benefit promised at the beginning of this section. But we emphasize that for practical pursposes it is only useful to minimize the norm of the mixed sensitivity function, in which case more than just simultaneous shaping of W_\\mathrm{p}S and WT must be achieved.\nWith this new interpretation, we can feel free to include other terms in the optimization criterion. In particular, the criterion can be extended to include the control effort as in (after reindexing the weighting filters) \\boxed\n{\\operatorname*{minimize}_{K \\text{ stabilizing}}  \n\\left\\|\n\\begin{bmatrix}\nW_1S\\\\W_2KS\\\\W_3T\n\\end{bmatrix}\n\\right\\|_{\\infty}.}\n\nThe middle term penalizes control (similarly as R term in LQ optimality criterion \\int(x^TQx+u^TRu)dt). Typically it is sufficient to set it equal to a nonnegative constant.\nAn important property of this method is that it extends to the multiple-input-multiple-output (MIMO) case. Nothing needs to be changes in the formal problem statement as the \\mathcal H_\\infty norm is defined for MIMO systems as well.\n\n\n\n Back to top",
    "crumbs": [
      "12. Robust control",
      "Mixed sensitivity design"
    ]
  },
  {
    "objectID": "cont_dp_DDP.html",
    "href": "cont_dp_DDP.html",
    "title": "Differential dynamic programming (DDP)",
    "section": "",
    "text": "#TODO\n\n\n\n Back to top",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Differential dynamic programming (DDP)"
    ]
  },
  {
    "objectID": "ext_H2.html",
    "href": "ext_H2.html",
    "title": "H2-optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "H2-optimal control"
    ]
  },
  {
    "objectID": "cont_dp_references.html",
    "href": "cont_dp_references.html",
    "title": "References",
    "section": "",
    "text": "Dynamic programming for continuous-time systems is typically not covered in standard texts on dynamic programming, because those mainly focus on discrete-time systems. But there is no shortage of discussions of HJB equation in control theory texts. Our introductory treatment here is based on Section 6.3 in [1].\nThe class [2] uses HJB equation as the main tool for solving various version of the LQR problem.\n[3] discusses the HJB equation in Chapter 5. In the section 5.2 they also discuss the connection with Pontryagin’s principle.\n\n\n\n\n Back to topReferences\n\n[1] F. L. Lewis, D. Vrabie, and V. L. Syrmo, Optimal Control, 3rd ed. John Wiley & Sons, 2012. Accessed: Mar. 09, 2022. [Online]. Available: https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf\n\n\n[2] B. D. O. Anderson and J. B. Moore, Optimal Control: Linear Quadratic Methods, Reprint of the 1989 edition. Dover Publications, 2007. Available: http://users.cecs.anu.edu.au/~john/papers/BOOK/B03.PDF\n\n\n[3] D. Liberzon, Calculus of Variations and Optimal Control Theory: A Concise Introduction. Princeton University Press, 2011. Available: http://liberzon.csl.illinois.edu/teaching/cvoc/cvoc.html",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "References"
    ]
  },
  {
    "objectID": "cont_indir_time_optimal.html",
    "href": "cont_indir_time_optimal.html",
    "title": "Time-optimal control",
    "section": "",
    "text": "The task of bringing the system from a given state to some given final state (either a single state or a set of states) can be formulated by setting \nL = 1,\n which turns the cost functional to \nJ = \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}1\\text{d}t = t_\\mathrm{f}-t_\\mathrm{i}.",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Time-optimal control"
    ]
  },
  {
    "objectID": "cont_indir_time_optimal.html#time-optimal-control-for-a-linear-system",
    "href": "cont_indir_time_optimal.html#time-optimal-control-for-a-linear-system",
    "title": "Time-optimal control",
    "section": "Time-optimal control for a linear system",
    "text": "Time-optimal control for a linear system\nWe restrict ourselves to an LTI system \n\\dot{\\bm x} = \\mathbf A\\bm x + \\mathbf B\\bm u,\\qquad t_\\mathrm{i} = 0, \\; \\bm x(t_\\mathrm{i}) = \\mathbf x_0,\n for which we set the desired final state as \n\\bm x(t_\\mathrm{f}) = 0.\n\nThis only makes sense if we impose some bounds on the control. We assume the control to be bounded by \n|u_i(t)| \\leq 1\\quad \\forall i, \\; \\forall t.\n\nThe necessary conditions can be assembled immediately by forming the Hamiltonian \nH = \\boldsymbol\\lambda^\\top \\,(\\mathbf A\\bm x+\\mathbf B\\bm u) - 1,\n and substituting into the (control) Hamilton canonical equations \n\\begin{aligned}\n\\dot{\\bm x} &= \\nabla_{\\boldsymbol\\lambda}H = \\mathbf A\\bm x + \\mathbf B\\bm u,\\\\\n\\dot{\\boldsymbol \\lambda} &= -\\nabla_{\\bm x}H = -\\mathbf A^\\top \\boldsymbol \\lambda.\n\\end{aligned}\n plus the Pontryagin’s statement about maximization of H with respect to \\bm u \nH(t, \\bm x^\\star ,\\bm u^\\star ,\\boldsymbol\\lambda^\\star ) \\geq  H(t, \\bm x^\\star ,\\bm u, \\boldsymbol\\lambda^\\star ), \\; u_i(t)\\in [-1,1]\\; \\forall i, \\; \\forall t.\n\nApplication of Pontryagin’s principle gives \n(\\boldsymbol\\lambda^\\star )^\\top \\, (\\mathbf A\\bm x^\\star +\\mathbf B\\bm u^\\star ) - 1 \\geq (\\boldsymbol\\lambda^\\star )^\\top \\, (\\mathbf A\\bm x^\\star +\\mathbf B\\bm u) - 1,\\quad u_i\\in [-1,1]\\; \\forall i, \\; \\forall t.\n\nCancelling the identical terms on both sides we are left with \n(\\boldsymbol\\lambda^\\star )^\\top \\, \\mathbf B\\bm u^\\star  \\geq (\\boldsymbol\\lambda^\\star )^\\top \\, \\mathbf B\\bm u,\\quad u_i\\in [-1,1]\\; \\forall i, \\; \\forall t.\n\nIt turns out that if this inequality is to hold then with the \\bm u arbitrary on the left (within the bounds), the only way to guarantee the validity is to have \n\\bm u^\\star  = \\text{\\textbf{sgn}}\\left( (\\boldsymbol\\lambda^\\star )^\\top \\, \\mathbf B\\right),\n where the signum function is applied elementwise. Clearly the optimal control is switching between the minimum and maximum values, which is 1 and -1. This is visualized in Fig. 1 for a scalar case (the \\mathbf B matrix has only a single column).\n\n\n\n\n\n\nFigure 1: Switching function and an optimal control derived from it.\n\n\n\nWell, in fact to support this claim, it must be rigorously excluded that the argument of the signum function, the so-called switching function can assume zero value for longer then just a time instant (although repeatedly). Check this by yourself in (or its online version). Search for .\n\nTime-optimal control for a double integrator system\nLet us analyze the situation for a double integrator. This corresponds to a system described by the second Newton’s law. For a normalized mass the state space model is \n\\begin{bmatrix}\n  \\dot y\\\\ \\dot v\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 & 1\\\\ 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  y\\\\ v\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n  0\\\\1\n\\end{bmatrix}\nu.\n\nThe switching function is obviously \\lambda_2(t) and an optimal control is given by \nu(t) = \\text{sgn} \\lambda_2(t).\n\nWe do not know \\lambda_2(t). In order to get it, we may need to solve the costate equations. Indeed, we can solve them independently of the state equations since it is decoupled from them \n\\begin{bmatrix}\n  \\dot \\lambda_1\\\\ \\dot \\lambda_2\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n0 & 0\\\\ 1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\lambda_1\\\\ \\lambda_2\n\\end{bmatrix}\n,\n from which it follows that \n\\lambda_1(t) = c_1\n and \n\\lambda_2(t) = c_1t+c_2.\n for some constants c_1 and c_2. To determine the constants, we will have to bring the boundary conditions finally into the game. The condition that H(t_\\mathrm{f}) = 0 gives \n\\lambda_2(t_\\mathrm{f})u(t_\\mathrm{f}) = 1.\n\nWe can now sketch possible profiles of the switching function. A few characteristic versions are in Fig. 2\n\n\n\n\n\n\nFigure 2: Possible evolutions of the costate \\lambda_2 in time-optimal control\n\n\n\nWhat we have learnt is that the costate \\lambda_2 would go through zero at most once during the whole control interval. Therefore we will have at most one switching of the control signal. This is a valuable observation.\nWe are approaching the final stage of the derivations. So far we have learnt that we can only consider u(t)=1 and u(t)=-1. The state equations can be easily integrated to get \nv(t) = v(0) + ut,\\quad y(t) = y(0) + v(0)t + \\frac{1}{2}ut^2.\n\nTo visualize this in y-v domain, express t from the first and subsitute into the second equation \nu(y-y(0)) = v(0) (v-v(0))+ \\frac{1}{2}(v-v(0))^2,\n which is a family of parabolas parameterized by (y(0),v(0)). These are visualized in Fig. 3.\n\n\nShow the code\nusing Plots\n# Nastavení vykreslování\nplot(legend=false, xlabel=\"y\", ylabel=\"v\")\n\n# Parabolas for u = 1\nu = 1\nv0 = -3\nv = -3:0.1:3\n\nfor y0 in -10:0.5:10\n    y = y0 .+ v0 .* (v .- v0) .+ 1/2 .* (v .- v0).^2\n    plot!(y, v, linewidth=0.5, color=:red)\nend\n\nv = -3:0.01:0\ny = 1/2 .* v.^2\nplot!(y, v, color=:red, linewidth=2)\n\nannotate!(2.5, -1, text(\"u=1\", 12, :red, halign=:right))\n\n# Parabolas for u = -1\nu = -1\nv0 = 3\nv = -3:0.1:3\n\nfor y0 in -10:0.5:10\n    y = -y0 .- v0 .* (v .- v0) .- 1/2 .* (v .- v0).^2\n    plot!(y, v, linewidth=0.5, color=:blue)\nend\n\nv = 0:0.01:3\ny = -1/2 .* v.^2\nplot!(y, v, color=:blue, linewidth=2)\n\nannotate!(-1.0, 1, text(\"u=-1\", 12, :blue, halign=:right))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Two families of (samples of) state trajectories corresponding to the minimum and maximum control for the double integrator system. Highlighted (thick) is the switching curve (composed of two branches).\n\n\n\n\nThere is a distinguished curve in the figure, which is composed of two branches. It is special in that for all the states starting on this curve, the system is brought to the origin for a corresponding setting of the control (and no further switching). This curve, called switching curve can be expressed as \ny = \\left\\{\n\\begin{array}{cl}\n\\frac{1}{2}v^2 & \\text{if} \\; v&lt;0\\\\\n-\\frac{1}{2}v^2 & \\text{if} \\; v&gt;0\n\\end{array}\n\\right.\n or \ny = - \\frac{1}{2}v|v|.\n\nThe final step can be done refering to the figure. We point a finger anywhere in the state plane. We follow the state trajectory that emanates from that particular point for which we can get to the origin with at maximum 1 switching. Clearly the strategy is to set u such that it brings us to the switching curve (the thick one in the figure), switch the control and then just follow the trajectory till the origin. That is it. This control strategy can be written as \\boxed{\nu(t) =\n\\left\\{\n\\begin{array}{cl}\n-1 & \\text{if } y(t)&gt;-\\frac{1}{2}v(t)|v(t)|\\text{ or if } y(t)= -\\frac{1}{2}v(t)|v(t)| \\text{ and }y&lt;0,\\\\\n1 & \\text{if } y(t) &lt; -\\frac{1}{2}v(t)|v(t)|\\text{ or if } y(t)= -\\frac{1}{2}v(t)|v(t)| \\text{ and }y&gt;0.\n\\end{array}\n\\right.}\n\n\n\nShow the code\nusing DifferentialEquations\nusing Plots\nfunction time_optimal_bang_bang_control(x,umin,umax)\n    y = x[1]                    # Position.\n    v = x[2]                    # Velocity.\n    s = y+1/2*v*abs(v)          # Switching curve s(y,v) = 0.\n    if s &gt; 0                    # If above the curve:\n        u = umin\n    elseif s &lt; 0                # If below the curve:\n        u = umax\n    elseif s == 0 && v &gt; 0      # If on the switching curve on the left:\n        u = umin\n    elseif s == 0 && v &lt; 0      # If on the switching curve on the right:\n        u = umax\n    else u = 0                  # If at (0,0).\n    end\n    return u\nend\n\nfunction simulate_time_optimal_double_integrator()\n    A = [0 1; 0 0]\n    B = [0, 1]\n    umin = -1.0\n    umax = 1.0\n    tfinal = 3.6\n    tspan = (0.0,tfinal)\n    x₀ = [1.0,1.0]\n    f(x,p,t) = A*x + B*time_optimal_bang_bang_control(x,umin,umax)\n    prob = ODEProblem(f,x₀,tspan)\n    sol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-8)\n    uopt = time_optimal_bang_bang_control.(sol.u,umin,umax);                         # Remember that the package uses `u` as the state variable.\n    p1 = plot(sol,linewidth=2,xaxis=\"\",yaxis=\"States\",label=[\"x\" \"v\"]) \n    p2 = plot(sol.t,uopt,linewidth=2,xaxis=\"Time\",yaxis=\"Control\",label=\"u\")\n    plot(p1,p2,layout=(2,1))\nend\n\nsimulate_time_optimal_double_integrator()\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Response of a double integrator with a time-optimal (bang-bang) feedback regulator to a nonzero initial state\n\n\n\n\nIn the plots you can find a confirmation of the fact that we derived rigorously—the fact that there will be at most one switch in the control signal… Ooops… This is actually not quite what we see in the plot above, is it? We can see that the control variable switches for the first time at about 2.2 s, but then it switches again at time close to 3.5 s. And it then keeps switching very fast. Consequently, the simulation gets significantly slower and the solver may even appear to get stuck.\nObviosly, what is going on is that the simulator is tempted to include not just two but in fact a huge number of switches in the control signal as it approaches the origin. This is quite characteristic of bang-bang control – a phenomenon called chattering. In this particular example we may decide to ignore it since both state variables are already close enough to the origin and we may want to declare the control task as finished. Generally, this chattering phenomenon needs to be handled somehow systematically. Any suggestion?",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Time-optimal control"
    ]
  },
  {
    "objectID": "intro_software.html",
    "href": "intro_software.html",
    "title": "Software for the course",
    "section": "",
    "text": "Although our course is heavily based on mathematical methods, it is a course designed for (control) engineers, which is to say that our main shared goal is to learn to solve engineering problems by using the mathematical methods. And except for the simplest textbook problems, the needed methods are numerical, which inevitably means that our course must have a strong software component.\nOn the other hand, our graduate course is not just a vocational training in using a dedicated software tool. Instead, we are going to implement the methods introduced in our course by ourselves, at least to some degree, and even if at a prototype level. We believe that this is the best way to learn the potentials and limitations of the methods, even if ultimately – when solving real industrial problems – we may want (or be forced) to use use software tools developed by specialists.",
    "crumbs": [
      "0. Introduction",
      "Software for the course"
    ]
  },
  {
    "objectID": "intro_software.html#julia",
    "href": "intro_software.html#julia",
    "title": "Software for the course",
    "section": "Julia",
    "text": "Julia\nIn our course we are going to use the Julia programming language for implementation of the methods and numerical experimentation. The language and its compiler and standard libraries are free (and open source), ane a rich ecosystem of packages is available. Julia is a modern language, similarly high-level as Matlab or Python, but with a performance comparable to C or Fortran.\nJulia is going to be used not only by the course lecturer(s) when demonstrating the methods in the lectures, these online lecture notes and exercises, but also by the students when programming the solutions to assigned homework problems. In order to reduce the load on students, incomplete code snippets will be provided, which students will have to complete only in the core algorithmic sections.\nBelow we give some basic information about Julia, including links to learning resources.\n\nInstall Julia\nInstructions for all platforms are at https://docs.julialang.org/en/v1/manual/installation/.\n\n\nInstall VS Code\nThe most recommendable IDE is VS Code https://code.visualstudio.com. Julia Extension https://code.visualstudio.com/docs/languages/julia can be installed within VS Code.\n\n\nDocumentation\nThe official documentation is available at https://docs.julialang.org/en/v1/. It can also be downloaded as a PDF file. Programmers fluent with Matlab or Python will find this overview of differences https://docs.julialang.org/en/v1/manual/noteworthy-differences/#Noteworthy-Differences-from-other-Languages useful.\n\n\nDiscussion forum\nAn active discussion forum exists https://discourse.julialang.org. But you may perhaps start by asking questions on our own course forum within the Teams platform.",
    "crumbs": [
      "0. Introduction",
      "Software for the course"
    ]
  },
  {
    "objectID": "intro_software.html#matlab-and-simulink",
    "href": "intro_software.html#matlab-and-simulink",
    "title": "Software for the course",
    "section": "Matlab and Simulink",
    "text": "Matlab and Simulink\nAs a course on computational methods for control design, we certainly cannot ignore the existence of Matlab and Simulink due their wide adoption both in academia and industry. In particular, when it comes to Simulink and its code generation capabilities, there is hardly any alternative. Therefore we will list in the respective sections of these lecture notes the appropropriate Matlab and Simulink functions and tools. After all, those students who decide to work on a semestral project in our automatic control lab will have to use Matlab and Simulink.",
    "crumbs": [
      "0. Introduction",
      "Software for the course"
    ]
  },
  {
    "objectID": "cont_indir_LQR_fin_horizon.html",
    "href": "cont_indir_LQR_fin_horizon.html",
    "title": "Indirect approach to LQR on a finite time horizon using differential Riccati equation",
    "section": "",
    "text": "To get some more insight and also to develop a practical design tool, let us consider the LQR version of this general problem. The Hamiltonian is  \nH(\\bm x, \\bm u, \\bm \\lambda) = \\bm \\lambda^\\top\\, (\\mathbf A\\bm x+\\mathbf B\\bm u) - \\frac{1}{2}\\left(\\bm x^\\top\\mathbf Q \\bm x + \\bm u^\\top\\mathbf R\\bm u\\right),\n in which we note that it does not explicitly depend on time.\nThe three necessary conditions of optimality (the state, costate and stationarity equations) are then \\boxed{\n\\begin{aligned}\n\\dot {\\bm x} &= \\nabla_{\\bm \\lambda} H(\\bm x,\\bm u,\\bm \\lambda) = \\mathbf A\\bm x + \\mathbf B\\bm u,\\\\\n\\dot{\\boldsymbol\\lambda} &= -\\nabla_{\\bm x} H(\\bm x,\\bm u,\\bm \\lambda) = \\mathbf Q\\bm x - \\mathbf A^\\top\\boldsymbol\\lambda,\\\\\n\\bm 0 &= \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda) = \\mathbf R\\bm u - \\mathbf B^\\top\\boldsymbol\\lambda.\n\\end{aligned}}\n\\tag{1}\nProvided \\mathbf R&gt;0, we can express \\bm u from the third equation (aka the stationarity equation) \\boxed{\n\\bm u = \\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda.}\n\\tag{2} and substitute into the first one. This leaves us with just two differential equations\n\\begin{aligned}\n\\dot {\\bm x} &= \\mathbf A\\bm x + \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda,\\\\\n\\dot{\\boldsymbol\\lambda} &= \\mathbf Q\\bm x - \\mathbf A^\\top\\boldsymbol\\lambda.\n\\end{aligned}\nThis is a set of 2n differential equations of the first order (we assume that \\bm x\\in \\mathbb R^n). We need 2n boundary values to fully specify the solution. Where do we get them?",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "LQR on a finite time horizon"
    ]
  },
  {
    "objectID": "cont_indir_LQR_fin_horizon.html#lqr-on-a-finite-and-fixed-time-horizon-with-a-fixed-final-state",
    "href": "cont_indir_LQR_fin_horizon.html#lqr-on-a-finite-and-fixed-time-horizon-with-a-fixed-final-state",
    "title": "Indirect approach to LQR on a finite time horizon using differential Riccati equation",
    "section": "LQR on a finite and fixed time horizon with a fixed final state",
    "text": "LQR on a finite and fixed time horizon with a fixed final state\nFor fixed initial and fixed final states, the boundary conditions are \\boxed{\n\\begin{aligned}\n\\bm x(0) &= \\mathbf x_0,\\\\\n\\bm x(t_\\mathrm{f}) &= \\mathbf x_\\mathrm{f}.\n\\end{aligned}}\n\\tag{3}\nLet’s now rewrite the equations as one large vector equation \n\\underbrace{\\begin{bmatrix}\n\\dot {\\bm x} \\\\ \\dot{\\boldsymbol\\lambda}\n\\end{bmatrix}}_{\\dot{\\bm{w}}}\n=\n\\underbrace{\\begin{bmatrix}\n\\mathbf A & \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\n\\mathbf Q & - \\mathbf A^\\top\n\\end{bmatrix}}_{\\mathbf{H}}\n\\underbrace{\\begin{bmatrix}\n\\bm x \\\\ \\boldsymbol\\lambda\n\\end{bmatrix}}_{\\mathbf{w}}\n\\tag{4}\nSimilarly as in the discrete-time case, we can find the solution numerically by relating the the state and costate at both ends of the time interval. In the continuous-time setting, it is the exponential of the matrix in the above equation that will be used for that purpose. Let’s do it now. By labelling the block matrix in the above as \\mathbf{H} and the vector composed of the state and costate vectors as \\mathbf{w}, we can write \n\\mathbf{w}(t_\\mathrm{f}) = \\underbrace{\\mathrm{e}^{\\mathbf{H}t_\\mathrm{f}}}_{\\boldsymbol\\Phi(t_\\mathrm{f})}\\mathbf{w}(0),\n where \\boldsymbol\\Phi(t) is a state-costate transition matrix. Labelling the blocks as in \n\\boldsymbol \\Phi(t) = \\begin{bmatrix}\n                        \\boldsymbol\\Phi_{11}(t) & \\boldsymbol\\Phi_{12}(t)\\\\\n                        \\boldsymbol\\Phi_{21}(t) & \\boldsymbol\\Phi_{22}(t)\n                       \\end{bmatrix},\n we can write from the equation for the state variable that \n\\bm \\lambda(0) = \\mathbf \\Phi_{12}^{-1}\\left(\\bm x(t_\\mathrm{f})-\\mathbf \\Phi_{11}\\bm x(0)\\right).\n\nNow, the initial value problem in Eq. 4 can be solved for both the state and the costate. Finally, the control signal can be computed using Eq. 2.\n\nExample 1 (LQR on a finite and fixed time horizon with a fixed final state approached as a two-poing BVP)  \n\n\nShow the code\nusing LinearAlgebra\nusing Random\nRandom.seed!(1234)\n\nn = 2                  # order of the system\nA = rand(n,n)\nB = rand(n,1)\n\nQ = diagm(0=&gt;rand(n))  # weighting matrices for the quadratic cost function\nR = rand(1,1)\n\nx0 = [1;2]             # initial states\nx1 = [0;3]             # final (desired) states\n\nt1 = 10                # final time\n\n# Building Hamiltonian system and solving for the missing boundary conditions using a state-transition matrix\n\nH = [A B/R*B'; Q -A']  # the combined matrix for the Hamiltonian cannonical equations\nP = exp(H*t1)          # \"state\"-transition matrix\n\nP11 = P[1:n,1:n]\nP12 = P[1:n,n+1:end]\nP21 = P[n+1:end,1:n]\nP22 = P[n+1:end,n+1:end]\n\nlambda0 = P12\\(x1-P11*x0);  # solving for the initial value of costate\n\n# Solving for the states and costates during the time interval\n\nusing ControlSystems\n\nG = ss(H,zeros(2n,1),Matrix{Float64}(I, 2n, 2n),0) # auxiliary system comprising the states and costates\n\nw0 = [x0; lambda0]\nt = 0:0.1:t1\nv(w,t) = [0]\ny, t, w, uout = lsim(G,v,t,x0=w0)\nx = w[1:n,:]            # state\nλ = w[n+1:end,:]        # costate\nu = (R\\B'*λ)'           # optimal control from the stationarity equation\n\n# Plotting the responses\n\nusing Plots\n\np1 = plot(t,x',ylabel=\"x\",label=\"\",lw=2)\np2 = plot(t,λ',ylabel=\"λ\",label=\"\",lw=2)\np3 = plot(t,u,ylabel=\"u\",label=\"\",lw=2)\n\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The solution to the LQR problem with a fixed final state approached as a two-point BVP. Plotted are the state, costate and control variables.\n\n\n\n\n\nThe procedure works, but we can get even more insight into the problem after setting \\mathbf Q=\\mathbf 0, which will essentially decouple the second equation from the first. Then we can follow the same procedure we demonstrated in the discrete-time setting. The optimal control problem is often referred to as minimum-energy problem as its goal is to control the system from a given initial state to some given final state while minimizing the supplied “energy”.\n\n\n\n\n\n\nIntegrals of squared states and or controls do not necessarily mean energy\n\n\n\nWhile it is quite common to encounter references to integrals of squares of some variables evolving in time such as states or controls as energy, it is useful to keep some reservation here and perceive it more like a jargon. What if the control variable is a position or orientation of some movable part? Squaring it and integrating over time surely does not mean energy.\n\n\nThe procedure for solving the problem \n\\begin{aligned}\n\\dot {\\bm x} &= \\mathbf A\\bm x + \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda,\\\\\n\\dot{\\boldsymbol\\lambda} &= - \\mathbf A^\\top\\boldsymbol\\lambda\n\\end{aligned}\n then proceeds by expressing from the costate equation the solution for \\bm \\lambda as a function of costate at the final time \n\\bm \\lambda(t) = \\mathrm{e}^{-\\mathbf A^\\top(t-t_\\mathrm{f})}\\bm \\lambda(t_\\mathrm{f}).\n\nWe substitute into the state equation \n\\dot {\\bm x}(t) = \\mathbf A\\bm x(t) + \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathrm{e}^{-\\mathbf A^\\top(t-t_\\mathrm{f})}\\bm \\lambda(t_\\mathrm{f})\n and solve for \\bm x(t) \n\\bm x(t) = \\mathrm{e}^{\\mathbf A\\mathbf(t-0)} \\mathbf x_0 + \\int_{0}^{t}\\left[\\mathrm{e}^{\\mathbf A^\\top(t-\\tau)}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathrm{e}^{-\\mathbf A^\\top(\\tau-t_\\mathrm{f})}\\bm \\lambda(t_\\mathrm{f})\\right]\\text{d}\\tau.\n\nEvaluating this at t=t_\\mathrm{f}, and moving the costate at the final time outside of the integral, we get \n\\mathbf x_\\mathrm{f} = \\mathrm{e}^{\\mathbf A\\mathbf(t_\\mathrm{f}-0)} \\mathbf x_0 + \\underbrace{\\int_{0}^{t_\\mathrm{f}}\\left[\\mathrm{e}^{\\mathbf A^\\top(t_\\mathrm{f}-\\tau)}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathrm{e}^{\\mathbf A^\\top(t_\\mathrm{f}-\\tau)}\\right]\\text{d}\\tau}_{\\mathbf G_R(0,t_\\mathrm{f})}\\bm \\lambda(t_\\mathrm{f}).\n\nThe as of yet unknown \\bm \\lambda(t_\\mathrm{f}) can finally be extracted \n\\bm \\lambda(t_\\mathrm{f}) = \\mathbf G_R(0,t_1)^{-1}\\left(\\mathbf x_\\mathrm{f} - \\mathrm{e}^{\\mathbf A t_\\mathrm{f}} \\mathbf x_0\\right),\n where \\mathbf G_R(0,t_1) is the weighted controllability/reachability Gramian. Remember that its inverse only exists if the system is controllable/reachable. The last step constitutes in bringing this value back into the formula for the optimal control \n\\begin{aligned}\n\\bm u(t) &= \\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda(t),\\\\\n              &= \\mathbf R^{-1}\\mathbf B^\\top\\mathrm{e}^{-\\mathbf A^\\top(t-t_\\mathrm{f})}\\bm \\lambda(t_\\mathrm{f})\\mathbf G_R(0,t_1)^{-1}\\left(\\bm x_\\mathrm{f} - \\mathrm{e}^{\\mathbf A\\mathbf(t_\\mathrm{f}-0)} x_0\\right).\n\\end{aligned}\n\nThe conclusions are similar if not identical to those in the discrete-time setting:\n\nThe necessary condition for the existence of optimal control is controllability of the system.\nThe minimum-energy control for a fixed final state assignment can be obtained in the form of a precomputed signal (well, in this case, if computation is to be performed numerically, a discrete-time approximation of the continuous-time variable could only be computed and stored).\n\nHonestly, from a practical viewpoint, it is hard to view the usefulness of the above procedure (exploiting the structure of the problem when \\mathbf Q=\\mathbf 0) as anything else than just giving us some insight. We have already learnt during our previous treatment of discrete-time systems that much more useful kind of a control – feedback control – can be computed if we relax the final state constraint. We are going to show this for continuous-time systems too.\nBefore we do that, we should address sufficiency. #TODO",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "LQR on a finite time horizon"
    ]
  },
  {
    "objectID": "cont_indir_LQR_fin_horizon.html#optimal-control-on-a-finite-and-fixed-time-interval-with-a-free-final-state",
    "href": "cont_indir_LQR_fin_horizon.html#optimal-control-on-a-finite-and-fixed-time-interval-with-a-free-final-state",
    "title": "Indirect approach to LQR on a finite time horizon using differential Riccati equation",
    "section": "Optimal control on a finite and fixed time interval with a free final state",
    "text": "Optimal control on a finite and fixed time interval with a free final state\nApparently, the open-loop nature of the optimal control for the fixed-final-state scenario is not quite satisfactory in most engineering applications. Similarly as in the discrete-time situation we may suspect that by relaxing the final state we may obtain a more useful control scheme. Relaxing the final state does not mean that we resign at the task of controlling the system behavior at the end of the control interval. It is only that now we will have to use the terminal costs to make the regulation error arbitrarily small at the end of the time horizon. Starting with the general case, the optimal control criterion is then \nJ = \\phi(\\bm x(t_\\mathrm{f}))+\\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}L(\\bm x,\\bm u,t)\\text{d}t.\n\nFirst we need to go back to the basic problem of calculus of variations and see how the solution to the basic problem changes if we set one of the ends free. As we are going to see in a minute, the optimal solution will still have to satisfy the Euler-Lagrange equation, the boundary condition will change, however.\n\nFree end in the calculus of variations\nSwitching to the notation in the calculus of variations temporarily, the set of candidate trajectories among which we search for and extremal is described in Fig. 2.\n\n\n\n\n\n\nFigure 2: Basic problem of calculus of variations with a free end\n\n\n\nLet us repeat here the previously derived expression for the first variation \n\\delta J(x,y(\\cdot),y'(\\cdot)) = \\left[\\frac{\\partial L(x,y,y')}{\\partial y'}\\delta(x)\\right]_{a}^b + \\int_a^b \\left( \\frac{\\partial L(x,y,y')}{\\partial y}-\\frac{\\text{d}}{\\text{d}x}\\frac{\\partial L(x,y,y')}{\\partial y'}\\right)\\delta(x)\\text{d}x.\n\nThis time, however, the first term on the right is not zero. In particular, \\delta y(a) = 0 but \\delta y(b) \\neq 0. Therefore the first variation is \n\\delta J = \\left.\\frac{\\partial L(x,y,y')}{\\partial y'}\\delta y(x)\\right|_{b} + \\int_a^b \\left( \\frac{\\partial L(x,y,y')}{\\partial y}-\\frac{\\text{d}}{\\text{d}x}\\frac{\\partial L(x,y,y')}{\\partial y'}\\right)\\delta y(x)\\text{d}x.  \n\nThe sum must be equal to zero. Note that even though we relaxed the condition on one end of the curve, the family of perturbations still contains (also) the functions that vanish at the end point, that is \\delta y(b)=0. Such extremals must still satisfy the EL equation, hence the second term (the integral) must vanish too. But if the EL equation is satisfied for perturbations vanishing at the end, the the integral must be zero also for the perturbations not vanishing at the end. As a consequence, the first term on the right must be equal to zero as well. To conclude, setting one of the ends free, the necessary conditions of optimality are still given by the EL equation, but the boundary condition y(b)=\\mathrm y_b is replaced by \n\\left.\\frac{\\partial L(x,y,y')}{\\partial y'}\\right|_{b} = 0.\n\\tag{5}\nThis result can be immediately applied to the free-final-state optimal control problem. The only deficiency is that the cost \nJ = \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}L(\\bm x,\\bm u,t)\\text{d}t\n does not include the term penalizing the final state. We will correct this in a moment. For the time being, note that the solution to the current problem is identical to the solution to the fixed final state problem with the final state condition \\bm x(t_\\mathrm{f}) = \\mathbf x_\\mathrm{f} replaced by the condition \n\\bm \\lambda(t_\\mathrm{f}) = \\mathbf 0.\n\nWhen deriving this, recall that we need to apply the condition Eq. 5 to the augmented Lagrangian in the optimal control problem.\nNow let’s include the term penalizing the final state. This is actually quite easy: what we need to do is to bring that term under the integral sign \n\\begin{aligned}\n\\phi(\\bm x(t_\\mathrm{f})) &= \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}\\frac{\\text{d}\\phi}{\\text{d}t}\\text{d}t + \\phi(\\bm x(t_\\mathrm{i})),\\\\\n              &= \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}\\left[\\frac{\\partial \\phi}{\\partial t}+(\\nabla_\\mathbf{x} \\phi)^\\top\\frac{\\text{d}\\bm x}{\\text{d}t}\\right]\\text{d}t + \\phi(\\bm x(t_\\mathrm{i})).\n\\end{aligned}\n\nNote that the last term on the right (the one corresponding to \\bm x(t_\\mathrm{i})) is constant and excluding it from the optimization has no impact on the optimal solution.\nRestricting our attention to time-invariant cases in favor of simplicity (assuming \\frac{\\partial \\phi}{\\partial t} = 0), the augmented Lagrangian is modified to \nJ^\\mathrm{aug}(t,\\bm x(\\cdot),\\dot{\\mathbf{x}}(\\cdot),\\bm u(\\cdot),\\bm \\lambda(\\cdot)) = \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}\\left[\\underbrace{ L(\\bm x,\\bm u,t)+(\\nabla_\\mathbf{x} \\phi)^\\top\\dot{\\bm x}+ \\bm \\lambda^\\top\\left( \\dot {\\bm x}(t) - \\mathbf f(\\bm x,\\bm u,\\mathbf t)\\right)}_{L^\\text{aug}}\\right ]\\text{d}t.\n\nSubstituting to Eq. 5, the new boundary condition corresponding to the final time is \\boxed{\n(\\nabla_\\mathbf{x} \\phi)(t_\\mathrm{f}) + \\boldsymbol\\lambda(t_\\mathrm{f}) = 0. }",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "LQR on a finite time horizon"
    ]
  },
  {
    "objectID": "cont_indir_LQR_fin_horizon.html#lqr-on-a-finite-and-fixed-time-horizon-with-a-free-final-state",
    "href": "cont_indir_LQR_fin_horizon.html#lqr-on-a-finite-and-fixed-time-horizon-with-a-free-final-state",
    "title": "Indirect approach to LQR on a finite time horizon using differential Riccati equation",
    "section": "LQR on a finite and fixed time horizon with a free final state",
    "text": "LQR on a finite and fixed time horizon with a free final state\nSpecializing the result to the LQ case with the final state penalization \n\\phi(\\bm x(t_\\mathrm{f})) = \\frac{1}{2}\\bm x^\\top(t_\\mathrm{f})\\mathbf S_\\mathrm{f}\\bm x(t_\\mathrm{f}),\n\nwe get the new boundary condition \\boxed{\n\\mathbf S_\\mathrm{f} \\bm x(t_\\mathrm{f}) + \\boldsymbol\\lambda(t_\\mathrm{f}) = 0.}\n\\tag{6}\nThis looks already familiar, doesn’t it? We found an identical relationship between the state and the costate at the final time in the discrete-time setting. The difference is in the sign, we will comment on this in a while. Suffice to say for now that this has (surprisingly) no impact on the solution.\nRestate here the full necessary conditions for the LQ problem. The state and the costate equations are \\boxed{\n\\begin{aligned}\n\\dot {\\bm x} &= \\mathbf A\\bm x + \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda,\\\\\n\\dot{\\boldsymbol\\lambda} &= \\mathbf Q\\bm x - \\mathbf A^\\top\\boldsymbol\\lambda.\n\\end{aligned}}\n\nThe stationarity equation is \\boxed{\n\\bm u = \\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda.}\n\nThe two sets of boundary equations are Eq. 6 and \\bm x(0)=\\mathbf x_0.\nSimilarly as in the previous scenario with fixed final state, here we can also proceed by numerically solving the linear boundary value problem. For completeness we describe it here, but in a few moments we will learn something more about this problem.\nBriefly, from the state-transition equation we have \n\\bm x(t_\\mathrm{f}) = \\boldsymbol \\Phi_{11} \\mathbf{x}(0) + \\boldsymbol \\Phi_{12} \\bm \\lambda (0),\n which after multiplication by \\mathbf S_\\mathrm{f} gives \n\\mathbf S_\\mathrm{f} \\bm x(t_\\mathrm{f}) = \\mathbf S_\\mathrm{f} \\boldsymbol \\Phi_{11} \\mathbf{x}(0) + \\mathbf S_\\mathrm{f} \\boldsymbol \\Phi_{12} \\bm \\lambda (0).\n\nThe boundary condition in the free final state case is \n\\mathbf{S_\\mathrm{f}}\\mathbf{x}(t_\\mathrm{f}) = -\\boldsymbol\\lambda(t_\\mathrm{f}),\n which immediately invites us to substitute to the right side the solution of the costate equation \n-\\boldsymbol \\Phi_{21} \\mathbf{x}(0) - \\boldsymbol \\Phi_{12} \\bm \\lambda (0) = \\mathbf S_\\mathrm{f} \\boldsymbol \\Phi_{11} \\mathbf{x}(0) + \\mathbf S_\\mathrm{f} \\boldsymbol \\Phi_{12} \\bm \\lambda (0),\n from which we can compute the initial value of the costate \n\\bm \\lambda(0) = -(\\mathbf S_\\mathrm{f} \\boldsymbol \\Phi_{12}+\\boldsymbol \\Phi_{22})^{-1}(\\mathbf S_\\mathrm{f} \\boldsymbol \\Phi_{11}+\\boldsymbol \\Phi_{12})\\bm x(0).\n\nHaving computed the initial value of the costate, we can solve the state and costate (differential) equations for the state and costate variables throughout the time horizon. The optimal control then follows from the stationarity equation. Done.\n\nExample 2 (LQR on a finite and fixed time horizon with a free final state approached as a two-point BVP)  \n\n\nShow the code\nusing LinearAlgebra\nusing Random\nRandom.seed!(1234)\n\nn = 2                  # order of the system\nA = rand(n,n)\nB = rand(n,1)\n\nQ = diagm(0=&gt;10*ones(n))    # weighting matrices for the quadratic cost function\nR = 1\nS = diagm(0=&gt;10*ones(n))\n\nx0 = [1;2]             # initial states\nt1 = 10                # final time\n\n# Building Hamiltonian system and solving for the missing boundary conditions using a state-transition matrix\n\nH = [A B/R*B'; Q -A'] # the combined matrix for the Hamiltonian cannonical equations\nP = exp(H*t1)        # \"state\"-transition matrix\n\nP11 = P[1:n,1:n]\nP12 = P[1:n,n+1:end]\nP21 = P[n+1:end,1:n]\nP22 = P[n+1:end,n+1:end]\n\nλ0 = -(S*P12+P22)\\(S*P11+P21)*x0  # solving for the initial value of costate\n\n# Solving for the states and costates during the time interval\n\nusing ControlSystems\n\nG = ss(H,zeros(2n,1),Matrix{Float64}(I, 2n, 2n),0) # auxiliary system comprising the states and costates\n\nw0 = [x0; λ0]\nt = 0:0.1:t1\nv(w,t) = [0]\ny, t, w, uout = lsim(G,v,t,x0=w0)\nx = w[1:n,:]           # state\nλ = w[n+1:end,:]  # costate\nu = (R\\B'*λ)'  # optimal control from the stationarity equation\n\n# Plotting the responses\n\nusing Plots\n\np1 = plot(t,x',ylabel=\"x\",label=\"\",lw=2)\np2 = plot(t,λ',ylabel=\"λ\",label=\"\",lw=2)\np3 = plot(t,u,ylabel=\"u\",label=\"\",lw=2)\n\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The solution to the LQR problem with a free final state approached as a two-point BVP. Plotted are the state, costate and control variables.\n\n\n\n\n\nNonetheless, computing the solution numerically, some important opportunity is escaping our attention. In order to discover it, we need to dig a bit deeper. First, we recall that the boundary condition at the end of the interval gives us a linear relation between the state and the costate. We now assume that this linear relation also holds throughout the time interval. This is the sweep method that we have already encountered in the discrete-time setting: \\boxed{\n\\bm S(t) \\bm x(t) = - \\boldsymbol\\lambda(t).}\n\\tag{7}\nWe differentiate both sides to obtain \n\\dot{\\bm S} \\bm x + \\bm S \\dot{\\bm x}= - \\dot{\\bm \\lambda}.\n\nSubstituting the state equation for \\dot{\\bm x} on the left and for \\dot{\\boldsymbol\\lambda} on the right we get \n\\dot{\\bm S} \\bm x + \\bm S (\\mathbf A\\bm x - \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\bm S\\bm x) = -\\mathbf Q\\bm x - \\mathbf A^\\top \\bm S\\bm x,\n which, since \\bm x can be arbitrary, translates to the condition on \\bm S \\boxed{\n- \\dot{\\bm S} =  \\bm S \\mathbf A + \\mathbf A^\\top \\bm S + \\mathbf Q - \\bm S\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\bm S.}\n\\tag{8}\nThis is another classical result called Riccati differential equation. Initiated at the final time t_\\mathrm{f} by \\bm S(t_\\mathrm{f}) = \\mathbf S_\\mathrm{f}, the differential equation is solved backwards to obtain a function (generally a matrix function) \\bm S(t), which is then substituted into the stationarity equation to obtain the optimal control \\bm u(t) \\boxed{\n\\bm u(t) = -\\mathbf R^{-1}\\mathbf B^\\top\\bm S(t)\\bm x(t).}\n\n\nExample 3 (Scalar Riccati differential equation) We now illustrate the these findings using a numerical example. For convenience we will use the scalar version of the Riccati differential equation. The scalar Riccati differential equation is given by \n- \\dot{s}(t) =  2as(t) + q - \\frac{b^2}{r}s^2(t), \\qquad s(t_\\mathrm{f}) = s_\\mathrm{f}.\n\\tag{9}\nFor some given values of the parameters a,b,q,r,s_\\mathrm{f}, we can solve the Riccati differential equation using a numerical ODE solver. And we can also compute the optimal gain for the state feedback controller using \nk(t) = -\\frac{b}{r}s(t).\n\n\n\nShow the code\nusing DifferentialEquations\n\nfunction solve_scalar_continuous_time_differential_riccati_equation(a,b,q,r,s₁,tspan)\n    f(s, p, t) = -2 * a * s - q + b^2 / r * s^2\n    prob = ODEProblem(f, s₁, reverse(tspan))\n    sol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-8)\n    return reverse(sol.t), reverse(sol.u)\nend\n\na = 1.0 \nb = 1.0 \nq = 1.0 \nr = 10.0 \ns₁ = 2.0;\n\nt₀ = 0.0\nt₁ = 30.0\n\nt,s = solve_scalar_continuous_time_differential_riccati_equation(a,b,q,r,s₁,(t₀,t₁)) \n\nk = [b/r*s[i] for i in eachindex(s)]\n\nusing Plots\np1 = plot(t,s,linewidth=2,xlabel=\"t\",ylabel=\"s(t)\",label=\"\")\nplot!([t₁],[s₁],marker=:circle,markersize=8,label=\"\",legend=:topright)\np2 = plot(t,k,linewidth=2,xlabel=\"t\",ylabel=\"k(t)\",label=\"\")\nplot(p1,p2,layout=(2,1),size=(600,600),legend=:topright)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: The solution to a scalar Riccati differential equation and the corresponding state feedback gain\n\n\n\n\nWe can now simulate the response to some nonzero initial state using this time-varying proportional state feedback control.\n\n\nShow the code\nx₀ = 1.0\n\nusing Interpolations\nKintp = LinearInterpolation(t,k)\n\nfclosed(x,k,t) = (a - b*k(t))*x\nprob = ODEProblem(fclosed,x₀,[0,t₁],t-&gt;Kintp(t))\nsol = solve(prob)\n\nplot(sol.t,sol.u,linewidth=2,xlabel=\"t\",ylabel=\"x(t)\",label=\"\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: The simulated response of the system to a nonzero initial state using the time-varying LQR\n\n\n\n\n\nThe story is now completely identical to the discrete-time case – the solution to the Riccati equation evolves in time, but it turns out that for a stabilizable system given by the matrices (\\mathbf A,\\mathbf B), it converges to some bounded limit. For a long enough time horizon it turns out that for major part of the time horizon \\bm S(t) rests around the limit value. Consequently, the state feedback gain \\bm K(t) also mostly remains constant and only changes towards the very end of the time horizon (towards the final time). The limit steady solution \\lim_{t\\rightarrow -\\infty} \\bm S(t) can be found either by running and ODE solver long enough, or it can be determined by solving the (continuous-time) algebraic Riccati equation (CARE).",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "LQR on a finite time horizon"
    ]
  },
  {
    "objectID": "reduction_order_hw.html",
    "href": "reduction_order_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Homework"
    ]
  },
  {
    "objectID": "roban_software.html",
    "href": "roban_software.html",
    "title": "Software",
    "section": "",
    "text": "ControlSystems.jl\n\ngangoffour\nlft\nmargin\nmarginplot\n\nRobustAndOptimalControl.jl\n\nUncertainty modelling\nhinfnorm2\nδr\nuss\nlft\ndiskmargin\nmakeweight\nstructured_singular_value\n\nMonteCarloMeasurements.jl",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Software"
    ]
  },
  {
    "objectID": "roban_software.html#julia",
    "href": "roban_software.html#julia",
    "title": "Software",
    "section": "",
    "text": "ControlSystems.jl\n\ngangoffour\nlft\nmargin\nmarginplot\n\nRobustAndOptimalControl.jl\n\nUncertainty modelling\nhinfnorm2\nδr\nuss\nlft\ndiskmargin\nmakeweight\nstructured_singular_value\n\nMonteCarloMeasurements.jl",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Software"
    ]
  },
  {
    "objectID": "roban_software.html#matlab",
    "href": "roban_software.html#matlab",
    "title": "Software",
    "section": "Matlab",
    "text": "Matlab\n\nControl System Toolbox for Matlab\n\nnorm\nlff\n\nRobust Control Toolbox for Matlab. A nice benefit is that accompanying video tutorials by Brian Douglas are available.\n\nUncertain system representation\nhinfnorm\nureal\nultidyn\nusample\nlftdata",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Software"
    ]
  },
  {
    "objectID": "roban_unstructured.html",
    "href": "roban_unstructured.html",
    "title": "Robustness analysis for unstructured uncertainty",
    "section": "",
    "text": "When we introduced the concept of robustness, we only vaguely hinted that it is always related to some property of interest. Now comes the time to specify these two properties:",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for unstructured uncertainty"
    ]
  },
  {
    "objectID": "roban_unstructured.html#internal-stability",
    "href": "roban_unstructured.html#internal-stability",
    "title": "Robustness analysis for unstructured uncertainty",
    "section": "Internal stability",
    "text": "Internal stability\nBefore we start discussing robust stability, we need to discuss one fine issue related to stability of a nominal system. We do it through the following example.\n\nExample 1 (Internal stability) Consider the following feedback system with a nominal plant G(s) and a nominal controller K(s).\n\n\n\n\n\n\nFigure 1: Feedback interconnection demonstrating that while some closed-loop transfer functions are stable, some others are not\n\n\n\nThe question is: is this closed-loop system stable? We determine stability by looking at the denominator of a closed-loop transfer function. But which one? There are several. Perhaps the most immediate one is the transfer function from the reference r to the plant output y. With the open-loop transfer function \nL(s) = G(s)K(s) = \\frac{s-1}{s+1} \\frac{k(s+1)}{s(s-1)} = \\frac{k}{s},\n the closed-loop transfer function is \nT(s) = \\frac{\\frac{k}{s}}{1+\\frac{k}{s}} = \\frac{k}{s+k},  \n which is perfectly stable. But note that for practical purposes, all possible closed-loop transfer functions must be stable. How about the one from the output disturbance d to the plant output y? \nS(s) = \\frac{1}{1+\\frac{k}{s}} = \\frac{s}{s+k},\n which is stable too. Doesn’t this indicate that we can stop worrying? Not yet. Consider now the closed-loop transfer function from the reference r to the control u. The closed-loop transfer function is \nK(s)S(s) = \\frac{\\frac{k(s+1)}{s(s-1)}}{1+\\frac{k}{s}} = \\frac{k(s+1)}{{\\color{red}(s-1)}(s+k)}.\n\nOops! This closed-loop transfer function is not stable.\nThe culprit here is our cancelling the zero in the right half-plane (RHP) with an unstable pole in the controller. But let’s emphasize that the trouble is not in imperfectness of this cancellation due to numerical errors. The trouble is in the very cancelling the zero in the RHP by the controller. Identical problem would arise if an unstable pole of the plant is cancelled by the RHP zero of the controller as we can see by modifying the assignment accordingly.\n\nThe example taught (or reminded) us that in order to guarantee stability of all closed-loop transfer functions, no cancellation of poles and zeros in the right half plane is allowed. The resulting closed-loop system is then called internally stable. Checking just (arbitrary) one closed-loop transfer function for stability is then enough to conclude that all of them are stable too.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for unstructured uncertainty"
    ]
  },
  {
    "objectID": "roban_unstructured.html#robust-stability-for-a-multiplicative-uncertainty",
    "href": "roban_unstructured.html#robust-stability-for-a-multiplicative-uncertainty",
    "title": "Robustness analysis for unstructured uncertainty",
    "section": "Robust stability for a multiplicative uncertainty",
    "text": "Robust stability for a multiplicative uncertainty\nWe consider a feedback system with a plant G(s) and a controller K(s), where the uncertainty in the plant modelled as multiplicative uncertainty, that is, G(s) = (1+W(s)\\Delta(s))\\,G_0(s).\nThe technique for analyzing closed-loop stability is based on Nyquist criterion. Instead of analyzing the Nyquist plot for the nominal plant G_0(s), we analyze the Nyquist plot for the uncertain plant G(s). The corresponding open-loop transfer function is \nL(s) = G(s)K(s) = (1+W(s)\\Delta(s))\\,G_0(s)K(s) = L_0(s) + W(s)L_0(s)\\Delta(s).\n\nWhen trying to figure out the conditions under which this family of Nyquist curves avoids the point -1, it is useful to interpret the last equation at a given frequency \\omega as a disc with the center at L_0(j\\omega) and the radius W(j\\omega)L_0(j\\omega). To see this, note that \\Delta(j\\omega) represents a complex number with a magnitude up to one, and with an arbitrary angle.\n\n\n\n\n\n\nFigure 2: Robust stability for multiplicative uncertainty\n\n\n\nThe geometric formulation of the condition is then that the distance from -1 to the nominal Nyquist plot of L_0(j\\omega) is greater than the radius W(j\\omega)L_0(j\\omega) of the disc centered at the nominal Nyquist curve With the distance from the point -1 to the nominal Nyquist plot of L_0(s) evaluated at a particular frequency \\omega a |-1-L_0(j\\omega)| = |1+L_0(j\\omega)|, the condition can be written as\n\n|W(j\\omega)L_0(j\\omega)| &lt; |1+L_0(j\\omega)|, \\;\\forall \\omega.\n\nDividing both sides by 1+L_0(j\\omega) we get \n\\frac{W(j\\omega)L_0(j\\omega)}{1+L_0(j\\omega)} &lt; 1, \\;\\forall \\omega.\n\nBut recalling the definition of the complementary sensitivity function, and dividing both sides by W, we can rewrite the condition as \\boxed\n{|T_0(j\\omega)| &lt; 1/|W(j\\omega)|, \\;\\; \\forall \\omega.}\n\nThis condition has clear interpretation in terms of the magnitude of the complementary sensitivity function – it must be smaller than the reciprocal of the magnitude of the uncertainty weight at all frequencies.\nFinally, we can also invoke the definition of the \\mathcal H_\\infty norm and reformulate the condition as \\boxed\n{\\|WT\\|_{\\infty}&lt; 1.}\n\nTo appreciate usefulness of the this format of the robust stability condition beyond mere notational compactness, we mention that \\mathcal H_\\infty norm of an LTI system can be reliably computed. Robust stability can then be then checked by computing a single number.\nIn fact, it is even better than that – there are methods for computing a feedback controller that minimizes the \\mathcal H_\\infty norm of a specified closed-loop transfer function, which suggests an optimization-based approach to design of robustly stabilizing controllers. We are going to build on this in the next chapter. But let’s stick to the analysis for now.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for unstructured uncertainty"
    ]
  },
  {
    "objectID": "roban_unstructured.html#robust-stability-for-an-lft-small-gain-theorem",
    "href": "roban_unstructured.html#robust-stability-for-an-lft-small-gain-theorem",
    "title": "Robustness analysis for unstructured uncertainty",
    "section": "Robust stability for an LFT – small gain theorem",
    "text": "Robust stability for an LFT – small gain theorem\nWe now aim at developing a similar robust stability condition for the (upper) LFT, as in Fig. 3.\n\n\n\n\n\n\nFigure 3: Upper LFT with the \\mathbf N term corresponding to the nominal closed-loop system structured into blocks\n\n\n\nThe term corresponding to the nominal closed-loop system is structured into blocks. It is only the N_{11} block that captures the interaction with the uncertainty in the model, that is why we are going to analyze the corresponding feedback loop. For convenience we rename this block as \nM \\coloneqq N_{11},\n and the open-loop transfer function is then M \\Delta. Following the same Nyquist criterion based reasoning as before, that is, asking for the conditions under which this open-loop transfer function does not touch the point -1, while the \\Delta term can introduce an arbitrary phase, we arrive at the following robust stability condition for the LFT \\boxed\n{|M(j\\omega)|&lt;1,\\;\\;\\forall \\omega.}\n\nInvoking the definition of the \\mathcal H_\\infty norm, we can rewrite the condition compactly as \\boxed\n{\\|M\\|_{\\infty}&lt;1.}\n\nOnce again, the formulation as an inequality over all frequencies can be useful for visualization and interpretation, while the inequality with the \\mathcal H_\\infty norm can be used for computation and optimization.\nThis condition of robust stability belongs to the most fundamental results in control theory. It is known as the small gain theorem.\n\n\n\n\n\n\nSmall gain theorem works for MIMO too\n\n\n\nSmall gain theorem works for a MIMO uncertainty \\boldsymbol \\Delta and a block \\mathbf N_{11} (or \\mathbf M) too \n\\|\\mathbf M\\|_{\\infty}&lt;1.\n\nBut we discuss in the next section that it is typically too conservative as the \\boldsymbol \\Delta block has typically some (block diagonal) structure, which should be exploited. But more on this in the section dedicated to structured uncertainty.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for unstructured uncertainty"
    ]
  },
  {
    "objectID": "roban_unstructured.html#nominal-performance",
    "href": "roban_unstructured.html#nominal-performance",
    "title": "Robustness analysis for unstructured uncertainty",
    "section": "Nominal performance",
    "text": "Nominal performance\nHaving discussed stability (and its robustness), it is now time to turn to performance (and its robustness). Performance can mean different things for different people, and it can be specified in a number of ways, but we would like to formulate performance requirements in the same frequency domain setting as we did for (robust) stability. Namely, we would like to specify the performance requirements in terms of the frequency response of some closed-loop transfer function. The sensitivity function seems to be a natural choice for this purpose. It turns out that by imposing upper bound constraints on |S(j\\omega)| (actually |S_0(j\\omega)| as we now focus on the nominal case with no uncertainty) we can specify a number of performance requirements:\n\nUp to which frequency the feedback controller attenuates the disturbance, that is, the bandwidth \\omega_\\mathrm{BW} of the system.\nHow much the feedback controller attenuates the disturbances over the bandwidth.\nHow does it behave at very low frequencies, that is, how well it regulates the steady-state error.\nWhat is the maximum amplification of the disturbance, that is, the resonance peak.\n\nThese four types of performance requirements can be pointed at in Fig. 4 below.\n\n\n\n\n\n\nFigure 4: Performance specifications through the shape of the magnitude frequency response of the sensitivity function\n\n\n\nBut these requirements can also be compactly expressed throug the performance weighting filter W_\\mathrm{p}(s) as \\boxed\n{|S_0(j\\omega)| &lt; 1/|W_\\mathrm{p}(j\\omega)|,\\;\\;\\forall \\omega,}\n\\tag{1}\nwhere S_0 = \\frac{1}{1+L_0} is the sensitivity function of the nominal closed-loop system. which can be compactly written as \\boxed\n{\\|W_\\mathrm{p}S_0\\|_{\\infty}&lt;1.}\n\nIt lends some insight if we visualize this condition in the complex plane. First, recall that S_0 = \\frac{1}{1+L_0}. Eq. 1 then translates to \n|W_\\mathrm{p}(j\\omega)|&lt;|1+L_0(j\\omega)|\\;\\;\\forall \\omega,\n which can be visualized as in Fig. 5.\n\n\n\n\n\n\nFigure 5: Nominal performance condition",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for unstructured uncertainty"
    ]
  },
  {
    "objectID": "roban_unstructured.html#robust-performance-for-a-multiplicative-uncertainty",
    "href": "roban_unstructured.html#robust-performance-for-a-multiplicative-uncertainty",
    "title": "Robustness analysis for unstructured uncertainty",
    "section": "Robust performance for a multiplicative uncertainty",
    "text": "Robust performance for a multiplicative uncertainty\nSo far we have the condition of robust stability and the condition of nominal performance. Simultaneous satisfaction of both gives… just robust stability and nominal performance. Robust performance obviously needs a stricter condition. We visualize it in Fig. 6.\n\n\n\n\n\n\nFigure 6: Robust performance condition\n\n\n\nIn words, the two circles, one corresponding to the uncertainty, the other corresponding to the performance specifications, must not even touch. This we can express algebraically as \n\\boxed{\n|W_\\mathrm{p}(j\\omega)S_0(j\\omega)| + |W(j\\omega)T_0(j\\omega)| &lt; 1\\;\\;\\forall \\omega.}\n\\tag{2}\nHow about rewriting this condition in terms of the \\mathcal H_\\infty norm? It appears this is not as straightforward as in the previous two cases of robust stability and nominal performance. But there is something we can do. We consider the following artificial/auxilliary/augmented closed-loop transfer function that we call mixed sensitivity function \n\\begin{bmatrix}\nW_\\mathrm{p}S_0\\\\\nWT_0\n\\end{bmatrix}.\n\nIts \\mathcal H_\\infty norm can be written as \n\\left\\|\n\\begin{bmatrix}\nW_pS\\\\\nWT\n\\end{bmatrix}\n\\right\\|_{\\infty}\n= \\sup_{\\omega\\in\\mathbb{R}}\\sqrt{|W_p(j\\omega)S(j\\omega)|^{2}+|W(j\\omega)T(j\\omega)|^{2}}.\n\nThis can be seen by using the known relation between the singular values and eigenvalues. Consider a matrix \\mathbf A \\in \\mathbb R^{n\\times m}. It holds that \n\\sigma_i(\\mathbf A) = \\sqrt{\\lambda_i(\\mathbf A^\\top \\mathbf A)},\n where \\sigma_i is the i-th singular value of a matrix, and \\lambda_i is the i-th eigenvalue. Although this is not how singular values are computed numerically (there are more reliable ways), we can use this result for our theoretical derivation. Note that if \\mathbf A is a one-column matrix (a column vector), the product \\mathbf A^\\top \\mathbf A is a scalar; it is equal to the square of the Euclidean norm of \\mathbf A. Hence, the only (nonzero) singular value of a one-column matrix is the Euclidean norm of the one-column \\mathbf A.\nWe have just learnt that \n\\left\\|\n\\begin{bmatrix}\nW_pS\\\\\nWT\n\\end{bmatrix}\n\\right\\|_{\\infty}\n= \\sup_{\\omega\\in\\mathbb{R}} \\left\\|\n\\begin{bmatrix}\nW_p(j\\omega)S(j\\omega)\\\\\nW(j\\omega)T(j\\omega)\n\\end{bmatrix}\n\\right\\|_{2}.\n\\tag{3}\n\n\n\n\n\n\nThe \\mathcal H_\\infty norm deals with transfer functions, the 2- (and 1- and \\infty-) norms deal with constant vectors\n\n\n\nIn the above equation, the norm on the left is a norm of a transfer function, the norm on the right is a norm of a constant vector (evaluated at a particular frequency \\omega). In order to emphasize it, some authors use the notation \\|\\cdot\\|_{\\mathcal H_\\infty} to distinguish the \\mathcal H_\\infty norm of a transfer function from the \\infty norm \\|\\cdot\\|_\\infty of a constant vector.\n\n\nThe condition of robust performance in Eq. 2 uses the 1-norm of a vector evaluated at a particular frequency \\omega. We can rewrite it as \n\\sup_{\\omega\\in\\mathbb{R}} \\left\\|\n\\begin{bmatrix}\nW_p(j\\omega)S(j\\omega)\\\\\nW(j\\omega)T(j\\omega)\n\\end{bmatrix}\n\\right\\|_{1} &lt; 1.\n\\tag{4}\nRecall the definitions of various norms of a planar vector \\bm x\\in \\mathbb R^2: \n\\|\\bm x\\|_2 = \\sqrt{x_1^2+x_2^2},\\quad \\|\\bm x\\|_1 = |x_1| + |x_2|,\\quad \\|\\bm x\\|_{\\infty} = \\max\\{|x_1|,|x_2|\\}.\n\nThese definitions are also visualized in Fig. 7 a.\n\n\n\n\n\n\nFigure 7: Norms in the Euclidean plane: a) definitions of 1-, 2-, and \\infty norms; b) unit norm “balls”\n\n\n\nFrom Fig. 7 b, we can deduce the following inequalities \n\\|\\bm x\\|_2 \\leq \\|\\bm x\\|_1\\leq \\sqrt{2}\\|\\bm x\\|_2.\n\nTherefore RP condition in the SISO case can be formulated as \\boxed\n{\\left\\|\n\\begin{bmatrix}\nW_\\mathrm{p}S_0\\\\\nWT_0\n\\end{bmatrix}\n\\right\\|_{\\infty}\n&lt;\\frac{1}{\\sqrt{2}},}\n where the augmented closed-loop system \\begin{bmatrix} W_\\mathrm{p}S\\\\ WT_0 \\end{bmatrix} is called mixed sensitivity function.\nIn the MIMO case we do not have a useful upper bound, but at least we have received a hint that it may be useful to minimize the \\mathcal H_\\infty norm of the mixed sensitivity function. This observation will directly lead to a control design method.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for unstructured uncertainty"
    ]
  },
  {
    "objectID": "roban_unstructured.html#robust-stability-for-mimo-multiplicative-uncertainty",
    "href": "roban_unstructured.html#robust-stability-for-mimo-multiplicative-uncertainty",
    "title": "Robustness analysis for unstructured uncertainty",
    "section": "Robust stability for MIMO multiplicative uncertainty",
    "text": "Robust stability for MIMO multiplicative uncertainty\nBack to the robust stability analysis. For MIMO systems, the general model of uncertainty is \n\\mathbf E = \\mathbf W_2\\boldsymbol \\Delta \\mathbf W_1,\\;\\ \\|\\boldsymbol\\Delta\\|_{\\infty}\\leq 1,\n and the robust stability condition depends on whether the uncertainty is modelled as the output or at the input of the plant.\nSay, the model of the uncertainty is just \\mathbf E = \\boldsymbol \\Delta \\mathbf W. For the output multiplicative uncertainty, the model of the uncertain plant is \n\\mathbf G = \\mathbf G_0 (\\mathbf I+\\boldsymbol \\Delta \\mathbf W),\n and the lower-upper LFT is in Fig. 8\n\n\n\n\n\n\nFigure 8: Output multiplicative uncertainty as an LFT\n\n\n\nThe transfer function of the M block is \n\\mathbf M_\\mathrm{O} = -\\mathbf W\\mathbf G_{0}\\mathbf K(\\mathbf I+\\mathbf G_{0}\\mathbf K)^{-1} = -\\mathbf W \\mathbf T_\\mathrm{O}.\n\nSimilarly, for the input multiplicative uncertainty, the model of the uncertain plant is \n\\mathbf G = (\\mathbf I+\\boldsymbol \\Delta \\mathbf W)\\mathbf G_0,  \n the lower-upper LFT is in Fig. 8\n\n\n\n\n\n\nFigure 9: Input multiplicative uncertainty as an LFT\n\n\n\nand the transfer function of the M block is \n\\mathbf M_\\mathrm{I} = -\\mathbf W\\mathbf K\\mathbf G_{0}(\\mathbf I+\\mathbf K\\mathbf G_{0})^{-1} = -\\mathbf W \\mathbf T_\\mathrm{I}.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for unstructured uncertainty"
    ]
  },
  {
    "objectID": "roban_unstructured.html#robust-performance-as-robust-stability-with-structured-uncertainty",
    "href": "roban_unstructured.html#robust-performance-as-robust-stability-with-structured-uncertainty",
    "title": "Robustness analysis for unstructured uncertainty",
    "section": "Robust performance as robust stability with structured uncertainty",
    "text": "Robust performance as robust stability with structured uncertainty\nA useful observation is that we can formulate the robust performance condition as a robust stability condition with a structured uncertainty. First, note that by refering to Fig. 10, the robust performance requirement can be state as: make the \\mathcal H_\\infty norm of the closed-loop transfer function from the disturbance d to the output d” small (smaller than one).\n\n\n\n\n\n\nFigure 10: Feedback configuration for expressing the requirements on robust performance\n\n\n\nBut we have already learnt that the condition on the \\mathcal H_\\infty norm can be interpreted as a condition on the robust stability of the given system and some \\Delta block. We introduce such artificial uncertainty block \\Delta_\\mathrm{p} here and plug it into the feedback loop with the whole system as in Fig. 11.\n\n\n\n\n\n\nFigure 11: Feedback configuration for expressing the requirements on robust performance by reformulating it as robust stability with a structured uncertainty\n\n\n\nWe emphasize that it does not represent any uncertainty but ruther serves to expresses the robust performance requirement, albeit indirectly via reformulating the whole problem as robust stability with a single augmented uncertainty block \n\\bm \\Delta^\\mathrm{aug} = \\begin{bmatrix}\n\\Delta & 0\\\\\n0 & \\Delta_\\mathrm{p}\n\\end{bmatrix}.\n\nIn this very special case of a system with a multiplicative uncertainty, we already showed how to write down the robust performance condition, hence it might not be clear why we have just reformulated the problem in a way that calls for the ability to analyze robust stability with a structured uncertainty (which we do not yet have). But still, it shows the very essential idea that we are going to use later within some more complicated setups, such as the ones with the original uncertainty block \\bm \\Delta already structured.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for unstructured uncertainty"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html",
    "href": "discr_indir_LQR_inf_horizon.html",
    "title": "Discrete-time LQR on an infinite horizon",
    "section": "",
    "text": "In this section we are going to solve the LQR problem on the time horizon extended to infinity, that is, our goal is to find an infinite (vector) control sequence \\bm u_0, \\bm u_{1},\\ldots, \\bm u_{\\infty} that minimizes \nJ_0^\\infty = \\frac{1}{2}\\sum_{k=0}^{\\infty}\\left[\\bm x_k^\\top \\mathbf Q \\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right],\n where, as before \\mathbf Q = \\mathbf Q^\\top \\succeq 0 and \\mathbf R = \\mathbf R^\\top \\succ 0, and the system is modelled by \n\\bm x_{k+1} = \\mathbf A \\bm x_{k} + \\mathbf B \\bm u_k, \\qquad \\bm x_0 = \\mathbf x_0.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on an infinite horizon using the DARE"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html#why-the-infinite-time-horizon",
    "href": "discr_indir_LQR_inf_horizon.html#why-the-infinite-time-horizon",
    "title": "Discrete-time LQR on an infinite horizon",
    "section": "Why the infinite time horizon?",
    "text": "Why the infinite time horizon?\nThe first question that inevitably pops up is the one about the motivation for introducing the infinite time horizon: does the introduction of an infinite time horizon meant that we do not care about when the controller accomplishes the task? No, certainly it does not.\nThe infinite time horizon can be used in scenarios when the final time is not given and we leave it up to the controller to take as much time as it needs to reach the desired state. But even then we can still express our preference for reaching the target state quickly by increasing the weights in \\mathbf Q against \\mathbf R.\nThe infinite time horizon can also be used to model the case when the system is expected to operate indefinitely. This is a common scenario in practice, for example, in the case of temperature control in a building – there is no explicit end of the control task.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on an infinite horizon using the DARE"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html#steady-state-solution-to-discrete-time-riccati-equation-yields-subpoptimal-control-on-a-finite-and-optimal-on-the-infinite-time-horizon",
    "href": "discr_indir_LQR_inf_horizon.html#steady-state-solution-to-discrete-time-riccati-equation-yields-subpoptimal-control-on-a-finite-and-optimal-on-the-infinite-time-horizon",
    "title": "Discrete-time LQR on an infinite horizon",
    "section": "Steady-state solution to discrete-time Riccati equation yields subpoptimal control on a finite and optimal on the infinite time horizon",
    "text": "Steady-state solution to discrete-time Riccati equation yields subpoptimal control on a finite and optimal on the infinite time horizon\nWe have seen in the previous section that the solution to the LQR problem with a free final state and a finite time horizon is given by a time-varying state feedback control law \\bm u_k = \\mathbf K_k \\bm x_k. The sequence of gains \\mathbf K_k for k=0,\\ldots, N-1, is given by the sequence of matrices \\mathbf S_k for k=0,\\ldots, N, which in turn is given as the solution to the (discrete-time) Riccati equation initialized by the matric \\mathbf S_N specifying the terminal cost, and solved backwards in time. But we have also seen, using an example, that if the time interval was long enough, the sequences \\mathbf K_k and \\mathbf S_k both converged to some steady state values as the time k proceeded backwards towards the beginning of the time interval.\nWhile using these steady-state values instead of the full sequences leads to a suboptimal solution on a finite time horizon, it turns out that it actually gives the optimal solution on an infinite time horizon. Although our argument here is as rather hand-wavy, it is intuitive — there is no end to the time interval, hence the steady-state values are not given a chance to change “towards the end”, as we observed in the finite time horizon case.\n\n\n\n\n\n\nOther approches to derivation of the infinite time horizon LQR exist\n\n\n\nOther approaches exist for solving the infinite time horizon LQR problem that do not make any reference to the finite time horizon problem. Some of them are very elegant and concise, but here we intentionally stick to viewing the infinite horizon problem as a natural extension of the finite horizon problem.\n\n\n\n\n\n\n\n\nNotation\n\n\n\nBefore we proceed with the discussion of how to find the steady-state values (the limits) of \\mathbf S_k and subsequently \\mathbf K_k, we must discuss the notation. While increasing the time horizon N, the solution to the Riccati equation settles towards the beginning of the time interval. We can thenk pick the steady-state values right at the initial time k=0, that is, \\mathbf S_0 and \\mathbf K_0. But thanks to time invariance, we can also fix the final time to some (arbitrary) N and strech the interval by moving its beginning toward -\\infty. The limits of the sequences \\mathbf S_k and \\mathbf K_k can be considered as k \\rightarrow -\\infty. It seems appropriate to denote these limits as \\mathbf S_{-\\infty} and \\mathbf K_{-\\infty} then. However, the notation used for these limits in the literature is just \\mathbf S_\\infty and \\mathbf K_\\infty \n\\mathbf S_\\infty \\coloneqq \\lim_{k\\rightarrow -\\infty} \\mathbf S_k, \\qquad \\mathbf K_\\infty \\coloneqq \\lim_{k\\rightarrow -\\infty} \\mathbf K_k.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on an infinite horizon using the DARE"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html#discrete-time-algebraic-riccati-equation",
    "href": "discr_indir_LQR_inf_horizon.html#discrete-time-algebraic-riccati-equation",
    "title": "Discrete-time LQR on an infinite horizon",
    "section": "Discrete-time algebraic Riccati equation",
    "text": "Discrete-time algebraic Riccati equation\nLeaving aside for the moment the important question whether and under which conditions such a limit \\mathbf S_\\infty exists, the immediate question is how to compute such limit. One straightforward strategy is to run the recurrent scheme (Riccati equation) and generate the sequence \\mathbf S_{N}, \\mathbf S_{N-1}, \\mathbf S_{N-2}, \\ldots so long as there is a nonnegligible improvement, that is, once \\mathbf S_{k}\\approx\\mathbf S_{k+1}, stop iterating. That is certainly doable.\nThere is, however, another idea. We apply the steady-state condition \n\\mathbf S_{\\infty} = \\mathbf S_k=\\mathbf S_{k+1}\n to the Riccati equation. The resulting equation \\boxed{\n\\mathbf S_{\\infty}=\\mathbf A^\\top\\left[\\mathbf S_{\\infty}-\\mathbf S_{\\infty}\\mathbf B(\\mathbf B^\\top\\mathbf S_{\\infty}\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top\\mathbf S_{\\infty}\\right]\\mathbf A+\\mathbf Q}\n is called discrete-time algebraic Riccati equation (DARE) and it is one of the most important equations in the field of computational control design.\nThe equation may look quite “messy” and does not necessarily offer any insight. But recall the good advice to shrink the problem to the scalar size while studying matrix-vector expressions and struggling to get some insight. Our DARE simplifies to \ns_\\infty = a^2s_\\infty - \\frac{a^2b^2s_\\infty^2}{b^2s_\\infty+r} + q\n\nMultiplying both sides by the denominator we get the equivalent quadratic (in s_\\infty) equation \\boxed{\nb^2s_\\infty^2 + (r - ra^2 - qb^2)s_\\infty - qr = 0.}\n\\tag{1}\nVoilà! A scalar DARE is just a quadratic equation, for which the solutions can be found readily.\nThere is a caveat here, though, reflected in using plural in “solutions” above. Quadratic equation can have two (or none) real solutions!\n\nExample 1 (Two solutions to the scalar DARE)  \n\na = 1/2\nb = 1\nq = 2\nr = 3\n\np₂ = b^2\np₁ = (r - r*a^2 - b^2*q)\np₀ = -r*q\n\ns₁, s₂ = (-p₁ + sqrt(p₁^2 - 4*p₂*p₀))/(2*p₂), (-p₁ - sqrt(p₁^2 - 4*p₂*p₀))/(2*p₂)\n\n(2.327677108793573, -2.577677108793573)\n\n\n\nBut the sequence produced by original discrete-time recurrent Riccati equation is determined uniquely by s_N! What’s up? How are the solutions to ARE related to the limiting solution of recursive Riccati equation? Answering this question will keep us busy for the rest of this lecture. We will structure this broad question into several sub-questions\n\nUnder which conditions is it guaranteed that there exists a (bounded) limiting solution \\mathbf S_\\infty to the recursive Riccati equation for all initial (actually final) values \\mathbf S_N?\nUnder which conditions is the limit solution unique for arbitrary \\mathbf S_N?\nUnder which conditions is it guaranteed that the time-invariant feedback gain \\mathbf K_\\infty computed from \\mathbf S_\\infty stabilizes the system (on the infinite control interval)?",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on an infinite horizon using the DARE"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html#boundedness-of-the-solution-to-recurrent-riccati-equation",
    "href": "discr_indir_LQR_inf_horizon.html#boundedness-of-the-solution-to-recurrent-riccati-equation",
    "title": "Discrete-time LQR on an infinite horizon",
    "section": "Boundedness of the solution to recurrent Riccati equation",
    "text": "Boundedness of the solution to recurrent Riccati equation\nLet us state the answer first: the system \\bm x_{k+1}=\\mathbf A \\bm x_k+\\mathbf B \\bm u_k must be stabilizable in order to guarantee existence of a bounded limiting solution \\mathbf S_\\infty to Riccati solution.\nTo see this, note that for a stabilizable system, we can find some time-invariant feedback gain, which guarantes that \\bm x_k\\rightarrow 0 as k\\rightarrow \\infty. Note, once again, that for LTI systems the situation with fixed and finite N and k going toward -\\infty, can be viewed equivalently as if k goes from 0 to \\infty. Invoking this and recalling also that our cost is \nJ_i = \\frac{1}{2}\\bm x_N^\\top\\mathbf S_N \\bm x_N + \\frac{1}{2}\\sum_{k=i}^{N-1}\\bm x_k^\\top\\mathbf Q \\bm x_k + \\bm u_k^\\top\\mathbf R \\bm u_k,\n we can argue that \\lim_{i\\rightarrow -\\infty} J_i is finite. At the same moment, the cost function J_i at any i for any suboptimal feedback gain must be higher or equal to the cost function for the optimal state feedback gain; this is the very definition of an optimal solution: \nJ_i \\geq J_i^\\star.\n\nTherefore \n\\mathbf x_i^\\top\\mathbf S_i \\mathbf x_i \\geq \\mathbf x_i^\\top\\mathbf S_i^\\star \\mathbf x_i,\n which can also be written as \n\\mathbf S_i \\succeq \\mathbf S_i^\\star.\n\nIn other words, we have just shown that for a stabilizable system the optimal sequence \\mathbf S_k is bounded at every k.\nTwo more properties of the optimal \\mathbf S_k can be identified upon consulting the Riccati equation:\n\n\\mathbf S_k is symmetric. This is obvious once we observe that by transposing the Riccati equation \n\\mathbf S_k = \\mathbf Q + \\mathbf A^\\top\\mathbf S_{k+1}\\mathbf A - \\mathbf A^\\top\\mathbf S_{k+1}\\mathbf B( \\mathbf B^\\top\\mathbf S_{k+1}\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf A,\n we obtain the same expression.\n\\mathbf S_k\\succeq \\mathbf 0 if \\mathbf S_N\\succeq \\mathbf 0. This is perhaps best shown with the form of Riccati equation\n\n\\mathbf S_k = \\mathbf Q+\\mathbf A^\\top\\mathbf S_{k+1}(\\mathbf I+\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1})^{-1}\\mathbf A,\n just use the rules such as that a squared matrix is always positive semidefinite, and that a sum of two semidefinite matrices is a semidefinite matrix. As usual, resorting to scalars will lend some insight.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on an infinite horizon using the DARE"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html#stabilizing-solution-of-are",
    "href": "discr_indir_LQR_inf_horizon.html#stabilizing-solution-of-are",
    "title": "Discrete-time LQR on an infinite horizon",
    "section": "Stabilizing solution of ARE",
    "text": "Stabilizing solution of ARE\nNow, let’s skip the second of our three original questions (the one about uniqueness) temporarily and focus on the third question. In order to answer it, let us extend our state-space model with the artificial output equation \n\\bm y_k = \\begin{bmatrix}\n                \\mathbf C \\\\ \\mathbf 0\n               \\end{bmatrix} \\bm x_k\n+\n\\begin{bmatrix}\n\\mathbf 0 \\\\ \\mathbf D\n\\end{bmatrix}\n\\bm u_k,\\quad k=0,1,\\ldots, N-1,\\qquad\n\\bm y_N = \\begin{bmatrix}\n                \\mathbf C \\\\ \\mathbf 0\n               \\end{bmatrix} \\bm x_N,\n where \\mathbf Q=\\mathbf C^\\top\\mathbf C, \\mathbf R=\\mathbf D^\\top\\mathbf D and \\mathbf S_N=\\mathbf C_N^\\top\\mathbf C_N. With this new artificial system, our original optimization criterion can be rewritten \nJ_i = \\frac{1}{2}\\bm y_N^\\top\\bm y_N + \\frac{1}{2}\\sum_{k=i}^{N-1} \\bm y_k^\\top\\bm y_k = \\frac{1}{2}\\sum_{k=i}^{N} \\bm y_k^\\top\\bm y_k.\n\nFrom the previous analysis we know that thanks to stabilizability, the optimal cost function is always bounded (by a finite cost for any suboptimal stabilizing controller) \nJ_{\\infty}^\\star = \\frac{1}{2}\\sum_{k=i}^\\infty\\bm y_k^\\top\\bm y_k&lt; \\infty.\n\nHaving a bounded sum of an infinite number of squared terms, it follows that \\|\\bm y_k\\|\\rightarrow 0 as k\\rightarrow \\infty, and therefore \\bm y_k\\rightarrow 0 as k\\rightarrow \\infty. The crucial question now is: does the fact that \\mathbf C \\bm x_k\\rightarrow 0 and \\mathbf D \\bm u_k\\rightarrow 0 for k\\rightarrow \\infty also imply that \\bm x_k \\rightarrow 0 and \\bm u_k \\rightarrow 0?\nSince |\\mathbf R|\\neq 0, \\bm u_k\\rightarrow 0. But we made no such restrictive assumption about \\mathbf Q. In the very extreme case assume that \\mathbf Q=\\mathbf 0. What will happen with an unstable system is that our optimization criterion only contains the control sequence \\bm u_k and it naturally minimizes the total cost by setting \\bm u_k = \\bm 0. The result is catastrophic — the system goes unstable. The fact that the state variables \\bm x_k of the system blow up goes unnoticed by the optimization cost function. The easiest fix of this situation is to require \\mathbf Q nonsigular as we did for \\mathbf  R (for other reasons), but this maybe too restrictive. We will shortly see an example where nonsingular weighting matrix \\mathbf Q might be useful.\nThe ultimate condition under which the blowing up of the system state variables is always reflected in the optimization cost is the detectability of the artificial system given by the matrix pair (\\mathbf A,\\mathbf C). In practice, we may check for observability (similarly as we do for controllability instead of detectability) but doing so we request more then is needed.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on an infinite horizon using the DARE"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html#uniqueness-of-the-stabilizing-solution",
    "href": "discr_indir_LQR_inf_horizon.html#uniqueness-of-the-stabilizing-solution",
    "title": "Discrete-time LQR on an infinite horizon",
    "section": "Uniqueness of the stabilizing solution",
    "text": "Uniqueness of the stabilizing solution\nThe last issue that we have to solve is uniqueness. Why do we need to care? We have already discussed that the ARE, being a quadratic equation, can have more than just one solution. In the scalar case it can have two real solutions (or no real because both complex). In general there are then several posibilities\n\nNone of them is nonnegative. Bad luck, no stabilizing solution can be found. This can be, however, excluded if the system (\\mathbf A,\\mathbf B) is stabilizable as discussed before.\nOnly one of them is nonnegative. We are lucky because this is our stabilizing solution.\n\nBoth solutions are nonnegative. Which one we shall pick? Both seem to be acceptable candidates but only one of them truly corresponds to the optimal solution.\n\nOur goal is to study if we can exclude the last scenario – multiple positive semidefinite solutions of ARE. Provided the system is stabilizable, we know that one of them is our optimal solution but we will have troubles to identify the correct solution. In other words, we are asking if there is a unique stabilizing solution to ARE. We will use ARE in the Joseph stabilized form \n\\mathbf S = (\\mathbf A-\\mathbf B\\mathbf K)^\\top  \\mathbf S(\\mathbf A-\\mathbf B\\mathbf K) + \\mathbf K^\\top  \\mathbf R\\mathbf K + \\mathbf Q,\n in which we stop using \\infty in the lower index for notational brevity. Using our factorization of \\mathbf Q and \\mathbf R we can write the Riccati equation as \n\\mathbf S = (\\mathbf A-\\mathbf B\\mathbf K)^\\top  \\mathbf S(\\mathbf A-\\mathbf B\\mathbf K) + \\begin{bmatrix}\\mathbf C^\\top& \\mathbf K\\mathbf D^\\top\\end{bmatrix}\\begin{bmatrix}\\mathbf C\\\\ \\mathbf D\\mathbf K\\end{bmatrix}.\n\nObserve that for a fixed (optimal) \\mathbf K the above equation is actually a Lyapunov equation for an equivalent system with the state matrices (\\mathbf A-\\mathbf B\\mathbf K,\\begin{bmatrix}\\mathbf C\\\\ \\mathbf D\\mathbf K\\end{bmatrix}). We have already refreshed the well-known facts about Lypunov equation that provided the system is stable (and in our case it is since (\\mathbf A-\\mathbf B\\mathbf K) matrix is) and observable, the equation has a unique positive definite solution. If observability is not guranteeed, there is a positive definite solution.\nClearly, what remains to be shown is that the system (\\mathbf A-\\mathbf B\\mathbf K,\\begin{bmatrix}\\mathbf C\\\\ \\mathbf D\\mathbf K\\end{bmatrix}) is observable. Let us invoke one of the tests of observability — the popular PBH test. For a system (\\mathbf A, \\mathbf C) it consists in creating a matrix \\begin{bmatrix}z\\mathbf I - \\mathbf A\\\\ \\mathbf C\\end{bmatrix} and checking if it is full rank for every complex z. In our case this test specializes to checking the rank of \n  \\begin{bmatrix}z\\mathbf I - (\\mathbf A-\\mathbf B\\mathbf K)\\\\ \\mathbf C\\\\ \\mathbf D\\mathbf K\\end{bmatrix},\n which can be shown to be equal to the rank of \n\\begin{bmatrix}z\\mathbf I - \\mathbf A\\\\ \\mathbf C\\\\ \\mathbf D\\mathbf K\\end{bmatrix}.\n\nThis expresses the well-known fact from linear systems that state feedback preserves observability. If \\mathbf K can be arbitrary, the only way to keep this rank full is to guarantee that \n\\begin{bmatrix}z\\mathbf I - \\mathbf A\\\\ \\mathbf C\\end{bmatrix}\n is full rank. In other words, the system (\\mathbf A, \\mathbf C), where \\mathbf C=\\sqrt{\\mathbf Q}, must be observable.\nLet us summarize the findings: although detectability of (\\mathbf A, \\mathbf C) is enough to guarantee the existence of a positive semidefinite \\mathbf S which stabilizes the system, if the detectability condition is made stronger by requiring observability, it is guaranteed that there will be a unique positive definite solution to ARE.\nWhy do we need to care about positive definiteness of \\mathbf S? Let us consider a scalar case for illustration again, in particular the scalar DARE in Eq. 1. Trivial analysis shows that for q=0, one of the roots is s_1=0 and the other is always s_2&lt;0. Hence the solution of ARE that represents the steady-state solution of the recurrent Riccati equation is s=0.\n\nExample 2 (Unique positive semidefinite solution to the scalar DARE)  \n\na = 1/2\nb = 1\nq = 0\nr = 3\n\np₂ = b^2\np₁ = (r - r*a^2 - b^2*q)\np₀ = -r*q\n\ns₁, s₂ = (-p₁ + sqrt(p₁^2 - 4*p₂*p₀))/(2*p₂), (-p₁ - sqrt(p₁^2 - 4*p₂*p₀))/(2*p₂)\n\n(0.0, -2.25)\n\n\n\nAs a consequence, the optimal state-feedback gain is k=0. For an unstable system this would be unacceptable but for a stable system this makes sense: the system is stable even withouth the control, therefore, when the state is not penalized in the criterion at all (q=0), the optimal strategy is not to regulate at all. Mathematically correct. Nonethelesss, from an engineering viewpoint we may be quite unhappy because the role of the feedback regulator is also to attenuate the influence of external disturbances. Our optimal state-feedback regulator does not help at all in these situation. That is why we may want to require positive definite solution of ARE (by strengtheing the condition from detectability to observability of (\\mathbf A,\\sqrt{\\mathbf Q})).\n\nExample 3 (LQR design for lateral-directional dynamics of the F16 aircraft) This is Example 5.3-1 from [1, p. 407]. A linear model of lateral-directional dynamics of F16 trimmed at: VT=502ft/s, 302psf dynamic pressure, cg @ 0.35cbar. It also includes the dynamics of ailerons and rudders and washout filter. The system has two inputs (the commands to the control surfaces), four state variables corresponding to the lateral-directional dynamics of the aircraft, two state variables corresponding to the servo dynamics, and one more corresponding to the washout filter. We assume that all the state variables are available for state feedback (they are measured or estimated).\n\n\n\n\n\n\nFigure 1: Block diagram of a model of lateral-directional dynamics of an aircraft\n\n\n\n\n\n\nTable 1: The two input and six state variables\n\n\n\n\n\nSymbol\nName\nUnits\n\n\n\n\nu_\\mathrm{a}\nreference for the aileron deflection\nrad\n\n\nu_\\mathrm{r}\nreference for the rudder deflection\nrad\n\n\n\\delta_\\mathrm{a}\ntrue aileron deflection\nrad\n\n\n\\delta_\\mathrm{r}\ntrue rudder deflection\nrad\n\n\n\\beta\nsideslip angle\nrad\n\n\n\\phi\nbank angle\nrad\n\n\np\nroll rate\nrad/s\n\n\nr\nyaw rate\nrad/s\n\n\nr_\\mathrm{w}\nmeasured yaw rate (after washout)\nrad/s\n\n\n\n\n\n\nThe code that defines the discrete-time state-space model follows\n\n\nShow the code\nA = [-0.3220  0.0640   0.0364    -0.9917    0.0003     0.0008   0;\n    0         0        1         0.0037     0          0        0;\n    -30.6492  0        -3.6784   0.6646     -0.7333    0.1315   0;\n    8.5396    0        -0.0254   -0.4764    -0.0319    -0.062   0;\n    0         0        0         0          -20.2      0        0;\n    0         0        0         0          0          -20.2    0;\n    0         0        0         57.2958    0          0       -1]\n\nB = [0     0;\n    0      0;\n    0      0;\n    0      0;\n    20.2   0;\n    0      20.2;\n    0      0]\n\nC = [0      0       0       57.2958 0      0    -1;\n    0       0       57.2958 0       0      0     0;\n    57.2958 0       0       0       0      0     0;\n    0       57.2958 0       0       0      0     0]\n\n# The `C` matrix is only defined for plotting purposes as it converts from radians to degrees.\n\nusing ControlSystems, LinearAlgebra\nG = ss(A,B,C,0)\n# Conversion of the model to discrete time \nTs = 0.1                     # sampling period\nGd = c2d(G,Ts)               # discretized system\nA,B,C,D = Gd.A,Gd.B,Gd.C,Gd.D\n\n\n([0.9226667967276305 0.0061993687563318516 … 0.0002121726335909019 0.0; -0.1324663143618211 0.9997066454002713 … 0.00031188493568957813 0.0; … ; 0.0 0.0 … 0.13265546508012172 0.0; 2.292290658677099 0.004976965360488232 … -0.009375212477843127 0.9048374180359594], [2.821613795894789e-5 0.00018468401795193416; -0.001449606139792144 0.00025320960350156183; … ; 0.0 0.8673445349198778; -0.0037522412534400328 -0.007373986216857326], [0.0 0.0 … 0.0 -1.0; 0.0 0.0 … 0.0 0.0; 57.2958 0.0 … 0.0 0.0; 0.0 57.2958 … 0.0 0.0], [0.0 0.0; 0.0 0.0; 0.0 0.0; 0.0 0.0])\n\n\nAnd in the following code we define the weights for the LQR controller and design the controller\n\n\nShow the code\nq_beta = 100.0\nq_phi = 100\nq_p = 1\nq_r = 1\nq_rw = 1\nq = [q_beta, q_phi, q_p, q_r, 0, 0, q_rw]\nQ = diagm(0=&gt;q)\n\nr_a = 10.0\nr_r = 10.0\nr = [r_a, r_r]\nR = diagm(0=&gt;r)\n\n\n2×2 Matrix{Float64}:\n 10.0   0.0\n  0.0  10.0\n\n\nNote how we do not penalize the fifth and sixth state variables, \\delta_\\mathrm{a} and \\delta_\\mathrm{r}, respectively, because these are just low-pass filtered control inputs, and the control inputs are already penalized with the \\mathbf R matrix. But we must check if the stabilizability and detectability – or actually controllability and observability – conditions are satisfied:\n\n\nShow the code\nrank(ctrb(A,B)), rank(obsv(A,Q))\n\n\n(7, 7)\n\n\nThe conditions are obviously satisfied, hence a unique stabilizing positive definite solution to DARE exists. We can compute it using a dedicated solver and we can form the matrix of state feedback gains:\n\n\nShow the code\nS = dare(A,B,Q,R)\nK = (R+B'*S*B)\\B'*S*A\n\n\n2×7 Matrix{Float64}:\n  0.32331  -2.90175   -0.73217    …  0.030565    0.00361445  -0.0242721\n -1.32507  -0.173493  -0.0398993     0.00352081  0.00384607  -0.0187199\n\n\nAlternatively, we could use the higher-level lqr or dlqr functions (K = lqr(Gd,Q,R), or K = dlqr(A,B,Q,R)). Finally we can use the computed gains to construct the controller and simulate and plot a response of the system to nonzero initial conditions:\n\n\nShow the code\nu(x,t) = -K*x # Control law\nt=0:Ts:10.0\nx0 = [1/10, 0, 0, 0, 0, 0, 0]\ny, t, x, uout = lsim(Gd,u,t,x0=x0)\n\nusing Plots, LaTeXStrings\np1 = plot(t,uout', label=[L\"u_\\mathrm{a} \\mathrm{(rad)}\" L\"u_\\mathrm{r} \\mathrm{(rad)}\"], xlabel=\"Time (s)\", ylabel=\"Controls\",markershape=:circle,markersize=1,linetype=:steppost)\np2 = plot(t,x[1:4,:]', label=[L\"\\beta \\mathrm{(rad)}\" L\"\\phi \\mathrm{(rad)}\" L\"p \\mathrm{(rad/s)}\" L\"r \\mathrm{(rad/s)}\" L\"\\delta_a\" L\"\\delta_r\" L\"r_w \\mathrm{(deg/s)}\"], xlabel=\"Time (s)\", ylabel=\"States\",markershape=:circle,markersize=1,linetype=:steppost)\np3 = plot(t,y', label=[L\"r_\\mathrm{w} \\mathrm{(deg/s)}\" L\"p \\mathrm{(deg/s)}\" L\"\\beta \\mathrm{(deg)}\" L\"\\phi \\mathrm{(deg)}\"], xlabel=\"Time (s)\", ylabel=\"Outputs\",markershape=:circle,markersize=1,linetype=:steppost)\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Simulation outcomes for the LQR control of lateral-directional dynamics of F16. Among the state variables we do not include \\delta_\\mathrm{a} and \\delta_\\mathrm{r} as these are just low-passs filtered out control inputs",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on an infinite horizon using the DARE"
    ]
  },
  {
    "objectID": "cont_indir_hw_2.html",
    "href": "cont_indir_hw_2.html",
    "title": "Homework",
    "section": "",
    "text": "A bee colony operates throughout a flowering season of length t_\\mathrm{f}. During this period, the colony must allocate its foraging effort between two key activities\n\nHive expansion: Increasing the number of worker bees to support future nectar collection (e.g., raising brood and constructing comb).\nNectar collection: Gathering nectar to convert into honey, which serves as the colony’s energy reserve for the coming non-flowering season.\n\nAt any time t \\in \\left[0, t_\\mathrm{f}\\right], the colony allocates a fraction u(t) \\in [0, 1] of its foraging effort toward hive expansion, with the remaining fraction 1 - u(t) used for nectar collection. The hive’s ability to perform either task depends on its current size, and bees die at a natural rate proportional to their population.\nLet\n\nx_1(t) denote the effective hive size (i.e., the number of active foragers), and\nx_2(t) the cumulative amount of nectar collected by time t.\n\nThe system dynamics are governed by\n\n\\begin{aligned}\n\\dot{x}_1(t) &= 2u(t)x_1(t) - x_1(t),\\\\\n\\dot{x}_2(t) &= (1 - u(t))x_1(t).\n\\end{aligned}\n\nThe hive size and nectar reserves are assumed to remain non-negative throughout the season, consistent with their physical interpretation. Also assume nonzero hive size at the beginning of the season, i.e., x_1(0) &gt; 0.\n\n\nFind the optimal allocation strategy u(t) that maximizes the total nectar stored by the end of the season, i.e., maximize x_2(t_\\mathrm{f}) for a fixed t_\\mathrm{f}.\n\n\n\n\n\n\nNote\n\n\n\nSimilar problems may appear on the final exam, so focus on understanding what you are doing and why.\n\n\n\n\n\n\nFormulate the Hamiltonian, derive the state and co-state equations.\nUse Pontryagin’s maximum principle to determine the optimal control strategy u^\\star as a function of the co-states, i.e., u^\\star(\\lambda_1, \\lambda_2). What can be immediately concluded about the optimal control?\nSolve the co-state equations, to determine the co-states \\boldsymbol{\\lambda}(t) on the interval [0, t_\\mathrm{f}].\nUse the co-states to express u^\\star as purely a function of time (and t_\\mathrm{f}).\n\n\n\n\n\n\n\nHints for solving the co-state equations\n\n\n\n\nThere is a way to determine the terminal condition \\boldsymbol{\\lambda}(t_\\mathrm{f}).\nThis allows you to immediately determine the value of one of the co-states.\nFrom \\boldsymbol{\\lambda}(t_\\mathrm{f}), you can also determine u^\\star(t_\\mathrm{f}) using your earlier result.\nSince all necessary information is given at the end of the control interval rather than the beginning, you’ll need to solve the co-state equations backwards in time. This can be done by reversing time and solving the system as a standard initial value problem.\n\n\n\nUpload the following files as a single .zip file to BRUTE system.\n\nhw.pdf containing your derivation of the problem solution. We do not require a \\LaTeX report; handwritten notes suffice, but they must be readable.\nhw.jl, based on the template below, containing a function u_opt(t, tf) that implements the final closed-form expression for your derived optimal control u^\\star(t).\n\n\nfunction u_opt(t, tf)\n    return 1\nend\n\nYour homework will be automatically evaluated by the BRUTE system based on your Julia implementation, so you can immediately see if you have arrived at the correct solution. However, full point will be awarded only after a manual review of the derivation by us.",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Homework"
    ]
  },
  {
    "objectID": "cont_indir_hw_2.html#nectar-storage-vs.-hive-expansion-in-a-bee-colony",
    "href": "cont_indir_hw_2.html#nectar-storage-vs.-hive-expansion-in-a-bee-colony",
    "title": "Homework",
    "section": "",
    "text": "A bee colony operates throughout a flowering season of length t_\\mathrm{f}. During this period, the colony must allocate its foraging effort between two key activities\n\nHive expansion: Increasing the number of worker bees to support future nectar collection (e.g., raising brood and constructing comb).\nNectar collection: Gathering nectar to convert into honey, which serves as the colony’s energy reserve for the coming non-flowering season.\n\nAt any time t \\in \\left[0, t_\\mathrm{f}\\right], the colony allocates a fraction u(t) \\in [0, 1] of its foraging effort toward hive expansion, with the remaining fraction 1 - u(t) used for nectar collection. The hive’s ability to perform either task depends on its current size, and bees die at a natural rate proportional to their population.\nLet\n\nx_1(t) denote the effective hive size (i.e., the number of active foragers), and\nx_2(t) the cumulative amount of nectar collected by time t.\n\nThe system dynamics are governed by\n\n\\begin{aligned}\n\\dot{x}_1(t) &= 2u(t)x_1(t) - x_1(t),\\\\\n\\dot{x}_2(t) &= (1 - u(t))x_1(t).\n\\end{aligned}\n\nThe hive size and nectar reserves are assumed to remain non-negative throughout the season, consistent with their physical interpretation. Also assume nonzero hive size at the beginning of the season, i.e., x_1(0) &gt; 0.\n\n\nFind the optimal allocation strategy u(t) that maximizes the total nectar stored by the end of the season, i.e., maximize x_2(t_\\mathrm{f}) for a fixed t_\\mathrm{f}.\n\n\n\n\n\n\nNote\n\n\n\nSimilar problems may appear on the final exam, so focus on understanding what you are doing and why.\n\n\n\n\n\n\nFormulate the Hamiltonian, derive the state and co-state equations.\nUse Pontryagin’s maximum principle to determine the optimal control strategy u^\\star as a function of the co-states, i.e., u^\\star(\\lambda_1, \\lambda_2). What can be immediately concluded about the optimal control?\nSolve the co-state equations, to determine the co-states \\boldsymbol{\\lambda}(t) on the interval [0, t_\\mathrm{f}].\nUse the co-states to express u^\\star as purely a function of time (and t_\\mathrm{f}).\n\n\n\n\n\n\n\nHints for solving the co-state equations\n\n\n\n\nThere is a way to determine the terminal condition \\boldsymbol{\\lambda}(t_\\mathrm{f}).\nThis allows you to immediately determine the value of one of the co-states.\nFrom \\boldsymbol{\\lambda}(t_\\mathrm{f}), you can also determine u^\\star(t_\\mathrm{f}) using your earlier result.\nSince all necessary information is given at the end of the control interval rather than the beginning, you’ll need to solve the co-state equations backwards in time. This can be done by reversing time and solving the system as a standard initial value problem.\n\n\n\nUpload the following files as a single .zip file to BRUTE system.\n\nhw.pdf containing your derivation of the problem solution. We do not require a \\LaTeX report; handwritten notes suffice, but they must be readable.\nhw.jl, based on the template below, containing a function u_opt(t, tf) that implements the final closed-form expression for your derived optimal control u^\\star(t).\n\n\nfunction u_opt(t, tf)\n    return 1\nend\n\nYour homework will be automatically evaluated by the BRUTE system based on your Julia implementation, so you can immediately see if you have arrived at the correct solution. However, full point will be awarded only after a manual review of the derivation by us.",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Homework"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html",
    "href": "opt_algo_unconstrained.html",
    "title": "Algorithms for unconstrained optimization",
    "section": "",
    "text": "Our motivation for studying numerical algorithms for unconstrained optimization remains the same as when we studied the conditions of optimality for such unconstrained problems – such algorithms constitute building blocks for constrained optimization problems. Indeed, many algorithms for constrained problems are based on reformulating the constrained problem into an unconstrained one and then applying the algorithms studied in this section.\nIt may be useful to recapitulate our motivation for studying optimization algorithms in general – after all, there are dozens of commercial or free&open-source software tools for solving optimization problems. Why not just use them? There are two answers beyond the traditional “at a grad school we should understand what we are using”:\nThere is certainly no shortage of algorithms for unconstrained optimization. In this crash course we can cover only a few. But the few we cover here certainly form a solid theoretical basis and provide practically usable tools.\nOne possible way to classify the algorithms is based on whether they use derivatives of the objective functions or not. In this course, we only consider the former approaches as they leads to more efficient algorithms. For the latter methods, we can refer to the literature (the prominent example is Nelder-Mead method).\nAll the relevant methods are iterative. Based on what happens within each iteration, we can classify them into two categories:",
    "crumbs": [
      "2. Optimization – algorithms",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html#descent-methods",
    "href": "opt_algo_unconstrained.html#descent-methods",
    "title": "Algorithms for unconstrained optimization",
    "section": "Descent methods",
    "text": "Descent methods\nA single iteration of a descent method consists of the following step: \n\\boxed{\n\\bm x_{k+1} = \\bm x_{k} + \\alpha_k \\bm d_k,}        \n\\tag{1} where \\bm x_k is the current solution, \\bm d_k is the search direction, and \\alpha_k is the step length.\nThe obvious quality that the search direction needs to satisfy, is that the cost function decreses along it, at least locally (for a small step length).\n\nDefinition 1 (Descent direction) At the current iterate \\bm x_k, the direction \\bm d_k is called a descent direction if \n\\nabla f(\\bm x_k)^\\top \\bm d_k &lt; 0,\n that is, the directional derivative is negative along the direction \\bm d_k.\n\nThe product above is an inner product of the two vectors \\bm d_k and \\nabla f(\\mathbf x_k). Recall that it is defined as \n\\nabla f(\\bm x_k)^\\top \\bm d_k = \\|\\nabla f(\\bm x_k)\\| \\|\\bm d_k\\| \\cos \\theta,\n where \\theta is the angle between the gradient and the search direction. This condition has a nice geometric interpretation in a contour plot for an optimization in \\mathbb R^2. Consider the line tangent to the function countour at \\bm x_k. A descent direction must be in the other half-plane generated by the tangent line than the one into which the gradient \\nabla f(\\bm x_k) points.\nBeware that it is only guaranteed that the cost function is reduced if the length of the step is sufficently small. For longer steps the higher-order terms in the Taylor’s series approximation of the cost function can dominate.\nBefore we proceed to the question of which descent direction to choose, we adress the question of how far to go along the chosen direction. This is the problem of line search.\n\nStep length determination (aka line search)\nNote that once the search direction has been fixed (whether we used the negative of the gradient or any other descent direction), the problem of finding the step length \\alpha_k is just a scalar optimization problem. It turns out, however, that besides finding the true minimum along the search directions, it is often sufficient to find the minimum only approximately, or not aiming at minimization at all and work with a fixed step length instead.\n\nFixed length of the step\nHere we give a guidance on the choice of the lenght of the step. But we need to introduce a useful concept first.\n\nDefinition 2 (L-smoothness) For a continuously differentiable function f, the gradient \\nabla f is said to be L-smooth if there exists a constant L&gt;0 such that \n\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L \\|x-y\\|.\n\n\nNot that if the second derivatives exist, L is an upper bound on the norm of the Hessian \n\\|\\nabla^2 f\\|\\leq L.\n\nFor quadratic functions, L is the largest eigenvalue of the Hessian \nL = \\max_i \\lambda_i (\\mathbf Q).\n\nThe usefulness of the concept of L-smoothness is that it provides a quadratic function that serves as an upper bound for the original function. This is formulated as the following lemma.\n\nLemma 1 (Descent lemma) Consider an L-smooth function f. Then for any \\mathbf x_k and \\mathbf x_{k+1}, the following inequality holds \nf(\\mathbf x_{k+1}) \\leq  f(\\mathbf x_{k}) + \\nabla f(\\mathbf x_k)^\\top (\\mathbf x_{k-1}-\\mathbf x_{k}) + \\frac{L}{2}\\|\\mathbf x_{k-1}-\\mathbf x_{k}\\|^2\n\n\nWhat implication does the result have on the determination of the step length?\n\n\\alpha = \\frac{1}{L}.\n\n\n\nExact line search\nA number of methods exist: bisection, golden section, Newton, … As finding the true minium in each iteration is often too computationally costly and hardly needed, we do not cover these methods here. One exception the Newton’s method, which for vector variables constitutes another descent method on its own and we cover it later.\nAnother exception is the case of a quadratic function in the following example.\n\nExample 1 Here we develop a solution for exact minimization of a quadratic functions f(\\bm x) = \\frac{1}{2} \\bm x^\\top\\mathbf Q \\bm x + \\mathbf c^\\top \\bm x along a given direction. We show that it leads to a closed-form formula. Although not particularly useful in practice (for a quadratic function we already know we can find the minimizer by solving a system of linear equations), it is a good exercise in understanding the problem of line search. Furthermore, we will use it later to demonstrate the behaviour of the steepest descent method. The problem is to \\operatorname*{minimize}_{\\alpha_k} f(\\bm x_k + \\alpha_k \\bm d_k). We express the cost as a function of the current iterate, the direction, and step length. \n\\begin{aligned}\nf(\\bm x_k + \\alpha_k \\bm d_k) &= \\frac{1}{2}(\\bm x_k + \\alpha_k\\bm d_k)^\\top\\mathbf Q (\\bm x_k + \\alpha_k\\bm d_k) +\\mathbf c^\\top(\\bm x_k + \\alpha_k\\bm d_k)\\\\\n&= \\frac{1}{2} \\bm x_k^\\top\\mathbf Q \\bm x_k + \\bm d_k^\\top\\mathbf Q\\bm x_k \\alpha_k + \\frac{1}{2} \\bm d_k^\\top\\mathbf Q\\bm d_k \\alpha_k^2+ \\mathbf c^\\top(\\bm x_k + \\alpha_k\\bm d_k).\n\\end{aligned}\n\nConsidering the current iterate and the search direction constant, by differentiating the function with respect to the length of the step, we get \n\\frac{\\mathrm{d}f(\\bm x_k + \\alpha_k\\bm d_k)}{\\mathrm{d}\\alpha_k} = \\bm d_k^\\top \\underbrace{(\\mathbf Q\\bm x_k + \\mathbf c)}_{\\nabla f(\\bm x_k)} + \\bm d_k^\\top\\mathbf Q\\bm d_k \\alpha_k.\n\nAnd now setting the derivative to zero, we find the optimal step length \n\\boxed{\n\\alpha_k = -\\frac{\\bm d_k^\\top \\nabla f(\\bm x_k)}{\\bm d_k^\\top\\mathbf Q\\bm d_k} = -\\frac{\\bm d_k^\\top (\\mathbf Q\\bm x_k + \\mathbf c)}{\\bm d_k^\\top\\mathbf Q\\bm d_k}.}\n\\tag{2}\nAs we have mentioned, this result will be useful for some benchmarking later.\n\n\n\nApproximate line search – backtracking\nThere are several methods for approximate line search. Here we describe the backtracking algorithm, which is based on the sufficient decrease condition (also known as Armijo condition), which reads \nf(\\bm x_k+\\alpha_k\\bm d_k) - f(\\bm x_k) \\leq \\gamma \\alpha_k \\mathbf d^T \\nabla f(\\bm x_k),\n where \\gamma\\in(0,1), typically \\gamma is very small, say \\gamma = 10^{-4}.\nThe term on the right can be be viewed as a linear function of \\alpha_k. Its negative slope is a bit less steep than the directional derivative of the function f at \\bm x_k. The condition of sufficient decrease thus requires that the cost function (as a function of \\alpha_k) is below the graph of this linear function.\nNow, the backtracking algorithm is parameterized by three parameters: the initial step lenght \\alpha_0&gt;0, the typically very small \\gamma\\in(0,1) that parameterizes the Armijo condition, and yet another parameter \\beta\\in(0,1).\nThe k-th iteration of the algorithm goes like this: failure of the sufficient decrease condition for a given \\alpha_k or, equivalently, satisfaction of the condition \nf(\\bm x_k) - f(\\bm x_k+\\alpha_k\\bm d_k) &lt; -\\gamma \\alpha_k \\mathbf d^T \\nabla f(\\bm x_k)\n sends the algorithm into another reduction of \\alpha_k by \\alpha_k = \\beta\\alpha_k. A reasonable choice for \\beta is 0.5, which corresponds to halving the step length upon failure to decrease sufficiently.\nThe backtracking algorithm can be implemented as follows\n\n\nShow the code\nfunction backtracking_line_search(f, ∇fₖ, xₖ, dₖ; α₀=1.0, β=0.5, γ=0.1)\n    αₖ = α₀\n    while f(xₖ)-f(xₖ+αₖ*dₖ) &lt; -γ*αₖ*dot(dₖ,∇fₖ)\n        αₖ *= β\n    end\n    return αₖ\nend\n\n\nbacktracking_line_search (generic function with 1 method)\n\n\nNow we are ready to proceed to the question of choosing a descent direction.\n\n\n\nSteepest descent (aka gradient descent) method\nA natural candidate for a descent direction is the negative of the gradient \n\\bm d_k = -\\nabla f(\\bm x_k).\n\nIn fact, among all descent directions, this is the one for which the descent is steepest (the gradient determines the direction of steepest ascent), though we will see later that this does not mean that the convergence of the method is the fastest.\nIn each iteration of the gradient method, this is the how the solution is updated\n\n\\boxed{\n\\bm x_{k+1} = \\bm x_{k} - \\alpha_k \\nabla f(\\bm x_{k}),}\n where the determination of the step length \\alpha_k has already been discussed in the previous section.\nLet’s now examine the behaviour of the method by applying it to minimization of a quadratic function. Well, for a quadratic function it is obviously an overkill, but we use it in the example because we can compute the step length exactly using Eq. 2, which then helps the methods show its best performance.\n\nExample 2 (Steepest descent method for a quadratic function with exact line search)  \n\n\nShow the code\nusing LinearAlgebra         # For dot() function.\nusing Printf                # For formatted output.\n\nx0 = [2, 3]                 # Initial vector.\nQ = [1 0; 0 3]              # Positive definite matrix defining the quadratic form.\nc = [1, 2]                   # Vector defining the linear part.\n\nxs = -Q\\c                   # Stationary point, automatically the minimizer for posdef Q. \n\nϵ  = 1e-5                   # Threshold on the norm of the gradient.\nN  = 100;                   # Maximum number of steps .\n\nfunction gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n    x = x0\n    iter = 0\n    f = 1/2*dot(x,Q*x)+dot(x,c)\n    ∇f = Q*x+c\n    while (norm(∇f) &gt; ϵ)\n        α = dot(∇f,∇f)/dot(∇f,Q*∇f)\n        x = x - α*∇f\n        iter = iter+1\n        f = 1/2*dot(x,Q*x)+dot(x,c)\n        ∇f = Q*x+c\n        @printf(\"i = %3d   ||∇f(x)|| = %6.4e   f(x) = %6.4e\\n\", iter, norm(∇f), f)\n        if iter &gt;= N\n            return f,x\n        end\n    end\n    return f,x\nend\n\nfopt,xopt = gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n\n\ni =   1   ||∇f(x)|| = 2.0229e+00   f(x) = 7.8495e-01\ni =   2   ||∇f(x)|| = 9.0210e-01   f(x) = -1.0123e+00\ni =   3   ||∇f(x)|| = 1.6005e-01   f(x) = -1.1544e+00\ni =   4   ||∇f(x)|| = 7.1374e-02   f(x) = -1.1657e+00\ni =   5   ||∇f(x)|| = 1.2663e-02   f(x) = -1.1666e+00\ni =   6   ||∇f(x)|| = 5.6470e-03   f(x) = -1.1667e+00\ni =   7   ||∇f(x)|| = 1.0019e-03   f(x) = -1.1667e+00\ni =   8   ||∇f(x)|| = 4.4679e-04   f(x) = -1.1667e+00\ni =   9   ||∇f(x)|| = 7.9269e-05   f(x) = -1.1667e+00\ni =  10   ||∇f(x)|| = 3.5350e-05   f(x) = -1.1667e+00\ni =  11   ||∇f(x)|| = 6.2718e-06   f(x) = -1.1667e+00\n\n\n(-1.1666666666479069, [-0.9999939492423319, -0.6666672167355456])\n\n\nWe can also decorate the code a bit to visualize how the iterations proceeded.\n\n\nShow the code\nfunction gradient_descent_quadratic_exact_decor(Q,c,x0,ϵ,N)\n    x = x0\n    X = x\n    f = 1/2*dot(x,Q*x)+dot(x,c)\n    F = [f,]\n    ∇f = Q*x+c\n    iter = 0\n    while (norm(∇f) &gt; ϵ)\n        α = dot(∇f,∇f)/dot(∇f,Q*∇f)\n        x = x - α*∇f\n        iter = iter+1\n        f = 1/2*dot(x,Q*x)+dot(x,c)\n        ∇f = Q*x+c\n        X = hcat(X,x)\n        push!(F,f)\n        if iter &gt;= N\n         return F,X\n        end\n    end\n    return F,X\nend\n\nF,X = gradient_descent_quadratic_exact_decor(Q,c,x0,ϵ,N)\n\nx1_grid = x2_grid = -4:0.01:4;\nf(x) = 1/2*dot(x,Q*x)+dot(x,c)\nz_grid = [f([x1,x2]) for x2=x2_grid, x1=x1_grid];\n\nusing Plots\ncontour(x1_grid,x2_grid,z_grid)\nplot!(X[1,:],X[2,:],label=\"xk\",marker=:diamond,aspect_ratio=1)\nscatter!([x0[1],],[x0[2],],label=\"x0\")\nscatter!([xs[1],],[xs[2],],label=\"xopt\")\nxlabel!(\"x1\");ylabel!(\"x2\");\nxlims!(-4,4); ylims!(-4,4)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Zigzagging of the steepest descent method for a quadratic function\n\n\n\n\n\nAltough the number of iterations in the above example is acceptable, a major characteristic of the method is visible. Its convergence is slowing down as we are approaching a local minimum, which is visually recognizable oscillations or zig-zagging. But it can be much worse for some data, as the next example shows.\n\nExample 3 (Steepest descent method for an ill-conditioned quadratic function with exact line search) Consider minimization of the following cost function f(\\bm x) = 1000x_1^2 + 40x_1x_2 + x_2^2.\n\n\nShow the code\nQ = [1000 20; 20 1]\nc = [0, 0]\nx0 = [1,1000]\n\nF,X = gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n\n\ni =   1   ||∇f(x)|| = 5.9951e+02   f(x) = 2.9939e+05\ni =   2   ||∇f(x)|| = 1.2093e+04   f(x) = 1.7221e+05\ni =   3   ||∇f(x)|| = 3.4484e+02   f(x) = 9.9052e+04\ni =   4   ||∇f(x)|| = 6.9560e+03   f(x) = 5.6974e+04\ni =   5   ||∇f(x)|| = 1.9835e+02   f(x) = 3.2771e+04\ni =   6   ||∇f(x)|| = 4.0011e+03   f(x) = 1.8850e+04\ni =   7   ||∇f(x)|| = 1.1409e+02   f(x) = 1.0842e+04\ni =   8   ||∇f(x)|| = 2.3014e+03   f(x) = 6.2364e+03\ni =   9   ||∇f(x)|| = 6.5623e+01   f(x) = 3.5872e+03\ni =  10   ||∇f(x)|| = 1.3237e+03   f(x) = 2.0633e+03\ni =  11   ||∇f(x)|| = 3.7746e+01   f(x) = 1.1868e+03\ni =  12   ||∇f(x)|| = 7.6141e+02   f(x) = 6.8264e+02\ni =  13   ||∇f(x)|| = 2.1711e+01   f(x) = 3.9265e+02\ni =  14   ||∇f(x)|| = 4.3796e+02   f(x) = 2.2585e+02\ni =  15   ||∇f(x)|| = 1.2488e+01   f(x) = 1.2991e+02\ni =  16   ||∇f(x)|| = 2.5191e+02   f(x) = 7.4722e+01\ni =  17   ||∇f(x)|| = 7.1831e+00   f(x) = 4.2980e+01\ni =  18   ||∇f(x)|| = 1.4490e+02   f(x) = 2.4722e+01\ni =  19   ||∇f(x)|| = 4.1317e+00   f(x) = 1.4220e+01\ni =  20   ||∇f(x)|| = 8.3344e+01   f(x) = 8.1791e+00\ni =  21   ||∇f(x)|| = 2.3765e+00   f(x) = 4.7046e+00\ni =  22   ||∇f(x)|| = 4.7939e+01   f(x) = 2.7061e+00\ni =  23   ||∇f(x)|| = 1.3670e+00   f(x) = 1.5565e+00\ni =  24   ||∇f(x)|| = 2.7574e+01   f(x) = 8.9529e-01\ni =  25   ||∇f(x)|| = 7.8627e-01   f(x) = 5.1497e-01\ni =  26   ||∇f(x)|| = 1.5861e+01   f(x) = 2.9621e-01\ni =  27   ||∇f(x)|| = 4.5226e-01   f(x) = 1.7038e-01\ni =  28   ||∇f(x)|| = 9.1229e+00   f(x) = 9.7999e-02\ni =  29   ||∇f(x)|| = 2.6014e-01   f(x) = 5.6369e-02\ni =  30   ||∇f(x)|| = 5.2474e+00   f(x) = 3.2423e-02\ni =  31   ||∇f(x)|| = 1.4963e-01   f(x) = 1.8649e-02\ni =  32   ||∇f(x)|| = 3.0183e+00   f(x) = 1.0727e-02\ni =  33   ||∇f(x)|| = 8.6065e-02   f(x) = 6.1701e-03\ni =  34   ||∇f(x)|| = 1.7361e+00   f(x) = 3.5490e-03\ni =  35   ||∇f(x)|| = 4.9504e-02   f(x) = 2.0414e-03\ni =  36   ||∇f(x)|| = 9.9859e-01   f(x) = 1.1742e-03\ni =  37   ||∇f(x)|| = 2.8475e-02   f(x) = 6.7539e-04\ni =  38   ||∇f(x)|| = 5.7439e-01   f(x) = 3.8848e-04\ni =  39   ||∇f(x)|| = 1.6378e-02   f(x) = 2.2345e-04\ni =  40   ||∇f(x)|| = 3.3038e-01   f(x) = 1.2853e-04\ni =  41   ||∇f(x)|| = 9.4207e-03   f(x) = 7.3928e-05\ni =  42   ||∇f(x)|| = 1.9003e-01   f(x) = 4.2523e-05\ni =  43   ||∇f(x)|| = 5.4188e-03   f(x) = 2.4459e-05\ni =  44   ||∇f(x)|| = 1.0931e-01   f(x) = 1.4069e-05\ni =  45   ||∇f(x)|| = 3.1168e-03   f(x) = 8.0922e-06\ni =  46   ||∇f(x)|| = 6.2873e-02   f(x) = 4.6546e-06\ni =  47   ||∇f(x)|| = 1.7928e-03   f(x) = 2.6773e-06\ni =  48   ||∇f(x)|| = 3.6164e-02   f(x) = 1.5400e-06\ni =  49   ||∇f(x)|| = 1.0312e-03   f(x) = 8.8578e-07\ni =  50   ||∇f(x)|| = 2.0801e-02   f(x) = 5.0949e-07\ni =  51   ||∇f(x)|| = 5.9314e-04   f(x) = 2.9306e-07\ni =  52   ||∇f(x)|| = 1.1965e-02   f(x) = 1.6856e-07\ni =  53   ||∇f(x)|| = 3.4117e-04   f(x) = 9.6958e-08\ni =  54   ||∇f(x)|| = 6.8821e-03   f(x) = 5.5769e-08\ni =  55   ||∇f(x)|| = 1.9624e-04   f(x) = 3.2078e-08\ni =  56   ||∇f(x)|| = 3.9585e-03   f(x) = 1.8451e-08\ni =  57   ||∇f(x)|| = 1.1288e-04   f(x) = 1.0613e-08\ni =  58   ||∇f(x)|| = 2.2769e-03   f(x) = 6.1045e-09\ni =  59   ||∇f(x)|| = 6.4925e-05   f(x) = 3.5113e-09\ni =  60   ||∇f(x)|| = 1.3097e-03   f(x) = 2.0197e-09\ni =  61   ||∇f(x)|| = 3.7345e-05   f(x) = 1.1617e-09\ni =  62   ||∇f(x)|| = 7.5331e-04   f(x) = 6.6821e-10\ni =  63   ||∇f(x)|| = 2.1480e-05   f(x) = 3.8435e-10\ni =  64   ||∇f(x)|| = 4.3330e-04   f(x) = 2.2107e-10\ni =  65   ||∇f(x)|| = 1.2355e-05   f(x) = 1.2716e-10\ni =  66   ||∇f(x)|| = 2.4923e-04   f(x) = 7.3142e-11\ni =  67   ||∇f(x)|| = 7.1068e-06   f(x) = 4.2071e-11\n\n\n(4.207097012988499e-11, [-2.37187654299357e-7, 1.1842143766173123e-5])\n\n\nWhile for the previous problem of the same kind and size the steepest descent method converged in just a few steps, for this particular data it takes many dozens of steps.\nThe culprit here are bad properties of the Hessian matrix Q. By “bad properties” we mean the so-called ill-conditioning, which is reflected in the very high condition number. Recall that condition number \\kappa for a given matrix \\mathbf A is defined as \n\\kappa(\\mathbf A) = \\|\\mathbf A^{-1}\\|\\cdot \\|\\mathbf A\\|,\n and it can be computed as ratio of the largest and smallest singular values, that is, \n\\kappa(\\mathbf A) = \\frac{\\sigma_{\\max}(\\mathbf A)}{\\sigma_{\\min}(\\mathbf A)}.\n\nIdeally this number should not be much larger than 1. In the example above it is\n\n\nShow the code\ncond(Q)\n\n\n1668.0010671466664\n\n\nwhich is well above 1000. Is there anything that we can do about it? The answer is yes. We can scale the original date to improve the conditioning.\n\n\nScaled gradient method for ill-conditioned problems\nUpon introducing a matrix \\mathbf S that relates the original vector variable \\bm x with a new vector variable \\bm y according to \n\\bm x = \\mathbf S \\bm y,\n the optimization cost function changes from f(\\bm x) to f(\\mathbf S \\bm y). Let’s rename the latter to g(\\bm y). And we will now examine how the steepest descent iteration changes. Straightforward application of the chain rule for finding derivatives of composite functions yields \ng'(\\bm y) = f'(\\mathbf S\\bm y) = f'(\\mathbf S\\bm y)\\mathbf S.\n\nKeeping in mind that gradients are transposes of derivatives, we can write \n\\nabla g(\\bm y) = \\mathbf S^\\top \\nabla f(\\mathbf S\\bm y).\n\nSteepest descent iterations then change accordingly\n\n\\begin{aligned}\n\\bm y_{k+1} &= \\bm y_k - \\alpha_k \\nabla g(\\bm y_k)\\\\\n\\bm y_{k+1} &= \\bm y_k - \\alpha_k \\mathbf S^T\\nabla f(\\mathbf S \\bm y_k)\\\\\n\\underbrace{\\mathbf S \\bm y_{k+1}}_{\\bm x_{k+1}} &= \\underbrace{\\mathbf S\\bm y_k}_{\\bm x_k} - \\alpha_k \\underbrace{\\mathbf S \\mathbf S^T}_{\\mathbf D}\\nabla f(\\underbrace{\\mathbf S \\bm y_k}_{\\bm x_k}).\n\\end{aligned}\n\nUpon renaming the product \\mathbf S \\mathbf S^T as a scaling matrix \\mathbf D, a single iteration changes to \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - \\alpha_k \\mathbf D\\nabla f(\\bm x_{k}).}\n\\tag{3}\nThe key question now is: how to choose the matrix \\mathbf D?\nWe would like to make the Hessian matrix \\nabla^2 f(\\mathbf S \\bm y) (which in the case of a quadratic matrix form is the matrix \\mathbf Q as we used it above) better conditioned. Ideally, \\nabla^2 f(\\mathbf S \\bm y)\\approx \\mathbf I.\nA simple way for improving the conditioning is to define the scaling matrix \\mathbf D as a diagonal matrix whose diagonal entries are given by \n\\mathbf D_{ii} = [\\nabla^2 f(\\bm x_k)]^{-1}_{ii}.\n\nIn words, the diagonal entries of the Hessian matrix are inverted and they then form the diagonal of the scaling matrix.\n\n\n\n\n\n\nHighlighting the structure of the scaled gradient method\n\n\n\nIt is worth emphasizing how the algorithm changed: the direction of steepest descent (the negative of the gradient) is premultiplied by some (scaling) matrix. We will see in a few moments that another method – Newton’s method – has a perfectly identical structure.\n\n\n\n\n\nNewton’s method\nNewton’s method is one of flagship algorithms in numerical computing. With confidence I include it in my personal Top 10 list of algorithms relevant for engineers and scientists. We may encounter the method in two settings:\n\nas a method for solving (systems of) nonlinear equations (aka rootfinding),\nand as a method for optimization.\n\nThe two are inherently related and it is useful to be able to see the connection.\n\nNewton’s method for rootfinding\nThe problem to be solved is that of finding x\\in\\mathbb R for which a given function g() vanishes. In other words, we solve the following equation \ng(x) = 0.\n\nThe above state scalar version has also its vector extension \n\\mathbf g(\\bm x) = \\mathbf 0,\n in which \\bm x stands for an n-tuple of variables, that is, \\bm x\\in \\mathbb R^n, and \\mathbf g() denotes an n-tuple of functions. Even more general version allows for different number of variables and equations.\nWe start with a scalar version. A single iteration of the method evaluates not only the value g(x_k) of the function g at the given point x_k but also its derivative g'(x_k). It then uses the two to approximate the function g() at x_k by a linear (actually affine) function and computes the intersection of this approximating function with the horizontal axis. This gives as x_{k+1}, that is, the (k+1)-th approximation to a solution (root). We can write this down as \n\\begin{aligned}\n\\underbrace{g(x_{k+1})}_{0} &= g(x_{k}) + g'(x_{k})(x_{k+1}-x_k)\\\\\n0 &= g(x_{k}) + g'(x_{k})x_{k+1}-g'(x_{k})x_k,\n\\end{aligned}\n from which the famous formula follows \n\\boxed{x_{k+1} = x_{k} - \\frac{g(x_k)}{g'(x_k)}.}\n\nIn the vector form, the formula is \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - [\\nabla \\mathbf g(\\bm x_k)^\\top]^{-1}\\mathbf g(\\bm x_k),}\n where \\nabla \\mathbf g(\\bm x_k)^\\top is the (Jacobian) matrix of the first derivatives of \\mathbf g at \\bm x_k, that is, \\nabla \\mathbf g() is a matrix with the gradient of the g_i(\\bm x) function as its i-th column.\n\n\nNewton’s method for optimization\nOnce again, we restrict ourselves to a scalar case first. The problem is \n\\operatorname*{minimize}_{x\\in\\mathbb{R}}\\quad f(x).\n\nAt the k-th iteration of the algorithm, the solution is x_k. The function to be minimized is approximated at x_k by a quadratic function m_k(). In order to find parameterization of this quadratic function, not only the function f() itself but also its first and second derivatives, f'() and f''(), respectively, must be evaluated at x_k. Using these three, a function m_k(x) approximating f(x) at some x not too far from x_k can be defined \nm_k(x) = f(x_k) + f'(x_k)(x-x_k) + \\frac{1}{2}f''(x_k)(x-x_k)^2.\n\nThe problem of minimizing this new function in the k-th iteration is then formulated, namely,\n\n\\operatorname*{minimize}_{x_{k+1}\\in\\mathbb{R}}\\quad m_k(x_{k+1}).\n\nThe way to find this solution is straightforward: find the derivative of m_k() and find the value of x_{k+1} for which this derivative vanishes. The result is \n\\boxed{x_{k+1} = x_{k} - \\frac{f'(x_k)}{f''(x_k)}.}\n\n\nExample 4 (Newton’s method for minimization in the scalar case) We consider the scalar function of a single (real) variable\n\n\nShow the code\n# f(x) = 3cos(x)*sin(2x+2)^2\nf(x) = -x^4 + 12x^3 - 47x^2 + 60x\n\n\nf (generic function with 1 method)\n\n\nLet’s have a look at its graph on some interval.\n\n\nShow the code\nx_grid = 1:0.01:5.5\nf_grid = f.(x_grid)\n\nusing Plots\nplot(x_grid,f_grid,label=\"f(x)\",xlabel=\"x\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, the essence of the method is that at a given point x_k we approximate the function locally by a quadratic function. Say, we choose\n\n\nShow the code\n#xₖ = 0.25\nxₖ = 2.4\n\n\n2.4\n\n\nWe can evaluate at this point not only the function but its first and second derivatives as well. We use one of the packages for AD.\n\n\nShow the code\nusing ForwardDiff\nDf = x -&gt; ForwardDiff.derivative(f,x)\nD²f = x -&gt; ForwardDiff.derivative(Df,x)\n\n\n#6 (generic function with 1 method)\n\n\nNow, we can write the quadratic “model function” as\n\n\nShow the code\nmₖ(x) = f(xₖ) + Df(xₖ)*(x-xₖ) + 1/2*D²f(xₖ)*(x-xₖ)^2\n\n\nmₖ (generic function with 1 method)\n\n\nWe now evaluate this model on the interval and plot it:\n\n\nShow the code\nmₖ_grid = mₖ.(x_grid)\n\nplot!(x_grid,mₖ_grid,label=\"mₖ(x)\")\nscatter!([xₖ,],[f(xₖ),],label=\"f(xₖ)\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the formula for the Newton’s step we get that\n\n\nShow the code\nxₖ₊₁ = xₖ - Df(xₖ)/D²f(xₖ)\n\n\n3.798347107438017\n\n\n\n\nShow the code\nscatter!([xₖ₊₁],[f(xₖ₊₁)],label=\"f(xₖ₊₁)\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s go for another iteration:\n\n\nShow the code\nplot(x_grid,f_grid,label=\"f(x)\",xlabel=\"x\")\n\nmₖ₊₁(x) = f(xₖ₊₁) + Df(xₖ₊₁)*(x-xₖ₊₁) + 1/2*D²f(xₖ₊₁)*(x-xₖ₊₁)^2\nmₖ₊₁_grid = mₖ₊₁.(x_grid)\nplot!(x_grid,mₖ₊₁_grid,label=\"mₖ₊₁(x)\")\nscatter!([xₖ,],[f(xₖ),],label=\"f(xₖ)\")\nscatter!([xₖ₊₁,],[f(xₖ₊₁),],label=\"f(xₖ₊₁)\")\n\nxₖ₊₂ = xₖ₊₁ - Df(xₖ₊₁)/D²f(xₖ₊₁)\nscatter!([xₖ₊₂],[f(xₖ₊₂)],label=\"f(xₖ₊₂)\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd the iterations would go on…\nBefore we leave this example, you are invited to experiment with setting different initial values of x.\n\nThe vector version of the Newton’s step is \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - [\\nabla^2 f(\\bm x_k)]^{-1} \\nabla f(\\bm x_k).}\n\nAlthough the mathematical formula contains a symbol for the inverse of a matrix, computationally it is better to formulate this computation in a way that a system of linear equations is solved. Namely, denoting the second term on the right by \\bm d_k, our symbol for the direction, that is, [\\nabla^2 f(\\bm x_k)]^{-1} \\nabla f(\\bm x_k) = \\bm d_k, we can find \\bm d_k by solving the following system of linear equations\n\n\\nabla^2 f(\\bm x_k) \\bm d_k = \\nabla f(\\bm x_k).\n\n\n\n\n\n\n\nInverse of a matrix is rarely needed in numerical computations\n\n\n\nIndeed, discussion forums for various programming languages and environments abound with questions about how to find the inverse of a matrix. The answer is almost always: are you sure you really need it? Most probably what you need is to solve a systems of linear equations, and that is a (slightly) different task.\n\n\n\nExample 5 (Newton’s method for minimization in the vector case) Consider the function\n\nf(x) = (x[1]+1)^4 + x[1]*x[2] + (x[2]+1)^4\n\nf (generic function with 1 method)\n\n\nThe graph of the function is in Fig. 2 below.\n\n\nShow the code\nx₁_grid = x₂_grid = -3:0.1:3;  \nf_grid = [f([x₁,x₂]) for x₁=x₂_grid, x₂=x₂_grid];\n\nusing Plots\nsurface(x₁_grid,x₂_grid,f_grid')   # Note the transpose.\nxlabel!(\"x₁\")\nylabel!(\"x₂\")\nxlims!(-3,3) \nylims!(-3,3)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Graph of the example function\n\n\n\n\nSome more insight can perhaps be obtained by plotting the contours as in Fig. 3.\n\n\nShow the code\ncontour(x₁_grid,x₂_grid,f_grid',levels=50)  # Note the transpose.\nxlabel!(\"x₁\")\nylabel!(\"x₂\")\nxlims!(-3,3) \nylims!(-3,3)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Contour graph of the example function\n\n\n\n\nWe choose to compute the gradient manually/symbolically:\n\n∇f(x) = [4*(1 + x[1])^3 + x[2], x[1] + 4*(1 + x[2])^3]\n\n∇f (generic function with 1 method)\n\n\nand similarly we do for the Hessian:\n\n∇²f(x) = [12*(1 + x[1])^2 1; 1 12*(1 + x[2])^2]\n\n∇²f (generic function with 1 method)\n\n\nWe now consider a particular value of the vector variable and use not only the function value but also the gradient and the Hessian to compute the local quadratic model:\n\n\nShow the code\nxₖ = [0.7,0.5]\n\nusing LinearAlgebra     # Because of the dot function\nmₖ(x) = f(xₖ) + dot(∇f(xₖ),(x-xₖ)) + 1/2*dot((x-xₖ),∇²f(xₖ),(x-xₖ))\n\n\nmₖ (generic function with 1 method)\n\n\nThe contours of the local quadratic model and the contours of the original function can be plotted together as in Fig. 4.\n\n\nShow the code\nmₖ_grid = [mₖ([x₁,x₂]) for x₁=x₁_grid, x₂=x₂_grid];\ncontour!(x₁_grid, x₂_grid, mₖ_grid',levels=10)\nscatter!([xₖ[1],],[xₖ[2],],label=\"xₖ\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Contours of a quadratic model over those for the original example function\n\n\n\n\nThe following step of the Newton’s method would constitute in finding the minimum of the quadratic function, that is, to localize the center of the contour ellipses. It can be guessed that such minimum will be located in the direction to the bottom left from the initial point, which is consistent with the contours of the original cost function.\n\n\nShow the code\nxₖ₊₁ = xₖ - ∇²f(xₖ)\\∇f(xₖ)\nscatter!([xₖ₊₁[1],],[xₖ₊₁[2],],label=\"xₖ₊₁\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now compute a whole sequence of such local minimizers of quadratic models. In other words, we demonstrate the functionality of a basic implementation of Newton’s method.\n\n\nShow the code\nusing Printf\n\nfunction newton_method(f,∇f,∇²f,x₀,ϵ,N)\n    x = x₀\n    k = 0\n    while (norm(∇f(x)) &gt; ϵ)\n        k = k+1\n        x = x - ∇²f(x)\\∇f(x)\n        @printf(\"iter = %3d     ||∇f|| = %17.6f     f = %19.6f \\n\",k,norm(∇f(x)),f(x))\n        if k &gt;= N\n         return f(x),x\n        end\n    end\n    return f(x),x\nend\n\n\nnewton_method (generic function with 1 method)\n\n\nAfter setting the termination parameters we are ready to start the iterative algorithm:\n\n\nShow the code\nϵ = 1e-8\nN = 100\nx₀ = xₖ\n\nf_opt,x_opt = newton_method(f,∇f,∇²f,x₀,ϵ,N)\n\n\niter =   1     ||∇f|| =          7.104527     f =            2.630340 \niter =   2     ||∇f|| =          1.872767     f =            0.650808 \niter =   3     ||∇f|| =          0.365832     f =            0.390200 \niter =   4     ||∇f|| =          0.034334     f =            0.375189 \niter =   5     ||∇f|| =          0.000780     f =            0.375000 \niter =   6     ||∇f|| =          0.000001     f =            0.375000 \niter =   7     ||∇f|| =          0.000000     f =            0.375000 \n\n\n(0.375, [-0.4999999999999264, -0.5000000000000205])\n\n\nFinally, plot the minimizer into the contour plots.\n\nscatter!([x_opt[1],],[x_opt[2],],label=\"x⋆\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient implementation by exploiting the symmetry and the positive definiteness of Hessian\nIn our straightforward implementation of the Newton’s method, we formulated a linear system of equations \\mathbf A\\bm x = \\mathbf b, and we used the backslash operator (introduced by K. Hensel in 1929, and adopted and popularized by Matlab, and then imported by other languages, including Juluia) to find the solution by writing x = A\\b. Although there is some decision tree behind the backslash operator (for example, depending on the shape of the matrix), more detailed analysis of the properties of the matrix is typically not performed automatically when calling x = A\\b. But the Hessian matrix \\nabla^2 f(\\bm x_k) (playing the role of the matrix \\mathbf A in the previous sentences) does possess important properties. First, it is symmetric. This can be exploited to solve the system of equations more efficiently. Second, well-behaved Hesians are positive definite in the vicinity of a local minimum. For this class of matrices, an efficient way to solve the corresponding system of equations is to use the Cholesky decomposition (factorization). (By the way, the pronounciation is ʃəˈlɛski.) A real positive-definite matrix \\mathbf A can be decomposed as\n\n\\mathbf A = \\bm L \\bm L^\\top,\n where \\bm L is a lower triangular matrix.\n\nExample 6 (Solving a system of linear equations with a symmetric positive definite matrix using Cholesky decomposition)  \n\n\nShow the code\nA = [10 1 0; 1 20 3; 0 3 30]\nb = [1, 2, 3]\n\nusing LinearAlgebra\nC = cholesky(A)\n\n\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 3.16228  0.316228  0.0\n  ⋅       4.46094   0.672504\n  ⋅        ⋅        5.43578\n\n\nC is a structure that contains the decomposition. We can access the lower triangular matrix \\bm L as\n\n\nShow the code\nL = C.L\n\n\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n 3.16228    ⋅         ⋅ \n 0.316228  4.46094    ⋅ \n 0.0       0.672504  5.43578\n\n\nHaving computed the lower triangular matrix \\bm L, we can write the original problem as \n\\bm L \\bm L^\\top \\bm x = \\mathbf b,\n which can be reformulated into solving two triangular systems by simple backsubstitution: \n\\begin{aligned}\n\\bm L \\bm y &= \\mathbf b,\\\\\n\\bm L^\\top \\bm x &= \\bm y.\n\\end{aligned}\n\n\n\nShow the code\ny = L\\b\n\n\n3-element Vector{Float64}:\n 0.31622776601683794\n 0.42591904767910926\n 0.49920457702436966\n\n\n\n\nShow the code\nx = L'\\y\n\n\n3-element Vector{Float64}:\n 0.09183673469387756\n 0.08163265306122447\n 0.09183673469387756\n\n\nBoth steps are realized by the following single line of code in Julia, but note that what is behind is really solving the two triangular systems by backsubstitution\n\n\nShow the code\nx = C\\b\n\n\n3-element Vector{Float64}:\n 0.09183673469387756\n 0.08163265306122447\n 0.09183673469387757\n\n\nYou can verify by yourself (using @btime macro from the BenchmarkTools package) that the decomposition followed by solving two triangular systems is faster than calling a general solver for linear systems of equations.\n\nBack to the Newton’s method. We can make a few observations:\n\nIf compared to the general prescription for descent direction methods (as described in Eq. 1), the Newton’s method determines the direction and the step lenght at once (both \\alpha_k and \\mathbf d_k are contained in the term - [\\nabla^2 f(\\mathbf x_k)]^{-1} \\nabla f(\\mathbf x_k)).\nIf compared with steepest descent (gradient) method, especially with its scaled version in Eq. 3, Newton’s method fits into the framework nicely because the inverse [\\nabla^2 f(\\mathbf x_k)]^{-1} of the Hessian can be regarded as a particular choice of a scaling matrix \\mathbf D. Indeed, you can find arguments in some textbooks that Newton’s method involves scaling that is optimal in some sense. We skip the details here because we only wanted to highlight the similarity in the structure of the two methods.\n\nThe great popularity of Newton’s method is mainly due to its nice convergence – quadratic. Although we skip any discussion of convergence rates here, note that for all other methods this is an ideal that is to be approached.\nThe plausible convergence rate of Newton’s method is paid for by a few disadvantages\n\nThe need to compute the Hessian. This is perhaps not quite obvious with simple problems but it can play some role with larger problems (recall our discussion of symbolic methods for finding derivatives).\nOnce the Hessian is computed, it must be inverted (actually, a linear system must by solved). Although we have already discussed the efficient method based on Cholesky decomposition, it is still quite some computational work.\nThe necessity to solve a linear system of equations requires that the Hessian be nonsingular. When can an optimization problem lose nonsingularity of its Hessian? And can anything be done, when it does so?\nAnd it is not only that the Hessian must be nonsingular, but it must also be positive (definite). Note that in the scalar case this corresponds to the situation when the second derivative is positive. Negativeness of the second derivative can send the algorithm in the opposite direction – away from the local minimum – , which would ruin the convergence of the algorithm completely.\n\nThe last two issues are handled by some modification of the standard Newton’s method.\n\n\nDamped Newton’s method\nA parameter \\alpha\\in(0,1) is introduced that shortens the step as in \n  \\bm x_{k+1} = \\bm x_{k} - \\alpha\\left[\\nabla^2 f(\\bm x_k)\\right]^{-1} \\nabla f(\\bm x_k).\n\n\n\nModifying the Hessian so that it is positive definite\n\n  \\bm x_{k+1} = \\bm x_{k} - \\left[\\nabla^2 f(\\bm x_k)+ \\lambda \\mathbf I\\right]^{-1} \\nabla f(\\bm x_k).\n\n\n\nFixed constant positive definite matrix instead of the inverse of the Hessian\nThe step is determined as \n  \\bm x_{k+1} = \\bm x_{k} - \\mathbf B \\nabla f(\\bm x_k).\n\nNote that the interpretation of the constant \\mathbf B in the position of the (inverse of the) Hessian in the rootfinding setting is that the slope of the approximating linear (affine) function is always constant.\nNow that we admitted to have something else then just the (inverse of the) Hessian in the formula for Newton’s method, we can explore further this new freedom. This will bring us into a family of methods called Quasi-Newton methods.\n\n\n\n\nQuasi-Newton’s methods\n#TODO In the meantime, have a look at [1, Sec. 4.4.4], or [2, Sec. 6.3], or [3, Ch. 13].\nSimilarly as we did when introducing the Newton’s method, we start our exposition with solving equations. Quasi-Newton methods (indeed, the plural is appropriate here because there is a whole family of methods under this name) generalize the key idea behind the (scalar) secant method for rootfinding. Let’s recall it here. The methods is based on secant approximation of the derivative: \n\\dot f(x_k) \\approx \\frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}.\n\nWe substitute this approximation into the Newton’s formula \nx_{k+1} = x_k - \\underbrace{\\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})}}_{\\approx \\dot f(x_k)}f(x_k).\n\nTransitioning from scalar rootfinding to optimization is as easy as increasing the order of the derivatives in the formula\n\n\\ddot f(x_k) \\approx \\frac{\\dot f(x_k)-\\dot f(x_{k-1})}{x_k-x_{k-1}} =: b_k,\n which can be rewritten into the secant condition \nb_k (\\underbrace{x_k-x_{k-1}}_{s_{k-1}}) = \\underbrace{\\dot f(x_k)-\\dot f(x_{k-1})}_{y_{k-1}}.\n\nThe vector version of the secant condition is\n\n\\begin{aligned}\\boxed{\n\\bm B_{k+1} \\mathbf s_k = \\mathbf y_k},\n\\end{aligned}\n where \\bm B_{k+1} is a matrix (to be determined) with Hessian-like properties\n\n\\bm B_{k+1} = \\bm B_{k+1}^\\top, \\qquad \\bm B_{k+1} \\succ \\mathbf 0.\n\nHow can we get it? Computing the matrix at every step anew is not computationally efficient. The preferred way is to compute the matrix \\bm B_{k+1} just by adding as small an update to the matrix \\bm B_{k+1} computed in the previous step as possible \n\\bm B_{k+1} = \\bm B_{k} + \\text{small update}.\n\nSeveral update schemes are documented in the literature. Particularly attractive are schemes that update not just \\bm B_{k+1} but \\bm B_{k+1}^{-1} directly. One popular update is BFGS:\n\\boxed{\n\\begin{aligned}\n\\bm H_{k+1} &= \\bm H_{k} + \\left(1+\\frac{\\mathbf y_k^\\top \\bm H_k \\mathbf y_k}{\\mathbf s_k^\\top\\mathbf y_k}\\right)\\cdot\\frac{\\mathbf s_k\\mathbf s_k^\\top}{\\mathbf s_k^\\top \\mathbf y_k} - \\frac{\\mathbf s_k \\mathbf y_k^\\top \\bm H_k + \\bm H_k\\mathbf y_k \\mathbf s_k^\\top}{\\mathbf y_k^\\top \\mathbf s_k}.\n\\end{aligned}}",
    "crumbs": [
      "2. Optimization – algorithms",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html#trust-region-methods",
    "href": "opt_algo_unconstrained.html#trust-region-methods",
    "title": "Algorithms for unconstrained optimization",
    "section": "Trust region methods",
    "text": "Trust region methods\n#TODO In the meantime, have a look at [1, Sec. 4.5], or [2, Sec. 4.4].\nThe key concept of trust region methods is that of… trust region. Trust region is a region (typically a ball or an ellipsoid) around the current point, in which we trust some approximation of the original cost function. We then find the minimum of this approximating function subject to the constraint on the norm of the step. Typically, the approximating function that is simple enough to minimize is a quadratic one: \nm_k(\\bm d) = f(\\bm x_k) + \\nabla f(\\bm x_k)^\\top \\bm d + \\frac{1}{2}\\bm d^\\top \\underbrace{\\nabla^2 f(\\bm x_k)}_{\\text{or} \\approx} \\bm d\n but trust the model only within \n\\|\\bm d\\|_2 \\leq \\delta_k.\n\nIn other words, we formulate the constrained optimization problem \\boxed{\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm d\\in\\mathbb R^n} &\\quad m_k(\\bm d)\\\\\n\\text{subject to} &\\quad \\|\\bm d\\|_2 \\leq \\delta_k.\n\\end{aligned}}\n\nFor later convenience (when differentiating the Lagrangian), we rewrite the constraint as \n\\frac{1}{2}\\left(\\|\\bm d\\|_2^2 - \\delta_k^2\\right) \\leq 0.\n\nLet’s write down the optimality conditions for this constrained problem. The Lagrangian is \nL(\\bm x_k, \\bm d) = f(\\bm x_k) + \\nabla f(\\bm x_k)^\\top \\bm d + \\frac{1}{2}\\bm d^\\top \\nabla^2 f(\\bm x_k)\\bm d + \\frac{\\mu}{2} (\\|\\bm d\\|^2-\\delta_k^2)\n\nThe necessary conditions (the KKT conditions) can be written upon inspection \n\\begin{aligned}\n\\nabla_{\\bm{d}}L(\\bm x_k, \\bm d) = \\nabla f(\\bm x_k) + \\nabla^2 f(\\bm x_k) \\bm d + \\mu \\bm d &= 0,\\\\\n\\|\\bm d\\|_2^2 - \\delta_k^2 &\\leq 0,\\\\\n\\mu &\\geq 0,\\\\\n\\mu \\left(\\|\\bm d\\|_2^2 - \\delta_k^2\\right) &= 0.\n\\end{aligned}\n\nNow, there are two scenarios: either the optimal step \\bm d keeps the updated \\bm x_{k+1} still strictly inside the trust region, or the updated \\bm x_{k+1} is at the boundary of the trust region. In the former case, since the constraint is satisfied strictly, the dual variable \\mu=0 and the optimality condition simplifies to \\nabla f(\\bm x_k) + \\nabla^2 f(\\bm x_k) \\bm d= 0, which leads to the standard Newton’s update \\bm d = -[\\nabla^2 f(\\bm x_k)]^{-1}\\nabla f(\\bm x_k). In the latter case the update is\n\n\\bm d = -[\\nabla^2 f(\\bm x_k) + \\mu \\mathbf I]^{-1}\\nabla f(\\bm x_k),\n which has the form that we have already discussed when mentioning modifications of Newton’s method.\nLet’s recall here our discussion of line search methods – we argued that there is rarely a need to compute the minimum at each step, and “good enough” reductions of the cost function typically suffice. The situation is similar for the trust region methods – approximate solution to the minimization (sub)problem is enough. However, here we are not going to discuss such methods here.\nOne issue that, however, requires discussion, is the issue of evaluationg the predictive performance of the (quadratic) model. If the model is not good enough, the trust region must be shrunk, if it is fairly good, the trust region must be expanded. In both cases, the constrained optimization (sub)problem must be solved again.\nOne metric we can use to evaluate the model is\n\n\\eta = \\frac{\\text{actual improvement}}{\\text{predicted improvement}} = \\frac{f(\\bm x_k)-f(\\bm x_{k+1})}{f(\\bm x_k)-m_k(\\bm x_{k+1})}.\n\nWe shrink the region for small \\eta (\\approx 0), and expand it for larger \\eta (\\approx 1).\nWe conclude this short discussion of trust region methods by comparing it with the descent methods. While in the descent methods we set the direction first, and the perform a line search in the chosen direction, in trust region methods this sequence is reversed. Kind of. By setting the radius of the trust region, we essentially set an upper bound on the step length. The subsequent optimization subproblem can be viewed as a search for a direction.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "ext_references.html",
    "href": "ext_references.html",
    "title": "References",
    "section": "",
    "text": "Accessible discussion of system and signal norms is in the slim (and freely downloadable) book [1].\n\n\n\n\n Back to topReferences\n\n[1] J. C. Doyle, B. A. Francis, and A. R. Tannenbaum, Feedback Control Theory, Reprint of the 1990 edition by Macmillan Publishing. Dover Publications, 2009. Available: https://www.control.utoronto.ca/people/profs/francis/dft.pdf",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "References"
    ]
  },
  {
    "objectID": "limitations_references.html",
    "href": "limitations_references.html",
    "title": "References",
    "section": "",
    "text": "The material here is mostly based on the chapters 5 (SISO systems) and 6 (MIMO systems) of [1].\nWhile the treatment of the book is more than sufficient for our course, we list here some other resources for further reading. The popular paper [2] provides a nice explanation of the waterbed effect(s).\nInterested readers may want to consult the specialized monograph [3], but it is certainly not necessary for our course.\nMore recent results are surveyed in [4].\n\n\n\n\n Back to topReferences\n\n[1] S. Skogestad and I. Postlethwaite, Multivariable Feedback Control: Analysis and Design, 2nd ed. Wiley, 2005. Available: https://folk.ntnu.no/skoge/book/\n\n\n[2] G. Stein, “Respect the unstable,” IEEE Control Systems, vol. 23, no. 4, pp. 12–25, Aug. 2003, doi: 10.1109/MCS.2003.1213600.\n\n\n[3] M. M. Seron, J. H. Braslavsky, and G. C. Goodwin, Fundamental Limitations in Filtering and Control. in Communications and Control Engineering. London: Springer, 1997. Available: https://doi.org/10.1007/978-1-4471-0965-5\n\n\n[4] J. Chen, S. Fang, and H. Ishii, “Fundamental limitations and intrinsic limits of feedback: An overview in an information age,” Annual Reviews in Control, vol. 47, pp. 155–177, Jan. 2019, doi: 10.1016/j.arcontrol.2019.03.011.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "References"
    ]
  },
  {
    "objectID": "discr_indir_hw.html",
    "href": "discr_indir_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Specifically, you should design a discrete-time state-feedback controller that performs fast “sideways” motion of a small indoor quadrotor (four-rotor drone). Namely, the controller should bring a quadrotor from one horizontal position to another. A 2D model is in the figure below\n\n\n\nQuadrotor model\n\n\nand the corresponding motion equations are\n\\begin{align*}\n\\ddot y(t) &= -a(t)\\sin \\theta(t)\\\\\n\\ddot z(t) & = a(t)\\cos \\theta(t) - g\\\\\n\\ddot \\theta(t) &= \\alpha(t)\n\\end{align*} where a(t) and \\alpha(t) represent the control inputs to the system, namely the linear and rotational acceleration. This assumes that the innermost control loops are already implemented and closed.\nThe gravitational acceleration g is approximated by 10\\,\\text{m}\\,\\text{s}^{-2}. The position variables y(t) and z(t) have units of m, \\theta is given in rad, and the inputs a(t) and \\alpha(t) are in \\text{m}\\,\\text{s}^{-2} and \\text{rad}\\,\\text{s}^{-2}, respectively. Only concentrating on the horizontal control, the input a(t) is set to\n a(t)=\\frac{10}{\\cos\\theta(t)} resulting in \\dot z(t)=0 and the simplified dynamics\n\\begin{align*}\n\\ddot y(t)&=-10 \\tan \\theta (t),\\\\\n\\ddot \\theta(t) &= \\alpha(t).\n\\end{align*}\nThe concrete control goal is to bring the quadrotor from the initial state y(0)=1,\\,\\dot y(0)=\\theta(0)=\\dot \\theta(0)=0 to the final state y(T) = \\dot y(T)=\\theta(T)=\\dot \\theta(T)=0.\nIn addition, there are constraints on the input \\alpha(t) and on the state variable \\theta(t):\n\\begin{align*}\n|\\alpha(t)| &\\leq 100,\\\\\n|\\theta(t)| &\\leq \\frac{\\pi}{6}.\n\\end{align*}\n\n\n\nLinearize the system around the equilibrium point y=0,\\,\\dot y=0,\\,\\theta=0,\\,\\dot \\theta=0 and discretize it with a sampling time T_s=0.01\\,\\text{s}.\nIn Julia, using the linearized model, design an LQ-optimal controller that gets the quadrotor from the initial to the vicinity of the final state (\\lvert x_i(t) \\rvert \\leq 0.01 for i=1,2,3,4) in the shortest possible time you can achieve while respecting the constraints on the input and the state variable.\nFor your solution to be accepted, you need to get to the vicinity of the final state within 3 seconds.\nWe advise you to use ControlSystemsBase package for the LQ-optimal controller synthesis.\nThe controller should be implemented as a function state_feedback(x, t) that takes the current state x and the current time step t as input and returns the control input \\alpha(t).\n\n\n\n\n\n\n\nRanking\n\n\n\nAll the solutions that meet the basic requirements will be ranked according to the time it takes for the quadrotor to reach the final state. The faster the solution, the higher the ranking.\nThe top three contenders will be awarded bonus 10% (grade increase) to the practical (open-book) part of the final exam.\n\n\nYour solution should be based on the following template and should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\n\nusing OrdinaryDiffEq, ControlSystemsBase, LinearAlgebra, Plots\n\nconst Ts = 1/100\n\nfunction quadrotor!(ẋ, x, u, t)\n\n    α = max(min(u, 100), -100)\n\n    ẋ[1] = x[2]\n    ẋ[2] = -10*tan(x[3])\n    ẋ[3] = x[4]\n    ẋ[4] = α\nend\n\n## TODO Linearize and discretize the system\n\n## TODO design the LQ-optimal controller\n\nfunction state_feedback(x, t)\n\n    ## TODO implement the state-feedback controller \n\n    return 0.0\nend\n\nYou may test your implementation using the following code snippet:\n\nT0 = 0;\nTf = 3;\nts = T0:Ts:Tf;\n\nN = length(ts)\nxs = zeros(4, N)\nus = zeros(1, N-1)\nxs[:, 1] = [1.0; 0.0; 0.0; 0.0]\n\nfor i = 1:N-1\n    us[:, i] .= state_feedback(xs[:, i], i)\n    prob = ODEProblem(quadrotor!, xs[:, i], [0, Ts], us[1, i])\n    sol = solve(prob, Tsit5())\n    xs[:, i+1] = sol.u[end]\nend\n\n\np1 = plot(ts, xs[1, :], label=\"y\")\nplot!(ts, xs[2, :], label=\"ẏ\")\nplot!(ts, xs[3, :], label=\"θ\")\nplot!(ts, xs[4, :], label=\"θ̇\")\n\nplot!([T0, Tf], [-pi/6, -pi/6], label=\"θ = -π/6\", linestyle=:dash)\nplot!([T0, Tf], [pi/6, pi/6], label=\"θ = π/6\", linestyle=:dash)\n\np2 = plot(ts[1:end-1], us[1, :], label=\"u\")\nplot(p1, p2, layout=(2, 1), size=(800, 600))",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Homework"
    ]
  },
  {
    "objectID": "discr_indir_hw.html#use-the-lq-optimal-control-methodology-to-design-a-discrete-time-state-feedback-regulator-for-a-given-lti-system",
    "href": "discr_indir_hw.html#use-the-lq-optimal-control-methodology-to-design-a-discrete-time-state-feedback-regulator-for-a-given-lti-system",
    "title": "Homework",
    "section": "",
    "text": "Specifically, you should design a discrete-time state-feedback controller that performs fast “sideways” motion of a small indoor quadrotor (four-rotor drone). Namely, the controller should bring a quadrotor from one horizontal position to another. A 2D model is in the figure below\n\n\n\nQuadrotor model\n\n\nand the corresponding motion equations are\n\\begin{align*}\n\\ddot y(t) &= -a(t)\\sin \\theta(t)\\\\\n\\ddot z(t) & = a(t)\\cos \\theta(t) - g\\\\\n\\ddot \\theta(t) &= \\alpha(t)\n\\end{align*} where a(t) and \\alpha(t) represent the control inputs to the system, namely the linear and rotational acceleration. This assumes that the innermost control loops are already implemented and closed.\nThe gravitational acceleration g is approximated by 10\\,\\text{m}\\,\\text{s}^{-2}. The position variables y(t) and z(t) have units of m, \\theta is given in rad, and the inputs a(t) and \\alpha(t) are in \\text{m}\\,\\text{s}^{-2} and \\text{rad}\\,\\text{s}^{-2}, respectively. Only concentrating on the horizontal control, the input a(t) is set to\n a(t)=\\frac{10}{\\cos\\theta(t)} resulting in \\dot z(t)=0 and the simplified dynamics\n\\begin{align*}\n\\ddot y(t)&=-10 \\tan \\theta (t),\\\\\n\\ddot \\theta(t) &= \\alpha(t).\n\\end{align*}\nThe concrete control goal is to bring the quadrotor from the initial state y(0)=1,\\,\\dot y(0)=\\theta(0)=\\dot \\theta(0)=0 to the final state y(T) = \\dot y(T)=\\theta(T)=\\dot \\theta(T)=0.\nIn addition, there are constraints on the input \\alpha(t) and on the state variable \\theta(t):\n\\begin{align*}\n|\\alpha(t)| &\\leq 100,\\\\\n|\\theta(t)| &\\leq \\frac{\\pi}{6}.\n\\end{align*}\n\n\n\nLinearize the system around the equilibrium point y=0,\\,\\dot y=0,\\,\\theta=0,\\,\\dot \\theta=0 and discretize it with a sampling time T_s=0.01\\,\\text{s}.\nIn Julia, using the linearized model, design an LQ-optimal controller that gets the quadrotor from the initial to the vicinity of the final state (\\lvert x_i(t) \\rvert \\leq 0.01 for i=1,2,3,4) in the shortest possible time you can achieve while respecting the constraints on the input and the state variable.\nFor your solution to be accepted, you need to get to the vicinity of the final state within 3 seconds.\nWe advise you to use ControlSystemsBase package for the LQ-optimal controller synthesis.\nThe controller should be implemented as a function state_feedback(x, t) that takes the current state x and the current time step t as input and returns the control input \\alpha(t).\n\n\n\n\n\n\n\nRanking\n\n\n\nAll the solutions that meet the basic requirements will be ranked according to the time it takes for the quadrotor to reach the final state. The faster the solution, the higher the ranking.\nThe top three contenders will be awarded bonus 10% (grade increase) to the practical (open-book) part of the final exam.\n\n\nYour solution should be based on the following template and should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\n\nusing OrdinaryDiffEq, ControlSystemsBase, LinearAlgebra, Plots\n\nconst Ts = 1/100\n\nfunction quadrotor!(ẋ, x, u, t)\n\n    α = max(min(u, 100), -100)\n\n    ẋ[1] = x[2]\n    ẋ[2] = -10*tan(x[3])\n    ẋ[3] = x[4]\n    ẋ[4] = α\nend\n\n## TODO Linearize and discretize the system\n\n## TODO design the LQ-optimal controller\n\nfunction state_feedback(x, t)\n\n    ## TODO implement the state-feedback controller \n\n    return 0.0\nend\n\nYou may test your implementation using the following code snippet:\n\nT0 = 0;\nTf = 3;\nts = T0:Ts:Tf;\n\nN = length(ts)\nxs = zeros(4, N)\nus = zeros(1, N-1)\nxs[:, 1] = [1.0; 0.0; 0.0; 0.0]\n\nfor i = 1:N-1\n    us[:, i] .= state_feedback(xs[:, i], i)\n    prob = ODEProblem(quadrotor!, xs[:, i], [0, Ts], us[1, i])\n    sol = solve(prob, Tsit5())\n    xs[:, i+1] = sol.u[end]\nend\n\n\np1 = plot(ts, xs[1, :], label=\"y\")\nplot!(ts, xs[2, :], label=\"ẏ\")\nplot!(ts, xs[3, :], label=\"θ\")\nplot!(ts, xs[4, :], label=\"θ̇\")\n\nplot!([T0, Tf], [-pi/6, -pi/6], label=\"θ = -π/6\", linestyle=:dash)\nplot!([T0, Tf], [pi/6, pi/6], label=\"θ = π/6\", linestyle=:dash)\n\np2 = plot(ts[1:end-1], us[1, :], label=\"u\")\nplot(p1, p2, layout=(2, 1), size=(800, 600))",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Homework"
    ]
  },
  {
    "objectID": "limitations_MIMO.html",
    "href": "limitations_MIMO.html",
    "title": "Limitations for MIMO systems",
    "section": "",
    "text": "Multiple-input-multiple-output (MIMO) systems are subject to limitations of the same origin as single-input-single-output (SISO) systems: unstable poles, “unstable” zeros, delays, disturbances, saturation, etc. However, the vector character of inputs and outputs introduces both opportunities to mitigate those limitations, and… new limitations.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#directions-in-mimo-systems",
    "href": "limitations_MIMO.html#directions-in-mimo-systems",
    "title": "Limitations for MIMO systems",
    "section": "Directions in MIMO systems",
    "text": "Directions in MIMO systems\nWith vector inputs and vector outputs, the input-output model of an LTI MIMO system is a matrix (of transfer functions). As such, it can be characterized not only by various scalar quantities (like poles, zeros, etc.), but also by the associated directions in the input and output spaces.\n\nExample 1 Consider the transfer function matrix (or matrix of transfer functions) \nG(s) = \\frac{1}{(0.2s+1)(s+1)}\\begin{bmatrix}1 & 1\\\\ 1+2s& 2\\end{bmatrix}.\n\nRecall that a complex number z\\in\\mathbb C is a zero of G if the rank of G(z) is less than the rank of G(s) for most s. While reliable numerical algorithms for computing zeros of MIMO systems work with state-space realizations, in this simple case we can easily verify that there is only one zero z=1/2.\nZeros in the RHP only exhibit themselves in some directions.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#conditioning-of-mimo-systems",
    "href": "limitations_MIMO.html#conditioning-of-mimo-systems",
    "title": "Limitations for MIMO systems",
    "section": "Conditioning of MIMO systems",
    "text": "Conditioning of MIMO systems\n\\boxed{\n\\gamma (G) = \\frac{\\bar{\\sigma}(G)}{\\underline{\\sigma}(G)}\n}\n\n\nIll-conditioned for \\gamma&gt;10\nBut depends on scaling!\n\nTherefore minimized conditioning number \\boxed{\n\\gamma^\\star(G) = \\min_{D_1, D_2}\\gamma(D_1GD_2)\n}\n but difficult do compute (=upper bound on \\mu)\nRGA can be used to give a reasonable estimate.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#relative-gain-array-rga",
    "href": "limitations_MIMO.html#relative-gain-array-rga",
    "title": "Limitations for MIMO systems",
    "section": "Relative gain array (RGA)",
    "text": "Relative gain array (RGA)\nRelative Gain Array (RGA) as an indicator of difficulties with control \\boxed{\\Lambda(G) = G \\circ (G^{-1})^T}\n\nindependent of scaling,\nsum of elements in rows and columns is 1,\nsum of absolute values of elements of RGA is very close to the minimized sensitivity number \\gamma^\\star, hence a system with large RGA entries is always ill-conditioned (but system with large \\gamma can have small RGA),\nRGA for a triangular system is an identity matrix,\nrelative uncertainty of an element of a transfer function matrix equal to (negative) inverse of the corresponding RGA entry makes the system singular.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#functional-controllability",
    "href": "limitations_MIMO.html#functional-controllability",
    "title": "Limitations for MIMO systems",
    "section": "Functional controllability",
    "text": "Functional controllability",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#interpolation-conditions-for-mimo-systems",
    "href": "limitations_MIMO.html#interpolation-conditions-for-mimo-systems",
    "title": "Limitations for MIMO systems",
    "section": "Interpolation conditions for MIMO systems",
    "text": "Interpolation conditions for MIMO systems",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#bandwidth-limitations-due-to-unstable-poles-and-zeros",
    "href": "limitations_MIMO.html#bandwidth-limitations-due-to-unstable-poles-and-zeros",
    "title": "Limitations for MIMO systems",
    "section": "Bandwidth limitations due to unstable poles and zeros",
    "text": "Bandwidth limitations due to unstable poles and zeros",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#limits-given-by-presence-of-disturbance-andor-reference",
    "href": "limitations_MIMO.html#limits-given-by-presence-of-disturbance-andor-reference",
    "title": "Limitations for MIMO systems",
    "section": "Limits given by presence of disturbance and/or reference",
    "text": "Limits given by presence of disturbance and/or reference",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#disturbance-rejection-by-a-plant-with-rhp-zero",
    "href": "limitations_MIMO.html#disturbance-rejection-by-a-plant-with-rhp-zero",
    "title": "Limitations for MIMO systems",
    "section": "Disturbance rejection by a plant with RHP zero",
    "text": "Disturbance rejection by a plant with RHP zero",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#limits-given-by-the-input-constraints-saturation",
    "href": "limitations_MIMO.html#limits-given-by-the-input-constraints-saturation",
    "title": "Limitations for MIMO systems",
    "section": "Limits given by the input constraints (saturation)",
    "text": "Limits given by the input constraints (saturation)",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#limits-given-by-uncertainty-in-the-model-in-open-loop",
    "href": "limitations_MIMO.html#limits-given-by-uncertainty-in-the-model-in-open-loop",
    "title": "Limitations for MIMO systems",
    "section": "Limits given by uncertainty in the model: in open loop",
    "text": "Limits given by uncertainty in the model: in open loop\n\nIn open loop\n\n\nIn closed loop",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "cont_dp_LQR.html",
    "href": "cont_dp_LQR.html",
    "title": "Using HJB equation to solve the continuous-time LQR problem",
    "section": "",
    "text": "As we have already discussed a couple of times, in the LQR problem we consider a linear time invariant (LTI) system modelled by \n\\dot{\\bm x}(t) = \\mathbf A\\bm x(t) + \\mathbf B\\bm u(t),\n and the quadratic cost function \nJ(\\bm x(t_\\mathrm{i}),\\bm u(\\cdot), t_\\mathrm{i}) = \\frac{1}{2}\\bm x^\\top(t_\\mathrm{f})\\mathbf S_\\mathrm{f}\\bm x(t_\\mathrm{f}) + \\frac{1}{2}\\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}\\left(\\bm x^\\top \\mathbf Q\\bm x + \\bm u^\\top \\mathbf R \\bm u\\right)\\mathrm{d}t.\n\nThe Hamiltonian is \nH(\\bm x,\\bm u,\\bm \\lambda) = \\frac{1}{2}\\left(\\bm x^\\top \\mathbf Q\\bm x + \\bm u^\\top \\mathbf R \\bm u\\right) + \\boldsymbol{\\lambda}^\\top \\left(\\mathbf A\\bm x + \\mathbf B\\bm u\\right).\n\nAccording to the HJB equation our goal is to minimize H at a given time t, which enforces the condition on its gradient \n\\mathbf 0 = \\nabla_{\\bm u} H = \\mathbf R\\bm u + \\mathbf B^\\top \\boldsymbol\\lambda,\n from which it follows that the optimal control must necessarily satisfy \n\\bm u^\\star = -\\mathbf R^{-1} \\mathbf B^\\top \\boldsymbol\\lambda.\n\nSince the Hessian of the Hamiltonian is positive definite by our assumption on positive definiteness of \\mathbf R \n\\nabla_{\\bm u \\bm u}^2 \\mathbf H = \\mathbf R &gt; 0,\n Hamiltonian is really minimized by the above choice of \\bm u^\\star.\nThe minimized Hamiltonian is \n\\min_{\\bm u(t)}H(\\bm x, \\bm u, \\bm \\lambda) = \\frac{1}{2}\\bm x^\\top \\mathbf Q \\bm x + \\boldsymbol\\lambda^\\top \\mathbf A \\bm x - \\frac{1}{2}\\boldsymbol\\lambda^\\top \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\boldsymbol\\lambda\n\nSetting \\boldsymbol\\lambda = \\nabla_{\\bm x} J^\\star, the HJB equation is \\boxed\n{-\\frac{\\partial J^\\star}{\\partial t} = \\frac{1}{2}\\bm x^\\top \\mathbf Q \\bm x + (\\nabla_{\\bm x} J^\\star)^\\top \\mathbf A\\bm x - \\frac{1}{2}(\\nabla_{\\bm x} J^\\star)^\\top \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\nabla_{\\bm x} J^\\star,}\n and the boundary condition is \nJ^\\star(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) = \\frac{1}{2}\\bm x^\\top (t_\\mathrm{f})\\mathbf S_\\mathrm{f}\\bm x(t_\\mathrm{f}).\n\nWe can now proceed by assuming that the optimal cost function is quadratic in \\bm x for all other times t, that is, there must exist a symmetric matrix function \\mathbf S(t) such that \nJ^\\star(\\bm x(t),t) = \\frac{1}{2}\\bm x^\\top (t)\\mathbf S(t)\\bm x(t).\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that we did something similar when making a sweep assumption to derive a Riccati equation following the indirect approach – we just make an inspired guess and see if it solves the equation. Here the inspiration comes from the observation made elsewhere, that the optimal cost function in the LQR problem is quadratic in \\bm x.\n\n\nWe now aim at substituting this into the HJB equation. Observe that \\frac{\\partial J^\\star}{\\partial t}=\\bm x^\\top(t) \\dot{\\mathbf{S}}(t) \\bm x(t) and \\nabla_{\\bm x} J^\\star = \\mathbf S \\bm x. Upon substitution to the HJB equation, we get\n\n-\\bm x^\\top \\dot{\\mathbf{S}} \\bm x = \\frac{1}{2}\\bm x^\\top \\mathbf Q \\bm x + \\bm x^\\top \\mathbf S \\mathbf A\\bm x - \\frac{1}{2}\\bm x^\\top \\mathbf S \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\mathbf S \\bm x.\n\nThis can be reformatted as \n-\\bm x^\\top \\dot{\\mathbf{S}} \\bm x = \\frac{1}{2} \\bm x^\\top \\left[\\mathbf Q + 2 \\mathbf S \\mathbf A - \\mathbf S \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\mathbf S \\right ] \\bm x.\n\nNotice that the middle matrix in the square brackets is not symmetric. Symmetrizing it (with no effect on the resulting value of the quadratic form) we get\n\n-\\bm x^\\top \\dot{\\mathbf{S}} \\bm x = \\frac{1}{2} \\bm x^\\top \\left[\\mathbf Q + \\mathbf S \\mathbf A + \\mathbf A^\\top \\mathbf S  - \\mathbf S \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\mathbf S \\right ] \\bm x.\n\nFinally, since the above single (scalar) equation should hold for all \\bm x(t), the matrix equation must hold too, and we get the familiar differential Riccati equation for the matrix variable \\mathbf S(t) \\boxed\n{-\\dot{\\mathbf S}(t) = \\mathbf A^\\top \\mathbf S(t) + \\mathbf S(t)\\mathbf A - \\mathbf S(t)\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\mathbf S(t) + \\mathbf Q}\n initialized at the final time t_\\mathrm{f} by \\mathbf S(t_\\mathrm{f}) = \\mathbf S_\\mathrm{f}.\nHaving obtained \\mathbf S(t), we can get the optimal control by substituting it into \\boxed\n{\n\\begin{aligned}\n    \\bm u^\\star(t) &= - \\mathbf R^{-1}\\mathbf B^\\top \\nabla_{\\bm x} J^\\star(\\bm x(t),t) \\\\\n                   &= - \\underbrace{\\mathbf R^{-1}\\mathbf B^\\top \\mathbf S(t)}_{\\bm K(t)}\\bm x(t).\n\\end{aligned}\n}\n\nWe have just rederived the continuous-time LQR problem using the HJB equation (previously we did it by massaging the two-point boundary value problem that followed as the necessary condition of optimality from the techniques of calculus of variations).\nNote that we have also just seen the equivalence between a first-order linear PDE and first-order nonlinear ODE.\n\n\n\n Back to top",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Using HJB equation to solve the continuous-time LQR problem"
    ]
  },
  {
    "objectID": "discr_indir_references.html",
    "href": "discr_indir_references.html",
    "title": "References",
    "section": "",
    "text": "While the indirect approaches to optimal control constitute the classical core of the optimal control theory, most treatments of the subject consider continuous-time systems. Our treatment was based on Chapter 2 in [1], which is one of a few resources that discuss discrete-time optimal control too.\n\n\n\n\n Back to topReferences\n\n[1] F. L. Lewis, D. Vrabie, and V. L. Syrmo, Optimal Control, 3rd ed. John Wiley & Sons, 2012. Accessed: Mar. 09, 2022. [Online]. Available: https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "References"
    ]
  },
  {
    "objectID": "intro_outline.html",
    "href": "intro_outline.html",
    "title": "Course outline",
    "section": "",
    "text": "The course is structured into 14 topics, each of them corresponding to one lecture. The topics are as follows:\n\nOptimization\n\nTheory: formulations, conditions, types of problems, optimization modellers, …\nAlgorithms: computing derivatives (symbolic, finite difference, autdiff), gradient, Newton, …, solvers\n\nDiscrete-time optimal control\n\nDirect approach (via optimization): on finite horizon, MPC\nIndirect approach (via Hamilton equations): finite and infinite horizon, LQR, Riccati equations, …\nDynamic programming: Bellman’s principle, …\nMore on MPC: combining direct and indirect approaches and dynamic programming\n\nContinuous-time optimal control\n\nIndirect approach (via calculus of variations): boundary value problem, Riccati equations, LQR\nIndirect approach (via Pontryagin’s principle of maximum): time-optimal constrained control\nNumerical methods for both direct and indirect approaches: shooting, multiple shooting, collocation\nSome extensions of LQ-optimal control: stochastic LQR, LQG, LTR, \\(\\mathcal{H}_2\\)\n\nRobust control\n\nModeling of uncertainty, robustness analysis: small gain theorem, structured singular values\nRobust control design: \\(\\mathcal{H}_\\infty\\)-optimal control, \\(\\mu\\)-synthesis\nAnalysis of achievable performance\n\nOther topics\n\nModel order reduction, controller order reduction\n\n\n\n\n\n Back to top",
    "crumbs": [
      "0. Introduction",
      "Course outline"
    ]
  },
  {
    "objectID": "discr_dir_mpc_hw.html",
    "href": "discr_dir_mpc_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "Homework"
    ]
  },
  {
    "objectID": "roban_goals.html",
    "href": "roban_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Explain and demonstrate how we can describe uncertainty in a linear model of a dynamical system in frequency domain (using a weighting filter W and the \\Delta term). Discuss the additive and multiplicative (input and output) models of uncertainty and their inverses.\nExplain and demonstrate the usage of Linear Fractional Transformation (LFT) as a general (unifying) framework for modeling uncertainty.\nDefine the \\mathcal H_\\infty norm of a system. Both SISO and MIMO. In the MIMO case this is related to SVD decomposition of a matrix, therefore be ready to explaining the decomposition itself (and, for example, to explain the difference between SVD and eigen decomposition).\nState the conditions of robust (closed-loop) stability in presence of additive or multiplicative uncertainty in the system. State them both in the form of a frequency-wise inequality but also in the form utilizing the \\mathcal H_\\infty norm of a closed-loop transfer function.\nState the conditions of robust (closed-loop) stability within the LFT framework (hint: small gain theorem).\nState the conditions of robust performance in presence of multiplicative uncertainty and discuss how it could be approximated using the bound on the \\mathcal H_\\infty norm of some related closed-loop transfer function.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Learning goals"
    ]
  },
  {
    "objectID": "roban_goals.html#knowledge-remember-and-understand",
    "href": "roban_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Explain and demonstrate how we can describe uncertainty in a linear model of a dynamical system in frequency domain (using a weighting filter W and the \\Delta term). Discuss the additive and multiplicative (input and output) models of uncertainty and their inverses.\nExplain and demonstrate the usage of Linear Fractional Transformation (LFT) as a general (unifying) framework for modeling uncertainty.\nDefine the \\mathcal H_\\infty norm of a system. Both SISO and MIMO. In the MIMO case this is related to SVD decomposition of a matrix, therefore be ready to explaining the decomposition itself (and, for example, to explain the difference between SVD and eigen decomposition).\nState the conditions of robust (closed-loop) stability in presence of additive or multiplicative uncertainty in the system. State them both in the form of a frequency-wise inequality but also in the form utilizing the \\mathcal H_\\infty norm of a closed-loop transfer function.\nState the conditions of robust (closed-loop) stability within the LFT framework (hint: small gain theorem).\nState the conditions of robust performance in presence of multiplicative uncertainty and discuss how it could be approximated using the bound on the \\mathcal H_\\infty norm of some related closed-loop transfer function.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Learning goals"
    ]
  },
  {
    "objectID": "roban_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "roban_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nCreate a model of an uncertain system in frequency domain, that is, you should provide a (model of a) nominal system and a weighting filter together with the specification which kind of uncertainty structure they are representing (additive, input/output multiplicative, inverse…). Design some decent controller using the techniques that you already master and check if the resulting feedback interconnection is robustly stable and if the performance is robust as well.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Learning goals"
    ]
  },
  {
    "objectID": "intro_similar.html",
    "href": "intro_similar.html",
    "title": "Similar courses at other universities",
    "section": "",
    "text": "Below we give a selection of courses on optimal and robust (oftentimes not together) control that are given by excellent instructors at research universities abroad:\n\nS. Boyd. EE363 - Linear Dynamical Systems. Stanford. Winter 2008-2009.\nR. D’Andrea. 151-0563-01L – Dynamic Programming and Optimal Control. ETH Zurich. Fall 2024.\nM. Diehl. Numerical Optimal Control. University of Freiburg, Winter semester 2024/2025.\nM. Egerstedt. ECE6553 - Optimal Control and Optimization. Georgia Tech. Spring 2017.\nT. Faulwasser, Y. Jiang. EE-736 – Control for Dynamic Systems. EPFL. 2024.\nA. Hanson. TSRT08 – Optimal control. Linkoping University. Fall 2024.\nM. Hovd. Robust control. Norwegian University of Science Technology, Trondheim, Norway.\nJ. How. 16323 – Principles of Optimal Control. MIT. Spring 2008.\nA. Karimi. Robust control. EPFL, Lausanne.\nS. Lall. Engr210a - Robust Control Analysis and Synthesis. Stanford. Autumn 2001-2002.\nZ. Manchester. 16-745 – Optimal Control and Reinforcement Learning. Carnegie Mellon University. 2025.\nA. Megretski. 6.245 - Multivariable Control Systems. MIT. Spring 2004.\nT. A. E. Oomen, and J.W. van Wingerden. Design Methods for Control Systems. Dutch Institute of Systems and Control (DISC), The Netherlands. 2025.\nM. Pavone, D. Gammelli, D. Morton, M. Foutter. AA 203 – Optimal and Learning-Based Control. Stanford University. Spring 2024.\nA. Rantzer. Robust Control. Lund University. 2015.\nM. Salapaka. EE5235/AEM8425 - Robust Control. University of Minnesota.\nC.W. Scherer. Robust control. TU Delft. Since 2006 not accessible outside TU Delft.\nS. Skogestad. Multivariable feedback control using frequency-domain methods. Norwegian University of Science Technology, Trondheim, Norway. Replaced by the course given by M. Hovd, last run 1999.\n\n\n\n\n Back to top",
    "crumbs": [
      "0. Introduction",
      "Similar courses"
    ]
  },
  {
    "objectID": "discr_dir_general.html",
    "href": "discr_dir_general.html",
    "title": "General finite-horizon nonlinear discrete-time optimal control as a nonlinear program",
    "section": "",
    "text": "In this section we formulate a finite-horizon optimal control problem (OCP) for a discrete-time dynamical system as a mathematical optimization problem (also mathematical program), which can then be solved numerically by a suitable numerical solver for nonlinear programming (NLP), or possibly quadratic programming (QP). The outcome of such numerical optimization is an optimal control trajectory (a sequence of controls), which is why this approach is called direct – we optimize directly over the trajectories.\nIn the following chapter we then present an alternative – indirect – approach, wherein the conditions of optimality are formulated first. These come in the form of a set of equations, some of them recurrent/recursive, some just algebraic. The indirect approach thus amounts to solving such equations.\nAnd then in another chapter we present the third approach – dynamic programming.\nThe three approaches form the backbone of the theory of optimal control for discrete-time systems, but later we are going to recognize the same triplet in the context of continuous-time systems.\n\n\n\n\n\n\n\n\nG\n\n\ndiscrete_time_optimal_control\n\nApproaches to discrete-time optimal control\n\n\n\ndirect_approach\n\nDirect approach\n\n\n\ndiscrete_time_optimal_control-&gt;direct_approach\n\n\n\n\n\nindirect_approach\n\nIndirect approach\n\n\n\ndiscrete_time_optimal_control-&gt;indirect_approach\n\n\n\n\n\ndynamic_programming\n\nDynamic programming\n\n\n\ndiscrete_time_optimal_control-&gt;dynamic_programming\n\n\n\n\n\n\n\n\nFigure 1: Three approaches to discrete-time optimal control\n\n\n\n\n\nBut now back to the direct approach. We will start with a general nonlinear discrete-time optimal control problem in this section, and then specialize to the linear quadratic regulation (LQR) problem in the next section. Finally, since the computed control trajectory constitutes an open-loop control scheme, something must be done about it if a feedback scheme is preferred – we introduce the concept of a receding horizon control (RHC), perhaps better known as model predictive control (MPC), which turns the direct approach into a feedback control scheme.\nWe start by considering a nonlinear discrete-time system modelled by the state equation \n\\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\n where\n\n\\bm x_k\\in \\mathbb R^n is the state at the discrete time k\\in \\mathbb Z,\n\\bm u_k\\in \\mathbb R^m is the control at the discrete time k,\n\\mathbf f_k: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb Z \\to \\mathbb{R}^n is a state transition function (in general not only nonlinear but also time-varying, with the convention that the dependence on k is expressed through the lower index).\n\nA general nonlinear discrete-time optimal control problem (OCP) is then formulated as \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm u_k \\in \\mathcal U_k,\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_k \\in \\mathcal X_k,\\quad k=i, \\ldots, N,\n\\end{aligned}\n where\n\ni is the initial discrete time,\nN is the final discrete time,\n\\phi() is a terminal cost function that penalizes the state at the final time,\nL_k() is a running (also stage) cost function,\nand \\mathcal U_k and \\mathcal X_k are sets of feasible controls and states – these sets are typically expressed using equations and inequalities. Should they be constant, the notation is just \\mathcal U and \\mathcal X.\n\nOftentimes it is convenient to handle the constraints of the initial and final states separately: \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm u_k \\in \\mathcal U_k,\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_k \\in \\mathcal X_k,\\quad k=i+1, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_i \\in \\mathcal X_\\mathrm{init},\\\\\n                    &\\quad \\bm x_N \\in \\mathcal X_\\mathrm{final}.\n\\end{aligned}\n\nIn particular, at the initial time just one particular state is often considered. At the final time, the state might be required to be equal to some given value, it might be required to be in some set defined through equations or inequalities, or it might be left unconstrained. Finally, the constraints on the control and states typically (but not always) come in the form of lower and upper bounds. The optimal control problem then specializes to \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_k \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\bm x_k \\leq \\mathbf x^{\\max},\\\\\n                    &\\quad\\bm x_i = \\mathbf x^\\text{init},\\\\\n                    &\\quad \\left(\\bm x_N = \\mathbf x^\\text{ref}, \\; \\text{or} \\; \\mathbf h^\\text{final}(\\bm x_N) =  \\mathbf 0, \\text{or} \\; \\mathbf g^\\text{final}(\\bm x_N) \\leq  \\mathbf 0\\right),                    \n\\end{aligned}\n where\n\nthe inequalities should be interpreted componentwise,\n\\mathbf u^{\\min} and \\mathbf u^{\\max} are lower and upper bounds on the control, respectively,\n\\mathbf x^{\\min} and \\mathbf x^{\\max} are lower and upper bounds on the state, respectively,\n\\mathbf x^\\text{init} is a fixed initial state,\n\\mathbf x^\\text{ref} is a required (reference) final state,\nand the functions \\mathbf g^\\text{final}() and \\mathbf h^\\text{final}() can be used to define the constraint set for the final state.\n\nThis optimal control problem is an instance of a general nonlinear programming (NLP) problem \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm x}\\in\\mathbb{R}^{n(N-i)},\\bar{\\bm u}\\in\\mathbb{R}^{m(N-i)}} &\\quad J(\\bar{\\bm x},\\bar{\\bm u})\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bar{\\bm x},\\bar{\\bm u}) =0,\\\\\n&\\quad \\mathbf g(\\bar{\\bm x},\\bar{\\bm u}) \\leq \\mathbf 0,\n\\end{aligned}\n where \\bar{\\bm u} and \\bar{\\bm x} are vectors obtained by stacking control and state vectors for individual times\n\n\\begin{aligned}\n\\bar{\\bm u} &= \\operatorname*{vec}(\\bm u_i,\\ldots, \\bm u_{N-1}) = \\begin{bmatrix}\\bm u_i\\\\ \\bm u_{i+1}\\\\ \\vdots \\\\ \\bm u_{N-1} \\end{bmatrix},\\\\\n\\bar{\\bm x} &= \\operatorname*{vec}(\\bm x_{i+1},\\ldots, \\bm x_N) = \\begin{bmatrix}\\bm x_{i+1}\\\\ \\bm x_{i+2}\\\\ \\vdots \\\\ \\bm x_{N} \\end{bmatrix}.\n\\end{aligned}\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlthought there may be applications where it is desirable to optimize over the initial state \\bm x_i as well, mostly the initial state \\bm x_i is fixed, and it does not have to be considered as an optimization variable. This can be even emphasized through the notation J(\\bar{\\bm x},\\bar{\\bm u}; \\bm x_i), where the semicolon separates the variables from (fixed) parameters.\nThe last control that affects the state trajectory on the interval [i,N] is \\bm u_{N-1}.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "General finite-horizon nonlinear optimal control as an NLP"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html",
    "href": "cont_numerical_direct.html",
    "title": "Numerical methods for direct approach",
    "section": "",
    "text": "We have already seen that direct methods for discrete-time optimal control problems essentially just regroup and reshape the problem data so that a nonlinear programming (NLP) solver can accept them. In order to follow the same approach with continuous-time optimal control problems, some kind of discretization is inevitable in order to formulate a nonlinear program. Depending of what is discretized and how, several groups of methods can be identified\nThe core principles behind the methods are identical to those discussed in the previous section on numerical methods for indirect approaches, but the adjective “direct” suggests that there must be some modifications.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-shooting-methods",
    "href": "cont_numerical_direct.html#direct-shooting-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct shooting methods",
    "text": "Direct shooting methods\nIn contrast with the application of shooting methods to the TP-BVP originating from the indirect approach, here the variables through whose values we aim the fictitious cannon are not the initial co-state vector but the (possibly long) vector parameterizing the whole control trajectory.\nIn the simplest – and fairly practical – case of a piecewise constant control trajectory, which is motivated by the eventual discrete-time implementation using a zero-order hold (ZOH), the control trajectory is parameterized just by the sequence of values u_0, u_1, \\ldots, u_{N-1}.\n\n\n\n\n\n\nFigure 1: Direct shooting – only the control trajectory is discretized and the parameters of this discretization serve as optimization variables\n\n\n\nThe state trajectory x(t) corresponding to the fixed initial state \\mathrm x_\\mathrm{i} and the sequence of controls is obtained by using some IVP ODE numerical solver.\nFor the chosen control trajectory and the simulated state trajectory, the cost function J(u(\\cdot);x_\\mathrm{i}) = \\phi(x(t_\\mathrm{f}),t_\\mathrm{f}) + \\int L(x,u) \\mathrm d t is then evaluated. This can also only be done numerically, typically using methods for numerical integration.\nOnce the cost function is evaluated, numerical optimization solver is used to update the vector parameterizing the control trajectory so that the cost is reduced. For the new control trajectory, the state response is simulated, … and so on.\nSince optimization is only done over the optimization variables that parameterize the control trajectory, the method resembles the sequential method of the direct approach to discrete-time optimal control.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-multiple-shooting-methods",
    "href": "cont_numerical_direct.html#direct-multiple-shooting-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct multiple shooting methods",
    "text": "Direct multiple shooting methods\n…",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-transcription-methods",
    "href": "cont_numerical_direct.html#direct-transcription-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct transcription methods",
    "text": "Direct transcription methods\nIn contrast with the direct shooting methods, here both the control and state trajectories are discretized. There is then no need for a numerical solver for IVP ODE.\n\n\n\n\n\n\nFigure 2: Direct transcription/discretization – both the control and state trajectories are discretized and the resulting vectors of parameterers of this discretization serve as optimization variables\n\n\n\nThe optimization is then done over the optimization variables that parameterize both the control trajectory and the state trajectory. In this regard the methods resemble the simultaneous method of the direct approach to discrete-time optimal control.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-collocation-methods",
    "href": "cont_numerical_direct.html#direct-collocation-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct collocation methods",
    "text": "Direct collocation methods\nSimilarly as in the direct transcription methods, both the control and state trajectories are discretized and the parameterization of this discretization enters the optimization.\nBut here it is not just the values of the state and the control variables at the discretization points that are the optimization variables, but rather the coefficients of the polynomials that approximate the corresponding variables at the intermediate times between the discretization points.\n\n\n\n\n\n\nFigure 3: Direct collocation – both the control and state trajectories are approximated by piecewise polynomials and parameters of these polynomials serve as optimization variables\n\n\n\nHowever, similarly as we have already seen while discussing the collocation methods for the indirect approach, every direct collocation methods is equivalent to some direct transcription method. For example, collocation with a quadratic polynomial and the collocation points at the beginning and end of the interval is equivalent to the implicit trapezoidal rule. Some people even say that the implicit trapezoidal method is a collocation method.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#minimumtime-optimal-control-in-direct-approach",
    "href": "cont_numerical_direct.html#minimumtime-optimal-control-in-direct-approach",
    "title": "Numerical methods for direct approach",
    "section": "Minimum‐time optimal control in direct approach",
    "text": "Minimum‐time optimal control in direct approach\nWhile the indirect approach to optimal control allows for relaxing the final time t_\\mathrm{f} (and possibly including it in the cost function), the direct approach does not seem to support it. The final time has to be finite and constant.\nHere we show one way to turn the final time is t_\\mathrm{f} into an optimization variable. We achieve it by augmenting the state equations with a new variable… guess what… t_\\mathrm{f} accompanied with the corresponding state equation \\dot t_\\mathrm{f} = 0.\nWe now introduce a new normalized time variable \\tau\\in [0,1] such that \n  t = \\tau t_\\mathrm{f}.\n\nThe differentials of time then relate correspondingly \n  \\mathrm{d} t = t_\\mathrm{f}\\mathrm{d}\\tau.\n\nThe original state equation \\frac{\\mathrm{d}x}{\\mathrm{d}t} = f(x) then modifies to \\boxed{\n  \\frac{\\mathrm{d}x}{\\mathrm{d}\\tau} = t_\\mathrm{f} f(x).}\n\nStrictly speaking this should be \\frac{\\mathrm{d}\\hat x}{\\mathrm{d}\\tau} = t_\\mathrm{f} f(\\hat x), reflecting that x(t) = x(t_\\mathrm{r} \\tau) = \\hat x(\\tau) . But the abuse of notation is perhaps acceptable.\nThe cost function will then include a term penalizing t_\\mathrm{f}. Or perhaps the whole cost function is just t_\\mathrm{f} .Or perhaps the final time does not even have to be penalized at all, it is just an optimization variable, it just adds a degree of freedom to the optimal control problem. Of course, in the latter case this is no longer a minimum-time problem, just a free-final time problem.\nIn direct transcription methods for optimal control, the number of discretization/integration (sub)intervals is fixed and not subject to optimization, otherwise the resulting NLP would have a varying size for a varying final time, which is not convenient. Instead, the length of the integration interval, or the discretization/sampling interval is varied hN = t_\\mathrm{f}.\nRecall that all (single-step) discretization methods can be interpreted as an approximate computation of the definite integral in x_{k+1} = x_k + \\int_{t_k}^{t_{k+1}} f(x(t),t)\\mathrm{d}t, which in s-stage Runge-Kutta methods with a fixed time step h always looks like x_{k+1} = x_k + h \\sum_{j=1}^s b_{j} f_{kj}.\nSince the length of the integration interval h is now an optimization variable, even a problem that is originally linear is now turned into a nonlinear one. Furthermore, we can see consistency between x_{k+1} = x_k + \\frac{t_\\mathrm{f}}{N} \\sum_{j=1}^s b_{j} f_{kj} and the above result for the continuous-time problem that does not consider discretization.\n\nExample 1 (Time-optimal river crossing) Sinusoidal bank shape, bell-shape river current, controlling only the orientation of the boat, but with some dynamics too.\n\n\nShow the code\nusing Plots\nusing JuMP\nusing Ipopt\nusing Interpolations\n\nfunction river_crossing_direct_transcription()\n    wmax = 1.0                                          # One half of the width of the river.\n    Rmax = 3.0                                          # Can set opposite direction of flow with MINUS sign.\n    R(w) = Rmax*exp(-(w/wmax)^2)                        # Velocity of the river as a function of the distance from the center.\n    u(x,y) = R(y-sin(x))/sqrt(1+cos(x)^2)               # x-component of the velocity of the flow.\n    v(x,y) = R(y-sin(x))*cos(x)/sqrt(1+cos(x)^2)        # y-component of the velocity of the flow.\n    Vmax = 4.0                                          # Maximum velocity of the boat with respect to the mass of water.\n    V̇max = 100.0\n    θ̇max = 5\n    (xinitial,yinitial) = (0.0,-wmax)                   # Initial position of the boat.\n    (xfinal,yfinal) = (2*pi,wmax)                       # Final required position of the boat.\n    tfinal_0 = 5.0                                      # Initial guess for the final time.\n    N = 50                                              # Number of time steps.\n    f₁(x,y,V,θ) = V*cos(θ) + u(x,y)                     # Dynamics of the boat.\n    f₂(x,y,V,θ) = V*sin(θ) + v(x,y)\n    model = Model(Ipopt.Optimizer)\n    set_silent(model)\n    @variable(model,tfinal&gt;=0,start=tfinal_0)           # Final time as an optimization variable.\n    h = tfinal/N                                        # Time step.\n    @variable(model, x[1:N+1])                          # N+1 values, because needed at the initial and final times.\n    @variable(model, y[1:N+1])                          # N+1 values, because needed at the initial and final conditions.\n    @variable(model, 0.0 &lt;= V[1:N] &lt;= Vmax)             # N values, because not needed at the final one.\n    @variable(model, θ[1:N])                            # N values, because not needed at the final one.\n    for i in 1:(N-1)\n        @constraint(model, x[i+1] == x[i] + h*(f₁(x[i],y[i],V[i],θ[i])+f₁(x[i+1],y[i+1],V[i+1],θ[i+1]))/2)\n        @constraint(model, y[i+1] == y[i] + h*(f₂(x[i],y[i],V[i],θ[i])+f₂(x[i+1],y[i+1],V[i+1],θ[i+1]))/2)\n    end\n    @constraint(model, x[N+1] == x[N] + h*(f₁(x[N],y[N],V[N],θ[N])+f₁(x[N+1],y[N+1],V[N],θ[N]))/2)  # Over the last integration interval.\n    @constraint(model, y[N+1] == y[N] + h*(f₂(x[N],y[N],V[N],θ[N])+f₂(x[N+1],y[N+1],V[N],θ[N]))/2)\n    @expression(model, southern_bank[i=1:N+1],sin(x[i])-wmax)\n    @expression(model, northern_bank[i=1:N+1],sin(x[i])+wmax)\n    @constraint(model, [i=1:N+1], southern_bank[i] &lt;= y[i])                 # Constraints on the y coordinate.\n    @constraint(model, [i=1:N+1], northern_bank[i] &gt;= y[i])                 # Constraints on the y coordinate. \n    @constraint(model, [i=1:N-1], -V̇max*h &lt;= V[i+1]-V[i])                   # Constraints on the rate of change of the forward velocity.\n    @constraint(model, [i=1:N-1], V̇max*h &gt;= V[i+1]-V[i])                    # Constraints on the rate of change of the forward velocity.\n    @constraint(model, [i=1:N-1], -θ̇max*h &lt;= θ[i+1]-θ[i])                   # Constraints on the rate of change of the angular velocity.\n    @constraint(model, [i=1:N-1], θ̇max*h &gt;= θ[i+1]-θ[i])                    # Constraints on the rate of change of the angular velocity.\n    interp_linear_x = linear_interpolation([1, N+1], [xinitial, xfinal])    # Linear growth.\n    interp_linear_y = linear_interpolation([1, N+1], [yinitial, yfinal])    # Linear growth.\n    interp_linear_θ = linear_interpolation([1, N÷2, N], [1.0, 0.0, 1.0])    # V profile.\n    initial_guess_x = mapreduce(transpose, vcat, interp_linear_x.(1:N+1))   # Converting a vector of vectors to a matrix.\n    initial_guess_y = mapreduce(transpose, vcat, interp_linear_y.(1:N+1))\n    initial_guess_V = Vmax*ones(N)                                          # Initial guess for the forward velocity: just max.\n    # initial_guess_θ = zero(initial_guess_V)\n    initial_guess_θ = mapreduce(transpose, vcat, interp_linear_θ.(1:N))\n    set_start_value.(x, initial_guess_x)\n    set_start_value.(y, initial_guess_y)\n    set_start_value.(V, initial_guess_V)\n    set_start_value.(θ, initial_guess_θ)\n    fix(x[1], xinitial)                                                     # Setting the initial positions.\n    fix(y[1], yinitial)\n    fix(x[N+1], xfinal)                                                     # Setting the final positions.\n    fix(y[N+1], yfinal) \n    @objective(model, Min, tfinal)                                          # Cośt function for the minimum-time control.\n    optimize!(model)\n    println(\"\"\"\n    termination_status = $(termination_status(model))\n    primal_status      = $(primal_status(model))\n    objective_value    = $(objective_value(model))\n    \"\"\")\n    gr(lw=2,label=\"\",ls=:solid,markershape=:circle)\n    t = value(h)*(0:N)\n    p1 = plot(t[1:end-1],value.(V),ylabel=\"V\",xlabel=\"t\",seriestype=:steppost,ylims=(-0.1,1.1*Vmax))\n    p2 = plot(t[1:end-1],value.(θ),ylabel=\"θ\",xlabel=\"t\",seriestype=:steppost)\n    pA = plot(p1,p2,layout=(2,1))\n    pB = plot(value.(x),value.(y),ylabel=\"y\",xlabel=\"x\")\n    xrange = 0:0.01:2*pi\n    ynorth = sin.(xrange) .- wmax\n    ysouth = sin.(xrange) .+ wmax\n    plot!(xrange,ynorth)\n    plot!(xrange,ysouth)\n    return pA,pB \nend\n\npA,pB = river_crossing_direct_transcription()\n\nplot(pB)\n\n\n\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit https://github.com/coin-or/Ipopt\n******************************************************************************\n\ntermination_status = LOCALLY_SOLVED\nprimal_status      = FEASIBLE_POINT\nobjective_value    = 1.2798568793864682",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_references.html",
    "href": "cont_numerical_references.html",
    "title": "References",
    "section": "",
    "text": "The indirect approach to the continuous-time optimal control problem (OCP), which formulates the necessary conditions of optimality as a two-point boundary value problem (TP-BVP), generally calls for numerical methods that iterate over trajectories. Besides some simple textbook problems allowing closed-form solutions, an regular exception is the LQR problem, for which a proportional feedback control is determined by solving (albeit also numerically) the continuous-time algebraic Riccati equation (CARE).\nThe direct approach to the continuous-time OCP proceeds by transcribing (discretizing) the continuous-time problem into a nonlinear programming problem (NLP), which is then solved numerically for the optimal (discretized) trajectories.\nNumerical methods for both direct and indirect approaches share a lot of common principles and tools. These are collectively presented in the literature as called numerical optimal control. A recommendable (and freely online available) introduction to these methods is [1]. Shorter version of this is in [2, Ch. 8], which is also freely available online. A more comprehensive treatment by another authors is in [3].\nSome survey papers such as [4] and [5] can also be useful, although now primarily as historical accounts. Similarly with the classics [6] and [7], which cover the indirect approach only.\nAnother name under which the numerical methods for the direct approach are presented is trajectory optimization. There are quite a few tutorials and surveys such as [8] and [9]. Recommendable is surely the dedicated chapter of the popular course [10, Ch. 10].\n\n\n\n\n Back to topReferences\n\n[1] S. Gros and M. Diehl, “Numerical Optimal Control (Draft).” Systems Control; Optimization Laboratory IMTEK, Faculty of Engineering, University of Freiburg, Apr. 2022. Available: https://www.syscop.de/files/2020ss/NOC/book-NOCSE.pdf\n\n\n[2] J. B. Rawlings, D. Q. Mayne, and M. M. Diehl, Model Predictive Control: Theory, Computation, and Design, 2nd ed. Madison, Wisconsin: Nob Hill Publishing, LLC, 2017. Available: http://www.nobhillpublishing.com/mpc-paperback/index-mpc.html\n\n\n[3] J. T. Betts, Practical Methods for Optimal Control Using Nonlinear Programming, 3rd ed. in Advances in Design and Control. Society for Industrial and Applied Mathematics, 2020. doi: 10.1137/1.9781611976199.\n\n\n[4] A. V. Rao, “A survey of numerical methods for optimal control,” Advances in the Astronautical Sciences, vol. 135, no. 1, pp. 497–528, 2009, Accessed: Jun. 09, 2016. [Online]. Available: http://vdol.mae.ufl.edu/ConferencePublications/trajectorySurveyAAS.pdf\n\n\n[5] O. von Stryk and R. Bulirsch, “Direct and indirect methods for trajectory optimization,” Annals of Operations Research, vol. 37, no. 1, pp. 357–373, Dec. 1992, doi: 10.1007/BF02071065.\n\n\n[6] D. E. Kirk, Optimal Control Theory: An Introduction, Reprint of the 1970 edition. Dover Publications, 2004.\n\n\n[7] A. E. Bryson Jr. and Y.-C. Ho, Applied Optimal Control: Optimization, Estimation and Control, Revised edition. CRC Press, 1975.\n\n\n[8] M. Kelly, “An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation,” SIAM Review, vol. 59, no. 4, pp. 849–904, Jan. 2017, doi: 10.1137/16M1062569.\n\n\n[9] M. P. Kelly, “Transcription Methods for Trajectory Optimization: A beginners tutorial,” arXiv:1707.00284 [math], Jul. 2017, Accessed: Apr. 06, 2021. [Online]. Available: http://arxiv.org/abs/1707.00284\n\n\n[10] R. Tedrake, “Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and Manipulation (Course Notes for MIT 6.832),” 2024. Accessed: Apr. 11, 2025. [Online]. Available: https://underactuated.mit.edu/",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "References"
    ]
  },
  {
    "objectID": "rocond_software.html",
    "href": "rocond_software.html",
    "title": "Software",
    "section": "",
    "text": "mixsyn: control design by minimnizing the \\mathcal{H}_\\infty norm the the mixed-sensitivity function.\nhinfsyn: control design by minimizing the \\mathcal{H}_\\infty norm of a closed-loop transfer function formulated using an LFT.\nncfsyn: another control design based on \\mathcal{H}_\\infty optimization, but this one considers a different uncertainty model not covered in our course. Although this uncertainty model does not have as intuitive an interpretation as the multiplicative uncertainty model use in mixed sensitivity synthesis, it captures a broad class of uncertainties. Furtheremore, the resulting controller enjoys the same decomposition into a state feedback and an observer as the popular LQG controller, which can be an advantage from an implementation viewpoint. Highly recommended method.\nmusyn: similar general setup as the hinfsyn method, but it considers a structure in the \\Delta term. It is regarded by some as the culmination of the \\mathcal{H}_\\infty control design methods. The disadvantage is that it is the most computationaly intensive of the methods we covered, and the resulting controller is typically of rather high order.",
    "crumbs": [
      "12. Robust control",
      "Software"
    ]
  },
  {
    "objectID": "rocond_software.html#matlab",
    "href": "rocond_software.html#matlab",
    "title": "Software",
    "section": "",
    "text": "mixsyn: control design by minimnizing the \\mathcal{H}_\\infty norm the the mixed-sensitivity function.\nhinfsyn: control design by minimizing the \\mathcal{H}_\\infty norm of a closed-loop transfer function formulated using an LFT.\nncfsyn: another control design based on \\mathcal{H}_\\infty optimization, but this one considers a different uncertainty model not covered in our course. Although this uncertainty model does not have as intuitive an interpretation as the multiplicative uncertainty model use in mixed sensitivity synthesis, it captures a broad class of uncertainties. Furtheremore, the resulting controller enjoys the same decomposition into a state feedback and an observer as the popular LQG controller, which can be an advantage from an implementation viewpoint. Highly recommended method.\nmusyn: similar general setup as the hinfsyn method, but it considers a structure in the \\Delta term. It is regarded by some as the culmination of the \\mathcal{H}_\\infty control design methods. The disadvantage is that it is the most computationaly intensive of the methods we covered, and the resulting controller is typically of rather high order.",
    "crumbs": [
      "12. Robust control",
      "Software"
    ]
  },
  {
    "objectID": "rocond_software.html#julia",
    "href": "rocond_software.html#julia",
    "title": "Software",
    "section": "Julia",
    "text": "Julia\n\nRobustAndOptimalControl.jl",
    "crumbs": [
      "12. Robust control",
      "Software"
    ]
  },
  {
    "objectID": "rocond_software.html#python",
    "href": "rocond_software.html#python",
    "title": "Software",
    "section": "Python",
    "text": "Python\n\nPython Control Systems Library",
    "crumbs": [
      "12. Robust control",
      "Software"
    ]
  },
  {
    "objectID": "cont_indir_notation.html",
    "href": "cont_indir_notation.html",
    "title": "On notation for Hamiltonians and variations",
    "section": "",
    "text": "Note that there was some ambiguity in our derivation when we were forming the augmented Lagrangian. Somehow arbitrarily we decided to define the augmented Lagrangian as \nL^\\text{aug}(\\bm x,\\bm u,\\boldsymbol \\lambda, t) =  L(\\bm x,\\bm u,t)+\\boldsymbol \\lambda^\\top (t)\\left[ \\dot {\\bm x}(t) - \\mathbf f(\\bm x,\\bm u,\\mathbf t)\\right]\n but we could easily formulate it also as \n\\hat L^\\text{aug}(\\bm x,\\bm u,\\boldsymbol \\lambda, t) =  L(\\bm x,\\bm u,t)+\\hat{\\boldsymbol \\lambda}^\\top (t)\\left[\\mathbf f(\\bm x,\\bm u,\\mathbf t)-\\dot {\\bm x}(t)\\right].\n\nBoth are clearly correct. Indeed, although the intermediate steps differ, the final results (Riccati equation, state feedback gain) are identical.\nBut was our choice really completely arbitrary? Well, not really. The former (the one used in this lecture) enables us to write the augmented Lagrangian using a Hamiltonian as \nL^\\text{aug}(\\bm x,\\bm u,\\boldsymbol \\lambda, t) =  \\boldsymbol \\lambda^\\top (t) \\dot {\\bm x}(t) - H(t,\\bm x,\\bm u,\\boldsymbol\\lambda)\n where the Hamiltonian is defined as \\boxed{\nH(\\bm x,\\bm u,\\boldsymbol \\lambda, t) = \\boldsymbol \\lambda^\\top (t) \\mathbf f(\\bm x,\\bm u,t) - L(\\bm x,\\bm u, t).}\n\nRecall that the concept of Hamiltonian was introduced in the context of the (physics-motivated) calculus of variations: \nH(x,y,y',p) = py'-L.\n\nReplacing the momentum p with the costate \\boldsymbol \\lambda, and replacing y' with the function \\mathbf f definining the state equation, we obtain the Hamiltonian in the context of optimal control. Indeed, this was admittedly our motivation: we wanted to adhere as much as possible to what had already been done well before the advent of optimal control theory.\nFor completeness we also show that the latter choice of the constraint function leads to expressing the augmented Lagrangian as \n\\hat L^\\text{aug}(\\bm x,\\bm u,\\boldsymbol \\lambda, t) =  \\hat H(\\bm x,\\bm u,\\hat{\\boldsymbol \\lambda}, t) - \\hat{\\boldsymbol \\lambda}^\\top (t) \\dot {\\bm x}(t),\n where \\boxed{\n\\hat H(\\bm x,\\bm u,\\hat{\\boldsymbol \\lambda},t) = L(\\bm x,\\bm u, t)+\\hat{\\boldsymbol \\lambda}^\\top (t) \\mathbf f(\\bm x,\\bm u,t).}\n\nDoes this definition of the Hamiltonian look familiar? Yep, this is the definition that we usee (again, somewhat arbitrarily) in our lecture on discrete-time systems.\nWhether one or the other, the canonical equations are identical. It is only that the second-order sufficiency conditions show maximization of the Hamiltonian in one case and minimization in the other. This can be concluded by observing that \nH(\\bm x,\\bm u,\\boldsymbol \\lambda, t) = -\\hat H(\\bm x,\\bm u,\\hat{\\boldsymbol \\lambda}, t).\n\nAlso \n\\hat{\\boldsymbol \\lambda} = -\\boldsymbol \\lambda.\n\nWhile I confess I am hesitating whether or not this complication due to two different definitions of the Hamiltonian is worth mentioning in an introductory text, the reality is that both conventions can be encountered in the literature and one should be aware of it. Most standard books rarely warn the reader about it. One of a few discussions of this frequent source of notational confusion is in [1, Sec. 3.4.4].",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "On notation for Hamiltonians and variations"
    ]
  },
  {
    "objectID": "cont_indir_notation.html#hamiltonian",
    "href": "cont_indir_notation.html#hamiltonian",
    "title": "On notation for Hamiltonians and variations",
    "section": "",
    "text": "Note that there was some ambiguity in our derivation when we were forming the augmented Lagrangian. Somehow arbitrarily we decided to define the augmented Lagrangian as \nL^\\text{aug}(\\bm x,\\bm u,\\boldsymbol \\lambda, t) =  L(\\bm x,\\bm u,t)+\\boldsymbol \\lambda^\\top (t)\\left[ \\dot {\\bm x}(t) - \\mathbf f(\\bm x,\\bm u,\\mathbf t)\\right]\n but we could easily formulate it also as \n\\hat L^\\text{aug}(\\bm x,\\bm u,\\boldsymbol \\lambda, t) =  L(\\bm x,\\bm u,t)+\\hat{\\boldsymbol \\lambda}^\\top (t)\\left[\\mathbf f(\\bm x,\\bm u,\\mathbf t)-\\dot {\\bm x}(t)\\right].\n\nBoth are clearly correct. Indeed, although the intermediate steps differ, the final results (Riccati equation, state feedback gain) are identical.\nBut was our choice really completely arbitrary? Well, not really. The former (the one used in this lecture) enables us to write the augmented Lagrangian using a Hamiltonian as \nL^\\text{aug}(\\bm x,\\bm u,\\boldsymbol \\lambda, t) =  \\boldsymbol \\lambda^\\top (t) \\dot {\\bm x}(t) - H(t,\\bm x,\\bm u,\\boldsymbol\\lambda)\n where the Hamiltonian is defined as \\boxed{\nH(\\bm x,\\bm u,\\boldsymbol \\lambda, t) = \\boldsymbol \\lambda^\\top (t) \\mathbf f(\\bm x,\\bm u,t) - L(\\bm x,\\bm u, t).}\n\nRecall that the concept of Hamiltonian was introduced in the context of the (physics-motivated) calculus of variations: \nH(x,y,y',p) = py'-L.\n\nReplacing the momentum p with the costate \\boldsymbol \\lambda, and replacing y' with the function \\mathbf f definining the state equation, we obtain the Hamiltonian in the context of optimal control. Indeed, this was admittedly our motivation: we wanted to adhere as much as possible to what had already been done well before the advent of optimal control theory.\nFor completeness we also show that the latter choice of the constraint function leads to expressing the augmented Lagrangian as \n\\hat L^\\text{aug}(\\bm x,\\bm u,\\boldsymbol \\lambda, t) =  \\hat H(\\bm x,\\bm u,\\hat{\\boldsymbol \\lambda}, t) - \\hat{\\boldsymbol \\lambda}^\\top (t) \\dot {\\bm x}(t),\n where \\boxed{\n\\hat H(\\bm x,\\bm u,\\hat{\\boldsymbol \\lambda},t) = L(\\bm x,\\bm u, t)+\\hat{\\boldsymbol \\lambda}^\\top (t) \\mathbf f(\\bm x,\\bm u,t).}\n\nDoes this definition of the Hamiltonian look familiar? Yep, this is the definition that we usee (again, somewhat arbitrarily) in our lecture on discrete-time systems.\nWhether one or the other, the canonical equations are identical. It is only that the second-order sufficiency conditions show maximization of the Hamiltonian in one case and minimization in the other. This can be concluded by observing that \nH(\\bm x,\\bm u,\\boldsymbol \\lambda, t) = -\\hat H(\\bm x,\\bm u,\\hat{\\boldsymbol \\lambda}, t).\n\nAlso \n\\hat{\\boldsymbol \\lambda} = -\\boldsymbol \\lambda.\n\nWhile I confess I am hesitating whether or not this complication due to two different definitions of the Hamiltonian is worth mentioning in an introductory text, the reality is that both conventions can be encountered in the literature and one should be aware of it. Most standard books rarely warn the reader about it. One of a few discussions of this frequent source of notational confusion is in [1, Sec. 3.4.4].",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "On notation for Hamiltonians and variations"
    ]
  },
  {
    "objectID": "cont_indir_notation.html#variation",
    "href": "cont_indir_notation.html#variation",
    "title": "On notation for Hamiltonians and variations",
    "section": "Variation",
    "text": "Variation\nUpon consulting numerous textbooks and monographs, it appears that the authors are far from the accord regarding the definition of variation (within the context of calculus of variations). Two main definitions appear\n\nThe one that we followed in this lecture defines the variation as an extension of the concept of a differential. That is, a variation \\delta J of a (cost) functional is the first-order approximation to the increment \\Delta J in the (cost) functional J. This we discussed in quite some detail in the text.\nThe other one defines variation as the derivative of the (cost) functional with respect to the real (perturbation) parameter. In our text, it is the \\frac{\\mathrm{d}J}{\\mathrm{d}\\alpha} (for fixed y(x) and \\eta(x)) that would be called a variation and labelled \\delta J. The increment in the (cost) functional would be then be approximated by \\delta J\\;\\alpha.\n\nBoth definitions are often encountered in the literature, but we prefer the former because the definitions of variation of a functional \\delta J and variation of a function \\delta y(x) are consistent. Both serve as first-order approximations to increments.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "On notation for Hamiltonians and variations"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html",
    "href": "opt_theory_unconstrained.html",
    "title": "Theory for unconstrained optimization",
    "section": "",
    "text": "We are going to analyze the optimization problem with no constraints \n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad  f(\\bm x).\nWhy are we considering such an unrealistic problem? After all, every engineering problem is subject to some constraints.\nBesides the standard teacher’s answer that we should always start with easier problems, there is another answer: it is common for analysis and algorithms for constrained optimization problems to reformulate them as unconstrained ones and then apply tools for unconstrained problems.",
    "crumbs": [
      "1. Optimization – theory",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#local-vs-global-optimality",
    "href": "opt_theory_unconstrained.html#local-vs-global-optimality",
    "title": "Theory for unconstrained optimization",
    "section": "Local vs global optimality",
    "text": "Local vs global optimality\nFirst, let’s define carefully what we mean by a minimum in the unconstrained problem.\n\n\n\n\n\n\nCaution\n\n\n\nFor those whose mother tongue does not use articles such as the and a/an in English, it is worth emphasizing that there is a difference between “the minimum” and “a minimum”. In the former we assume that there is just one minimum, in the latter we make no such assumption.\n\n\nConsider a (scalar) function of a scalar variable for simplicity. Something like the function whose graph is shown in Fig. 1 below.\n\n\n\n\n\n\nFigure 1: An example of a scalar function of a scalar real argument that exhibits several local minima and maxima\n\n\n\nWe say, that the function has a local minimum at x^\\star if f(x)\\geq f(x^\\star) in an \\varepsilon neighbourhood. All the red dots in the above figure are local minima. Similarly, of course, the function has a local maximum at x^\\star if f(x)\\leq f(x^\\star) in an \\varepsilon neighbourhood. Such local maxima are the green dots in the figure. The smallest and the largest of these are global minima and maxima, respectively.",
    "crumbs": [
      "1. Optimization – theory",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#conditions-of-optimality",
    "href": "opt_theory_unconstrained.html#conditions-of-optimality",
    "title": "Theory for unconstrained optimization",
    "section": "Conditions of optimality",
    "text": "Conditions of optimality\nHere we consider two types of conditions of optimality for unconstrained minimization problems: necessary and sufficient conditions. Necessary conditions must be satisfied at the minimum, but even when they are, the optimality is not guaranteed. On the other hand, the sufficient conditions need not be satisfied at the minimum, but if they are, the optimality is guaranteed. We show the necessary conditions of the first and second order, while the sufficient condition only of the second order.\n\nScalar optimization variable\nYou may want to have a look at the video, but below we continue with the text that covers (and polishes and sometimes even extends a bit) the content of the video.\n\nWe recall here the fundamental assumption made at the beginning of our introduction to optimization – we only consider optimization variables that are real-valued (first, just scalar x \\in \\mathbb R, later vectors \\bm x \\in \\mathbb R^n), and objective functions f() that are sufficiently smooth – all the derivatives exist. Then the conditions of optimality can be derived upon inspecting the Taylor series approximation of the cost function around the minimum.\n\nTaylor series approximation around the optimum\nDenote x^\\star as the (local) minimum of the function f(x). The Taylor series expansion of f(x) around x^\\star is\n\nf(x^\\star+\\alpha) = f(x^\\star)+\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star}\\alpha + \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2 + {\\color{blue}\\mathcal{O}(\\alpha^3)},\n where \\mathcal{O}() is called Big O and has the property that \n\\lim_{\\alpha\\rightarrow 0}\\frac{\\mathcal{O}(\\alpha^3)}{\\alpha^3} \\leq M&lt;\\infty.\n\nAlternatively, we can write the Taylor series expansion as \nf(x^\\star+\\alpha) = f(x^\\star)+\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star}\\alpha + \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2 + {\\color{red}o(\\alpha^2)},\n using the little o with the property that \n\\lim_{\\alpha\\rightarrow 0}\\frac{o(\\alpha^2)}{\\alpha^2} = 0.\n\nWhether \\mathcal{O}() or \\mathcal{o}() concepts are used, it is just a matter of personal preference. They both express that the higher-order terms in the expansion tend to be negligible compare to the first- and second-order term as \\alpha is getting smaller. \\mathcal O(\\alpha^3) goes to zero at least as fast as a cubic function, while o(\\alpha^2) goes to zero faster than a quadratic function.\nIt is indeed important to understand that this negligibility of the higher-order terms is only valid asymptotically – for a particular \\alpha it may easily happend that, say, the third-order term is still dominating.\n\n\nFirst-order necessary conditions of optimality\nFor \\alpha sufficiently small, the first-order Taylor series expansion is a good approximation of the function f(x) around the minimum. Since \\alpha enters this expansion linearly, the cost function can increase or decrease with \\alpha, depending on the sign of the first derivative. The only way to ensure that the function as a (local) minimu at x^\\star is to have the first derivative equal to zero, that is \\boxed{\n\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star} = 0.}\n\n\n\nSecond-order necessary conditions of optimality\nOnce the first-order necessary condition of optimality is satisfied, the dominating term (as \\alpha is getting smalle) is the second-order term \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2. Since \\alpha is squared, it is the sign of the second derivative that determines the contribution of the whole second-order term to the cost function value. For the minimum, the second derivative must be nonnegative, that is\n\n\\boxed{\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star} \\geq 0.}\n\nFor completeness we state that the sign must be nonpositive for the maximum.\n\n\nSecond-order sufficient condition of optimality\nFollowing the same line of reasoning as above, the if the second derivative is positive, the miniumum is guaranteed, that is, the sufficient condition of optimality is \n\\boxed{\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star} &gt; 0.}\n\nIf the second derivative fails to be positive and is just zero (thus still satisfying the necessary condition), does it mean that the point is not a minimum? No. We must examine higher order terms.\n\n\n\nVector optimization variable\nOnce again, should you prefer watching a video, here it is, but below we continue with the text that covers the content of the video.\n\n\nFirst-order necessary conditions of optimality\nOne way to handle the vector variables is to convert the vector problem into a scalar one by fixing a direction to an arbitrary vector \\bm d and then considering the scalar function of the form f(\\bm x^\\star + \\alpha \\bm d). For convenience we define a new function \ng(\\alpha) \\coloneqq f(\\bm x^\\star + \\alpha \\bm d)\n and from now on we can invoke the results for scalar functions. Namely, we expand the g() function around zero as \ng(\\alpha) = g(0) + \\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0}\\alpha + \\frac{1}{2}\\frac{\\mathrm{d}^2 g(\\alpha)}{\\mathrm{d}\\alpha^2}\\bigg|_{\\alpha=0}\\alpha^2 + \\mathcal{O}(\\alpha^3),\n and argue that the first-order necessary condition of optimality is \n\\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0} = 0.\n\nNow, invoking the chain rule, we go back from g() to f() \n\\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0} = \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} \\frac{\\partial(\\bm x^\\star + \\alpha \\bm d)}{\\partial\\alpha}\\bigg|_{\\alpha=0} = \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star}\\,\\bm d = 0,\n where \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} is a row vector of partial derivatives of f() evaluated at \\bm x^\\star. Since the vector \\bm d is arbitrary, the necessary condition is that\n\n\\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} = \\mathbf 0,  \n\nMore often than not we use the column vector to store partial derivatives. We call it the gradient of the function f() and denoted it as \n\\nabla f(\\bm x) \\coloneqq \\begin{bmatrix}\\frac{\\partial f(\\bm x)}{\\partial x_1} \\\\ \\frac{\\partial f(\\bm x)}{\\partial x_n} \\\\ \\vdots \\\\ \\frac{\\partial f(\\bm x)}{\\partial x_n}\\end{bmatrix}.  \n\nThe first-order necessary condition of optimality using gradients is then \n\\boxed{\\left.\\nabla f(\\bm x)\\right|_{x=x^\\star} = \\mathbf 0.  }\n\n\n\n\n\n\n\nGradient is a column vector\n\n\n\nIn some literature the gradient \\nabla f(\\bm x) is defined as a row vector. For the condition of optimality it does not matteer since all we require is that all partial derivatives vanish. But for other purposes in our text we regard the gradient as a vector living in the same vector space \\mathbb R^n as the optimization variable. The row vector is sometimes denoted as \\mathrm Df(\\bm x).\n\n\n\nComputing the gradient of a scalar function of a vector variable\nA convenient way is to compute the differential fist and then to identify the derivative in it. Recall that the differential is the first-order approximation to the increment of the function due to a change in the variable\n\n\\Delta f \\approx \\mathrm{d}f = \\nabla f(x)^\\top \\mathrm d \\bm x.\n\nFinding the differential of a function is conceptually easier than finding the derivative since it is a scalar quantity. When searching for the differential of a composed function, we follow the same rules as for the derivative (such as that the one for finding the differential of a product). Let’s illustrate it using an example.\n\nExample 1 For the function \nf(\\mathbf x) = \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x},\n where \\mathbf Q is symetric, the differential is \n\\mathrm{d}f = \\frac{1}{2}\\mathrm d\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\mathrm d\\bm{x} + \\mathbf{r}^\\top\\mathrm{d}\\bm{x},\n in which the first two terms can be combined thanks to the fact that they are scalars \n\\mathrm{d}f = \\left(\\bm{x}^\\top\\frac{\\mathbf{Q} + \\mathbf{Q}^\\top}{2} + \\mathbf{r}^\\top\\right)\\mathrm{d}\\bm{x},\n and finally, since we assumed that \\mathbf Q is a symmetric matrix, we get \n\\mathrm{d}f = \\left(\\mathbf{Q}\\bm{x} + \\mathbf{r}\\right)^\\top\\mathrm{d}\\bm{x},\n from which we can identify the gradient as \n\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\mathbf{x} + \\mathbf{r}.\n\nThe first-order condition of optimality is then \n\\boxed{\\mathbf{Q}\\mathbf{x} = -\\mathbf{r}.}\n\nAlthough this was just an example, it is actually a very useful one. Keep this result in mind – necessary condition of optimality of a quadratic function comes in the form of a set of linear equations.\n\n\n\n\nSecond-order necessary conditions of optimality\nAs before, we fix the direction \\bm d and consider the function g(\\alpha) = f(\\bm x^\\star + \\alpha \\bm d). We expand the expression for the first derivative as \n\\frac{\\mathrm d g(\\alpha)}{\\mathrm d \\alpha} = \\sum_{i=1}^{n}\\frac{\\partial f(\\bm x)}{\\partial x_i}\\bigg|_{\\bm x = \\bm x^\\star} d_i,\n and differentiating this once again, we get the second derivative \n\\frac{\\mathrm d^2 g(\\alpha)}{\\mathrm d \\alpha^2} = \\sum_{i,j=1}^{n}\\frac{\\partial^2 f(\\bm x)}{\\partial x_ix_j}\\bigg|_{\\bm x = \\bm x^\\star}d_id_j\n\n\n\\begin{aligned}\n\\frac{\\mathrm d^2 g(\\alpha)}{\\mathrm d \\alpha^2}\\bigg|_{\\alpha=0} &= \\sum_{i,j=1}^{n}\\frac{\\text{d}^2}{\\text{d}x_ix_j}f(\\bm x)\\bigg|_{\\bm x = \\bm x^\\star}d_id_j\\\\\n&= \\mathbf d^\\text{T} \\underbrace{\\nabla^\\text{2}f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star}}_\\text{Hessian} \\mathbf d.\n\\end{aligned}\n where \\nabla^2 f(\\mathbf x) is the Hessian (the symmetrix matrix of the second-order mixed partial derivatives) \n\\nabla^2 f(x) = \\begin{bmatrix}\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1^2} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1\\partial x_2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1\\partial x_n}\\\\\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2\\partial x_1} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2^2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2\\partial x_n}\\\\\n                 \\vdots\\\\\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_1} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_n}.\n                \\end{bmatrix}\n\nSince \\bm d is arbitrary, the second-order necessary condition of optimality is then \n\\boxed{\\nabla^\\text{2}f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star} \\succeq 0,}\n where, once again, the inequality \\succeq reads that the matrix is positive semidefinite.\n\n\nSecond-order sufficient condition of optimality\n\n\\boxed{\\nabla^2 f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star} \\succ 0,}\n where, once again, the inequality \\succ reads that the matrix is positive definite.\n\nExample 2 For the quadratic function f(\\mathbf x) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{Q}\\mathbf{x} + \\mathbf{r}^\\mathrm{T}\\mathbf{x}, the Hessian is \n\\nabla^2 f(\\mathbf{x}) = \\mathbf{Q}\n and the second-order necessary condition of optimality is \n\\boxed{\\mathbf{Q} \\succeq 0.}\n\nSecond-order sufficient condition of optimality is then \n\\boxed{\\mathbf{Q} \\succ 0.}\n\nOnce again, this was more than just an example – quadratic functions are so important for us that it is worth remembering this result.",
    "crumbs": [
      "1. Optimization – theory",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#classification-of-stationary-points",
    "href": "opt_theory_unconstrained.html#classification-of-stationary-points",
    "title": "Theory for unconstrained optimization",
    "section": "Classification of stationary points",
    "text": "Classification of stationary points\nFor a stationary (also critical) point \\bm x^\\star, that is, one that satisfies the first-order necessary condition \n\\nabla f(\\bm x^\\star) = 0,\n\nwe can classify it as\n\nMinimum: if \\nabla^2 f(\\bm x^\\star)\\succ 0,\nMaximum: if \\nabla^2 f(\\bm x^\\star)\\prec 0,\nSaddle point: if \\nabla^2 f(\\bm x^\\star) is indefinite,\nSingular point: if \\nabla^2 f(\\bm x^\\star) is singular and \\nabla^2 f(\\bm x^\\star)\\succeq 0 (or \\nabla^2 f(\\bm x^\\star)\\prec 0).\n\n\n\nExample 3 (Minimum of a quadratic function) We consider a quadratic function f(\\mathbf x) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{Q}\\mathbf{x} + \\mathbf{r}^\\mathrm{T}\\mathbf{x} for a particular \\mathbf{Q} and \\mathbf{r}.\n\n\nShow the code\nQ = [1 1; 1 2]\nr = [0, 1]\n\nusing LinearAlgebra\nx_stationary = -Q\\r\neigvals(Q)\n\n\n2-element Vector{Float64}:\n 0.38196601125010515\n 2.618033988749895\n\n\nThe matrix is positive definite, so the stationary point is a minimum. In fact, the minimum. Surface and contour plots of the function are shown below.\n\nShow the code\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([x_stationary[1]], [x_stationary[2]], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\nGKS: cannot open display - headless operation mode active\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 2: Minimum of a quadratic function\n\n\n\n\n\nExample 4 (Saddle point of a quadratic function)  \n\n\nShow the code\nQ = [-1 1; 1 2]\nr = [0, 1]\n\nusing LinearAlgebra\nx_stationary = -Q\\r\neigvals(Q)\n\n\n2-element Vector{Float64}:\n -1.3027756377319946\n  2.302775637731995\n\n\nThe matrix is indefinite, so the stationary point is a saddle point. Surface and contour plots of the function are shown below.\n\nShow the code\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([x_stationary[1]], [x_stationary[2]], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 3: Saddle point of a quadratic function\n\n\n\n\n\nExample 5 (Singular point of a quadratic function)  \n\n\nShow the code\nQ = [2 0; 0 0]\nr = [3, 0]\n\nusing LinearAlgebra\neigvals(Q)\n\n\n2-element Vector{Float64}:\n 0.0\n 2.0\n\n\nThe matrix Q is singular, which has two consequences:\n\nWe cannot compute the stationary point since Q is not invertible. In fact, there is a whole line (a subspace) of stationary points.\nThe matrix Q is positive semidefinite, which generally means that optimality cannot be concluded. But in this particular case of a quadratic function, there are no higher-order terms in the Taylor series expansion, so the stationary point is a minimum.\n\nSurface and contour plots of the function are shown below.\n\nShow the code\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ndisplay(contour(x1_data,x2_data,f_data))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 4: Singular point of a quadratic function\n\n\n\n\n\nExample 6 (Singular point of a non-quadratic function) Consider the function f(\\bm x) = x_1^2 + x_2^4. Its gradient is \\nabla f(\\bm x) = \\begin{bmatrix}2x_1\\\\ 4x_2^3\\end{bmatrix} and it vanishes at \\bm x^\\star = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}. The Hessian is \\nabla^2 f(\\bm x) = \\begin{bmatrix}2 & 0\\\\ 0 & 12x_2^2\\end{bmatrix}, which when evaluated at the stationary point is \\nabla^2 f(\\bm x)\\bigg|_{\\bm x=\\mathbf 0} = \\begin{bmatrix}2 & 0\\\\ 0 & 0\\end{bmatrix}, which is positive semidefinite. We cannot conclude if the function attains a minimum at \\bm x^\\star.\nWe need to examine higher-order terms in the Taylor series expansion. The third derivatives are \n\\frac{\\partial^3 f}{\\partial x_1^3} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_1^2\\partial x_2} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_1\\partial x_2^2} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_2^3} = 24x_2,\n and when evaluated at zero, they all vanish.\nAll but one fourth derivatives also vanish. The one that does not is \n\\frac{\\partial^4 f}{\\partial x_2^4} = 24,\n which is positive, and since the associated derivative is of the even order, the power si also even, hence the function attains a minimum at \\bm x^\\star = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}.\nThis can also be visually confirmed by the surface and contour plots of the function.\n\nShow the code\nf(x) = x[1]^2 + x[2]^4\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([0.0], [0.0], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 5: Singular point of a non-quadratic function that turrns out to be a minimum",
    "crumbs": [
      "1. Optimization – theory",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "discr_dir_goals.html",
    "href": "discr_dir_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Give some examples of practically useful optimal control cost functions (aka performance indices).\nExplain the challenges in designing controllers with a fixed structure (e.g. PID controllers) by optimizing over their coefficients.\nFormulate the general (nonlinear) problem of optimal control design for a discrete-time system as a numerical optimization over (finite) control sequences. Discuss the two possible variants: simultaneous and sequential.\nFormulate the problem of optimal regulator design for a discrete-time linear time-invariant (LTI) system over a finite time horizon and with a quadratic performance index as a quadratic program (QP). Develop fully both the simultaneous and sequential forms of the optimization problem and consider also including the inequality constraints. What is the major disadvantage of the control strategy based on the offline optimization over a control sequence?\nExplain the essence of receding horizon control also known as model predictive control (MPC). What are the major advantages and disadvantages?\nFormulate the MPC regulation for a linear system and a quadratic cost as a quadratic program. Give both the simultaneous and sequential versions.\nFormulate the MPC tracking for a linear system and a quadratic cost as a quadratic program. Explain the need for replacement of the control signals by their increments in the optimization problem. Give both the simultaneous and sequential versions.\nDiscuss the anticipatory reference tracking (aka preview control) and show how this could be achieved using MPC.\nShow how soft constraints can be included in the MPC optimization problem and discuss the motivation for their introduction.\nExplain the difference between the prediction horizon and control horizon.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "discr_dir_goals.html#knowledge-remember-and-understand",
    "href": "discr_dir_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Give some examples of practically useful optimal control cost functions (aka performance indices).\nExplain the challenges in designing controllers with a fixed structure (e.g. PID controllers) by optimizing over their coefficients.\nFormulate the general (nonlinear) problem of optimal control design for a discrete-time system as a numerical optimization over (finite) control sequences. Discuss the two possible variants: simultaneous and sequential.\nFormulate the problem of optimal regulator design for a discrete-time linear time-invariant (LTI) system over a finite time horizon and with a quadratic performance index as a quadratic program (QP). Develop fully both the simultaneous and sequential forms of the optimization problem and consider also including the inequality constraints. What is the major disadvantage of the control strategy based on the offline optimization over a control sequence?\nExplain the essence of receding horizon control also known as model predictive control (MPC). What are the major advantages and disadvantages?\nFormulate the MPC regulation for a linear system and a quadratic cost as a quadratic program. Give both the simultaneous and sequential versions.\nFormulate the MPC tracking for a linear system and a quadratic cost as a quadratic program. Explain the need for replacement of the control signals by their increments in the optimization problem. Give both the simultaneous and sequential versions.\nDiscuss the anticipatory reference tracking (aka preview control) and show how this could be achieved using MPC.\nShow how soft constraints can be included in the MPC optimization problem and discuss the motivation for their introduction.\nExplain the difference between the prediction horizon and control horizon.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "discr_dir_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "discr_dir_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\nImplement a simple model predictive controller (MPC) in both the simultaneous and sequential formats using Matlab. Rely on the availability of numerical solvers for quadratic programming, that is, you do not have to write your own optimization solver.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "discr_dir_mpc_explicit.html",
    "href": "discr_dir_mpc_explicit.html",
    "title": "Explicit MPC",
    "section": "",
    "text": "Model predictive control (MPC) is not computationally cheap (compared to, say, PID or LQG control) as it requires solving optimization problem – typically a quadratic program (QP) - online. The optimization solver needs to be a part of the controller.\nThere is an alternative, though, at least in some cases. It is called explicit MPC. The computationally heavy optimization is only perfomed only during the design process and the MPC controller is then implemented just as an affine state feedback\n\\boxed{\n\\bm u_k(\\bm x_k) = \\mathbf F^i \\bm x_k + \\mathbf g^i,\\; \\text{if}\\; \\bm x_k\\in \\mathcal R^i,}\n where \\mathcal R^i, \\; i=1, 2, \\ldots, p are polyhedral regions, into which the state space \\mathbb R^n is partitioned, and \\mathbf F^i and \\mathbf g^i are the coefficient matrices and vectors that parameterize the affine state feedback controller in that region.\nThe regions \\mathcal R^i and the corresponding coefficients \\mathbf F^i and \\mathbf g^i are determined during the offline design process. Which set of coefficients is chosen is determined online (in real time) based on which region \\mathcal R^i the state \\bm x_k is located in.",
    "crumbs": [
      "6. More on MPC",
      "Explicit MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_explicit.html#multiparametric-programming",
    "href": "discr_dir_mpc_explicit.html#multiparametric-programming",
    "title": "Explicit MPC",
    "section": "Multiparametric programming",
    "text": "Multiparametric programming\nThe key technique for explicit MPC is multi-parametric programming. Consider the following optimization problem\n\nJ^\\star(\\bm x) = \\inf_{\\bm z\\in \\mathbb R^m} J(\\bm z;\\bm x),\n where \\bm z\\in \\mathbb R^m is an optimization (vector) variable, while \\bm x\\in \\mathbb R^n is a vector of parameters (it is a common notational convention to separate variables and parameters). For a given parameter \\bm x, the cost function J is to be minimized. We now want to study how the optimal cost J^\\star depends on the parameter. For a scalar parameter, this task is called parametric programming, for vector parameters, the name of the problem changes to multiparametric programming.\n\nExample 1 (Parametric programming) Consider the following cost function J(z;x) in z\\in\\mathbb R, parameterized by x\\in \\mathbb R. The optimization variable z is subject to an inequality constraint, and this constraint is also parameterized by x. \n\\begin{aligned}\nJ(z;x) &= \\frac{1}{2} z^2 + 2zx + 2x^2 \\\\\n\\text{subject to} &\\quad  z \\leq 1 + x.\n\\end{aligned}\n\nIn this simple case we can aim at analytical solution. We proceed in the standard way – we introduce a Lagrange multiplicator \\lambda and form the augmented cost function \nL(z,\\lambda; x) = \\frac{1}{2} z^2 + 2zx + 2x^2 + \\lambda (z-1-x).\n\nThe necessary conditions of optimality for the inequality-constrained problem come in the form of KKT conditions \n\\begin{aligned}\nz + 2x + \\lambda &= 0,\\\\\nz - 1 - x &\\leq  0,\\\\\n\\lambda & \\geq 0,\\\\\n\\lambda (z - 1 - x) &= 0.\n\\end{aligned}\n\nThe last condition – the complementarity condition – gives rise to two scenarios: one corresponding to \\lambda = 0, and the other corresponding to z - 1 - x = 0. We consider them separately below.\nAfter substituting \\lambda = 0 into the KKT conditions, we get \n\\begin{aligned}\nz + 2x &= 0,\\\\\nz - 1 - x & \\leq  0.\n\\end{aligned}\n\nFrom the first equation we get how z depends on x, and from the second we obtain a bound on x. Finally, we can also substitute the expression for z into the cost function J to get the optimal cost J^\\star as a function of x. All these are summarized here \n\\begin{aligned}\nz &= -2x,\\\\\nx & \\geq -\\frac{1}{3},\\\\\nJ^\\star(x) &= 0.\n\\end{aligned}\n\nNow, the other scenario. Upon substituting z - 1 - x = 0 into the KKT conditions we get \n\\begin{aligned}\nz + 2x + \\lambda &= 0,\\\\\nz - 1 - x &=  0,\\\\\n\\lambda & \\geq 0.\n\\end{aligned}\n\nFrom the second equation we get the expression for z in terms of x, substituting into the first equation and invoking the condition on nonnegativity of \\lambda we get the bound on x (not suprisingly it complements the one obtained in the previous scenario). Finally, substituting for z in the cost function J we get a formula for the cost J^\\star as a function of x.\n\n\\begin{aligned}\nz &= 1 + x,\\\\\n\\lambda &= -z - 2x \\geq 0 \\quad \\implies \\quad x \\leq -\\frac{1}{3},\\\\\nJ^\\star(x) &= \\frac{9}{2}x^2 + 3x + \\frac{1}{2}.\n\\end{aligned}\n\nThe two scenarios can now be combined into a single piecewise affine function z^\\star(x) \nz^\\star(x) = \\begin{cases}\n1+x & \\text{if } x \\leq -\\frac{1}{3},\\\\\n-2x & \\text{if } x &gt; -\\frac{1}{3}.\n\\end{cases}\n\n\n\nShow the code\nx = range(-1, 1, length=100)\nz(x) = x &lt;= -1/3 ? 1 + x : -2x\nJstar(x) = x &lt;= -1/3 ? 9/2*x^2 + 3x + 1/2 : 0\n\nusing Plots\nplot(x, z.(x), label=\"\",lw=2)\nvline!([-1/3],line=:dash, label=\"\")\nxlabel!(\"x\")\nylabel!(\"z⋆(x)\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Piecewise affine dependence of the minimizer on the parameter\n\n\n\n\nand a piecewise quadratic cost function J^\\star(x) \nJ^\\star(x) = \\begin{cases}\n\\frac{9}{2}x^2 + 3x + \\frac{1}{2} & \\text{if } x \\leq -\\frac{1}{3},\\\\\n0 & \\text{if } x &gt; -\\frac{1}{3}.\n\\end{cases}\n\n\n\nShow the code\nplot(x, Jstar.(x), label=\"\",lw=2)\nvline!([-1/3],line=:dash, label=\"\")\nxlabel!(\"x\")\nylabel!(\"J⋆(x)\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Piecewise quadratic dependence of the optimal cost on the parameter\n\n\n\n\n\n\nExample 2 (Multiparametric programming) #TODO",
    "crumbs": [
      "6. More on MPC",
      "Explicit MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_explicit.html#explicit-mpc",
    "href": "discr_dir_mpc_explicit.html#explicit-mpc",
    "title": "Explicit MPC",
    "section": "Explicit MPC",
    "text": "Explicit MPC\nAs we have discussed a few times, the cost of a finite-horizon optimal control problem is a function of the control trajectory \\bm u_0, \\bm u_1, \\ldots, \\bm u_{N-1}, while the initial state \\bm x_0 is a parameter (not subject to optimization)\n\nJ\\left(\\begin{bmatrix}\\bm u_0\\\\ \\bm u_1\\\\ \\vdots \\\\ \\bm u_{N-1}\\end{bmatrix}; \\bm x_0\\right) = \\phi(\\bm x_N) + \\sum_{k=0}^{N-1} L(\\bm x_k, \\bm u_k).\n\nYou may now perhaps appreciate our choice of notation in the previous paragraphs in that we used \\bm x as the parameter. It fits here perfectly. The state at the beginning of the time horizon over which the optimal control is computed is a parameter.\nConsidering that the cost function is quadratic, and parameterized by a vector, we can apply the multiparametric programming techniquest to find the optimal control law as a function of the state (here playing the role of a vector parameter).\n#TODO\n[1], [2], [3], [4]",
    "crumbs": [
      "6. More on MPC",
      "Explicit MPC"
    ]
  },
  {
    "objectID": "intro_references.html",
    "href": "intro_references.html",
    "title": "Literature for the course",
    "section": "",
    "text": "The two subdomains of control theory - optimal control and robust control -, which we explore in this course, are both quite mature, and are covered by a wealth of books, introductory and advanced. The same holds for the doman of numerical optimization, which provides important components for both control subdomains.\nIn our course we do not follow any single textbook chapter by chapter.\nInstead, besides our own lecture notes we are going to refer to various study resources once we study the individual (weekly) topics. In doing that we prefer freely available online resources and books available in the university library, but we also give some recommendations for relevant and helpful books that are not freely available.\nBelow we give a general recommendation on literature categorized into the three domains. Students are not required to obtain those books but perhaps such curated and commented lists might do some service to interested students.",
    "crumbs": [
      "0. Introduction",
      "Literature for the course"
    ]
  },
  {
    "objectID": "intro_references.html#numerical-optimization",
    "href": "intro_references.html#numerical-optimization",
    "title": "Literature for the course",
    "section": "Numerical optimization",
    "text": "Numerical optimization\nThe popular [1] offers deep but clear explanations, and [2] is commonly regarded as a comprehensive reference, and yet it is readable. The broad scope of the recently published [3] fits our course nearly perfectly, even including the optimal control part. Very didactic and also accompanied by Matlab and Python codes is [4]. [5] is also recommendable. Unfortunately, none of these books is freely available online.\nHowever, there are a few recently published textbooks that are also legally available online and that are fairly comprehensive and readable: [6], [7] and [8], the latter including code in Julia language. The freely available “convex optimization bible” [9] contains perfect explanations of many advanced optimization concepts such as duality.",
    "crumbs": [
      "0. Introduction",
      "Literature for the course"
    ]
  },
  {
    "objectID": "intro_references.html#optimal-control",
    "href": "intro_references.html#optimal-control",
    "title": "Literature for the course",
    "section": "Optimal control",
    "text": "Optimal control\nOne of a few optimal control textbooks that covers also discrete-time problems is the classical textboook [10], which has been lately made available online on the author’s web page.\nMajority of classical optimal control textbooks and monographs focus on continuous-time systems, for which we need to invoke calculus of variations. While it is not a trivial subject, an accessible treatment tailored to continuous-time optimal control is provided by [11] (a draft available online). The monograph [12] is regarded a classic, but [13] is perhaps a bit more readable (and affordable as it was reprinted by a low-cost publisher). A few copies of another reprinted classic [14] are available in the library, but freely downloadable electronic version are also available.\nWhile all these classical monographs provide fundamental theoretical insight, computational tools they provide (mostly in one way or another reformulating the optimal control problem into a Riccati equation) are mostly restricted to linear systems. For nonlinear continuous-time systems, hardly anything is as practical as numerical methods as covered for example by [15]. When restricted to discrete-time systems, model predictive control is covered by [16], which is also available online.",
    "crumbs": [
      "0. Introduction",
      "Literature for the course"
    ]
  },
  {
    "objectID": "intro_references.html#robust-control",
    "href": "intro_references.html#robust-control",
    "title": "Literature for the course",
    "section": "Robust control",
    "text": "Robust control\nThe topic of robust control is also described in a wealth of books. But unlike in the area of optimization and general optimal control, here we do have a strong preference for a single book, and our choice is fairly confident: [17]. We are going to use it in the last third of the course. A few copies of this book are available in the university library (reserved for the students of this course). In addition, the first three chapters are freely downloadable on the author’s web page. But we strongly recommend considering purchasing the book. It might turn out very useful as a reference even after passing an exam in this course.\nOther classical references are [18], [19], [20], [21], [22], but we are not going to used them in our course.",
    "crumbs": [
      "0. Introduction",
      "Literature for the course"
    ]
  },
  {
    "objectID": "ext_hw.html",
    "href": "ext_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Homework"
    ]
  },
  {
    "objectID": "dynamic_programming_hw.html",
    "href": "dynamic_programming_hw.html",
    "title": "Homework",
    "section": "",
    "text": "In this homework, you will implement a dynamic programming solver for a general discrete-time optimal control problem in Julia. Specifically, you will solve the following problem: \n\\begin{align*}\n    \\underset{\\mathbf{u}_k}{\\text{minimize}} \\quad & \\phi(\\mathbf{x}_N) + \\sum_{k=1}^{N-1} L(\\bm{x}_k, \\bm{u}_k)\\\\\n    \\text{subject to} \\quad & \\bm{x}_{k+1} = \\mathbf{f}(\\bm{x}_k, \\bm{u}_k), \\qquad k = 1, 2, \\ldots, N-1,\\\\\n    & \\mathbf{u}_{\\text{min}} \\leq \\bm{u}_k \\leq \\mathbf{u}_{\\text{max}}, \\qquad k = 1, 2, \\ldots, N-1,\\\\\n    & \\mathbf{x}_{\\text{min}} \\leq \\bm{x}_k \\leq \\mathbf{x}_{\\text{max}}, \\qquad k = 1, 2, \\ldots, N.\\\\\n\\end{align*}\n where \\phi is the terminal penalty function, L is the additive cost, \\mathbf{f} is state transition function, \\bm{x}_k \\in \\mathbb{R}^{n} is the state, and \\bm{u}_k \\in \\mathbb{R}^{m} is the control input. The state and control input constraints are given by \\mathbf{x}_{\\text{min}}, \\mathbf{x}_{\\text{max}}, \\mathbf{u}_{\\text{min}}, and \\mathbf{u}_{\\text{max}}, respectively. The goal is to construct the optimal control policy \\bm{u}_k^*(\\bm{x}) that minimizes the cost-to-go function J_k(\\bm{x}) for each state \\bm{x} at time k.\nYour solver will work with discretized state and control input spaces \n    X = X_1 \\times X_2 \\times \\ldots \\times X_n, \\quad U = U_1 \\times U_2 \\times \\ldots \\times U_m,\n where each X_i and U_i are finite sets created by uniformly discretizing intervals [\\mathbf{x}_{\\text{min}}^{(i)}, \\mathbf{x}_{\\text{max}}^{(i)}] and [\\mathbf{u}_{\\text{min}}^{(i)}, \\mathbf{u}_{\\text{max}}^{(i)}] into n_{x}^{(i)} and n_{u}^{(i)} points, respectively.\n\n\n\nImplement the dynprog function in the code cell below by filling in the missing parts. Specifically, you need to:\n\nInitialize the cost-to-go function J_N at the terminal time N.\nImplement the Bellman recursion to compute J_k(\\bm{x}) and \\bm{u}_k^*(\\bm{x}) for all states \\bm{x} \\in X at time k.\n\nYour solution should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\n\n\n\nSince the next state \\bm{x}_{k+1} = \\mathbf{f}(\\bm{x}_k, \\bm{u}_k) is generally not on the discretized grid, interpolation is required. In the template below, we provide the function ss_itp, which creates an interpolator for a function (array) defined on the state space grid. This function can be used to interpolate the cost-to-go function J to an arbitrary state \\bm{x}. The optimal control policy \\bm{u}_k^*(\\bm{x}) is also interpolated in a similar way.\nInterpolating J introduces its own set of problems. For example, we cannot truly enforce the state constraints. This is due to the fact that if \\bm{x}_{k+1} lies outside the bounds for all possible \\bm{u}_k, then we would like to evaluate J_k as \\infty or appropropriately high value. However, this high value would, in time, propagate to the entire state space, leading to incorrect results. We, therefore, opt for evaluating J_k for the out of bounds states by extrapolating the value from the nearest grid point. This is already implemented for you, but you should be aware of this limitation.\n\nusing LinearAlgebra, Interpolations\n\n\"\"\"\n    dynprog(L, ϕ, f, x_min, x_max, u_min, u_max, nxs, nus, N)\n\nApplies dynamic programming to solve the discrete-time optimal control problem:\n\n    minimize ∑ₖ L(xₖ, uₖ) + ϕ(x_N)\n    subject to xₖ₊₁ = f(xₖ, uₖ),  k = 1, ..., N-1\n               x_min ≤ xₖ ≤ x_max, k = 1, ..., N\n               u_min ≤ uₖ ≤ u_max, k = 1, ..., N-1.\n\nwhere\n- `x` is the state,\n- `u` is the control input,\n- `L(x, u)` is the stage cost function,\n- `ϕ(x)` is the terminal cost function,\n- `f(x, u)` is the state transition function,\n- `x_min` and `x_max` are vectors of the lower and upper bounds on the state,\n- `u_min` and `u_max` are vectors of the lower and upper bounds on the control input,\n- `nxs` is a tuple of the number of grid points along each axis of the state space,\n- `nus` is a tuple of the number of grid points along each axis of the control input space,\n- `N` is the number of time steps.\n\n# Output\n- `policy(x, k)`: A function that returns the optimal control action at state `x` and time `k`.\n- `J_opt`: The optimal cost-to-go function on the state space grid.\n- `u_opt`: The optimal control policy on the state space grid.\n  - This function uses interpolation to approximate the optimal control for continuous `x`.\n\n\"\"\"\nfunction dynprog(\n    L::Function,\n    ϕ::Function,\n    f::Function,\n    x_min::Vector{Float64},\n    x_max::Vector{Float64},\n    u_min::Vector{Float64},\n    u_max::Vector{Float64},\n    nxs::NTuple,\n    nus::NTuple,\n    N::Int64\n)\n\n    n = length(nxs)\n    m = length(nus)\n\n    # Discretize each axis of the state space\n    xs = map(i -&gt; LinRange(x_min[i], x_max[i], nxs[i]), 1:n)\n\n    # Take the Cartesian product of the axes\n    X = Iterators.product(xs...)\n\n    # Discretize each axis of the control input space\n    us = map(i -&gt; LinRange(u_min[i], u_max[i], nus[i]), 1:m)\n\n    # Take the Cartesian product of the axes\n    U = Iterators.product(us...)\n\n    # The optimal cost-to-go function\n    J_opt = zeros(length(X), N) # J[i, k] is the cost-to-go at state X[i] and time k\n\n    # TODO Initialize the cost-to-go function at the terminal time\n    J_opt[:, N] = zeros(length(X))\n\n    # The optimal control policy\n    u_opt = zeros(m, length(X), N-1) # u_opt[:, i, k] is the optimal control at state X[i] and time k\n\n    # Creates an interpolator for an array defined on a grid onto the entire state space\n    function ss_itp(A)\n        itp = scale(extrapolate(interpolate(reshape(A, nxs...), BSpline(Cubic(Line(OnGrid())))), Flat()),Tuple(xs))\n        fun_itp(x) = itp(x...)\n        return fun_itp\n    end\n\n    for k = N-1:-1:1 # Backward recursion\n     \n        # Interpolate the cost-to-go function outside of the grid points\n        next_cost_itp = ss_itp(J_opt[:, k+1]) # e.g. next_cost_itp(x) returns J_opt(x, k+1)\n        \n        for (i, x) in enumerate(X) # Loop over the state space\n            x = collect(x) # tuple -&gt; vector\n\n            # TODO: Implement the Bellman recursion to compute J_opt[i, k] and u_opt[:, i, k] for all states x ∈ X.\n            J_opt[i, k] = 0.0\n            u_opt[:, i, k] .= 0.0  \n        end\n\n    end\n\n    # Creates an interpolator for the optimal control policy\n    u_opt_itps = [ss_itp(u_opt[i, :, k]) for i in 1:m, k in 1:N-1]\n    policy(x, k) = map(i -&gt; u_opt_itps[i, k](x), 1:m)\n\n    return J_opt, u_opt, policy\nend\n\nFor testing your implementation, you can use the following example problem: \n   \\begin{align*}\n    \\underset{u_k}{\\text{minimize}} \\quad & 2\\lvert x_3 - 1 \\rvert + \\sum_{k=1}^{2} \\lvert x_k \\rvert + \\lvert u_k \\rvert\\\\\n    \\text{subject to} \\quad & x_{k+1} = x_k + u_k, \\qquad k = 1, 2,\\\\\n    & -2 \\leq x_k \\leq 2, \\qquad k = 1, 2, 3,\\\\\n    & -1 \\leq u_k \\leq 1, \\qquad k = 1, 2.\n\\end{align*}\n Furthermore, n_x = 5, n_u = 3, leading to the discretized state and control input spaces X = \\{-2, -1, 0, 1, 2\\} and U = \\{-1, 0, 1\\}, respectively. The problem is already implemented for you in the code cell below.\n\nf(x,u) = [x[1] + u[1]]\nL(x,u) = abs(x[1]) + abs(u[1])\nϕ(x) = 2.0abs(x[1] - 1.0)\n\nx_min = [-2.0]\nx_max = [2.0]\nu_min = [-1.0]\nu_max = [1.0]\nnxs = (5,)\nnus = (3,)\n\nN = 3\n\nJ_opt, u_opt, policy = dynprog(L, ϕ, f, x_min, x_max, u_min, u_max, nxs, nus, N)\n\nx0 = [-1.0]\nxhist = zeros(1, N)\nxhist[1] = x0[1]\nuhist = zeros(1, N-1)\n\nfor k = 1:N-1\n    uhist[:, k] = policy(xhist[:, k], k)\n    xhist[:, k+1] = f(xhist[:, k], uhist[:, k])\nend\n\nusing Plots\n\nplot(0:N-1, xhist[1,:], linetype=:steppost)\nxlabel!(\"Time\")\nylabel!(\"State\")\ntitle!(\"State trajectory\")",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Homework"
    ]
  },
  {
    "objectID": "dynamic_programming_hw.html#dynamic-programming-solver-implementation",
    "href": "dynamic_programming_hw.html#dynamic-programming-solver-implementation",
    "title": "Homework",
    "section": "",
    "text": "In this homework, you will implement a dynamic programming solver for a general discrete-time optimal control problem in Julia. Specifically, you will solve the following problem: \n\\begin{align*}\n    \\underset{\\mathbf{u}_k}{\\text{minimize}} \\quad & \\phi(\\mathbf{x}_N) + \\sum_{k=1}^{N-1} L(\\bm{x}_k, \\bm{u}_k)\\\\\n    \\text{subject to} \\quad & \\bm{x}_{k+1} = \\mathbf{f}(\\bm{x}_k, \\bm{u}_k), \\qquad k = 1, 2, \\ldots, N-1,\\\\\n    & \\mathbf{u}_{\\text{min}} \\leq \\bm{u}_k \\leq \\mathbf{u}_{\\text{max}}, \\qquad k = 1, 2, \\ldots, N-1,\\\\\n    & \\mathbf{x}_{\\text{min}} \\leq \\bm{x}_k \\leq \\mathbf{x}_{\\text{max}}, \\qquad k = 1, 2, \\ldots, N.\\\\\n\\end{align*}\n where \\phi is the terminal penalty function, L is the additive cost, \\mathbf{f} is state transition function, \\bm{x}_k \\in \\mathbb{R}^{n} is the state, and \\bm{u}_k \\in \\mathbb{R}^{m} is the control input. The state and control input constraints are given by \\mathbf{x}_{\\text{min}}, \\mathbf{x}_{\\text{max}}, \\mathbf{u}_{\\text{min}}, and \\mathbf{u}_{\\text{max}}, respectively. The goal is to construct the optimal control policy \\bm{u}_k^*(\\bm{x}) that minimizes the cost-to-go function J_k(\\bm{x}) for each state \\bm{x} at time k.\nYour solver will work with discretized state and control input spaces \n    X = X_1 \\times X_2 \\times \\ldots \\times X_n, \\quad U = U_1 \\times U_2 \\times \\ldots \\times U_m,\n where each X_i and U_i are finite sets created by uniformly discretizing intervals [\\mathbf{x}_{\\text{min}}^{(i)}, \\mathbf{x}_{\\text{max}}^{(i)}] and [\\mathbf{u}_{\\text{min}}^{(i)}, \\mathbf{u}_{\\text{max}}^{(i)}] into n_{x}^{(i)} and n_{u}^{(i)} points, respectively.\n\n\n\nImplement the dynprog function in the code cell below by filling in the missing parts. Specifically, you need to:\n\nInitialize the cost-to-go function J_N at the terminal time N.\nImplement the Bellman recursion to compute J_k(\\bm{x}) and \\bm{u}_k^*(\\bm{x}) for all states \\bm{x} \\in X at time k.\n\nYour solution should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\n\n\n\nSince the next state \\bm{x}_{k+1} = \\mathbf{f}(\\bm{x}_k, \\bm{u}_k) is generally not on the discretized grid, interpolation is required. In the template below, we provide the function ss_itp, which creates an interpolator for a function (array) defined on the state space grid. This function can be used to interpolate the cost-to-go function J to an arbitrary state \\bm{x}. The optimal control policy \\bm{u}_k^*(\\bm{x}) is also interpolated in a similar way.\nInterpolating J introduces its own set of problems. For example, we cannot truly enforce the state constraints. This is due to the fact that if \\bm{x}_{k+1} lies outside the bounds for all possible \\bm{u}_k, then we would like to evaluate J_k as \\infty or appropropriately high value. However, this high value would, in time, propagate to the entire state space, leading to incorrect results. We, therefore, opt for evaluating J_k for the out of bounds states by extrapolating the value from the nearest grid point. This is already implemented for you, but you should be aware of this limitation.\n\nusing LinearAlgebra, Interpolations\n\n\"\"\"\n    dynprog(L, ϕ, f, x_min, x_max, u_min, u_max, nxs, nus, N)\n\nApplies dynamic programming to solve the discrete-time optimal control problem:\n\n    minimize ∑ₖ L(xₖ, uₖ) + ϕ(x_N)\n    subject to xₖ₊₁ = f(xₖ, uₖ),  k = 1, ..., N-1\n               x_min ≤ xₖ ≤ x_max, k = 1, ..., N\n               u_min ≤ uₖ ≤ u_max, k = 1, ..., N-1.\n\nwhere\n- `x` is the state,\n- `u` is the control input,\n- `L(x, u)` is the stage cost function,\n- `ϕ(x)` is the terminal cost function,\n- `f(x, u)` is the state transition function,\n- `x_min` and `x_max` are vectors of the lower and upper bounds on the state,\n- `u_min` and `u_max` are vectors of the lower and upper bounds on the control input,\n- `nxs` is a tuple of the number of grid points along each axis of the state space,\n- `nus` is a tuple of the number of grid points along each axis of the control input space,\n- `N` is the number of time steps.\n\n# Output\n- `policy(x, k)`: A function that returns the optimal control action at state `x` and time `k`.\n- `J_opt`: The optimal cost-to-go function on the state space grid.\n- `u_opt`: The optimal control policy on the state space grid.\n  - This function uses interpolation to approximate the optimal control for continuous `x`.\n\n\"\"\"\nfunction dynprog(\n    L::Function,\n    ϕ::Function,\n    f::Function,\n    x_min::Vector{Float64},\n    x_max::Vector{Float64},\n    u_min::Vector{Float64},\n    u_max::Vector{Float64},\n    nxs::NTuple,\n    nus::NTuple,\n    N::Int64\n)\n\n    n = length(nxs)\n    m = length(nus)\n\n    # Discretize each axis of the state space\n    xs = map(i -&gt; LinRange(x_min[i], x_max[i], nxs[i]), 1:n)\n\n    # Take the Cartesian product of the axes\n    X = Iterators.product(xs...)\n\n    # Discretize each axis of the control input space\n    us = map(i -&gt; LinRange(u_min[i], u_max[i], nus[i]), 1:m)\n\n    # Take the Cartesian product of the axes\n    U = Iterators.product(us...)\n\n    # The optimal cost-to-go function\n    J_opt = zeros(length(X), N) # J[i, k] is the cost-to-go at state X[i] and time k\n\n    # TODO Initialize the cost-to-go function at the terminal time\n    J_opt[:, N] = zeros(length(X))\n\n    # The optimal control policy\n    u_opt = zeros(m, length(X), N-1) # u_opt[:, i, k] is the optimal control at state X[i] and time k\n\n    # Creates an interpolator for an array defined on a grid onto the entire state space\n    function ss_itp(A)\n        itp = scale(extrapolate(interpolate(reshape(A, nxs...), BSpline(Cubic(Line(OnGrid())))), Flat()),Tuple(xs))\n        fun_itp(x) = itp(x...)\n        return fun_itp\n    end\n\n    for k = N-1:-1:1 # Backward recursion\n     \n        # Interpolate the cost-to-go function outside of the grid points\n        next_cost_itp = ss_itp(J_opt[:, k+1]) # e.g. next_cost_itp(x) returns J_opt(x, k+1)\n        \n        for (i, x) in enumerate(X) # Loop over the state space\n            x = collect(x) # tuple -&gt; vector\n\n            # TODO: Implement the Bellman recursion to compute J_opt[i, k] and u_opt[:, i, k] for all states x ∈ X.\n            J_opt[i, k] = 0.0\n            u_opt[:, i, k] .= 0.0  \n        end\n\n    end\n\n    # Creates an interpolator for the optimal control policy\n    u_opt_itps = [ss_itp(u_opt[i, :, k]) for i in 1:m, k in 1:N-1]\n    policy(x, k) = map(i -&gt; u_opt_itps[i, k](x), 1:m)\n\n    return J_opt, u_opt, policy\nend\n\nFor testing your implementation, you can use the following example problem: \n   \\begin{align*}\n    \\underset{u_k}{\\text{minimize}} \\quad & 2\\lvert x_3 - 1 \\rvert + \\sum_{k=1}^{2} \\lvert x_k \\rvert + \\lvert u_k \\rvert\\\\\n    \\text{subject to} \\quad & x_{k+1} = x_k + u_k, \\qquad k = 1, 2,\\\\\n    & -2 \\leq x_k \\leq 2, \\qquad k = 1, 2, 3,\\\\\n    & -1 \\leq u_k \\leq 1, \\qquad k = 1, 2.\n\\end{align*}\n Furthermore, n_x = 5, n_u = 3, leading to the discretized state and control input spaces X = \\{-2, -1, 0, 1, 2\\} and U = \\{-1, 0, 1\\}, respectively. The problem is already implemented for you in the code cell below.\n\nf(x,u) = [x[1] + u[1]]\nL(x,u) = abs(x[1]) + abs(u[1])\nϕ(x) = 2.0abs(x[1] - 1.0)\n\nx_min = [-2.0]\nx_max = [2.0]\nu_min = [-1.0]\nu_max = [1.0]\nnxs = (5,)\nnus = (3,)\n\nN = 3\n\nJ_opt, u_opt, policy = dynprog(L, ϕ, f, x_min, x_max, u_min, u_max, nxs, nus, N)\n\nx0 = [-1.0]\nxhist = zeros(1, N)\nxhist[1] = x0[1]\nuhist = zeros(1, N-1)\n\nfor k = 1:N-1\n    uhist[:, k] = policy(xhist[:, k], k)\n    xhist[:, k+1] = f(xhist[:, k], uhist[:, k])\nend\n\nusing Plots\n\nplot(0:N-1, xhist[1,:], linetype=:steppost)\nxlabel!(\"Time\")\nylabel!(\"State\")\ntitle!(\"State trajectory\")",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Homework"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html",
    "href": "opt_algo_derivatives.html",
    "title": "Computing the derivatives",
    "section": "",
    "text": "We have already argued that using derivatives gives optimization algorithms a boost. There are three methods to compute derivatives (and gradients, Jacobians, Hessians):",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing the derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#symbolic-methods",
    "href": "opt_algo_derivatives.html#symbolic-methods",
    "title": "Computing the derivatives",
    "section": "Symbolic methods",
    "text": "Symbolic methods\nThese are essentially the methods that we have all learnt to apply using a pen and paper. A bunch of rules. The input for the procedure is a function and the output from the procedure is another function. For example, for f(x) = x^2, the derivative is f'(x) = 2x.\nAlthough straightforward and automatable, symbolic methods are not always the best choice. When does this happen?\n\nThe function to be differentiated is already rather complicated, and the derivative will typically be even more complicated. Its evaluation then may be computationally expensive. We will see that in the example.\nThe function to be differentiated is not available in the closed form (as a formula), but only as a software implementation, however open-source.\n\n\nExample 1 (Symbolic computation of the gradient of a function of the simulated trajectory) Consider the state-space model of a pendulum \n\\underbrace{\n\\begin{bmatrix}\n\\dot \\theta\\\\ \\dot \\omega\n\\end{bmatrix}}_{\\dot{\\bm x}}\n=\n\\underbrace{\n\\begin{bmatrix}\n\\omega\\\\ -\\frac{g}{l}\\sin\\theta\n\\end{bmatrix}}_{\\mathbf f(\\bm x)},\n where l=1\\,\\mathrm{m} is the length of the pendulum, g=9.81\\,\\mathrm{m}/\\mathrm{s}^2 is the acceleration due to gravity, \\theta and \\omega are the angle and angular velocity of the pendulum, respectively. We are going to simulate the trajectory of the pendulum that is initially at some nonzero angle, say, \\theta(0) = \\pi/4 = \\theta_0, and zero velocity, that is, \\omega(0) = 0 = \\omega_0. And we are going to consider the 2-norm (actually its square for convenience) of the state vector at the end of the simulation interval as the cost function to be minimized, for which we need to evaluate the gradient at the initial state.\nFirst, we write an ODE solver that obtains an approximation of the final point \\bm x(t_\\mathrm{f}) of the state trajectory on the interval [0,t_\\mathrm{f}], and a function that computes the cost as a function of the initial state J(\\bm x_0).\n\n\n\n\n\n\nAlternative (and still imperfect) notation\n\n\n\nThe state at the final time is a function of the state at the initial time, hence we could also write it as \\bm x(t_\\mathrm{f};\\bm x_0), in which case the cost cold be written as J(\\bm x(t_\\mathrm{f};\\bm x_0)). The dependence of the cost on the initial state is still visible, but the notation is a bit more clumsy (and abusive anyway).\n\n\n\n\nShow the code\nfunction solve_for_final_state_fd(f, x₀, tspan, h)\n    t0, tf = tspan\n    t = t0\n    x = x₀\n    while t &lt; tf\n        x = x + h * f(x)\n        t = t + h\n    end\n    return x\nend\n\nfunction J(x₀) \n    x_final = solve_for_final_state_fd(f, x₀, tspan, h)\n    return x_final[1]^2 + x_final[2]^2\nend\n\n\nAnd now we use the solver to compute the trajectory and the cost\n\n\nShow the code\ng = 9.81\nl = 1.0\nf(x) = [x[2], -g/l*sin(x[1])]  \nθ₀ = π/4\nω₀ = 0.0   \ntspan = (0.0, 1.0)\nh = 0.1\n\nJ([θ₀, ω₀])\n\n\n1.6184460563562304\n\n\nWe now use the Symbolics.jl package to compute the gradient of the cost function at the initial state. We first define symbolic state variables and and then obtain the symbolic expression for the cost function just by evaluating the function we already have at these symbolic state variables.\n\n\nShow the code\nusing Symbolics\n@variables θ₀ ω₀\nJ_sym = J([θ₀, ω₀])\n\n\n(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀)^2 + (-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀)^2\n\n\nIf the shortcomings of symbolic methods have not yet started surfacing, scroll to the right in the output field. Rather long, n’est-ce pas? And we have just started, because we now must differentiate this long expression symbolically (and then we convert it from a symbolic expression back to a standard function in Julia):\n\n\nShow the code\n∇J_sym = Symbolics.gradient(J_sym,[θ₀, ω₀])\n∇J_sym_expr = build_function(∇J_sym, [θ₀, ω₀])\n∇J_sym_fun = eval(∇J_sym_expr[1])\n\n\nFinally, let’s do some benchmarking of evaluation of the gradient at the given (initial) state:\n\n\nShow the code\nusing BenchmarkTools\n@btime ∇J_sym_fun([π/4,0.0])\n\n\n  265.498 μs (4 allocations: 160 bytes)\n\n\n2-element Vector{Float64}:\n 3.9096539447898264\n 0.020083627236371047\n\n\nAs a teaser for what what is to come, we also benchmark the solution based on AD:\n\n\nShow the code\nusing ForwardDiff\n\n@btime ForwardDiff.gradient(J, [π/4,0.0])\n\n\n  3.148 μs (133 allocations: 5.28 KiB)\n\n\n2-element Vector{Float64}:\n 3.909653944789824\n 0.020083627236369115\n\n\nNote that while for the former quite some work must have been done before the timing analysis was started (namely, the formula for the derivative had to be found), for the latter we started only with the function definition. And yet the latter approach wins hands down. But before we start exploring the AD methods, we give a brief overview of numerical methods based on finite-difference (FD) approximations.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing the derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#numerical-finite-difference-fd-methods",
    "href": "opt_algo_derivatives.html#numerical-finite-difference-fd-methods",
    "title": "Computing the derivatives",
    "section": "Numerical finite-difference (FD) methods",
    "text": "Numerical finite-difference (FD) methods\nThese methods approximate the derivative by computing differences between the function values at different points, hence the name finite-difference (FD) methods. The simplest FD methods follow from the definition of the derivative after omiting the limit:\n\n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x+h)-f(x)}{h}\\qquad\\qquad \\text{forward difference}\n or \n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x)-f(x-h)}{h}\\qquad\\qquad \\text{backward difference}\n or \n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x+\\frac{h}{2})-f(x-\\frac{h}{2})}{h}\\qquad\\qquad \\text{central difference}\n\nFor functions of vector variables, the same idea applies, but now we have to compute the difference for each component of the vector.\n\nDependence of the error on the step size\nThe finite-difference methods only approximate the derivatives. The smaller the h in the above formulas, the smaller the approximation error. Really? Not quite. Let’s explore it through an example.\n\nExample 2 (Dependence of the error on the step size) Consider a scalar function of a vector argument f(\\bm x) = \\sum_{i=1}^n x_i^2.\n\n\nShow the code\nf(x) = sum(xᵢ^2 for xᵢ in x)\n\n\nf (generic function with 1 method)\n\n\nNow, in order to compute the gradient \\nabla f, we need to compute all the individual partial derivatives, the individual components of the vector. Let’s now restrict ourselves just to one component, say, the first one, that is, let’s compute \\frac{\\partial f(\\mathbf x)}{\\partial x_1}.\nIn this simple example, a formula can be written down upon inspection: \\frac{\\partial f(\\mathbf x)}{\\partial x_1} = 2x_1:\n\n\nShow the code\nx₀ = rand(10)\n∇f_1_exact = 2*x₀[1]\n\n\n1.0701669603742112\n\n\nbut let’s pretend that this answer is not available to us (we will only use it for evaluation of approximation errors of of chosen FD methods).\nWe now give a function for computing the first entry in the gradient (vector) by using the forward FD method. Note that in defining the function we exploit the multiple dispatch functionality of Julia, thanks to which the function will handled the floating point model of the input appropriately. That is, the input vector could be IEEE-754 double-precision floating-point format, or IEEE-754 single-precision floating-point format (or perhaps even something else).\n\n\nShow the code\nfunction ∇f_1(f,x₀::Vector{T},h::T) where T&lt;:Real\n    (f(x₀[1]+h)-f(x₀[1]))/h\nend\n\n\n∇f_1 (generic function with 1 method)\n\n\nWe can now compute the first entry of the gradient for the particular vector given in the IEEE double format (default for Julia)\n\n\nShow the code\n∇f_1(f,x₀,h)\n\n\n1.1701669603742109\n\n\nWe can also compute the same quantity for the same vector represented in the IEEE single format:\n\n\nShow the code\n∇f_1(f,Vector{Float32}(x₀),Float32(h))\n\n\n1.1701673f0\n\n\nObviously, both answers differ from the accurate one computed above.\nNow, let’s examine the error as a function of the size of the interval h:\n\n\nShow the code\nh_range = exp10.(range(-13, stop=-1, length=1000));\nabs_err_64 = [abs((∇f_1_exact - ∇f_1(f,x₀,h))) for h in h_range];\nabs_err_32 = [abs((∇f_1_exact - ∇f_1(f,Vector{Float32}(x₀),Float32(h)))) for h in h_range];\n\nusing Plots\nplot(h_range, abs_err_64,xaxis=:log, yaxis=:log, xlabel=\"h\", ylabel = \"|e|\", label = \"IEEE double\")\nplot!(h_range, abs_err_32,xaxis=:log, yaxis=:log, xlabel=\"h\", ylabel = \"|e|\", label = \"IEEE single\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we read the graph from right to left – as h is getting smaller –, we observe that initially the error decreases for both 64- and 32-bit floating-point format, and it decreases at the same rate. The reason for this decrease is that the trunction error (essentially what we commit here by doing FD approximation is that we truncate the Taylor series) dominates here, and this error goes down with h.\nThe major intended takeaway from this example is that this reduction of the error only takes place down to some h below which the error no longer decreases; in fact, it actally increases as h gets smaller. The reason is that for h this small, the rounding errors dominate. Apparently, they start exhibitting themselves for smaller values with the 64-bit format than with 32-bit format. The rounding errors are getting dominant here as we are subtracting two numbers that are getting more and more similar as h decreaces. This is known as the phenomenon of catastrophic cancellation.\nIt is known from rigorous numerical analysis that the error in the case of the simple backward or forward finite difference approximation to a scalar derivative scales with \\sqrt{\\epsilon}, where \\epsilon is the machine epsilon. Here we can expect even worse outcome as the dimension n of the vector grows. Note that \\epsilon for double precision IEEE is\n\n\nShow the code\n2^(-53)\n\n\n1.1102230246251565e-16\n\n\nwhich is available in Julia through eps() function with the type as the input argument (if no argument is given, it assumes Float64):\n\n\nShow the code\nsqrt(eps())\n\n\n1.4901161193847656e-8\n\n\nSimilarly, the 32-bit version is\n\n\nShow the code\nsqrt(eps(Float32))\n\n\n0.00034526698f0\n\n\nApparently, these are roughly the “cutt-off” values of h.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing the derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#automatic-also-algorithmic-differentiation-ad-methods",
    "href": "opt_algo_derivatives.html#automatic-also-algorithmic-differentiation-ad-methods",
    "title": "Computing the derivatives",
    "section": "Automatic (also Algorithmic) differentiation (AD) methods",
    "text": "Automatic (also Algorithmic) differentiation (AD) methods\n#TODO: in the meantime, have a look at [1, Ch. 5], or [2, Sec. 6.6], or [3, Sec. 2].\n\nForward AD\n#TODO\n\nImplementation of Forward AD by dual numbers\nSimilar to a complex number, a dual number has two components, one corresponding to the value, the other corresponding to the derivative:\n\nx = v + d\\epsilon,\n where the special property of \\epsilon is \n\\epsilon^2=0.\n\n(Compare it with the property of the imaginary unit: i^2=-1.)\nMultiplication of two dual numbers y = x_1\\cdot x_2 is the defined naturally as \n\\begin{aligned}\nx &= (v_1 + d_1\\epsilon)\\cdot (v_2 + d_2\\epsilon)\\\\\n  &= v_1v_2 + (v_1d_2+d_1v_2) \\epsilon.\n\\end{aligned}\n\nSimilarly for other functions. We illustrate this using the following example.\n\nExample 3 Consider a function y(x) = \\cos(x^2). Its derivative is trivially \\frac{\\mathrm d y}{\\mathrm d x} = -2x\\sin(x^2).\nLet’s now evaluate this result at a particular value of x, say\n\nx = 2\n\n2\n\n\nFirst, we are going to develop our own data class (actually struct) in Julia for dual numbers. The code below is inspired by a code from [3] (they even provide Jupyter Notebooks).\nThen we show the usage of a functionality already provided in ForwardDiff.jl package implementing the forward mode AD.\n\n\nShow the code\nstruct Dual\n    v        # the VALUE part\n    d        # the DERIVATIVE part\nend\n\n\nNow we need to overload the involved basic operations such as addition and multiplication of two dual numbers, multiplication by a scalar, squaring and finding the value of cosine function.\n\n\nShow the code\nBase.:+(a::Dual,b::Dual) = Dual(a.v+b.v,a.d+b.d)\nBase.:*(a::Dual,b::Dual) = Dual(a.v*b.v,a.d*b.v+b.d*a.v)\nBase.:*(a::Number,b::Dual) = Dual(a*b.v,a*b.d)\nBase.:^(a::Dual,b::Int) = Dual(a.v^b,b*a.v^(b-1))\nBase.:cos(a::Dual) = Dual(cos(a.v),-sin(a.v)*a.d)\n\n\nLet’s now check the functionality of the individual functions\n\nX = Dual(x,1)\n\nDual(2, 1)\n\n\n\nY = Dual(3,0)\n\nDual(3, 0)\n\n\n\n3*X\n\nDual(6, 3)\n\n\n\nY*X\n\nDual(6, 3)\n\n\n\nX^2\n\nDual(4, 4)\n\n\n\ncos(X)\n\nDual(-0.4161468365471424, -0.9092974268256817)\n\n\nFinally, let’s use the new functionality to compute the derivative of the assigned function \\cos(x^2)\n\ncos(X^2)\n\nDual(-0.6536436208636119, 3.027209981231713)\n\n\nIn practice, you will hardly feel a need to implement your own library for algorithmic differentiation. Instead, you may want to use one of those avaialable ones, such as ForwardDiff.jl.\n\nusing ForwardDiff\nX = ForwardDiff.Dual(x,1)\nY = cos(X^2)\nY.value\n\n-0.6536436208636119\n\n\n\nY.partials\n\n1-element ForwardDiff.Partials{1, Float64}:\n 3.027209981231713\n\n\nCompare with the “exact”\n\n-2*x*sin(x^2)\n\n3.027209981231713\n\n\n\n\n\n\nReverse AD\n#TODO",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing the derivatives"
    ]
  },
  {
    "objectID": "cont_indir_hw.html",
    "href": "cont_indir_hw.html",
    "title": "Homework",
    "section": "",
    "text": "In this homework, we will solve a simple continuous optimal control problem using the indirect method. The goal is to find the control input u(t) that brings a simple pendulum from the origin to the upright position in a given time interval while minimizing the control effort. Formally, the problem can be stated as follows: \n\\begin{align*}\n    \\underset{u(t)}{\\text{minimize}} \\quad & \\int_{0}^{5} u^2(t)\\, \\mathrm{d}t\\\\\n    \\text{subject to} \\quad & \\dot{x}_1(t) = x_2(t), \\quad t \\in [0, 5],\\\\\n    & \\dot{x}_2(t) = -a_1\\sin{x_1(t)} - a_2x_2(t) + a_3u(t), \\quad t \\in [0, 5],\\\\\n    & x_1(0) = 0,\\\\\n    & x_2(0) = 0,\\\\\n    & x_1(5) = \\pi,\\\\\n    & x_2(5) = 0,\n\\end{align*}\n where x_1(t) is the angle of the pendulum, x_2(t) is the angular velocity, and u(t) is the control input, and a_1 and a_2 are positive constants.\n\n\n\nFormulate the Hamiltonian for the problem above.\nUse the Hamiltonian to derive the state, costate equations and the expression for the optimal control input.\nSubstitute the optimal control input into the state and costate equations to formulate a two-point boundary value problem (TPBVP).\nUse the template code solve the TPBVP numerically using DifferentialEquations.jl. Specifically, take a look at Boundary Value Problems and TwoPointBVProblem in the documentation.\nYou may also find useful how to work with the solution struct from DifferentialEquations.jl here.\nYour solution should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\n\n\nusing DifferentialEquations\n\nconst m = 1\nconst l = 1\nconst g = 9.81\nconst b = 0.1\nconst tf = 5\n\na₁ = g / l\na₂ = b / (m * l^2)\na₃ = 1 / (m * l^2)\n\nfunction find_optimal_trajectory()\n\n    function dynamics!(du, u, p, t)\n        # u = [x₁, x₂, λ₁, λ₂]\n        # du = [ẋ₁, ẋ₂, λ̇₁, λ̇₂]\n        # TODO: implement the state and costate equations here\n    end\n\n    function bcl!(residual, u_l, p)\n        # u_l = [x₁(0), x₂(0), λ₁(0), λ₂(0)]\n        # TODO: enforce initial conditions for the state\n    end\n\n    function bcr!(residual, u_r, p)\n        # u_r = [x₁(tf), x₂(tf), λ₁(tf), λ₂(tf)]\n        # TODO: enforce final conditions for the state\n    end\n\n    # TODO: Set up time span and initial guess for the state+costate trajectory\n\n    # TODO: Create and solve the boundary value problem\n\n    # TODO: Extract state trajectory and reconstruct optimal control\n\n    return t, x_opt, u_opt  # where t is a N-length vector, x_opt is a 2×N matrix and u_opt is a 1×N matrix\nend\n\nYou can plot your results using the following code snippet:\n\nusing Plots\n\nt, x_opt, u_opt = find_optimal_trajectory()\n\np1 = plot(t, x_opt[1, :], label=\"x₁\")\nplot!(t, x_opt[2, :], label=\"x₂\")\n\np2 = plot(t, u_opt[1,:], label=\"u\")\n\nplot(p1, p2, layout=(2, 1))",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Homework"
    ]
  },
  {
    "objectID": "cont_indir_hw.html#unconstrained-pendulum-swing-up-problem",
    "href": "cont_indir_hw.html#unconstrained-pendulum-swing-up-problem",
    "title": "Homework",
    "section": "",
    "text": "In this homework, we will solve a simple continuous optimal control problem using the indirect method. The goal is to find the control input u(t) that brings a simple pendulum from the origin to the upright position in a given time interval while minimizing the control effort. Formally, the problem can be stated as follows: \n\\begin{align*}\n    \\underset{u(t)}{\\text{minimize}} \\quad & \\int_{0}^{5} u^2(t)\\, \\mathrm{d}t\\\\\n    \\text{subject to} \\quad & \\dot{x}_1(t) = x_2(t), \\quad t \\in [0, 5],\\\\\n    & \\dot{x}_2(t) = -a_1\\sin{x_1(t)} - a_2x_2(t) + a_3u(t), \\quad t \\in [0, 5],\\\\\n    & x_1(0) = 0,\\\\\n    & x_2(0) = 0,\\\\\n    & x_1(5) = \\pi,\\\\\n    & x_2(5) = 0,\n\\end{align*}\n where x_1(t) is the angle of the pendulum, x_2(t) is the angular velocity, and u(t) is the control input, and a_1 and a_2 are positive constants.\n\n\n\nFormulate the Hamiltonian for the problem above.\nUse the Hamiltonian to derive the state, costate equations and the expression for the optimal control input.\nSubstitute the optimal control input into the state and costate equations to formulate a two-point boundary value problem (TPBVP).\nUse the template code solve the TPBVP numerically using DifferentialEquations.jl. Specifically, take a look at Boundary Value Problems and TwoPointBVProblem in the documentation.\nYou may also find useful how to work with the solution struct from DifferentialEquations.jl here.\nYour solution should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\n\n\nusing DifferentialEquations\n\nconst m = 1\nconst l = 1\nconst g = 9.81\nconst b = 0.1\nconst tf = 5\n\na₁ = g / l\na₂ = b / (m * l^2)\na₃ = 1 / (m * l^2)\n\nfunction find_optimal_trajectory()\n\n    function dynamics!(du, u, p, t)\n        # u = [x₁, x₂, λ₁, λ₂]\n        # du = [ẋ₁, ẋ₂, λ̇₁, λ̇₂]\n        # TODO: implement the state and costate equations here\n    end\n\n    function bcl!(residual, u_l, p)\n        # u_l = [x₁(0), x₂(0), λ₁(0), λ₂(0)]\n        # TODO: enforce initial conditions for the state\n    end\n\n    function bcr!(residual, u_r, p)\n        # u_r = [x₁(tf), x₂(tf), λ₁(tf), λ₂(tf)]\n        # TODO: enforce final conditions for the state\n    end\n\n    # TODO: Set up time span and initial guess for the state+costate trajectory\n\n    # TODO: Create and solve the boundary value problem\n\n    # TODO: Extract state trajectory and reconstruct optimal control\n\n    return t, x_opt, u_opt  # where t is a N-length vector, x_opt is a 2×N matrix and u_opt is a 1×N matrix\nend\n\nYou can plot your results using the following code snippet:\n\nusing Plots\n\nt, x_opt, u_opt = find_optimal_trajectory()\n\np1 = plot(t, x_opt[1, :], label=\"x₁\")\nplot!(t, x_opt[2, :], label=\"x₂\")\n\np2 = plot(t, u_opt[1,:], label=\"u\")\n\nplot(p1, p2, layout=(2, 1))",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Homework"
    ]
  },
  {
    "objectID": "opt_algo_solvers.html",
    "href": "opt_algo_solvers.html",
    "title": "Numerical solvers",
    "section": "",
    "text": "The number of numerical solvers is huge. First, we give a short biased list of solvers which we may use within this course.\n\nOptimization Toolbox for Matlab: fmincon, fminunc, linprog, quadpro, … Available within the all-university Matlab license for all students and employees at CTU.\nGurobi Optimizer: LP, QP, SOCP, MIP, commercial (but free academic license available).\nIBM ILOG CPLEX: LP, QP, SOCP, MIP, commercial (but free academic license available).\nMOSEK: LP, QP, MIP, SOCP, SDP, commercial (but free academic license available).\nHIGHS: LP, QP, MIP, open source.\nKnitro: NLP, commercial.\nIpopt: NLP, open source.\nSEDUMI: SOCP, SDP, open source.\n…\n\nSecond, for a reasonably comprehensive and well maintained list of solvers, consult the NEOS Guide to Optimization web page (in particular the link at the bottom of that page). Similar list is maintained within Hans Mittelman’s Decision Tree for Optimization Software web page.\nWorking in Matlab and using Yalmip for defining and solving optimization problems, the list of optimization solvers supported by Yalmip shows what is available.\nSimilarly, users of Julia and JuMP will find the list of solvers supported by JuMP useful. The list is worth consulting even if Julia is not the tool of choice, as many solvers are indepdenent of Julia.\n\n\n\n Back to top",
    "crumbs": [
      "2. Optimization – algorithms",
      "Numerical solvers"
    ]
  },
  {
    "objectID": "discr_indir_software.html",
    "href": "discr_indir_software.html",
    "title": "Software for discrete-time LQR and DARE",
    "section": "",
    "text": "ControlSystems.jl\n\nlqr\nare – actually uses MatrixEquations.jl.\n\nMatrixEquations.jl\n\nared",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Software for discrete-time LQR and DARE"
    ]
  },
  {
    "objectID": "discr_indir_software.html#julia",
    "href": "discr_indir_software.html#julia",
    "title": "Software for discrete-time LQR and DARE",
    "section": "",
    "text": "ControlSystems.jl\n\nlqr\nare – actually uses MatrixEquations.jl.\n\nMatrixEquations.jl\n\nared",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Software for discrete-time LQR and DARE"
    ]
  },
  {
    "objectID": "discr_indir_software.html#matlab",
    "href": "discr_indir_software.html#matlab",
    "title": "Software for discrete-time LQR and DARE",
    "section": "MATLAB",
    "text": "MATLAB\n\nControl System Toolbox\n\nidare (dare deprecated)\ndlqr",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Software for discrete-time LQR and DARE"
    ]
  },
  {
    "objectID": "discr_indir_software.html#python",
    "href": "discr_indir_software.html#python",
    "title": "Software for discrete-time LQR and DARE",
    "section": "Python",
    "text": "Python\n\nPython Control\n\ndare\ndlqr",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Software for discrete-time LQR and DARE"
    ]
  },
  {
    "objectID": "intro_projects.html",
    "href": "intro_projects.html",
    "title": "Projects",
    "section": "",
    "text": "The motivation is to experience a more complete procedure than what we can have within tightly focused weekly homework problems. It is an excellent opportunity to apply the knowledge and skills acquired during the course to a more complex problem.",
    "crumbs": [
      "0. Introduction",
      "Projects"
    ]
  },
  {
    "objectID": "intro_projects.html#organization",
    "href": "intro_projects.html#organization",
    "title": "Projects",
    "section": "Organization",
    "text": "Organization\n\nTeam work allowed, in fact even encouraged, teams of 3-5. One suggestion for the organization of the team work is that every team member can design a different kind of a controller than the other members of the team. For instance, one member can design a reference tracking LQR, another one can do an MPC and yet another can try a robust controller using Hinf optimization. The benefit of working in a team is that you can share the unavoidable pain (or fun) of learning to operate the particular laboratory experiment and possibly making sense of its model. Later you can also share the load of writing the report.\nEach team or individual has to submit a report organized as follows:\n\nA very brief description of the laboratory model and the mathematical model you used. No need to replicate the full description provided in the model repositories, but the description should be sufficient for the report to be understandable without the need to consult the model repository. In particular, included should be the explicit enumeration of all the input, state, and output variables, their ranges, units, some characterization of uncertainty in the model and the measurements. The requirements on the control system shoud be clearly stated.\nDocumentation of the computational procedure(s) leading to the design of controllers.\nDiscussion of the performance of the designed controllers with the aid of figures showing and comparing the simulated and measured responses.\n\nIt is highly recommendable, that each project has their own repository within the gitlab.fel.cvut.cz. The final report (6 pages at maximum) should then refer to the content (scripts, models, data) of the repository to support reproducibility.",
    "crumbs": [
      "0. Introduction",
      "Projects"
    ]
  },
  {
    "objectID": "intro_projects.html#time",
    "href": "intro_projects.html#time",
    "title": "Projects",
    "section": "Time",
    "text": "Time\nWhile the project can be expected to demand higher time investment then a weekly homework, it should not be another diploma thesis. Admittedly, some of the assignments rather are open-ended, which means that it is perfectly appropriate to upper bound the total time invested into the project – just do what you can within the time. But then include the timing information in the final report.",
    "crumbs": [
      "0. Introduction",
      "Projects"
    ]
  },
  {
    "objectID": "intro_projects.html#assessment",
    "href": "intro_projects.html#assessment",
    "title": "Projects",
    "section": "Assessment",
    "text": "Assessment\nThe final report will be evaluated as either sufficient or insufficient, it will be either accepted or not. We will not assess contributions of individual students to the report, we will consider the report as a whole. There will be two attempts to submit the report, so that even if the report fails to be accepted when submitted for the first time, some feedback will be given by the instructor and there will be one more chance to submit the report.",
    "crumbs": [
      "0. Introduction",
      "Projects"
    ]
  },
  {
    "objectID": "intro_projects.html#proposed-projects",
    "href": "intro_projects.html#proposed-projects",
    "title": "Projects",
    "section": "Proposed projects",
    "text": "Proposed projects\nThe proposed project consists of laboratory experimental systems in our KN:E-26 laboratory. The essence of the projects is to demonstrate one or preferrably several control design methods using these systems.\n\nQuanser Active Suspension\nGitlab repository: https://gitlab.fel.cvut.cz/aa4cc/edu-ctrl-lab-models/quanser-active-suspension\n\n\n\nQuanser Active Mass Dampers on Shake Table\nGitlab repository: https://gitlab.fel.cvut.cz/aa4cc/edu-ctrl-lab-models/quanser-amd\n\n\n\nQuanser 2DOF Serial Flexible Link\nGitlab repository: https://gitlab.fel.cvut.cz/aa4cc/edu-ctrl-lab-models/quanser-2-dof-serial-flexible-link\n\n\n\nQuanser 3DOF Helicopter\nGitlab repository: https://gitlab.fel.cvut.cz/aa4cc/edu-ctrl-lab-models/quanser-3dof-helicopter",
    "crumbs": [
      "0. Introduction",
      "Projects"
    ]
  },
  {
    "objectID": "rocond_hw.html",
    "href": "rocond_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "Homework"
    ]
  },
  {
    "objectID": "cont_indir_free_final_time.html#free-final-time-and-free-final-state",
    "href": "cont_indir_free_final_time.html#free-final-time-and-free-final-state",
    "title": "Optimal control with a free final time",
    "section": "Free final time and free final state",
    "text": "Free final time and free final state\nWe start by considering the optimal control problem where both the final time and the state at this final time are subject to optimization. Later we will consider modification in which the final state is constrained in one way or another.\n\nFree end in calculus of variations\nWe adhere to our style of developing the results within the framework of calculus of variations first. The problem we are going to solve is visualized in Fig. 1.\n\n\n\n\n\n\nFigure 1: Optimizing over functions with one of the end points of the interval set free\n\n\n\nThe key trick is that stretching or shrinking the interval of the independent variable is done by perturbing the stationary value of the right end b of the interval in proportion to the same \\alpha we use to perturb the functions y and y'. That is, b is perturbed by \\Delta b = \\alpha \\Delta x and the perturbed cost functional is then \nJ(y^\\star+{\\color{blue}\\alpha} \\eta) = \\int_a^{{\\color{red}b}+{\\color{blue}\\alpha}\\Delta x} L(x,y^\\star +{\\color{blue}\\alpha} \\eta,(y^\\star )'+{\\color{blue}\\alpha} \\eta')\\text{d}x.\n\n\n\n\n\n\n\nb is the stationary value of the right end of the interval\n\n\n\nWe do not use b^\\star for the stationary value of the right end of the interval in favour of notational simplicity. But this comes at the cost of losing a bit of consistency in the notation.\n\n\nNote that we have a minor technical problem here since the function y^\\star is only defined on the interval [a,b]. But there is an easy fix: we define a continuation of the function to the right of b in the form of a linear approximation given by the derivative of y^\\star at b. We will exploit it in a while.\nNow, in order to find the variation \\delta J, we can proceed by fitting the Taylor’s expansion of the above perturbed cost function to the general Taylor’s expansion and identifying the first-order term in \\alpha. Equivalently, we can use the earlier stated fact that \n\\delta J = \\left.\\frac{\\text{d}}{\\text{d}\\alpha}J(y^\\star +\\alpha\\eta)\\right|_{\\alpha=0}\\alpha.\n\nIn order to find the derivative, we observe that the variable with respect to which we are differentiating is included in the upper bound of the integral. Therefore we cannot just change the order of differentiation and integration. This situation is handled by the well-known Leibniz rule for differentiation under the integral sign (look it up yourself in the full generality). In our case it leads to \n\\left.\\frac{\\text{d}}{\\text{d}\\alpha}J(y^\\star +\\alpha\\eta)\\right|_{\\alpha=0} = \\int_a^{b} \\left( L_y-\\frac{\\text{d}}{\\text{d}x}L_{y'}\\right)\\eta(x)\\text{d}x + \\left.L_{y'}\\right|_{b}\\eta(b) + \\left.L\\right|_{b}\\Delta x,\n which after multiplication by \\alpha gives the variation of the functional \\boxed{\n\\delta J = \\int_a^{b} \\left( L_y-\\frac{\\text{d}}{\\text{d}x}L_{y'}\\right)\\delta y(x)\\text{d}x + \\left.L_{y'}\\right|_{b}\\delta y(x) + \\left.L\\right|_{b}\\underbrace{\\Delta x\\alpha}_{\\Delta b},}\n where the first two terms on the right are already known to us, and the only new term is the third one. The reasoning now is pretty much the same as it was in the fixed-interval free-end case. We argue that among the variations \\delta y there are also those that vanish at b, hence the conditions must be satisfied even if the last two terms are zero. But then the integral must be zero, which gives rise to the familiar Euler-Lagrange equation. The last two terms must together be zero and it does not hurt to rewrite them in a complete notation to dispell any ambiquity \\boxed{\n\\left.\\frac{\\partial L(x,y(x),y'(x))}{\\partial y'}\\right|_{x=b}\\delta y(b) + \\left.L(x,y(x),y'(x))\\right|_{x=b}\\Delta b = 0.}\n\\tag{1}\nNow, in order to get some more insight into the above condition, the relation between the participating objects can be further explored. We will do it using Fig. 1 but we augment it a bit with a few labels, see Fig. 2 below.\n\n\n\n\n\n\nFigure 2: Optimizing over curves with one end of the interval of the independent variable x set free and relaxing also the value of the function there\n\n\n\nNote that we have included a brand new label here, namely, \\mathrm{d}y_\\mathrm{f} for the perturbation of the value of the function y() at the end of the interval (taking into consideration that the length of the interval can change as well). We can now write \ny^\\star (b+\\Delta b) + \\delta y(b+\\Delta b) = y^\\star (b)+\\mathrm{d}y_\\mathrm{f},\n which after approximating each term with the first two terms of its Taylor expansion gives \n\\cancel{y^\\star (b)}+{y^\\star }'(b)\\Delta b + \\delta y(b)+\\cancel{\\delta'(b) \\Delta b} = \\cancel{y^\\star (b)}+\\mathrm{d}y_\\mathrm{f}.\n\nNote that the third product on the left can be neglected since it contains two terms that are both of order one in the perturbation variable \\alpha. In other words, we approximate \\delta y(b+\\Delta b) by \\delta y(b). In addition, the term y^\\star (b) can be subtracted from both sides. From what remains after these cancelations, we can conclude that \n{y^\\star }'(b)\\Delta b + \\delta y(b) = \\mathrm{d}y_\\mathrm{f},\n or, equivalently, \n\\delta y(b) = \\mathrm{d}y_\\mathrm{f} - {y^\\star }'(b)\\Delta b.\n\nWe will now substitute this into the general form of the boundary equation in () \nL_{y'}(b,y(b),y'(b))\\,\\left[\\mathrm{d}y_\\mathrm{f} - {y^\\star }'(b)\\Delta b\\right] + L(b,y(b),y'(b))\\Delta b = 0.\n\\tag{2}\nCollecting now the terms with the two perturbation variables \\mathrm d y_\\mathrm{f} and \\Delta b, we reformat the above expression into \\boxed{\nL_{y'}(b,y(b),y'(b))\\mathrm{d}y_\\mathrm{f} + \\left[L(b,y(b),y'(b))-L_{y'}(b,y(b),y'(b)) {y^\\star }'(b)\\right]\\Delta b = 0.}\n\\tag{3}\nNow, since \\mathrm d y_\\mathrm{f} and \\Delta b are assumed independent, the corresponding terms must be simultaneously and independently equal zero, that is, \n\\begin{aligned}\nL_{y'}(b,y(b),y'(b)) &= 0,\\\\\nL(b,y(b),y'(b))-L_{y'}(b,y(b),y'(b)) {y^\\star }'(b) &= 0.\n\\end{aligned}\n\nNote that the first condition actually constitutes n scalar conditions whereas the second one is just a scalar condition itself, hence, n+1 boundary conditions.\n\n\nOptimal control setting\nLet’s now switch back to the optimal control setting with t as the independent variable. Recall that the optimal control problem is \n\\min_{\\bm x(),\\bm u(),t_\\mathrm{f}} \\left[\\phi(\\bm x(t_\\mathrm{f}))+\\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}L(\\bm x,\\bm u,t)\\text{d}t\\right].\n subject to \n\\dot{\\bm x}(t)= \\mathbf f(\\bm x,\\bm u,t),\\qquad  \\bm x(t_\\mathrm{i}) = \\mathbf x_\\mathrm{i}.\n\nWe have already seen that the integrand of the augmented cost function now contains not only the term that corresponds to the Lagrange multiplier but also the term that penalizes the state at the final time, that is, \nL^\\mathrm{aug}(\\bm x,\\bm u,\\boldsymbol \\lambda, t) = L(\\bm x,\\bm u,t) + \\underbrace{\\frac{\\partial \\phi}{\\partial t} + (\\nabla_{\\bm x}\\phi)^\\top \\frac{\\mathrm d \\bm x }{\\mathrm d t}}_{\\frac{\\mathrm d \\phi(\\bm x(t), t)}{\\mathrm d t}} + \\boldsymbol{\\lambda}^\\top (\\dot{\\bm{x}} - \\mathbf f(\\bm x,\\bm u,t))\n\\tag{4}\nWe then rewrite the boundary conditions Eq. 3 as \\boxed{\n\\left.(\\nabla_\\mathbf{x}\\phi+\\boldsymbol \\lambda)\\right|_{t=t_\\mathrm{f}}^\\top \\mathrm{d}\\mathbf{x}_\\mathrm{f} + \\left.\\left(L+\\frac{\\partial \\phi}{\\partial t}-\\boldsymbol \\lambda^\\top  f(\\bm x, \\bm u, t)\\right)\\right|_{t=t_\\mathrm{f}}\\mathrm d t_\\mathrm{f}.}\n\nSince here we assume that the final time and the state at the final time are independent, this single conditions breaks down into two boundary conditions\n\n\\begin{aligned}\n\\nabla_\\mathbf{x}\\phi(x(t_\\mathrm{f}),t_\\mathrm{f})+\\boldsymbol \\lambda(t_\\mathrm{f}) &=0\\\\\nL(x(t_\\mathrm{f}),u(t_\\mathrm{f}),t_\\mathrm{f})+\\frac{\\partial \\phi(\\mathbf{x}(t_\\mathrm{f}),t_\\mathrm{f})}{\\partial t}-\\boldsymbol \\lambda^\\top (t_\\mathrm{f}) f(\\bm x(t_\\mathrm{f}), \\bm u(t_\\mathrm{f}), t_\\mathrm{f}) &=0.\n\\end{aligned}\n\nThe first one is actually representing n scalar conditions, the second one is just a single scalar condition. Hence, altogether we have n+1 boundary conditions.\nLet’s try to get some more insight into this. Let’s assume now that the term penalizing the state at the final time does not explicitly depend on time, that is, \\frac{\\partial \\phi}{\\partial t}=0. Then the boundary condition modifies to \n\\left.(\\nabla_\\mathbf{x}\\phi+\\boldsymbol \\lambda)\\right|_{t=t_\\mathrm{f}}^\\top \\mathrm{d}\\mathbf{x}_\\mathrm{f} + \\left.\\left(L-\\boldsymbol \\lambda^\\top  f(\\bm x, \\bm u, t)\\right)\\right|_{t=t_\\mathrm{f}}\\mathrm d t_\\mathrm{f},\n which can be rewritten as \n\\left.(\\nabla_\\mathbf{x}\\phi+\\boldsymbol \\lambda)\\right|_{t=t_\\mathrm{f}}^\\top \\mathrm{d}\\mathbf{x}_\\mathrm{f} - \\left.H(\\bm x,\\bm u,\\boldsymbol \\lambda,t)\\right|_{t=t_\\mathrm{f}}\\mathrm d t_\\mathrm{f},\n which, in turn, enforces the scalar boundary condition (on top of those other n conditions) \\boxed{\n\\left.H(\\bm x(t),\\bm u(t),\\boldsymbol \\lambda(t),t)\\right|_{t=t_\\mathrm{f}}\\mathrm d t_\\mathrm{f}=0}.\n\nThis is an observation that is worth memorizing—for a free final time optimal control problem, Hamiltonian vanishes at the end of the time interval.\nLet’s now add one more observation. We could have mentioned it even in the previous lecture since it is a general property of a Hamiltonian evaluated along the optimal solution—the total derivative of a Hamiltonian (evaluated along the solution) with respect to time is equal to its partial derivative with respect to time: \n\\frac{\\mathrm{d}H}{\\mathrm{d}t} = \\underbrace{\\frac{\\partial H}{\\partial \\bm x}}_{-\\dot{\\boldsymbol \\lambda}}\\frac{\\mathrm{d}\\bm x}{\\mathrm{d}t} + \\underbrace{\\frac{\\partial H}{\\partial \\boldsymbol \\lambda}}_{\\dot{\\mathbf{x}}}\\frac{\\mathrm{d}\\boldsymbol \\lambda}{\\mathrm{d}t} + \\underbrace{\\frac{\\partial H}{\\partial \\bm u}}_{0}\\frac{\\mathrm{d}\\bm u}{\\mathrm{d}t} + \\frac{\\partial H}{\\partial t} = \\frac{\\partial H}{\\partial t}.\n\nNow, if neither the system equations nor the optimal control cost function depend on time, that is, if \\frac{\\partial H}{\\partial t}=0, the Hamiltonian remains constant along the optimal solution (trajectory), that is, \\boxed{\nH(\\bm x(t),\\bm u(t),\\boldsymbol \\lambda(t)) = \\mathrm{const.}\\quad \\forall t}\n\nCombined with the previous result (boundary value of H at the end of the free time interval is zero), we obtain the powerful conclusion that the Hamiltonian evaluated alon the optimal trajectory is always zero in the free final time scenario: \\boxed{\nH(\\bm x(t),\\bm u(t),\\boldsymbol \\lambda(t)) = 0\\quad \\forall t}\n\nThis is a pretty insightful piece of information. Since some (numerical) techniques for optimal control are based on iterative minimization of a Hamiltonian, here we already know the minimum value.\n\n\nThe boundary condition for the other definition of Hamiltonian\nIn the previous lecture/chapter we already discussed the unfortunate discrepancy in the definition of Hamiltonian in the literature. Perhaps there is no need to come back to this topic because you are now aware of the problem, but we just want to have the formulas corresponding to the other definition of the Hamiltonian at hand. So, if instead of Eq. 4 we write the augmented Lagrangian as \n\\hat L^\\mathrm{aug}(\\bm x,\\bm u,\\hat{\\boldsymbol \\lambda}, t) = L(\\bm x,\\bm u,t) + \\underbrace{\\frac{\\partial \\phi}{\\partial t} + (\\nabla_{\\bm x}\\phi)^\\top \\frac{\\mathrm d \\bm x }{\\mathrm d t}}_{\\left.\\frac{\\mathrm d \\phi(\\bm x(t), t)}{\\mathrm d t}\\right|_{t=t_{\\mathrm f}}} + \\hat{\\boldsymbol{\\lambda}}^\\top  (f(\\bm x,\\bm u,t) - \\dot{\\mathbf{x}}),\n the boundary condition would then modify to \\boxed{\n\\left.(\\nabla_\\mathbf{x}\\phi-\\hat{\\boldsymbol \\lambda})\\right|_{t=t_\\mathrm{f}}^\\top \\mathrm{d}\\mathbf{x}_\\mathrm{f} + \\left.\\left(L+\\frac{\\partial \\phi}{\\partial t}+\\hat{\\boldsymbol \\lambda}^\\top  f(\\bm x, \\bm u, t)\\right)\\right|_{t=t_\\mathrm{f}}\\mathrm d t_\\mathrm{f},}\n which can be rewritten in the case of \\frac{\\partial \\phi(\\mathbf{x}(t_\\mathrm{f}),t_\\mathrm{f})}{\\partial t}=0 and using the alternative definition of the Hamiltonian \\hat H = L+\\hat{\\boldsymbol \\lambda}^\\top  \\mathbf f as \n\\left.(\\nabla_\\mathbf{x}\\phi-\\hat{\\boldsymbol \\lambda})\\right|_{t=t_\\mathrm{f}}^\\top \\mathrm{d}\\mathbf{x}_\\mathrm{f} + \\left.\\hat H(\\bm x,\\bm u,\\hat{\\boldsymbol \\lambda},t)\\right|_{t=t_\\mathrm{f}}\\mathrm d t_\\mathrm{f},",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Optimal control with a free final time"
    ]
  },
  {
    "objectID": "cont_indir_free_final_time.html#free-final-time-but-the-final-state-on-a-prescribed-curve",
    "href": "cont_indir_free_final_time.html#free-final-time-but-the-final-state-on-a-prescribed-curve",
    "title": "Optimal control with a free final time",
    "section": "Free final time but the final state on a prescribed curve",
    "text": "Free final time but the final state on a prescribed curve\n\nCalculus of variations setting\nWe will now investigate the case when the final value of the solution y(x) is to be on the curve described by \\psi(x), that is \ny^\\star (b+\\Delta b) + \\delta y(b+\\Delta b) = \\psi(b+\\Delta b).\n\\tag{5}\nThis corresponds to the situation depicted in Fig. 3.\n\n\n\n\n\n\nFigure 3: The value of the function at the free end of the interval is constrained by another function\n\n\n\nWe already discussed the terms on the left. What is new here is the term on the right. It can also be approximated by the first two terms in the Taylor’s expansion \n\\psi(b+\\Delta b) = \\psi(b) + \\psi'(b)\\Delta b.\n\nTherefore, we can expand Eq. 5 into \n\\cancel{y^\\star (b)} + (y^\\star )'(b)\\Delta b + \\delta y(b) = \\underbrace{\\cancel{y^\\star (b)}}_{\\psi(b)}+\\psi'(b)\\Delta b,\n from which we can express \\delta y(b) as \n\\delta y(b) = \\psi'(b)\\Delta b - (y^\\star )'(b)\\Delta b,\n and substitute to the boundary condition Eq. 1, which after cancelling the common \\Delta b term yields \\boxed{\nL_{y'}(b,y(b),y'(b))\\,\\left[\\psi'(b)-y'(b)\\right] + L(b,y(b),y'(b)) = 0.}\n\\tag{6}\nThis is just one scalar boundary conditions. But the original n conditions that the state that y(x) = \\psi(x) at the right end of the interval must be added. Altogether, we have n+1 boundary conditions.\nThe above single equation Eq. 6 is called transversality condition for the reason to be illuminated by the next example.\n\nExample 1 To get an insight, consider again the minimum distance problem. This time we want to find the shortest distance from a point to a curve given by \\psi(x). The answer is intuitive, but let us see what our rigorous tools offer here. The EL equation stays intact, therefore we know that the shortest path is a line. It starts at (a,0) but in order to determine its end, we need to invoke the other boundary condition. Remember that the Lagrangian is \nL = \\sqrt{1+(y')^2}\\text{d}x\n and \nL_{y'} = \\frac{y'}{\\sqrt{1+(y')^2}}\\text{d}x.\n\nThe transversality condition boils down to \n1+y'(b)\\psi'(b)=0,\n which can also be visualized using vectors in the plane \n\\begin{bmatrix}1 & y'(b)\\end{bmatrix}\\, \\begin{bmatrix}1 \\\\ \\psi'(b)\\end{bmatrix} = 0.\n\nThe interpretation of this result is that our desired curve y hits the target curve \\psi in a perpendicular (transverse) direction.\n\nUnderstanding the boundary conditions is crucial. Let us have yet another look at the result just derived. It can be written as \\boxed{\n\\left.L_{y'}\\psi'\\right|_{b} - \\left.H\\right|_{b} = 0.}\n\nIt follows that for a free length of the interval and fixed value of the variable at the end of the interval, in which \n\\psi(x) = c,\\; c\\in\\mathbb R,\n the transversality condition simplifies to \\boxed{\nH(b) = 0.}\n\n\n\nOptimal control setting\nOnce again, let’s recall that the optimal control problem is \n\\min_{\\bm x(\\cdot),\\bm u(\\cdot),t_\\mathrm{f}} \\left[\\phi(\\bm x(t_\\mathrm{f}))+\\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}L(\\bm x,\\bm u,t)\\text{d}t\\right].\n subject to \n\\begin{aligned}\n\\dot{\\bm x}(t)&= \\mathbf f(\\bm x,\\bm u,t),\\\\  \n\\bm x(t_\\mathrm{i}) &= \\mathbf x_\\mathrm{i}\\\\\n\\bm x(t_\\mathrm{f}) &= \\boldsymbol \\psi(t_\\mathrm{f}).\n\\end{aligned}\n\nTranslating the above derived transversality condition from the domain (and notation) of calculus of variations into the optimal control setting gives \\boxed{\n\\left.(\\nabla_\\mathbf{x}\\phi+\\boldsymbol \\lambda)^\\top \\dot{\\boldsymbol\\psi}(t) + L+\\frac{\\partial \\phi}{\\partial t}-\\boldsymbol \\lambda^\\top  \\mathbf f(\\bm x, \\bm u, t)\\right|_{t=t_\\mathrm{f}}=0.}\n\nOf course, on top of this single condition, the 2n boundary conditions shown above must be added \\boxed{\n\\begin{aligned}\n\\bm x(t_\\mathrm{i}) &= \\mathbf x_\\mathrm{i}\\\\\n\\bm x(t_\\mathrm{f}) &= \\boldsymbol \\psi(t_\\mathrm{f}).\n\\end{aligned}}",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Optimal control with a free final time"
    ]
  },
  {
    "objectID": "discr_dir_mpc_references.html",
    "href": "discr_dir_mpc_references.html",
    "title": "References",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "References"
    ]
  },
  {
    "objectID": "cont_numerical_goals.html",
    "href": "cont_numerical_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Enumerate numerical method for indirect approach to continuous-time optimal control (gradient method, shooting, multiple shooting, collocation). Explain the essence of individual methods.\nEnumerate numerical method for indirect approach to continuous-time optimal control (direct shooting, direct multiple shooting, direct collocation). Explain the essence of individual methods.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_numerical_goals.html#knowledge-remember-and-understand",
    "href": "cont_numerical_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Enumerate numerical method for indirect approach to continuous-time optimal control (gradient method, shooting, multiple shooting, collocation). Explain the essence of individual methods.\nEnumerate numerical method for indirect approach to continuous-time optimal control (direct shooting, direct multiple shooting, direct collocation). Explain the essence of individual methods.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_numerical_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "cont_numerical_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nSolve a continuous-time optimal control problem for a nonlinear system using a direct method (at least using the method of direct collocation).",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_indir_fuel_optimal.html",
    "href": "cont_indir_fuel_optimal.html",
    "title": "Fuel-optimal control",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "discr_dir_mpc_software.html",
    "href": "discr_dir_mpc_software.html",
    "title": "Numerical solvers for MPC",
    "section": "",
    "text": "The extra features needed for MPC:\n\nwarmstarting requires fesibility of the previous solution. Some methods the iterations may temporarily lose feasibility, which can be a problem if only a fixed number of iterations is allowed (in favor of predictable timing).\n…\n\nA curated list of QP solvers is maintained at https://github.com/qpsolvers/qpbenchmark. Below are a few most popular from the open-source domain. Most if not all of them can be interfaced from various programming languages.\n\nqpOASES\nOSQP\nDAQP\nqpSWIFT\nProxQP\nPiQP\nECOS\nHPIPM",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Numerical solvers for MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_software.html#qp-solvers-suitable-for-linear-mpc",
    "href": "discr_dir_mpc_software.html#qp-solvers-suitable-for-linear-mpc",
    "title": "Numerical solvers for MPC",
    "section": "",
    "text": "The extra features needed for MPC:\n\nwarmstarting requires fesibility of the previous solution. Some methods the iterations may temporarily lose feasibility, which can be a problem if only a fixed number of iterations is allowed (in favor of predictable timing).\n…\n\nA curated list of QP solvers is maintained at https://github.com/qpsolvers/qpbenchmark. Below are a few most popular from the open-source domain. Most if not all of them can be interfaced from various programming languages.\n\nqpOASES\nOSQP\nDAQP\nqpSWIFT\nProxQP\nPiQP\nECOS\nHPIPM",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Numerical solvers for MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_software.html#higher-level-tools",
    "href": "discr_dir_mpc_software.html#higher-level-tools",
    "title": "Numerical solvers for MPC",
    "section": "Higher-level tools",
    "text": "Higher-level tools\n\nacados – free & open source; interfaces to Octave/Matlab, Python, C++.\nCasADi – free & open source; interfaces to Octave/Matlab, Python, C++.\nModelPredictiveControl.jl – free & open source; Julia.\nModel Predictive Control Toolbox for Matlab – commercial (by The Mathworks); Matlab; the generated C code follows industrial standards.\nMultiparametric Toolbox 3 (MPT3) – free & open source; Matlab.\nYalmip – free & open source; Matlab.\nForcesPro – commercial (by Embotech).",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Numerical solvers for MPC"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html",
    "href": "opt_theory_reformulations.html",
    "title": "Problem reformulations",
    "section": "",
    "text": "There is bag of trick that can be used to reformulate an optimization problem into an equivalent a form that is more suitable for a theoretical analysis or a particular numerical solver. Here we only pick a few.",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#maximization-into-minimization",
    "href": "opt_theory_reformulations.html#maximization-into-minimization",
    "title": "Problem reformulations",
    "section": "Maximization into minimization",
    "text": "Maximization into minimization\nGiven a function f(\\bm x), we can maximize it by minimizing -f(\\bm x).",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#equality-into-inequality-constraints",
    "href": "opt_theory_reformulations.html#equality-into-inequality-constraints",
    "title": "Problem reformulations",
    "section": "Equality into inequality constraints",
    "text": "Equality into inequality constraints\nAs a matter of fact, we can declare an NLP problem only inequality constraints as the most general one. This is because we can always transform an equality constraint into two inequality constraints. Given an equality constraint h(\\bm x) = 0, we can write it as h(\\bm x) \\leq 0 and -h(\\bm x) \\leq 0, that is,\n\n\\underbrace{\\begin{bmatrix}\nh(\\bm x) \\\\\n-h(\\bm x)\n\\end{bmatrix}}_{\\mathbf g(\\bm x)} \\leq \\mathbf 0.\n\nOn the other hand, it is typically useful to keep the equality constraints explicit in the problem formulation for the benefit of theoretical analysis, numerical methods and convenience of the user/modeller.",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#inequality-into-sort-of-equality-constraints",
    "href": "opt_theory_reformulations.html#inequality-into-sort-of-equality-constraints",
    "title": "Problem reformulations",
    "section": "Inequality into “sort-of” equality constraints",
    "text": "Inequality into “sort-of” equality constraints\nConsider the inequality constraint g(\\bm x) \\leq 0. By introducing a slack variable s and imposing the nonnegativity condition, we can turn the inequality into the equality g(\\bm x) + s = 0. Well, we have not completely discarded an inequality because now we have s \\geq 0. But this new problem may be better suited for some theoretical analysis or numerical methods.\nIt is also possible to express the nonnegativity constraint implicitly by considering an unrestricted variable s and using it within the inequality through its square s^2:\n\ng(\\bm x) + s^2 = 0.",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#linear-cost-function-always-possible",
    "href": "opt_theory_reformulations.html#linear-cost-function-always-possible",
    "title": "Problem reformulations",
    "section": "Linear cost function always possible",
    "text": "Linear cost function always possible\nGiven a cost function f(\\bm x) to be minimized, we can always upper-bound it by a new variable \\gamma accompanied by a new constraint f(\\bm x) \\leq \\gamma and then minimize just \\gamma \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm{x}\\in\\mathbb R^n, \\gamma\\in\\mathbb R} & \\quad \\gamma \\\\\n\\text{subject to} & \\quad f(\\bm x) \\leq \\gamma.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#absolute-value",
    "href": "opt_theory_reformulations.html#absolute-value",
    "title": "Problem reformulations",
    "section": "Absolute value",
    "text": "Absolute value\nConsider an optimization problem in which the cost function contains the absolute value of a variable \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\sum_i c_i|x_i|\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x \\geq \\mathbf b.\n\\end{aligned}\n\nWe also impose the restriction that all the coefficients c_i are nonnegative. The cost function is then a sum of piecewise linear convex function, which can be shown to be convex.\nThe trouble with the absolute value function is that it is not linear, it is not even smooth. And yet, as we will see below, this optimization with the absolute value can be reformulated as a linear program.\nOne possible reformulation introduces two new nonnegative (vector) variables \\bm x^+\\geq 0 and \\bm x^-\\geq 0, with which the original variables can be expressed as x_i = x_i^+ - x_i^-, \\; i=1, \\ldots, n. The cost function can then be written as \\sum c_i|x_i| = \\sum_i c_i (x_i^+ + x_i^-).\nThis may look surprising (and incorrect) at first, but we argue that at an optimum, x_i^+ or x_i^- must be zero. Otherwise we could subtract (in case c_i&gt;0) the same amount from/to both, which would not change the satisfaction of the constraints (this modification cancels in x_i = x_i^+ - x_i^-), and the cost would be further reduced.\nThe LP in the standard form then changes to\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x^+\\in \\mathbb R^n, \\bm x^-\\in \\mathbb R^n} &\\quad \\mathbf c^\\top (\\bm x^+ + \\bm x^-)\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x^+ - \\mathbf A \\bm x^- \\geq \\mathbf b,\\\\\n&\\quad \\bm x^+ \\geq \\mathbf 0,\\\\\n&\\quad \\bm x^- \\geq \\mathbf 0.\n\\end{aligned}\n\nAnother possibility is to exploit the reformulation of z_i = |x_i| as x_i\\leq z and -x_i\\leq z. The original problem then transforms into\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm z\\in \\mathbb R^n, \\bm x\\in \\mathbb R^n} &\\quad \\mathbf c^\\top \\bm z\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x \\geq \\mathbf b,\\\\\n&\\qquad \\bm x \\leq \\bm z,\\\\\n&\\quad -\\bm x \\leq \\bm z.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#piecewise-linear",
    "href": "opt_theory_reformulations.html#piecewise-linear",
    "title": "Problem reformulations",
    "section": "Piecewise linear",
    "text": "Piecewise linear\n#TODO",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#quadratic",
    "href": "opt_theory_reformulations.html#quadratic",
    "title": "Problem reformulations",
    "section": "Quadratic",
    "text": "Quadratic\n#TODO",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_algo_hw.html",
    "href": "opt_algo_hw.html",
    "title": "Homework",
    "section": "",
    "text": "In this assignment, you will implement the BFGS (Broyden–Fletcher–Goldfarb–Shanno) algorithm—a popular quasi-Newton method for unconstrained optimization—in the Julia programming language. Specifically, you will be implementing the inverse Hessian update variant, which can be, e.g., found in the Broyden–Fletcher–Goldfarb–Shanno algorithm Wikipedia page.\nThe instructions for the assignment are as follows\n\nYour solution should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\nYou should use Automatic Differentiation (AD) to compute the gradients of the objective function. You can use the Zygote package for this purpose.\nYou should use backtracking line search with the Armijo condition to find the step size, see Approximate line search – backtracking.\nThe implementation should be based on the provided template below.\n\n\nusing LinearAlgebra\nusing Zygote # for Automatic Differentiation (AD)\n\n\"\"\"\n    BFGS(J::Function, x₀::Vector{T}, ε::T=sqrt(eps(T)), maxiter::Int=50) where T &lt;: AbstractFloat\n\nPerforms unconstrained optimization using the Broyden–Fletcher–Goldfarb–Shanno (BFGS) quasi-Newton method.\n\n# Arguments\n- `f`: The objective function to be minimized. It should take a vector `x` and return a scalar.\n- `x₀`: A vector representing the initial guess for the optimization. The element type `T` must be a subtype of `AbstractFloat`.\n- `ε`: The convergence tolerance for the gradient norm (default: `√eps(T)`, where `T` is the element type of `x₀`).\n- `maxiter`: The maximum number of iterations allowed (default: 50).\n- `verbose`: A boolean indicating whether to print the optimization progress (default: `false`).\n\n# Returns\nA tuple containing:\n- A vector of type `Vector{T}` representing the optimized solution.\n- The optimal function value at the solution.\n- The number of iterations performed.\n- A symbol indicating the termination status (`:converged` or `:maxiter_reached`)\n\nThis implementation supports arbitrary precision arithmetic if `x₀` is of a higher precision type (e.g., `BigFloat`).\n\"\"\"\nfunction BFGS(f::Function, x₀::Vector{T}, ε::T=√(eps(T)), maxiter::Int=50; verbose::Bool=false) where T &lt;: AbstractFloat\n\n    xₖ = x₀ # Initial guess\n\n    # Preallocate memory - all the variables that will be used in the loop\n    xₖ₊₁ = similar(xₖ)\n    ∇fₖ = zeros(T, length(xₖ))\n    ∇fₖ₊₁ = similar(∇fₖ)\n    Hₖ = Matrix{T}(I, length(x₀), length(x₀)) \n    Hₖ₊₁ = similar(Hₖ)\n    yₖ = similar(∇fₖ)\n    sₖ = similar(xₖ)\n    pₖ = similar(xₖ)\n\n    # TODO Compute ∇f(xₖ) using AD\n    # ∇fₖ =\n\n    if norm(∇fₖ, Inf) &lt; ε # Convergence check - Inital guess is already optimal\n        return xₖ, f(xₖ), 0, :converged\n    end\n\n    Hₖ ./= norm(∇fₖ)  # Initial inverse Hessian approximation\n\n    for k = 1:maxiter\n\n        if norm(∇fₖ, Inf) &lt; ε # Convergence check\n            return xₖ, f(xₖ), k, :converged\n        end\n\n        if verbose\n            println(\"Iteration: \", k,  \" | f(xₖ): \", f(xₖ),\" | ǁ∇f(xₖ)ǁ∞: \", norm(∇fₖ, Inf))\n        end\n\n        # TODO Complete the BFGS update, i.e., compute xₖ₊₁, ∇fₖ₊₁, and Hₖ₊₁\n        # For the linesearch use the Armijo condition (https://hurak.github.io/orr/opt_algo_unconstrained.html#approximate-line-search-backtracking)\n\n        # Prepare for the next iteration\n        xₖ .= xₖ₊₁\n        ∇fₖ .= ∇fₖ₊₁\n        Hₖ .= Hₖ₊₁\n\n    end\n\n    return xₖ, f(xₖ), maxiter, :maxiter_reached\n\nend\n\nTo test your implementation, take a look at common test functions in the Optimization Test Functions Wikipedia page. For example, you can use the Rosenbrock function: \nf(\\bm{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2\\right],\n which has the global minimum at \\bm{x} = (1, 1, \\ldots, 1).",
    "crumbs": [
      "2. Optimization – algorithms",
      "Homework"
    ]
  },
  {
    "objectID": "opt_algo_hw.html#implementation-of-the-bfgs-method-for-unconstrained-optimization",
    "href": "opt_algo_hw.html#implementation-of-the-bfgs-method-for-unconstrained-optimization",
    "title": "Homework",
    "section": "",
    "text": "In this assignment, you will implement the BFGS (Broyden–Fletcher–Goldfarb–Shanno) algorithm—a popular quasi-Newton method for unconstrained optimization—in the Julia programming language. Specifically, you will be implementing the inverse Hessian update variant, which can be, e.g., found in the Broyden–Fletcher–Goldfarb–Shanno algorithm Wikipedia page.\nThe instructions for the assignment are as follows\n\nYour solution should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\nYou should use Automatic Differentiation (AD) to compute the gradients of the objective function. You can use the Zygote package for this purpose.\nYou should use backtracking line search with the Armijo condition to find the step size, see Approximate line search – backtracking.\nThe implementation should be based on the provided template below.\n\n\nusing LinearAlgebra\nusing Zygote # for Automatic Differentiation (AD)\n\n\"\"\"\n    BFGS(J::Function, x₀::Vector{T}, ε::T=sqrt(eps(T)), maxiter::Int=50) where T &lt;: AbstractFloat\n\nPerforms unconstrained optimization using the Broyden–Fletcher–Goldfarb–Shanno (BFGS) quasi-Newton method.\n\n# Arguments\n- `f`: The objective function to be minimized. It should take a vector `x` and return a scalar.\n- `x₀`: A vector representing the initial guess for the optimization. The element type `T` must be a subtype of `AbstractFloat`.\n- `ε`: The convergence tolerance for the gradient norm (default: `√eps(T)`, where `T` is the element type of `x₀`).\n- `maxiter`: The maximum number of iterations allowed (default: 50).\n- `verbose`: A boolean indicating whether to print the optimization progress (default: `false`).\n\n# Returns\nA tuple containing:\n- A vector of type `Vector{T}` representing the optimized solution.\n- The optimal function value at the solution.\n- The number of iterations performed.\n- A symbol indicating the termination status (`:converged` or `:maxiter_reached`)\n\nThis implementation supports arbitrary precision arithmetic if `x₀` is of a higher precision type (e.g., `BigFloat`).\n\"\"\"\nfunction BFGS(f::Function, x₀::Vector{T}, ε::T=√(eps(T)), maxiter::Int=50; verbose::Bool=false) where T &lt;: AbstractFloat\n\n    xₖ = x₀ # Initial guess\n\n    # Preallocate memory - all the variables that will be used in the loop\n    xₖ₊₁ = similar(xₖ)\n    ∇fₖ = zeros(T, length(xₖ))\n    ∇fₖ₊₁ = similar(∇fₖ)\n    Hₖ = Matrix{T}(I, length(x₀), length(x₀)) \n    Hₖ₊₁ = similar(Hₖ)\n    yₖ = similar(∇fₖ)\n    sₖ = similar(xₖ)\n    pₖ = similar(xₖ)\n\n    # TODO Compute ∇f(xₖ) using AD\n    # ∇fₖ =\n\n    if norm(∇fₖ, Inf) &lt; ε # Convergence check - Inital guess is already optimal\n        return xₖ, f(xₖ), 0, :converged\n    end\n\n    Hₖ ./= norm(∇fₖ)  # Initial inverse Hessian approximation\n\n    for k = 1:maxiter\n\n        if norm(∇fₖ, Inf) &lt; ε # Convergence check\n            return xₖ, f(xₖ), k, :converged\n        end\n\n        if verbose\n            println(\"Iteration: \", k,  \" | f(xₖ): \", f(xₖ),\" | ǁ∇f(xₖ)ǁ∞: \", norm(∇fₖ, Inf))\n        end\n\n        # TODO Complete the BFGS update, i.e., compute xₖ₊₁, ∇fₖ₊₁, and Hₖ₊₁\n        # For the linesearch use the Armijo condition (https://hurak.github.io/orr/opt_algo_unconstrained.html#approximate-line-search-backtracking)\n\n        # Prepare for the next iteration\n        xₖ .= xₖ₊₁\n        ∇fₖ .= ∇fₖ₊₁\n        Hₖ .= Hₖ₊₁\n\n    end\n\n    return xₖ, f(xₖ), maxiter, :maxiter_reached\n\nend\n\nTo test your implementation, take a look at common test functions in the Optimization Test Functions Wikipedia page. For example, you can use the Rosenbrock function: \nf(\\bm{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2\\right],\n which has the global minimum at \\bm{x} = (1, 1, \\ldots, 1).",
    "crumbs": [
      "2. Optimization – algorithms",
      "Homework"
    ]
  },
  {
    "objectID": "cont_indir_LQR_inf_horizon.html",
    "href": "cont_indir_LQR_inf_horizon.html",
    "title": "Indirect approach to LQR on an infinite time horizon using CARE",
    "section": "",
    "text": "When solving the initial (actually final) value problem \n- \\dot{\\bm S}(t) =  \\bm S(t) \\mathbf A + \\mathbf A^\\top \\bm S(t) + \\mathbf Q - \\bm S(t)\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\bm S(t), \\qquad \\bm S(t_\\mathrm{f}) = \\mathbf S_\\mathrm{f}\n for \\bm S(t) at time t\\rightarrow -\\infty, we recall that a steady (or settled) solution does not evolve any further, that is, \\dot{\\bm S}(t) = \\bm 0. Substituting this to the differential Riccati equation, we obtain the continuous-time algebraic Riccati equation (CARE) \\boxed{\n\\mathbf 0=  \\bm S \\mathbf A + \\mathbf A^\\text{T} \\bm S + \\mathbf Q - \\bm S\\mathbf B\\mathbf R^{-1}\\mathbf B^\\text{T} \\bm S.}\n\\tag{1}\nThis is a prominent equation in optimal control theory. Unline its discrete-time counterpart (the DARE), the CARE is obviously a quadratic equation in the matrix variable \\bm S.\nNumerical solution can be found using specialized solvers (see the section on software). But a surprisingly nontrivial question is: how does the solution to CARE relate to the limiting solution \\bm S_\\infty = \\lim_{t\\rightarrow -\\infty} \\bm S(t)? After all, the CARE is a quadratic equation, which even in the scalar case has more then one solution. For convenience, we write down the scalar version explicitly: \\boxed{\n0 =  2as + q - \\frac{b^2}{r}s^2.}\n\nWhich of the (at maximum) two real solutions is the one that we are looking for? We can afford to be rather short in the remaining analysis since the similarity to the discrete-time case is truly very strong. Thus we can conclude this section by merely stating that a unique stabilizing solution of the ARE exists if and only if the artificial system modelled by (\\mathbf A,\\sqrt{\\mathbf Q}) is detectable (or observable if we require positive definiteness of \\bm S(t)).\nOnce the solution \\bm S is found, the optimal control law is given by \n\\boxed{\n\\bm u(t) = -\\underbrace{\\mathbf R^{-1}\\mathbf B^\\top\\bm S}_{\\bm K}\\,\\bm x(t).}\n\n\nExample 1 (LQR on an infinite horizon)  \n\n\nShow the code\nusing ControlSystems\nusing LinearAlgebra\n\nn = 2\nm = 2\n\nA = rand(n,n)\nB = rand(n,m)\nC = Matrix{Float64}(I, n, n)\n\nQ = 100* Matrix{Float64}(I,n,n);\nR = Matrix{Float64}(I, m, m);\n\nK = lqr(A,B,Q,R)\n\nG = ss(A,B,C,0)\n\nu(x,t) = -K*x\n\nt = 0:0.1:5\nx₀ = [1,3]\n\ny, t, x, uout = lsim(G,u,t,x0=x₀)\n\nusing Plots\nplot(t,x',xlabel=\"t\",ylabel=\"x(t)\", label=[\"x₁\" \"x₂\"],linewidth=2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The simulated response of the system to a nonzero initial state using the time-invariant LQR computed by solving CARE\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "LQR on an infinite time horizon using the CARE"
    ]
  },
  {
    "objectID": "rocond_goals.html",
    "href": "rocond_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Give the general optimization criterion for the control-design procedure based on minimizing the \\mathcal H_\\infty-norm of the mixed sensitivity function.\nExplain the control design procedure based on minimizing the \\mathcal H_\\infty-norm of the linear fractional transformation (LFT) of a generalized plant and a stabilizing controller.\nExplain the control design procedure called 𝜇-synthesis and aiming at achieving robust stability in presence of structured uncertainty. Show how the requirement of robust performance can be included in this robust stabilization framework (the answer: by including an artificial uncertainty block corresponding to performance specs).",
    "crumbs": [
      "12. Robust control",
      "Learning goals"
    ]
  },
  {
    "objectID": "rocond_goals.html#knowledge-remember-and-understand",
    "href": "rocond_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Give the general optimization criterion for the control-design procedure based on minimizing the \\mathcal H_\\infty-norm of the mixed sensitivity function.\nExplain the control design procedure based on minimizing the \\mathcal H_\\infty-norm of the linear fractional transformation (LFT) of a generalized plant and a stabilizing controller.\nExplain the control design procedure called 𝜇-synthesis and aiming at achieving robust stability in presence of structured uncertainty. Show how the requirement of robust performance can be included in this robust stabilization framework (the answer: by including an artificial uncertainty block corresponding to performance specs).",
    "crumbs": [
      "12. Robust control",
      "Learning goals"
    ]
  },
  {
    "objectID": "rocond_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "rocond_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nDesign a robust controller by \\mathcal H_\\infty norm minimization (your task is actually just to formulate the control design problem as the optimization problem, the actual numerical job of optimization can be relegated to an available numerical solver). Namely, you should master the technique of control design by minimizing the \\mathcal H_\\infty norm of mixed sensitivity function but you should also be able to formulate (and use Matlab to solve) the general \\mathcal H_\\infty-optimal control problem. A closely related design procedure that you should master is that of 𝜇 synthesis.",
    "crumbs": [
      "12. Robust control",
      "Learning goals"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html",
    "href": "opt_algo_constrained.html",
    "title": "Algorithms for constrained optimization",
    "section": "",
    "text": "We keep adhering to our previous decision to focus on the algorithms that use derivatives. But even then the number of derivative-based algorithms for constrained optimization – considering both equality and inequality constraints – is huge. They can be classified in many ways. Here we choose the very pragmatic point of view of the immediate use within our course, and within the discipline of optimal control in general. It is certainly a bit narrow point of view, but it will get us going… In this viewpoint we admit inspiration by the overview paper [1]. And there is a wealth of literature providing a more rigorous classification, which we give references to.\nThere are essentially two types of optimization problems (aka mathematical programms) that dominate the discipline of optimal control:\nWe will therefore focus our discussion of methods to these two.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#quadratic-programming",
    "href": "opt_algo_constrained.html#quadratic-programming",
    "title": "Algorithms for constrained optimization",
    "section": "Quadratic programming",
    "text": "Quadratic programming\nWe consider the problem\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} &\\quad \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{c}^\\top\\bm{x}\\\\\n\\text{subject to} &\\quad \\mathbf A_\\text{eq} \\bm x = \\mathbf b_\\text{eq},\\\\\n&\\quad \\mathbf A_\\text{ineq} \\bm x \\leq \\mathbf b_\\text{ineq}.\n\\end{aligned}\n\n\nActive set methods\n#TODO\n\n\nInterior point methods\n#TODO\n\n\nFirst-order methods\nThe first-order methods (the methods using only the first derivatives) seem to be going through some rennaissance in the last two decades or so. Being computationally simpler than their higher-order counterparts (such as the Newton’s and Quasi-Newton methods), and enhanced with some clever acceleration modifications, they are the methods of choice in machine learning applications, where the size of the problems (the number of variables and the number of equations and inequalities) can easily reach millions and more. Although in optimal control we are typically not encountering optimization problems of this size, we can still benefit from the trend. While the size of the optimization problem can be medium or even small (a few dozens of variables and constraints), the time budget for its solution can be extremely small (easily bellow a millisecond, or even down to a few tens of microseconds). Furthermore, such optimization may be performed on some embedded hardware with limited resource. Computational simplicity of first-order methods (they do not rely on more advance linear algebra computations such as matrix decompositions) makes them particularly suited for these applications.\n\nProjected gradient method\nOne way to extend the standard gradient method to constraint optimization is to combine it with a suitable projection operator. The idea behind the algorithm is that after the standard gradient descent update, a projection onto the feasible set is performed.\nCommonly, an orthogonal projection is used, which is defined as \nP_\\mathcal{C}(x) \\coloneqq \\arg\\min_{\\bm y\\in\\mathcal{C}} \\|\\bm y - \\bm x\\|_2.\n\nFor a general set \\mathcal C, the projection can be computationaly expensive. But for some simple yet useful sets, the projection is trivial. The prominent example is a box (a multidimensional interval):\n\nfunction projection_on_box!(x,xₗ,xᵤ)\n    for i=1:length(x)\n        if x[i] &lt; xₗ[i]\n            x[i] = xₗ[i]\n        elseif x[i] &gt; xᵤ[i]\n            x[i] = xᵤ[i]\n        end\n    end\nend\n\nprojection_on_box! (generic function with 1 method)\n\n\nIn our implementation of the algorithm we use a fixed step lenght based on the maximum curvature of the Hessian.\n\n\nShow the code\nusing LinearAlgebra\n\nfunction projected_gradient_quadratic(Q,c,xₗ,xᵤ,x₀,ϵ,N)\n    x = x₀                           # initializing x\n    f(x) = 1/2*dot(x,Q,x)+dot(x,c)\n    ∇f(x) = Q*x+c                    # defining the gradient\n    L = maximum(diag(Q,0))           # maximum curvature (here we assume just a diagonal Q, otherwise max(eigvals))\n    α = 1/L                          # step length\n    k = 0\n    d = 1+ϵ                          # initial value of the distance between two solutions (epsilon plus whatever)\n    while (norm(d) &gt; ϵ/L)\n        xold = x\n        x = x - α*∇f(x)              # the step in the descent direction\n        projection_on_box!(x,xₗ,xᵤ)  # the projection of the descent step on the box\n        d = x-xold                   # the current step (after the projection)\n        k = k+1\n        if k &gt;= N\n         return f(x),x\n        end\n    end\n    return f(x),x\nend\n\n\nprojected_gradient_quadratic (generic function with 1 method)\n\n\n\n\nShow the code\nx₀ = [1.5, 1.5]     # the initial vector\n\nxₗ = [0.0, 0.0]     # the lower bound\nxᵤ = [2.0, 2.0]     # the upper bound\n\nQ = [1 0; 0 3]      # the positive definite matrix defining the quadratic form\nc = [1; 2]          # the vector defining the linear part\n\nϵ  = 1e-5           # the tolerance\nN  = 100;           # the maximum number of steps\n\n\n\n\nShow the code\nf_opt,x_opt = projected_gradient_quadratic(Q,c,xₗ,xᵤ,x₀,ϵ,N)\n\n\n(0.0, [0.0, 0.0])\n\n\nBelow we also give a bit more “decorated” version that produces the sequence of solutions that we can also plot.\n\n\nShow the code\nusing Printf\n\nfunction projected_gradient_quadratic(Q,c,xₗ,xᵤ,x₀,ϵ,N)\n    x = x₀                    # initializing x\n    X = x                     # the vector of vectors that will be output\n    f(x) = 1/2*dot(x,Q,x)+dot(x,c)\n    fx = f(x)\n    F = [fx,]\n    ∇f(x) = Q*x+c              # building the gradient\n    gx = ∇f(x)\n    L = maximum(diag(Q,0))    # maximum curvature (here I assume just diagonal Q, otherwise max(eigvals))\n    α = 1/L                   # step length\n    #α = 1/5                  # just to explore the behaviour when the step is longer or shorter than to the boundary\n    k = 0\n    d = 1\n    while (norm(d) &gt; ϵ/L)\n        k = k+1\n        xold = x\n        x = x - α*gx          # step in the descent direction\n        projection_on_box!(x,xₗ,xᵤ)\n        d = x-xold\n        @printf(\"iter = %3d   ||∇f(x)|| = %6.4e   f(x) = %6.4e\\n\",k,norm(gx),fx)\n        gx = ∇f(x)\n        fx = f(x)\n        X = hcat(X,x)\n        push!(F,fx)\n        if k &gt;= N\n         return F,X\n        end\n    end\n    return F,X\nend\n\n\nprojected_gradient_quadratic (generic function with 1 method)\n\n\n\nF,X = projected_gradient_quadratic(Q,c,xₗ,xᵤ,x₀,ϵ,N)\n\niter =   1   ||∇f(x)|| = 6.9642e+00   f(x) = 9.0000e+00\niter =   2   ||∇f(x)|| = 2.6034e+00   f(x) = 8.8889e-01\niter =   3   ||∇f(x)|| = 2.2879e+00   f(x) = 1.1728e-01\niter =   4   ||∇f(x)|| = 2.2361e+00   f(x) = 0.0000e+00\n\n\n([9.0, 0.8888888888888891, 0.117283950617284, 0.0, 0.0], [1.5 0.6666666666666667 … 0.0 0.0; 1.5 0.0 … 0.0 0.0])\n\n\n\n\nShow the code\nx1_grid = x2_grid = -2:0.01:4;\nf(x) = 1/2*dot(x,Q,x)+dot(x,c)\nz_grid = [f([x1,x2]) for x2=x2_grid, x1=x1_grid];\n\nxs = -Q\\c           # the stationary point of the unconstrained problem \n\nusing Plots\nplot(Shape([(2,2),(2,0),(0,0),(0,2),(2,2)]),opacity=0.2,label=\"bounds\")\ncontour!(x1_grid,x2_grid,z_grid)\nplot!(X[1,:],X[2,:],label=\"xₖ\",marker=:diamond,aspect_ratio=1)\nscatter!([x₀[1],],[x₀[2],],label=\"x₀\")\nscatter!([xs[1],],[xs[2],],label=\"x⋆ unconstrained\")\nxlabel!(\"x₁\");ylabel!(\"x₂\")\n#xlims!(-4,4); ylims!(-4,4)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSplitting methods\n#TODO",
    "crumbs": [
      "2. Optimization – algorithms",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#nonlinear-programming",
    "href": "opt_algo_constrained.html#nonlinear-programming",
    "title": "Algorithms for constrained optimization",
    "section": "Nonlinear programming",
    "text": "Nonlinear programming\n\nSequential quadratic programming (SQP)\nKKT conditions for a nonlinear program with equality constraints solved by Newton’s method. Interpretation: at each iteration, we solve a quadratic program (QP) with linear constraints.\n\n\nInterior point methods\n#TODO",
    "crumbs": [
      "2. Optimization – algorithms",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "dynamic_programming_goals.html",
    "href": "dynamic_programming_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Understand and explain the Bellman’s principle of optimality.\nShow how dynamic programming and Bellman’s principle of optimality can be used to give analytical solution to a discrete-time LQ-optimal control on a finite control interval.\nGive the Hamilton-Jacobi-Bellman (HJB) equation and explain it as a reformulation of the principle of optimality for continuous-time systems. Give also its version featuring the Hamiltonian function. (will only be covered later in the course when discussing continuous-time systems)",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Learning goals"
    ]
  },
  {
    "objectID": "dynamic_programming_goals.html#knowledge-remember-and-understand",
    "href": "dynamic_programming_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Understand and explain the Bellman’s principle of optimality.\nShow how dynamic programming and Bellman’s principle of optimality can be used to give analytical solution to a discrete-time LQ-optimal control on a finite control interval.\nGive the Hamilton-Jacobi-Bellman (HJB) equation and explain it as a reformulation of the principle of optimality for continuous-time systems. Give also its version featuring the Hamiltonian function. (will only be covered later in the course when discussing continuous-time systems)",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Learning goals"
    ]
  },
  {
    "objectID": "dynamic_programming_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "dynamic_programming_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nUse dynamic programming to design an optimal feedback controller in the form of a lookup table for a general (possibly nonlinear) discrete-time dynamical system.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Learning goals"
    ]
  },
  {
    "objectID": "limitations_SISO.html",
    "href": "limitations_SISO.html",
    "title": "Limitations for SISO systems",
    "section": "",
    "text": "For a given system, there may be some inherent limitations of achievable performance. However hard we try to design/tune a feedback controller, certain closed-loop performance indicators such as bandwidth, steady-state accuracy, or resonant peaks may have inherent limits. We are going to explore these. The motivation is that once we know what is achievable, we do not have to waste time by trying to achieve the impossible.\nAt first it may look confusing that we are only formulating this problem of learning the limits towards the end of our course, since one view of the whole optimal control theory is that is that it provides a systematic methodology for learning what is possible to achieve. Shall we need to know the shortest possible time in which the drone can be brought from one position and orientation to another, we just formulate the minimum-time optimal control problem and solve it. Even if at the end of the day we intend to use a different controller – perhaps one supplied commercially with a fixed structure like a PID controller – at least we can assess the suboptimality of such controller by comparing its performance with the optimal one.\nIn this section we are going to restrict ourselves to SISO systems. Then in the next section we will extend the results to MIMO systems.\nS+T = 1",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#clarification-of-the-definition-of-bandwidth",
    "href": "limitations_SISO.html#clarification-of-the-definition-of-bandwidth",
    "title": "Limitations for SISO systems",
    "section": "Clarification of the definition of bandwidth",
    "text": "Clarification of the definition of bandwidth",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#interpolation-conditions-of-internal-stability",
    "href": "limitations_SISO.html#interpolation-conditions-of-internal-stability",
    "title": "Limitations for SISO systems",
    "section": "Interpolation conditions of internal stability",
    "text": "Interpolation conditions of internal stability\nConsider that the plant modelled by the transfer function G(s) has a zero in the right half-plane (RHP), that is,\n\nG(z) = 0, \\; z\\in \\text{RHP}.\n\nIt can be shown that the closed-loop transfer functions S(s) and t(s) satisfy the interpolation conditions \\boxed{\nS(z)=1,\\;\\;\\;T(z)=0\n}\n\n\nProof. Showing this is straightforward and insightful: since no unstable pole-zero cancellation is allowed if internal stability is to be guaranteed, the open-loop transfer function L=KG must inherit the RHP zero of G, that is,\n\nL(z) = K(z)G(z) = 0, \\; z\\in \\text{RHP}.\n\nBut then the sensitivity function S=1/(1+L) must satisfy \nS(z) = \\frac{1}{1+L(z)} = 1.\n\nConsequently, the complementary sensitivity function T=1-S must satisfy the interpolation condition T(z)=0.\n\nSimilarly, assuming that the plant transfer function G(s) has a pole in the RHP, that is,\n\nG(p) = \\infty, \\; p\\in \\text{RHP},\n which can also be formulated in a cleaner way (avoiding the infinity in the definition) as \n\\frac{1}{G(p)} = 0, \\; p\\in \\text{RHP},\n the closed-loop transfer functions S(s) and T(s) satisfy the interpolation conditions \\boxed\n{T(p) = 1,\\;\\;\\;S(p) = 0.}\n\nThe interpolation conditions that we have just derived constitute the basis on which we are going to derive the limitations of achievable closed-loop magnitude frequency responses. But we need one more technical results before we can proceed. Most probably you have already encountered it in some course on complex analysis - maximum modulus principle. We state this result in the jargon of control theory.\n\nTheorem 1 (Maximum modulus principle) For a stable transfer function F(s), that is, for a function with no pole in the closed right half-plane (RHP) it holds that\n\n\\sup_{\\omega}|F(j\\omega)|\\geq |F(s_0)|\\;\\;\\; \\forall s_0\\in \\text{RHP}.\n\nThis can also be expressed compactly as \n\\|F(s)\\|_\\infty \\geq |F(s_0)|\\;\\;\\; \\forall s_0\\in \\text{RHP}.\n\n\nNow instead of some general F(s) we consider the weighted sensitivity function W_\\mathrm{p}(s)S(s). And the complex number s in the RHP equals to a zero z of the plant transfer function G(s), that is, G(z)=0, \\; z\\in\\mathbb C, \\; \\Re(z)\\geq 0. Then the maximum modulus principle together with the interpolation condition S(z)=1 implies that\n\n\\|W_\\mathrm{p}S\\|_{\\infty}\\geq |W_\\mathrm{p}(z)|.\n\nSimilar result holds for the weighted complementary sensitivity function W(s)T(s) and an unstable pole p of the plant transfer function G(s), when combining the maximum modulus principle with the interpolation condition T(p)=1\n\n\\|WT\\|_{\\infty}\\geq |W(p)|.\n\nThese two simple results can be further generalized to the situations in which the plant transfer function G(s) has multiple zeros and poles in the RHP. Namely, if G(s) has N_p unstable poles p_i and N_z unstable zeros z_j,\n\n\\|W_\\mathrm{p}S\\|_{\\infty}\\geq c_{1j}|W_\\mathrm{p}(z_j)|, \\;\\;\\;c_{1j}=\\prod_{i=1}^{N_p}\\frac{|z_j+\\bar{p}_i|}{|z_j-p_i|}\\geq 1,\n \n\\|WT\\|_{\\infty}\\geq c_{2i}|W(p_i)|, \\;\\;\\;c_{2i}=\\prod_{j=1}^{N_z}\\frac{|\\bar{z}_j+p_i|}{|z_j-p_i|}\\geq 1.\n\nAs a special case, consider the no-weight cases W_\\mathrm{p}(s)=1 and W(s)=1 with just a single unstable pole and zero. Then the limitations on the achievable closed-loop magnitude frequency responses can be formulated as \n\\|S\\|_{\\infty} &gt; c, \\;\\; \\|T\\|_{\\infty} &gt; c, \\;\\;\\;c=\\frac{|z+p|}{|z-p|}.\n\n\nExample 1 For G(s) = \\frac{s-4}{(s-1)(0.1s+1)}, the limitations are \n\\|S\\|_{\\infty}&gt;1.67, \\quad \\|T\\|_{\\infty}&gt;1.67.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#limitations-of-the-achievable-bandwidth-due-to-zeros-in-the-right-half-plane",
    "href": "limitations_SISO.html#limitations-of-the-achievable-bandwidth-due-to-zeros-in-the-right-half-plane",
    "title": "Limitations for SISO systems",
    "section": "Limitations of the achievable bandwidth due to zeros in the right half-plane",
    "text": "Limitations of the achievable bandwidth due to zeros in the right half-plane\nThere are now two requirements on the weighted sensitivity function that must be reconciled. First, the performance requirements \n|S(j\\omega)|&lt;\\frac{1}{|W_\\mathrm{p}(j\\omega)|}\\;\\;\\forall\\omega\\;\\;\\;\\Longleftrightarrow \\|W_\\mathrm{p}S\\|_{\\infty}&lt;1\n and second, the just derived consequence of the interpolation condition \n\\|W_\\mathrm{p}S\\|_{\\infty}\\geq |W_\\mathrm{p}(z)|.\n\nThe only way to satisfy both is to guarantee that \n|W_\\mathrm{p}(z)|&lt;1.\n\nNow, consider the popular first-order weight \nW_\\mathrm{p}(z)=\\frac{s/M+\\omega_\\mathrm{B}}{s+\\omega_\\mathrm{B} A}.\n\nFor one real zero in the RHP, the inequality |W_\\mathrm{p}(z)|&lt;1 can be written as \n\\omega_\\mathrm{B}(1-A) &lt; z\\left(1-\\frac{1}{M}\\right).\n\nSetting A=0 a M=2, the upper bound on the bandwidth follows\n\\boxed\n{\\omega_\\mathrm{B}&lt;0.5z.}\n\nFor complex conjugate pair \n\\omega_\\mathrm{B}=|z|\\sqrt{1-\\frac{1}{M^2}}\n M=2: \\omega_\\mathrm{B}&lt;0.86|z|.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#limitation-of-the-achievable-bandwidth-due-to-poles-in-the-right-half-plane",
    "href": "limitations_SISO.html#limitation-of-the-achievable-bandwidth-due-to-poles-in-the-right-half-plane",
    "title": "Limitations for SISO systems",
    "section": "Limitation of the achievable bandwidth due to poles in the right half-plane",
    "text": "Limitation of the achievable bandwidth due to poles in the right half-plane\nUsing \n|T(j\\omega)|&lt;\\frac{1}{|W(j\\omega)|}\\;\\;\\;\\forall\\omega\\;\\;\\;\\Longleftrightarrow \\|WT\\|_{\\infty}&lt;1\n and the interpolation condition \\|WT\\|_{\\infty}\\geq |W(p)|: \n|W(p)|&lt;1\n With weight \nW(s)= \\frac{s}{\\omega_{BT}^*}+\\frac{1}{M_T}\n we get a lower bound on the bandwidth \n\\omega_{BT}^* &gt; p\\frac{M_T}{M_T-1}\n M_T=2: {\\omega_{BT}^*&gt;2p}\\ For complex conjugate pair: \\omega_{BT}^*&gt;1.15|p|.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#limitations-due-to-time-delay",
    "href": "limitations_SISO.html#limitations-due-to-time-delay",
    "title": "Limitations for SISO systems",
    "section": "Limitations due to time delay",
    "text": "Limitations due to time delay\nConsider the problem of designing a feedback controller for reference tracking. An ideal closed-loop transfer function T(s) from the reference to the output satisfies T(s)=1. If the plant has a time delay, the best achievable closed-loop transfer function T(s) is given by \nT(s) = e^{-\\theta s},\n that is, the reference is perfectly tracked, albeit with some delay. The best achievable sensitivity function S(s) is then given by \nS(s) = 1-e^{-\\theta s}.\n\nIn order to make the analysis simpler, we approximate the sensitivity function by the first-order Taylor expansion \nS(s) \\approx \\theta s,\n from which we can see that the magnitude frequency response of the sensitivity function is approximated by a linear function of frequency. Unit gain is achieved at about\n\n\\omega_{c}=1/\\theta.\n From this approximation, we can see that the bandwidth of the system is limited by the time delay \\theta as\n\\boxed{\n\\omega_c &lt; \\frac{1}{\\theta}.\n}",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#limitations-due-presence-of-disturbance",
    "href": "limitations_SISO.html#limitations-due-presence-of-disturbance",
    "title": "Limitations for SISO systems",
    "section": "Limitations due presence of disturbance",
    "text": "Limitations due presence of disturbance",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#limitations-due-to-saturation-of-actuators",
    "href": "limitations_SISO.html#limitations-due-to-saturation-of-actuators",
    "title": "Limitations for SISO systems",
    "section": "Limitations due to saturation of actuators",
    "text": "Limitations due to saturation of actuators",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "reduction_order_benchmarks.html",
    "href": "reduction_order_benchmarks.html",
    "title": "Benchmarks",
    "section": "",
    "text": "Model-Order-Reduction (MOR) Wiki\nSLICOT Benchmark Examples for Model Reduction\n\n\n\n\n Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Benchmarks"
    ]
  },
  {
    "objectID": "ext_goals.html",
    "href": "ext_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Explain the necessary changes to the LQR framework in the case when the system is exposed to some random disturbances and the initial state is random as well (this is called stochastic LQR problem). The answer is that actually no changes are needed, the same formulas for the optimal state-feedback gain can be used as in the deterministic case.\nState the guarantees on the stability margins (GM and PM) for the LQR state-feedback regulator.\nDiscuss the possible extensions of the LQR framework in the situation when not all the states are measured. In particular, explain the idea behind the LQG controller, that is, a combination of a LQR state-feedback controller and a Kalman filter.\nDiscuss the guarantees on the stability margins for an LQG controller. Here, John Doyle’s famously short abstract gives the answer…\nExplain the key idea behind the Loop Transfer Recovery (LTR) control strategy as a heuristic means of restoring the robustness of an LQG controller.\nReformulate both the LQR and the LQG problems within the new configuration featuring a generalized system and a feedback controller in the feedback loop.\nGive the definition of the H2 system norm and explain how its minimization relates to the LQR/LQG-optimal control.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Learning goals"
    ]
  },
  {
    "objectID": "ext_goals.html#knowledge-remember-and-understand",
    "href": "ext_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Explain the necessary changes to the LQR framework in the case when the system is exposed to some random disturbances and the initial state is random as well (this is called stochastic LQR problem). The answer is that actually no changes are needed, the same formulas for the optimal state-feedback gain can be used as in the deterministic case.\nState the guarantees on the stability margins (GM and PM) for the LQR state-feedback regulator.\nDiscuss the possible extensions of the LQR framework in the situation when not all the states are measured. In particular, explain the idea behind the LQG controller, that is, a combination of a LQR state-feedback controller and a Kalman filter.\nDiscuss the guarantees on the stability margins for an LQG controller. Here, John Doyle’s famously short abstract gives the answer…\nExplain the key idea behind the Loop Transfer Recovery (LTR) control strategy as a heuristic means of restoring the robustness of an LQG controller.\nReformulate both the LQR and the LQG problems within the new configuration featuring a generalized system and a feedback controller in the feedback loop.\nGive the definition of the H2 system norm and explain how its minimization relates to the LQR/LQG-optimal control.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Learning goals"
    ]
  },
  {
    "objectID": "ext_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "ext_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nDesign an LQG/LTR regulator/controller.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_indir_trajectory_stabilization.html",
    "href": "cont_indir_trajectory_stabilization.html",
    "title": "Trajectory stabilization and neigboring extremals",
    "section": "",
    "text": "Indirect methods for optimal control reformulate the optimal control problem into a set of equtions – boundary value problems with differential and algebraic equations in the case of continuous-time systems – and by solving these (typically numerically) we obtain the optimal state and control trajectories. Practical usefullness of these is rather limited as such optimal control trajectory constitutes an open-loop control – there is certainly no need to advocate the importance of feedback in this advanced control course.\nOne way to introduce feedback is to regard the computed optimal state trajectory \\bm x^\\star(t) as a reference trajectory and design a feedback controller to track this reference. To our advantage, we already have the corresponding control trajectory \\bm u^\\star(t) too, and theferore we can formulate such reference tracking problem as a problem of regulating the deviation \\delta \\bm x(t) of the state from its reference by means of superposing a feedback control \\delta \\bm u(t) onto the (open-loop) optimal control.\nWhile this problem – also known as the problem of stabilization of a (reference) trajectory – can be solved by basically any feedback control scheme, one elegant way is to linearize the system around the reference trajectory and formulate the problem as the LQR problem for a time-varying linear system.\n\n\n\n\n\n\nLinearization around a trajectory\n\n\n\nDon’t forget that when linearizing a nonlinear system \\dot{\\bm x} = \\mathbf f(\\bm x,\\bm u) around a point that is not equilibrium – and this inevitably happens when linearizing along the state trajectory \\bm x^\\star(t) obtained from indirect approach to optimal control – the linearized system \\frac{\\mathrm d}{\\mathrm d t} \\delta \\bm x= \\mathbf A(t) \\delta \\bm x + \\mathbf B(t) \\delta \\bm u considers not only the state variables but also the control variables as increments \\delta \\bm x(t) and \\delta \\bm u(t) that must be added to the nominal values \\bm x^\\star(t) and \\bm u^\\star(t) of the state and control variables determining the operating point. That is, \\bm x(t) = \\bm x^\\star(t) + \\delta \\bm x(t) and \\bm u(t) = \\bm u^\\star(t) + \\delta \\bm u(t).\n\n\nHaving decided on an LQR framework, we can now come up with the three matrices \\mathbf Q, \\mathbf R and \\mathbf S that set the quadratic cost function. Once this choice is made, we can just invoke the solver for continuous-time Riccati equation with the ultimate goal of finding the time-varying state feedback gain \\mathbf K(t).\n\n\n\n\n\n\nLQR for trajectory stabilization can be done in discrete time\n\n\n\nIf discrete-time feedback control is eventually desired, which it mostly is, the whole LQR design for a time-varying linear system will have to be done using just periodically sampled state and control trajectories and applying recursive formulas for the discrete-time Riccati equation and state feedback gain.\n\n\nThe three weighting matrices \\mathbf Q, \\mathbf R and \\mathbf S, if chosen arbitrarily, are not related to the original cost function that is minimized by the optimal state and control trajectories. The matrices just parameterize a new optimal control problem. And it can be acceptable. It turns out, however, that there is a clever (and insightful) way of choosing these matrices so that the trajectory stabilization problem inherits the original cost function. In other words, even when the system fails to stay on the optimal trajectory perfectly, the LQR state-feedback controller will keep minimizing the same cost function when regulating the deviation from the optimal trajectory.\nRecall that using the conventional definition of Hamiltonian H(\\bm x, \\bm u, \\bm \\lambda) = L(\\bm x, \\bm u) + \\bm \\lambda^\\top \\mathbf f(\\bm x, \\bm u), in which we now assume time invariance of both the system and the cost function for notational simplicity, the necessary conditions of optimality are \n\\begin{aligned}\n\\dot{\\bm x} &= \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\boldsymbol \\lambda)  = \\mathbf f(\\bm x, \\bm u), \\\\\n\\dot{\\bm \\lambda} &= -\\nabla_{\\bm x} H(\\bm x,\\bm u,\\boldsymbol \\lambda), \\\\\n\\mathbf 0 &= \\nabla_{\\bm u} H(\\bm x,\\bm u,\\boldsymbol \\lambda),\\\\\n\\bm x(t_\\mathrm{i})&=\\mathbf x_\\mathrm{i},\\\\\n\\bm x(t_\\mathrm{f})&=\\mathbf x_\\mathrm{f} \\quad \\text{or}\\quad \\bm \\lambda(t_\\mathrm{f})=\\nabla\\phi(\\bm{x}(t_\\mathrm{f})),\n\\end{aligned}\n where the option on the last line is selected based on whether the state at the final time is fixed or free.\n\n\n\n\n\n\nThe state at the final time can also be restricted by a linear equation\n\n\n\nThe conditions of optimality stated above correspond to one of the two standard situations, in which the state in the final time is either fixed to a single value or completely free. The conditions can also be modified to consider the more general situation, in which the state at the final time is restricted to lie on a manifold defined by an equality constraint \\psi(\\bm x(t_\\mathrm{f})) = 0.\n\n\nLet’s now consider some tiny perturbation to the initial state \\bm x(t_\\mathrm{i}) from its prescribed nominal value \\mathbf x_\\mathrm{i}. It will give rise to deviations all all the variables in the above equations from their nominal – optimal – trajectories. Assuming the deviations are small, linear model suffices to describe them. In other words, what we are now after is linearization of the above equations\n\n\\begin{aligned}\n\\delta \\dot{\\bm x} &= \\nabla_{\\bm x} \\mathbf f \\; \\delta \\bm x + \\nabla_{\\bm u} \\mathbf f \\; \\delta \\bm u, \\\\\n\\delta \\dot{\\bm \\lambda} &= -\\nabla^2_{\\bm{xx}} H \\; \\delta \\bm x -\\nabla^2_{\\bm{xu}} H \\; \\delta \\bm u -\\underbrace{\\nabla^2_{\\bm{x\\lambda}} H}_{(\\nabla_{\\bm x} \\mathbf f)^\\top} \\; \\delta \\bm \\lambda, \\\\\n\\mathbf 0 &= \\nabla^2_{\\bm{ux}} H \\; \\delta \\bm x + \\nabla^2_{\\bm{uu}} H \\; \\delta \\bm u + \\underbrace{\\nabla^2_{\\bm{u\\lambda}} H}_{(\\nabla_{\\bm u} \\mathbf f)^\\top} \\; \\delta \\bm \\lambda,\\\\\n\\delta \\bm x(t_\\mathrm{i}) &= \\text{specified},\\\\\n\\delta \\bm \\lambda(t_\\mathrm{f}) &= \\nabla^2_{\\bm{xx}}\\phi(\\mathbf{x}(t_\\mathrm{f}))\\; \\delta \\bm x(t_\\mathrm{f}).\n\\end{aligned}\n\\tag{1}\n\n\n\n\n\n\nNote on notation\n\n\n\nLet’s recall for convenience here that since \\mathbf f(\\bm x, \\bm u) is a vector function of a vector argument(s), \\nabla_{\\bm x} \\mathbf f is a matrix whose columns are gradients of individual components of \\mathbf f. Equivalently, (\\nabla_{\\bm x} \\mathbf f)^\\top stands for the Jacobian of the function \\mathbf f with respect to \\bm x. Similarly, \\nabla_{\\bm{xx}} H is the Hessian of \\mathbf f with respect to \\bm x. That is, it is a matrix composed of second derivatives. It is a symmetric matrix, hence no need to transpose it. Finally, the terms \\nabla_{\\bm{ux}} H and \\nabla_{\\bm{xu}} H are matrices containing mixed second derivatives.\n\n\nWith hindsight we relabel the individual terms in Eq. 1 as \n\\begin{aligned}\n\\mathbf A(t) &\\coloneqq (\\nabla_{\\bm x} \\mathbf f)^\\top\\\\\n\\mathbf B(t) &\\coloneqq (\\nabla_{\\bm u} \\mathbf f)^\\top\\\\\n\\mathbf Q(t) &\\coloneqq \\nabla^2_{\\bm{xx}} H\\\\\n\\mathbf R(t) &\\coloneqq \\nabla^2_{\\bm{uu}} H\\\\\n\\mathbf N(t) &\\coloneqq \\nabla^2_{\\bm{xu}} H\\\\\n\\mathbf S_\\mathrm{f} &\\coloneqq \\nabla^2_{\\bm{xx}}\\phi(\\mathbf{x}(t_\\mathrm{f})).\n\\end{aligned}\n\nLet’s rewrite the perturbed necessary conditions of optimality using these new symbols\n\n\\begin{aligned}\n\\delta \\dot{\\bm x} &= \\mathbf A(t) \\; \\delta \\bm x + \\mathbf B(t) \\; \\delta \\bm u, \\\\\n\\delta \\dot{\\bm \\lambda} &= - \\mathbf Q(t) \\; \\delta \\bm x - \\mathbf N(t) \\; \\delta \\bm u - \\mathbf A^\\top (t)\\; \\delta \\bm \\lambda, \\\\\n\\mathbf 0 &= \\mathbf N(t) \\; \\delta \\bm x + \\mathbf R(t) \\; \\delta \\bm u + \\mathbf B^\\top (t) \\; \\delta \\bm \\lambda,\\\\\n\\delta \\bm x(t_\\mathrm{i}) &= \\text{specified},\\\\\n\\delta \\bm \\lambda(t_\\mathrm{f}) &= \\mathbf S_\\mathrm{f}    \\; \\delta \\bm x(t_\\mathrm{f}).\n\\end{aligned}\n\nAssuming that \\nabla^2_{\\bm{uu}} H is nonsingular, which can solve the third equation for \\bm u \n\\bm u = -\\nabla^2_{\\bm{uu}} H^{-1} \\left( \\nabla^2_{\\bm{ux}} H \\; \\delta \\bm x + (\\nabla_{\\bm u} \\mathbf f)^\\top \\; \\delta \\bm \\lambda\\right ).\n\n#TODO: finish.\n\n\n\n Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Trajectory stabilization and neigboring extremals"
    ]
  },
  {
    "objectID": "dynamic_programming_tabular.html",
    "href": "dynamic_programming_tabular.html",
    "title": "Tables as outcomes of dynamic programming",
    "section": "",
    "text": "Based on what we have seen so far, it turns out that the key to solving the discrete-time optimal control problem is to find some… functions. Either the optimal cost function J_k^\\star(\\bm x_k) or the optimal Q-factor Q_k^\\star(\\bm x_k,\\bm u_k). Once we have them, we can easily find the optimal control \\bm u_k^\\star(\\bm x_k). The question is, however, how to find these functions. We have seen some recursions for both of them, but it is not clear how to turn these into practical algorithms. We do it here.\nWe are going to use \nJ_k^\\star(\\bm x_k) = \\min_{\\bm u_k}\\left(L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1})\\right)\n backwards in (discrete) time at a grid of states. Indeed, gridding the state space is the key technique in dynamic programming, because DP assumes a finite state space. If it is not finite, we must grid it.\nWe start with the final time N. We evaluate the terminal cost function \\phi(\\bm x_N) at a grid of states, which directly yields the optimal costs J_N^\\star(\\bm x_N).\nWe then proceed to the time N-1. Evaluating the optimal cost function J^\\star_{N-1} at each grid point in the state space calls for some optimization, namely \n\\min_{u_{N-1}} \\left(L_{N-1}(\\bm x_{N-1},\\bm u_{N-1}) + J_{N}^\\star(\\mathbf f_{N-1}(\\bm x_{N-1}, \\bm u_{N-1}))\\right).\n\nWe save the optimal costs and the corresponding controls at the given grid points (giving two arrays of values), decrement the time to N-2, and repeat. All the way down to the initial time i.\nLet’s summarize that as an outcome of this whole procedure we have two tables – one for the optimal cost, the other for the optimal control.\n\n\n\n Back to top",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Tabular outcomes of DP"
    ]
  },
  {
    "objectID": "cont_numerical_hw.html",
    "href": "cont_numerical_hw.html",
    "title": "Homework",
    "section": "",
    "text": "In this week’s homework, you will plan and stabilize a swing-up trajectory for a cart-pole system. The cart-pole is a classic control problem in which a pole is attached to a cart that moves along a horizontal track.\nThe system is show on the figure\n\n\n\nCart-pole system\n\n\nThe dynamics of the system are given by the equations \n\\begin{align}\n\\ddot{x} &= \\frac{1}{m_\\text{c} + m_\\text{p}\\sin^2{\\theta}} \\left[F_x + m_\\text{p}\\sin{\\theta}\\left(l\\dot{\\theta}^2+g\\cos{\\theta}\\right)\\right],\\\\\n\\ddot{\\theta} &= \\frac{1}{l(m_\\text{c} + m_\\text{p}\\sin^2{\\theta})} \\left[-F_x\\cos{\\theta} - m_\\text{p}l\\dot{\\theta}^2\\sin{\\theta}\\cos{\\theta} - (m_\\text{c}+m_\\text{p})g\\sin{\\theta}\\right],\n\\end{align}\n where\n\nx is the position of the cart,\n\n\\dot{x} is the velocity of the cart,\n\n\\theta is the angle of the pole (with \\theta = 0 when the pole is hanging downward),\n\n\\dot{\\theta} is the angular velocity of the pole,\n\nm_\\text{c} is the mass of the cart,\n\nm_\\text{p} is the mass of the pole,\n\nl is the length of the pole,\n\ng is the gravitational acceleration.\n\nYou control the system by applying a horizontal force F_x to the cart.\nFormally, let \\mathbf{x} = (x, \\dot{x}, \\theta, \\dot{\\theta}) be the state of the system, and u = F_x be the control input. Your objective is to drive the system from the initial state {\\mathbf{x}_\\text{initial} = (0, 0, 0, 0)} (cart at rest at the origin, pole hanging down) to the final state {\\mathbf{x}_\\text{final} = (0, 0, \\pi, 0)} (cart at rest at the origin, pole upright).\nThe cart is constrained to move within bounds, and the force is subject to actuator limits, i.e, {\\lvert x \\rvert \\leq 1\\,\\text{m}} and {\\lvert F_x \\rvert \\leq 3\\,\\text{N}}. You should get the system to the final state under 10 seconds.\n\n\nBase your implementation on the template provided below. Upload your solution as a single file named hw.jl to the BRUTE system.\nYour task are the following\n\n\nFormulate the swing-up task as an optimal control problem (OCP), and numerically solve it. Specifically, you need to\n\nChoose a suitable cost functional.\nFormulate the OCP as a Nonlinear program (NLP) (e.g., using direct transcription, collocation, pseudo-spectral methods, etc.).\nModel the NLP in JuMP and use Ipopt to solve it.\nThe planned trajectory must reach the final state in under 10 seconds, and respect the state contraints and actuator limits.\nReturn the planned trajectory as\n\nx_opt (4 x N matrix) for the state trajectory, and\nu_opt (1 x N-1 matrix) for the control input trajectory. where N corresponds to the number of simulation steps (e.g., N = 1001 for 10 seconds with time step Δt = 0.01).\n\nComplete this part in the plan_cartpole_swingup function.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe OCP does not need to be “discretized” using the simulation time step Δt = 0.01. If you use, for example, direct collocation you may solve the OCP with a different number or spacing of knot points. For global pseudo-spectral methods, time steps do not make sense at all, as you evalute the dynamics at the Chebyshev/Legendre nodes.\nHowever, the resulting trajectory must be resampled to produce x_opt and u_opt on a uniform grid with time step Δt = 0.01, as this is what the controller and simulator expect.\n\n\n\n\n\nYour next task is to design a controller that tracks and stabilizes the planned trajectory. Specifically, complete the step! function in the provided template.\nYou must\n\nImplement feedback around the nominal trajectory (x_opt, u_opt). The controller should correct for deviations from the planned path using the current state.\nYou may use any stabilizing strategy. However, we recommend time-varying state feedback obtained by solving the LQR problem along the planned trajectory.\nNear the target state, you should switch to a simpler static controller (e.g., LQR-based state feedback designed at the upright equilibrium). This improves stability and reduces sensitivity to trajectory mismatches.\nThe applied control input must respect the actuator limits of ±3 N. You may clamp the input if necessary.\nThe cart should also not exceed the bounds of ±1 m.\n\nYou may extend the CartPoleController struct to store any data your feedback controller needs (e.g., gain matrices, thresholds, linearized models, etc.).\n\n\n\n\n\n\nImportant\n\n\n\nYour controller will be tested on a simulation with slightly perturbed system parameters (e.g., different masses or pole length) to reflect real-world model uncertainty. Your design should be robust enough to handle such variations and still stabilize the system.\n\n\n\n\n\nTo wrap up the assignment, your controllers will compete in a swing-up challenge.\n\nEach submission will be evaluated in closed-loop on a perturbed version of the cart-pole system (i.e., with slightly different parameters than those used for planning).\nThe goal is simple—Swing the pole up and stabilize it in the shortest time possible.\nControllers will be ranked based on how quickly they reach a small neighborhood of the upright equilibrium and remain there without violating constraints.\nThe authors of the top three fastest controllers will receive +10% to the practical part of the exam.\n\n\n\n\n\n\n\nTip\n\n\n\nYou are free to tune everything — be it the NLP formulation, cost function, or feedback design.\n\n\n\n\n\n\n\nRegarding the translation of the OCP into NLP, you may find find usefull Russ Tedrake’s lecture notes from his Underactuated Robotics course @ MIT, which can be found here. We also recommend Matthew Kelly’s tutorial paper dubbed An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation (available here).\nWhen designing the time-varying LQR state feedback controller, you will need to linearize the system dynamics around the planned trajectory. We discourage you from doing this by hand. Instead, use the Zygote package to compute the Jacobians of the system dynamics using automatic differentiation.\n\n\n\n\n\nusing LinearAlgebra\nusing JuMP, Ipopt, Zygote\nusing ControlSystemsBase\n\nconst Δt = 0.01 # Time step for the simulation\nx0 = [0.0, 0.0, 0.0, 0.0] # Initial state (cart at rest, pole hanging down)\nxf = [0.0, 0.0, π, 0.0] # Final state (cart at rest, pole upright)\n\n# Cart-pole dynamics function\nfunction dynamics(x, p, t, u)\n    mc, mp, l, g = p\n\n    dx₁ = x[2]\n    dx₂ = 1 / (mc + mp * sin(x[3])^2) * (u[1] .- mp * sin(x[3]) * (l * x[4]^2 + g * cos(x[3])))\n    dx₃ = x[4]\n    dx₄ = 1 / (l * (mc + mp * sin(x[3])^2)) * (-u[1] * cos(x[3]) .- mp * l * x[4]^2 * sin(x[3]) * cos(x[3]) .- (mc + mp) * g * sin(x[3]))\n\n    return [dx₁, dx₂, dx₃, dx₄]\nend\n\n# Wrapper with nominal parameters (You may find this useful for modeling the NLP)\nfunction f(x, u)\n    mc = 0.5\n    mp = 0.2\n    l = 0.3\n    g = 9.81\n\n    p = [mc, mp, l, g]\n\n    return dynamics(x, p, 0.0, u)\nend\n\n\n# Function for trajectory planning (to be completed by the student)\nfunction plan_cartpole_swingup()\n\n    model = Model(Ipopt.Optimizer)\n\n    # TODO: Use JuMP to define the NLP model and Ipopt to solve it\n\n    # TODO: Extract the optimal trajectory from the solution (e.g. sample the trajectory at Δt intervals)\n\n    N = 100  # Number of time steps (can be adjusted, e.g., computed from minimum time formulation)\n\n    # These will be filled with the planned trajectory\n    x_opt = zeros(4, N)\n    u_opt = zeros(1, N-1)\n\n    return x_opt, u_opt\nend\n\n\n# Controller structure to store trajectory and any additional data (e.g. feedback gains)\nmutable struct CartPoleController\n    x_opt::Matrix{Float64}\n    u_opt::Matrix{Float64}\n    # TODO: Add any other variables you may need (e.g. gain matrices, thresholds)\nend\n\n# Constructor for the controller\nfunction CartPoleController(x_opt::Matrix{Float64}, u_opt::Matrix{Float64})\n    # TODO: Initialize and store anything else you need here\n    return CartPoleController(x_opt, u_opt)\nend\n\n# Evaluate the controller at state x and time step k\nfunction step!(controller::CartPoleController, x::Vector{Float64}, k::Int64)\n    # Nominal state and input from the planned trajectory\n    x_nom = controller.x_opt[:, k]\n    u_nom = controller.u_opt[:, k]\n\n    # TODO: Implement feedback to stabilize the trajectory.\n    # You may use time-varying LQR, constant gain, or another strategy.\n    #\n    # Consider switching to a simpler static controller (e.g., LQR around the upright equilibrium)\n    # once the system is sufficiently close to the target state.\n    #\n    # The input should not exceed the actuator limits of ± 3 N (you may clamp it if necessary).\n\n    # You may also update `controller` struct if needed (e.g., for gain scheduling)\n\n    u = u_nom  # Replace with your feedback control law\n\n    return u\nend\n\n\n\n\nYour planner and controller will be used in the BRUTE system in a similar way as the snippet below.\n\nusing OrdinaryDiffEq\n\nx_opt, u_opt = plan_cartpole_swingup()\ncontroller = CartPoleController(x_opt, u_opt)\n\nts_sim = 0:Δt:Δt*1000\nx_sim = zeros(4, length(ts_sim))\nu_sim = zeros(1, length(ts_sim)-1)\n\nfor (k, t) in enumerate(ts_sim[1:end-1])\n    u_sim[:, k] = step!(controller, x_sim[:, k], k)\n\n    polecart(x, p, t)  = dynamics(x, p, t, u_sim[:, k])\n    prob = ODEProblem(polecart, x_sim[:, k], (0, Δt), [0.50, 0.2, 0.3, 9.81])\n    sol = solve(prob, Tsit5())\n\n    x_sim[:, k+1] = sol[:, end]\n\nend\n\nThe following code snippet shows how to visualize the results of your simulation. You can also edit this to visualize your planned trajectory.\n\nusing CairoMakie\n\nfig = Figure( size = (600, 500))\n\nax = Axis(fig[1, 1], title = \"Cart-Pole Trajectory\", xlabel = \"Time (s)\")\nlines!(ax, ts_sim, x_sim[1, :], label = \"x\")\nlines!(ax, ts_sim, x_sim[2, :], label = \"ẋ\")\nlines!(ax, ts_sim, x_sim[3, :], label = \"θ\")\nlines!(ax, ts_sim, x_sim[4, :], label = \"θ̇\")\n\naxislegend(ax)\n\n\nax = Axis(fig[2, 1], title = \"Control Input\", xlabel = \"Time\", ylabel = \"Force\")\nlines!(ax, ts_sim[1:end-1], u_sim[1, :], label = \"Control Input\")\n\n\nfig\n\nLast but not least, you can animate the cart-pole system using the following code snippet.\n\nl = 0.3 # pendulum length\n\nx = x_sim[1, :]\nθ = x_sim[3, :]\n\npx = x .+ l .* sin.(θ)\npy = .-l .* cos.(θ)\n\nfig = Figure(resolution = (800, 300))\nax = Axis(fig[1, 1]; xlabel = \"x\", ylabel = \"y\", aspect = DataAspect())\n\n# Cart dimensions\ncart_width = 0.2\ncart_height = 0.1\n\nx_line = Observable([x[1], px[1]])\ny_line = Observable([cart_height, py[1]])\n\nx_blob = Observable(px[1])\ny_blob = Observable(py[1])\n\n# Initial shapes\ncart_obs = Observable(Rect(x[1] - cart_width/2, 0.0, cart_width, cart_height))\n\npendulum_line = lines!(ax, x_line, y_line, color=:black)\npendulum_bob = scatter!(ax, x_blob, y_blob; markersize=15, color=:red)\n\n# Cart patch\ncart_patch = poly!(ax, cart_obs, color = :blue)\n\n# Set axis limits\nxlims!(ax, -1.2, 1.2)\nylims!(ax, -0.5, 0.5)\n\n# Animation parameters\nframes = length(ts_sim)\nframerate = Int(round(frames / 10))  # approximate real-time\n\nfor i in 1:frames\n    # Update pendulum\n    x_line[] = [x[i], px[i]]\n    y_line[] = [cart_height, py[i]]\n    x_blob[] = px[i]\n    y_blob[] = py[i]\n    cart_obs[] = Rect(x[i] - cart_width/2, 0.0, cart_width, cart_height)\n\n    display(fig)\n\n    sleep(1/framerate)\nend",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Homework"
    ]
  },
  {
    "objectID": "cont_numerical_hw.html#cart-pole-swing-up",
    "href": "cont_numerical_hw.html#cart-pole-swing-up",
    "title": "Homework",
    "section": "",
    "text": "In this week’s homework, you will plan and stabilize a swing-up trajectory for a cart-pole system. The cart-pole is a classic control problem in which a pole is attached to a cart that moves along a horizontal track.\nThe system is show on the figure\n\n\n\nCart-pole system\n\n\nThe dynamics of the system are given by the equations \n\\begin{align}\n\\ddot{x} &= \\frac{1}{m_\\text{c} + m_\\text{p}\\sin^2{\\theta}} \\left[F_x + m_\\text{p}\\sin{\\theta}\\left(l\\dot{\\theta}^2+g\\cos{\\theta}\\right)\\right],\\\\\n\\ddot{\\theta} &= \\frac{1}{l(m_\\text{c} + m_\\text{p}\\sin^2{\\theta})} \\left[-F_x\\cos{\\theta} - m_\\text{p}l\\dot{\\theta}^2\\sin{\\theta}\\cos{\\theta} - (m_\\text{c}+m_\\text{p})g\\sin{\\theta}\\right],\n\\end{align}\n where\n\nx is the position of the cart,\n\n\\dot{x} is the velocity of the cart,\n\n\\theta is the angle of the pole (with \\theta = 0 when the pole is hanging downward),\n\n\\dot{\\theta} is the angular velocity of the pole,\n\nm_\\text{c} is the mass of the cart,\n\nm_\\text{p} is the mass of the pole,\n\nl is the length of the pole,\n\ng is the gravitational acceleration.\n\nYou control the system by applying a horizontal force F_x to the cart.\nFormally, let \\mathbf{x} = (x, \\dot{x}, \\theta, \\dot{\\theta}) be the state of the system, and u = F_x be the control input. Your objective is to drive the system from the initial state {\\mathbf{x}_\\text{initial} = (0, 0, 0, 0)} (cart at rest at the origin, pole hanging down) to the final state {\\mathbf{x}_\\text{final} = (0, 0, \\pi, 0)} (cart at rest at the origin, pole upright).\nThe cart is constrained to move within bounds, and the force is subject to actuator limits, i.e, {\\lvert x \\rvert \\leq 1\\,\\text{m}} and {\\lvert F_x \\rvert \\leq 3\\,\\text{N}}. You should get the system to the final state under 10 seconds.\n\n\nBase your implementation on the template provided below. Upload your solution as a single file named hw.jl to the BRUTE system.\nYour task are the following\n\n\nFormulate the swing-up task as an optimal control problem (OCP), and numerically solve it. Specifically, you need to\n\nChoose a suitable cost functional.\nFormulate the OCP as a Nonlinear program (NLP) (e.g., using direct transcription, collocation, pseudo-spectral methods, etc.).\nModel the NLP in JuMP and use Ipopt to solve it.\nThe planned trajectory must reach the final state in under 10 seconds, and respect the state contraints and actuator limits.\nReturn the planned trajectory as\n\nx_opt (4 x N matrix) for the state trajectory, and\nu_opt (1 x N-1 matrix) for the control input trajectory. where N corresponds to the number of simulation steps (e.g., N = 1001 for 10 seconds with time step Δt = 0.01).\n\nComplete this part in the plan_cartpole_swingup function.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe OCP does not need to be “discretized” using the simulation time step Δt = 0.01. If you use, for example, direct collocation you may solve the OCP with a different number or spacing of knot points. For global pseudo-spectral methods, time steps do not make sense at all, as you evalute the dynamics at the Chebyshev/Legendre nodes.\nHowever, the resulting trajectory must be resampled to produce x_opt and u_opt on a uniform grid with time step Δt = 0.01, as this is what the controller and simulator expect.\n\n\n\n\n\nYour next task is to design a controller that tracks and stabilizes the planned trajectory. Specifically, complete the step! function in the provided template.\nYou must\n\nImplement feedback around the nominal trajectory (x_opt, u_opt). The controller should correct for deviations from the planned path using the current state.\nYou may use any stabilizing strategy. However, we recommend time-varying state feedback obtained by solving the LQR problem along the planned trajectory.\nNear the target state, you should switch to a simpler static controller (e.g., LQR-based state feedback designed at the upright equilibrium). This improves stability and reduces sensitivity to trajectory mismatches.\nThe applied control input must respect the actuator limits of ±3 N. You may clamp the input if necessary.\nThe cart should also not exceed the bounds of ±1 m.\n\nYou may extend the CartPoleController struct to store any data your feedback controller needs (e.g., gain matrices, thresholds, linearized models, etc.).\n\n\n\n\n\n\nImportant\n\n\n\nYour controller will be tested on a simulation with slightly perturbed system parameters (e.g., different masses or pole length) to reflect real-world model uncertainty. Your design should be robust enough to handle such variations and still stabilize the system.\n\n\n\n\n\nTo wrap up the assignment, your controllers will compete in a swing-up challenge.\n\nEach submission will be evaluated in closed-loop on a perturbed version of the cart-pole system (i.e., with slightly different parameters than those used for planning).\nThe goal is simple—Swing the pole up and stabilize it in the shortest time possible.\nControllers will be ranked based on how quickly they reach a small neighborhood of the upright equilibrium and remain there without violating constraints.\nThe authors of the top three fastest controllers will receive +10% to the practical part of the exam.\n\n\n\n\n\n\n\nTip\n\n\n\nYou are free to tune everything — be it the NLP formulation, cost function, or feedback design.\n\n\n\n\n\n\n\nRegarding the translation of the OCP into NLP, you may find find usefull Russ Tedrake’s lecture notes from his Underactuated Robotics course @ MIT, which can be found here. We also recommend Matthew Kelly’s tutorial paper dubbed An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation (available here).\nWhen designing the time-varying LQR state feedback controller, you will need to linearize the system dynamics around the planned trajectory. We discourage you from doing this by hand. Instead, use the Zygote package to compute the Jacobians of the system dynamics using automatic differentiation.\n\n\n\n\n\nusing LinearAlgebra\nusing JuMP, Ipopt, Zygote\nusing ControlSystemsBase\n\nconst Δt = 0.01 # Time step for the simulation\nx0 = [0.0, 0.0, 0.0, 0.0] # Initial state (cart at rest, pole hanging down)\nxf = [0.0, 0.0, π, 0.0] # Final state (cart at rest, pole upright)\n\n# Cart-pole dynamics function\nfunction dynamics(x, p, t, u)\n    mc, mp, l, g = p\n\n    dx₁ = x[2]\n    dx₂ = 1 / (mc + mp * sin(x[3])^2) * (u[1] .- mp * sin(x[3]) * (l * x[4]^2 + g * cos(x[3])))\n    dx₃ = x[4]\n    dx₄ = 1 / (l * (mc + mp * sin(x[3])^2)) * (-u[1] * cos(x[3]) .- mp * l * x[4]^2 * sin(x[3]) * cos(x[3]) .- (mc + mp) * g * sin(x[3]))\n\n    return [dx₁, dx₂, dx₃, dx₄]\nend\n\n# Wrapper with nominal parameters (You may find this useful for modeling the NLP)\nfunction f(x, u)\n    mc = 0.5\n    mp = 0.2\n    l = 0.3\n    g = 9.81\n\n    p = [mc, mp, l, g]\n\n    return dynamics(x, p, 0.0, u)\nend\n\n\n# Function for trajectory planning (to be completed by the student)\nfunction plan_cartpole_swingup()\n\n    model = Model(Ipopt.Optimizer)\n\n    # TODO: Use JuMP to define the NLP model and Ipopt to solve it\n\n    # TODO: Extract the optimal trajectory from the solution (e.g. sample the trajectory at Δt intervals)\n\n    N = 100  # Number of time steps (can be adjusted, e.g., computed from minimum time formulation)\n\n    # These will be filled with the planned trajectory\n    x_opt = zeros(4, N)\n    u_opt = zeros(1, N-1)\n\n    return x_opt, u_opt\nend\n\n\n# Controller structure to store trajectory and any additional data (e.g. feedback gains)\nmutable struct CartPoleController\n    x_opt::Matrix{Float64}\n    u_opt::Matrix{Float64}\n    # TODO: Add any other variables you may need (e.g. gain matrices, thresholds)\nend\n\n# Constructor for the controller\nfunction CartPoleController(x_opt::Matrix{Float64}, u_opt::Matrix{Float64})\n    # TODO: Initialize and store anything else you need here\n    return CartPoleController(x_opt, u_opt)\nend\n\n# Evaluate the controller at state x and time step k\nfunction step!(controller::CartPoleController, x::Vector{Float64}, k::Int64)\n    # Nominal state and input from the planned trajectory\n    x_nom = controller.x_opt[:, k]\n    u_nom = controller.u_opt[:, k]\n\n    # TODO: Implement feedback to stabilize the trajectory.\n    # You may use time-varying LQR, constant gain, or another strategy.\n    #\n    # Consider switching to a simpler static controller (e.g., LQR around the upright equilibrium)\n    # once the system is sufficiently close to the target state.\n    #\n    # The input should not exceed the actuator limits of ± 3 N (you may clamp it if necessary).\n\n    # You may also update `controller` struct if needed (e.g., for gain scheduling)\n\n    u = u_nom  # Replace with your feedback control law\n\n    return u\nend\n\n\n\n\nYour planner and controller will be used in the BRUTE system in a similar way as the snippet below.\n\nusing OrdinaryDiffEq\n\nx_opt, u_opt = plan_cartpole_swingup()\ncontroller = CartPoleController(x_opt, u_opt)\n\nts_sim = 0:Δt:Δt*1000\nx_sim = zeros(4, length(ts_sim))\nu_sim = zeros(1, length(ts_sim)-1)\n\nfor (k, t) in enumerate(ts_sim[1:end-1])\n    u_sim[:, k] = step!(controller, x_sim[:, k], k)\n\n    polecart(x, p, t)  = dynamics(x, p, t, u_sim[:, k])\n    prob = ODEProblem(polecart, x_sim[:, k], (0, Δt), [0.50, 0.2, 0.3, 9.81])\n    sol = solve(prob, Tsit5())\n\n    x_sim[:, k+1] = sol[:, end]\n\nend\n\nThe following code snippet shows how to visualize the results of your simulation. You can also edit this to visualize your planned trajectory.\n\nusing CairoMakie\n\nfig = Figure( size = (600, 500))\n\nax = Axis(fig[1, 1], title = \"Cart-Pole Trajectory\", xlabel = \"Time (s)\")\nlines!(ax, ts_sim, x_sim[1, :], label = \"x\")\nlines!(ax, ts_sim, x_sim[2, :], label = \"ẋ\")\nlines!(ax, ts_sim, x_sim[3, :], label = \"θ\")\nlines!(ax, ts_sim, x_sim[4, :], label = \"θ̇\")\n\naxislegend(ax)\n\n\nax = Axis(fig[2, 1], title = \"Control Input\", xlabel = \"Time\", ylabel = \"Force\")\nlines!(ax, ts_sim[1:end-1], u_sim[1, :], label = \"Control Input\")\n\n\nfig\n\nLast but not least, you can animate the cart-pole system using the following code snippet.\n\nl = 0.3 # pendulum length\n\nx = x_sim[1, :]\nθ = x_sim[3, :]\n\npx = x .+ l .* sin.(θ)\npy = .-l .* cos.(θ)\n\nfig = Figure(resolution = (800, 300))\nax = Axis(fig[1, 1]; xlabel = \"x\", ylabel = \"y\", aspect = DataAspect())\n\n# Cart dimensions\ncart_width = 0.2\ncart_height = 0.1\n\nx_line = Observable([x[1], px[1]])\ny_line = Observable([cart_height, py[1]])\n\nx_blob = Observable(px[1])\ny_blob = Observable(py[1])\n\n# Initial shapes\ncart_obs = Observable(Rect(x[1] - cart_width/2, 0.0, cart_width, cart_height))\n\npendulum_line = lines!(ax, x_line, y_line, color=:black)\npendulum_bob = scatter!(ax, x_blob, y_blob; markersize=15, color=:red)\n\n# Cart patch\ncart_patch = poly!(ax, cart_obs, color = :blue)\n\n# Set axis limits\nxlims!(ax, -1.2, 1.2)\nylims!(ax, -0.5, 0.5)\n\n# Animation parameters\nframes = length(ts_sim)\nframerate = Int(round(frames / 10))  # approximate real-time\n\nfor i in 1:frames\n    # Update pendulum\n    x_line[] = [x[i], px[i]]\n    y_line[] = [cart_height, py[i]]\n    x_blob[] = px[i]\n    y_blob[] = py[i]\n    cart_obs[] = Rect(x[i] - cart_width/2, 0.0, cart_width, cart_height)\n\n    display(fig)\n\n    sleep(1/framerate)\nend",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Homework"
    ]
  },
  {
    "objectID": "ext_LQR_margins.html",
    "href": "ext_LQR_margins.html",
    "title": "Guaranteed gain and phase margins for LQR",
    "section": "",
    "text": "There is one important and fairly plausible property of the LQR state-feedback controller that we have not yet mentioned: its robustness to perturbations in the system model. Although we will postpone the introduction of a general robustness framework until the next chapter, here we can already refer to the classical concepts of gain and phase margins (GM and PM, respectively) as introduced in introductory courses on automatic control. The aformentioned property of an LQR is that it guarantees very nice GM and PM. For an arbitrary linear system parameterized by the matrices \\mathbf A and \\mathbf B and arbitrary cost function parameterized by the matrices \\mathbf Q and \\mathbf R – as long as the standard LQR conditions are satisfied – the LQR controller guarantees some decent values of GM and PM.\nIn order to see this, we consider the feedback interconnection of a plant whose all state variables are available at the output, and a proportional (state) feedback controller. The system is depicted in Fig. 1.\nThe plant is described by the state-space model \n\\mathbf G(s) = (s\\mathbf I-\\mathbf A)^{-1}\\mathbf B,\n in which the output matrix \\mathbf C is implicitly assumed to be the identity matrix, and the feedthrough matrix \\mathbf D is assumed to be zero.\nWe now need to break the loop to get an open-loop transfer function. We can break the loop either at the input or at the output of the plant, leading to input and output open-loop transfer functions, respectively. For the purpose of stability analysis, both are acceptable, and we have the freedom to choose the one that is more convenient to work with. We choose to break the loop at the input to the plant as sketched in Fig. 1, which leads to the the (input) open-loop transfer function \n\\mathbf L(s) = \\mathbf K\\mathbf G(s) = \\mathbf K(s\\mathbf I-\\mathbf A)^{-1}\\mathbf B.\nThe convenience is in that if the control u is just a scalar variable, then \\mathbf L is a scalar transfer function.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Guaranteed GM and PM for LQR"
    ]
  },
  {
    "objectID": "ext_LQR_margins.html#return-difference",
    "href": "ext_LQR_margins.html#return-difference",
    "title": "Guaranteed gain and phase margins for LQR",
    "section": "Return difference",
    "text": "Return difference\nStarting with the scalar (input) open-loop transfer function, the term \n1+L(s),\n whose zeros are the closed-loop poles, is known as the return difference.\nWe analyze it on the imaginary axis, that is, for s=j\\omega, \\omega\\in\\mathbb R. The magnitude \n|1+L(j\\omega)| = |-1-L(j\\omega)|\n can be interpreted as the distance of the Nyquist plot of the open-loop transfer function evaluated at some \\omega \\in \\mathbb R from the point -1 in the complex plane. It turns out, that it is better from an analysis viewpoint to evaluate the squared magnitude instead, that is,\n\n|1+L(j\\omega)|^2 =\\overline{(1+L(j\\omega))}(1+L(j\\omega)).",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Guaranteed GM and PM for LQR"
    ]
  },
  {
    "objectID": "ext_LQR_margins.html#paraconjugate-system",
    "href": "ext_LQR_margins.html#paraconjugate-system",
    "title": "Guaranteed gain and phase margins for LQR",
    "section": "Paraconjugate system",
    "text": "Paraconjugate system\nIn order to compute the squared magnitude, we used a complex conjugate of a function evaluated on the imaginary axis. Given a transfer function, say L(s), we now define an auxiliary transfer function called paraconjugate transfer function as \\boxed{\n\\tilde{L}(s) \\coloneqq L(-s).}\n\nThe crucial property of a paraconjugate transfer function is exhibited when it is evaluated on the imaginary axis \n  \\tilde L(j\\omega) = L(-j \\omega) = \\overline{L(j \\omega)}.\n\nUsefulness of this property for evaluating the square of the magnitude of the frequency response is now perhaps obvious \n|L(j\\omega)|^2 = \\overline{L(j\\omega)}L(j\\omega)| = \\left.\\tilde L(s) L(s)\\right |_{s = j\\omega}.\n\nAn obvious question might pop up here: why not introducing a transfer function \\bar{L} such that \\bar{L}(s) = \\overline{L(s)}? For a real-parameter transfer function this would require complex conjugation of the powers of s only. The trouble of this approach is, however, that \\bar s is not analytic in the complex-variable sense, while -s is (this may not be obvious, but here we only state it as a fact). This also explains the terminology: the term conjugate, which generally means that two things are related in some sense, is often used to denote a complex conjugate number (either a bar over the number or an asterisk next to it). Therefore calling our \\tilde L just a conjugate transfer function might be misleading (although some people do it), because it would suggest that it is equal to \\overline{L(s)} , which is only the case on the imaginary axis but not anywhere else! That is why there is the “para” prefix in the name. But the fact is that neither the terminology nor the notation is not standard here.\nIn the complex-coefficient case, the definition must be modified to \\boxed{\n  \\tilde L(s) \\coloneqq \\bar L(-s),}\n\nwhere \\bar L denotes complex conjugation of the coefficients only (note that here we use the symbol in a different way than in the discussion above).\nIn the case of a real-coefficient matrix transfer function, the paraconjugation also contains a transpose of the matrix, that is, \n  \\widetilde{\\mathbf L}(s) \\coloneqq \\mathbf L^\\top(-s).\n\nThe case of a complex-coefficient matrix transfer function, (complex) conjugate transpose instead of a transpose must be used \\boxed{\n  \\widetilde{\\mathbf L}(s) \\coloneqq \\mathbf L^\\ast(-s) = \\overline{\\mathbf L}^\\top(-s).}",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Guaranteed GM and PM for LQR"
    ]
  },
  {
    "objectID": "ext_LQR_margins.html#kalmans-identity-for-return-difference-for-lqr",
    "href": "ext_LQR_margins.html#kalmans-identity-for-return-difference-for-lqr",
    "title": "Guaranteed gain and phase margins for LQR",
    "section": "Kalman’s identity for return difference for LQR",
    "text": "Kalman’s identity for return difference for LQR\nNow we are ready to state the Kalman’s identity for the return difference of the LQR controller. For simplicity we now restrict ourselves to \\mathbf R=\\rho I, but the result is available for a general \\mathbf R too.\n\n\\left(\\mathbf I+\\widetilde{\\mathbf L}\\right) \\left(\\mathbf I+\\mathbf L\\right) = \\mathbf I + \\frac{1}{\\rho} \\mathbf B^\\top (-sI-\\mathbf A)^{-\\top} \\mathbf Q (sI-\\mathbf A)^{-1} \\mathbf B.\n\nThe proof can be found elsewhere. Here we focus on the implications – relying on the positive semidefiniteness of \\mathbf Q and positive definiteness of \\mathbf R (positiveness of \\rho), we argue that the second matrix on the right is positive semidefinite. Hence \\boxed{\n\\left(\\mathbf I+\\widetilde{\\mathbf L}\\right) \\left(\\mathbf I+\\mathbf L\\right) \\succeq \\mathbf I.}\n\nWe may develop better appreciation for the result if we consider its SISO version. Restricting the function arguments to the imaginary axis we get \n\\boxed{\n|1+KG(j\\omega)|\\geq 1,\\quad \\forall \\omega.\n}\n\nThis is powerful, isn’t it? This reads that the distance between the Nyquist plot of the open-loop transfer function and the point -1 in the complex plane is at least 1. The interpretation of this result is visualized in the series of figures Fig. 2, Fig. 3, Fig. 4 below. In words, there is a circle of radius 1 centered at the point -1 in the complex plane, which is guaranteed not to be entered by the Nyquist curve.\n\n\n\n\n\n\n\nFigure 2: Guaranteed gain and phase margins for LQR\n\n\n\n\n\n\n\n\n\nFigure 3: Guaranteed gain and phase margins for LQR\n\n\n\n\n\n\n\n\n\nFigure 4: Guaranteed gain and phase margins for LQR\n\n\n\nRecalling the definitions of GM and PM, and using some straightforward trigonometry, it follows that the following lower and upper bounds on the GM and PM hold\n\\boxed{\nGM_+ = \\infty,\\quad GM_- \\leq \\frac{1}{2},\\quad PM_+ \\geq 60^\\circ,\\quad PM_- \\leq -60^\\circ.\n}\n\nFinally, this can be checked using the following code.\n\n\nShow the code\nusing LinearAlgebra\nusing ControlSystems\nusing Plots\nusing Random\nRandom.seed!(1234)\n\nn = 5\nG = ssrand(n,1,n)\nA,B,C,D = ssdata(G)\nC = Matrix{Float64}(I, n, n)\nD = 0.0 \nG = ss(A,B,C,D) # Recreate the state-space system with the new C matrix\n\nQ = 5I\nR = I\nK = lqr(G,Q,R)\n\nL = K*G # Open-loop system\n\nmarginplot(L)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Bode plot of a system with an LQR feedback controller – GM and PM indeed comply with the theoretically guaranteed bounds\n\n\n\n\n\n\nShow the code\nnyquistplot(L,lw=2,legend=false,xlabel=\"Real\",ylabel=\"Imaginary\",title=\"\",aspect_ratio=1)\n\n\n\n┌ Warning: Keyword argument hover not supported with Plots.GRBackend().  Choose from: annotationcolor, annotationfontfamily, annotationfontsize, annotationhalign, annotationrotation, annotations, annotationvalign, arrow, aspect_ratio, axis, background_color, background_color_inside, background_color_outside, background_color_subplot, bar_width, bins, bottom_margin, camera, clims, color_palette, colorbar, colorbar_entry, colorbar_scale, colorbar_title, colorbar_titlefont, colorbar_titlefontcolor, colorbar_titlefontrotation, colorbar_titlefontsize, connections, contour_labels, discrete_values, fill, fill_z, fillalpha, fillcolor, fillrange, fillstyle, flip, fontfamily, fontfamily_subplot, foreground_color, foreground_color_axis, foreground_color_border, foreground_color_grid, foreground_color_subplot, foreground_color_text, formatter, framestyle, grid, gridalpha, gridlinewidth, gridstyle, group, guide, guidefont, guidefontcolor, guidefontfamily, guidefonthalign, guidefontrotation, guidefontsize, guidefontvalign, html_output_format, inset_subplots, label, layout, left_margin, legend_background_color, legend_column, legend_font, legend_font_color, legend_font_family, legend_font_halign, legend_font_pointsize, legend_font_rotation, legend_font_valign, legend_foreground_color, legend_position, legend_title, legend_title_font_color, legend_title_font_family, legend_title_font_pointsize, legend_title_font_rotation, legend_title_font_valigm, levels, lims, line, line_z, linealpha, linecolor, linestyle, linewidth, link, margin, marker_z, markeralpha, markercolor, markershape, markersize, markerstrokealpha, markerstrokecolor, markerstrokewidth, minorgrid, minorgridalpha, minorgridlinewidth, minorgridstyle, minorticks, mirror, normalize, orientation, overwrite_figure, permute, plot_title, plot_titlefontcolor, plot_titlefontfamily, plot_titlefontrotation, plot_titlefontsize, plot_titlelocation, plot_titlevspan, polar, primary, projection, quiver, ribbon, right_margin, rotation, scale, series_annotations, seriesalpha, seriescolor, seriestype, show, show_empty_bins, showaxis, size, smooth, subplot, subplot_index, thickness_scaling, tick_direction, tickfontcolor, tickfontfamily, tickfonthalign, tickfontrotation, tickfontsize, tickfontvalign, ticks, title, titlefontcolor, titlefontfamily, titlefonthalign, titlefontrotation, titlefontsize, titlefontvalign, top_margin, unitformat, weights, widen, window_title, x, xdiscrete_values, xerror, xflip, xforeground_color_axis, xforeground_color_border, xforeground_color_grid, xforeground_color_text, xformatter, xgrid, xgridalpha, xgridlinewidth, xgridstyle, xguide, xguidefontcolor, xguidefontfamily, xguidefonthalign, xguidefontrotation, xguidefontsize, xguidefontvalign, xlims, xlink, xminorgrid, xminorgridalpha, xminorgridlinewidth, xminorgridstyle, xminorticks, xmirror, xrotation, xscale, xshowaxis, xtick_direction, xtickfontcolor, xtickfontfamily, xtickfonthalign, xtickfontrotation, xtickfontsize, xtickfontvalign, xticks, xunitformat, xwiden, y, ydiscrete_values, yerror, yflip, yforeground_color_axis, yforeground_color_border, yforeground_color_grid, yforeground_color_text, yformatter, ygrid, ygridalpha, ygridlinewidth, ygridstyle, yguide, yguidefontcolor, yguidefontfamily, yguidefonthalign, yguidefontrotation, yguidefontsize, yguidefontvalign, ylims, ylink, yminorgrid, yminorgridalpha, yminorgridlinewidth, yminorgridstyle, yminorticks, ymirror, yrotation, yscale, yshowaxis, ytick_direction, ytickfontcolor, ytickfontfamily, ytickfonthalign, ytickfontrotation, ytickfontsize, ytickfontvalign, yticks, yunitformat, ywiden, z, z_order, zdiscrete_values, zerror, zflip, zforeground_color_axis, zforeground_color_border, zforeground_color_grid, zforeground_color_text, zformatter, zgrid, zgridalpha, zgridlinewidth, zgridstyle, zguide, zguidefontcolor, zguidefontfamily, zguidefonthalign, zguidefontrotation, zguidefontsize, zguidefontvalign, zlims, zlink, zminorgrid, zminorgridalpha, zminorgridlinewidth, zminorgridstyle, zminorticks, zmirror, zrotation, zscale, zshowaxis, ztick_direction, ztickfontcolor, ztickfontfamily, ztickfonthalign, ztickfontrotation, ztickfontsize, ztickfontvalign, zticks, zunitformat, zwiden\n└ @ Plots ~/.julia/packages/Plots/MR7sb/src/args.jl:1557\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Nyquist plot of a system with an LQR feedback controller – GM and PM indeed comply with the theoretically guaranteed bounds\n\n\n\n\nEven after running the code a few times, for different models and weighting matrices, the GM and PM are always satisfying the bounds stated above. What a convenience! Recall that when designing a PID controller, it can easily happen that for some values of the controller parameters, the GM and PM are poor. In fact, the controller can even destabilize the system. This is not the case for LQR controllers. For arbitrary values of the weighting matrices, the LQR controller guarantees very decent GM and PM.\nWe will see later in this chapter, that this property is far from common for other kinds of controllers. Namely, it is lost in the case of the LQR controller, which is just the LQR combined with a Kalman filter.",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Guaranteed GM and PM for LQR"
    ]
  },
  {
    "objectID": "intro_rules.html",
    "href": "intro_rules.html",
    "title": "Rules of the course",
    "section": "",
    "text": "In this course we follow, at least partially, the concept of flipped learning. This means that you are expected to study a bit on your own even before the lecture. The study material is made available through this web site. It consists primarily of texts, codes and frequently also videos. At the lecture we will not repeat the introductory material, instead we will discuss it, and focus on the more advanced topics and examples.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#studying-starts-before-the-lecture",
    "href": "intro_rules.html#studying-starts-before-the-lecture",
    "title": "Rules of the course",
    "section": "",
    "text": "In this course we follow, at least partially, the concept of flipped learning. This means that you are expected to study a bit on your own even before the lecture. The study material is made available through this web site. It consists primarily of texts, codes and frequently also videos. At the lecture we will not repeat the introductory material, instead we will discuss it, and focus on the more advanced topics and examples.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#learning-goals",
    "href": "intro_rules.html#learning-goals",
    "title": "Rules of the course",
    "section": "Learning goals",
    "text": "Learning goals\nTo guide you in your self-study before the lecture, we always provide a set of learning goals for the given (weakly) block. These learning goals are structured into two parts – knowledge and understading, and problem-solving skills. These are formulated in a way that guides you while studying. Always start by reading this list and only then proceed to the study material. Then come back to the learning goals and check if you can tick them off.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#online-quizzes",
    "href": "intro_rules.html#online-quizzes",
    "title": "Rules of the course",
    "section": "Online quizzes",
    "text": "Online quizzes\nIn order to push you gently towards such studying before the lecture, you will be asked to fill in a short online quiz (in Brute system) even before the actual lecture.\n\n\n\n\n\n\nDeadlines for online quizzes\n\n\n\nThe deadlines are always set to the very beginning of the lectures, that is, Wednesday, 11:00 am.\n\n\nThese quizzes will not test if you achieved all the learning goals – you are not certainly not expected to learn everything by yourself. Typically only some easier learning goals of the “knowledge and understanding” type will be tested.\nThe role of such quizzes in the overall grading is described later on this page.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#howework-problems",
    "href": "intro_rules.html#howework-problems",
    "title": "Rules of the course",
    "section": "Howework problems",
    "text": "Howework problems\nIn our course, homework problems are assigned weekly. Homework problems are typically related to the “skills” part of the learning goals, and as such they will almost exclusively consist of writing some computer code.\nThe second (\\(\\approx\\)) half of the exercises session will be always dedicated to the homework problem(s). Ideally you will be able to solved the problems before leaving the classroom, but at least you will have an opportunity to get started with solving the problem, while taking advantage of having a lecturer and other students nearby.\nA challenging – but certainly rewarding as well – attribute of our homework problems is that their solutions are expected to be programmed in Julia language (see Software for the course).\nSimilarly as the online quizzes, you will be expected to submit the solutions through the Brute system, which will give an immediate feedback if the submitted solution is accepted or not. #TODO: what if the submitted solution is evaluated as incorrect? Can it be resubmitted? Immediately? How many times?\n\n\n\n\n\n\nDeadlines for submission of solutions to homework problems\n\n\n\nThe deadlines are always set to Wednesday, just before the beginning of the corresponding exercises, e.g., 12:45, 14:30, 16:15.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#semestral-project",
    "href": "intro_rules.html#semestral-project",
    "title": "Rules of the course",
    "section": "Semestral project",
    "text": "Semestral project\nStudents aiming at the final grading A must work on one project from our list during the semester (details will be made available shortly after the beginning of the semester). Students with no ambitions for the A grade do not have to work on the projects.\nAlthough students are encouraged to collaborate, no need to hide it, the submitted . #TODO: this should be made more precise.\nAt the end of the semester, a short report will have to be presented to the teacher. #TODO: students are typically finishing it during the exam period, do they have to do it before their own exam?\nAn ideal format of such report is a git repository within the FEE Gitlab containing a short text, software code, possibly data sets (from simulations or experiments), and graphs, photos, videos (from experiments).\nIts grading is binary too –⁠ either it is accepted by the teacher or not. In case the report is not accepted, details will be given and the student can correct/improve/extend the report and resubmit. Note however, that at least one week (5 working days) is needed for the teacher to evaluate the report. A report submitted a few days before the deadline stands no chance to be modified and resubmitted.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#grading",
    "href": "intro_rules.html#grading",
    "title": "Rules of the course",
    "section": "Grading",
    "text": "Grading\n\nOnline quizzes and homework problems\nThe primary motivation for introduction of the online quizzes and homework problems is to help your learning process (you receive feedback in real time). Your performance in quizzes and homework problems does not affect the final grade. Feel free to make some mistakes while learning.\nHowever, some thresholds do have to be passed in quizzes and homework problems to earn the credit (“zápočet” in Czech), and to be allowed to take an exam. The bar is not high, but if you are neglecting the quizzes and homework assignments, your passing will be in danger. Namely, at least 70% of competences (demonstrated in quizzes and homework problems) must be provably mastered. #TODO: how about retaking the quizzes and resubmitting the hw solutions?\n\n\nFinal exam\nThe final grade will be based on the exam organized at the end of the semester (in the exam period). The final exam will have both an open-book and closed-book parts. #TODO: details will be added.\n\n\nOverall grade\nGrading of the two partial exams will be done according to the following standard table, where the percentages refer to the number of maximum points that could be obtained in the given exam.\n\n\n\nGrading\nOpen-book exam\nClosed-book exam\n\n\n\n\nA\n90 %\n90 %\n\n\nB\n80 %\n80 %\n\n\nC\n70 %\n70 %\n\n\nD\n60 %\n60 %\n\n\nE\n50 %\n50 %\n\n\nF\nbelow 50 %\nbelow 50 %\n\n\n\nUltimately, in order to arrive at the overall final grade (A through F) we take the minimum of the two partial exam grades.\n\n\nRole of the semestral project in the grading\nWhile the semestral projects are not mandatory, without them you will not be able to get the highest grade A. On the other hand, if you do the project and your report is accepted, and you pass the exam, the grade you receive in the exam will be automatically improved by one (e.g., from C do B).",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "discr_dir_mpc_economic.html",
    "href": "discr_dir_mpc_economic.html",
    "title": "Economic MPC",
    "section": "",
    "text": "(Ellis, Liu, and Christofides 2017), (Ellis, Durand, and Christofides 2014), (Faulwasser, Grüne, and Müller 2018), (Rawlings, Angeli, and Bates 2012)",
    "crumbs": [
      "6. More on MPC",
      "Economic MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_economic.html#literature",
    "href": "discr_dir_mpc_economic.html#literature",
    "title": "Economic MPC",
    "section": "",
    "text": "(Ellis, Liu, and Christofides 2017), (Ellis, Durand, and Christofides 2014), (Faulwasser, Grüne, and Müller 2018), (Rawlings, Angeli, and Bates 2012)",
    "crumbs": [
      "6. More on MPC",
      "Economic MPC"
    ]
  },
  {
    "objectID": "cont_indir_goals.html",
    "href": "cont_indir_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Formulate the general problem of calculus of variations. Explain the difference between the variation and differential.\nWrite down the Euler-Lagrange equation and explain its role in optimal control (that it constitutes the first-order necessary condition of optimality for the problem of calculus of variations).\nGive the first-order necessary conditions of optimality for a general (possibly nonlinear) optimal control problem on a fixed and finite time interval. Highlight that it comes in the form of a set of differential and algebraic equations (DAE), together with the boundary conditions that reflect the type of the problem.\nGive the first-order necessary conditions for an optimal control problem on a fixed and finite time horizon with a continous-time LTI system and a quadratic cost - the so-called LQR problem. Discuss the form of boundary conditions if the final state is fixed or free.\nCharacterize qualitatively the solution to the LQ-optimal control problem on a fixed and finite time horizon with a fixed final state. Namely, you should emphasize that it is an open-loop control.\nCharacterize qualitatively the solution to the LQ-optimal control problem on a fixed and finite time horizon with a free final state. Namely, you should emphasize that it is a proportional time-varying state-feedback control and that the time-varying feedback gains are computed from the solution to the differential Riccati equation.\nExplain the basic facts about LQ-optimal control on an infinite time interval with a free final state. Namely, you should explain that it comes in the form of a proportional state feedback and that the feedback gain can be computed either as the limiting solution to the differential Riccati equation or (and this is preferrable) as a solution to Algebraic Riccati Equation (ARE). The latter option brings in some issues related to existence and uniqueness of a stabilizing controller, which you should discuss.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_indir_goals.html#knowledge-remember-and-understand",
    "href": "cont_indir_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Formulate the general problem of calculus of variations. Explain the difference between the variation and differential.\nWrite down the Euler-Lagrange equation and explain its role in optimal control (that it constitutes the first-order necessary condition of optimality for the problem of calculus of variations).\nGive the first-order necessary conditions of optimality for a general (possibly nonlinear) optimal control problem on a fixed and finite time interval. Highlight that it comes in the form of a set of differential and algebraic equations (DAE), together with the boundary conditions that reflect the type of the problem.\nGive the first-order necessary conditions for an optimal control problem on a fixed and finite time horizon with a continous-time LTI system and a quadratic cost - the so-called LQR problem. Discuss the form of boundary conditions if the final state is fixed or free.\nCharacterize qualitatively the solution to the LQ-optimal control problem on a fixed and finite time horizon with a fixed final state. Namely, you should emphasize that it is an open-loop control.\nCharacterize qualitatively the solution to the LQ-optimal control problem on a fixed and finite time horizon with a free final state. Namely, you should emphasize that it is a proportional time-varying state-feedback control and that the time-varying feedback gains are computed from the solution to the differential Riccati equation.\nExplain the basic facts about LQ-optimal control on an infinite time interval with a free final state. Namely, you should explain that it comes in the form of a proportional state feedback and that the feedback gain can be computed either as the limiting solution to the differential Riccati equation or (and this is preferrable) as a solution to Algebraic Riccati Equation (ARE). The latter option brings in some issues related to existence and uniqueness of a stabilizing controller, which you should discuss.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_indir_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "cont_indir_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nSolve the continuous-time LQR problem using solvers available in your software of choice (Matlab, Julia, Python).",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_indir_overview.html",
    "href": "cont_indir_overview.html",
    "title": "Overview of continuous-time optimal control",
    "section": "",
    "text": "Through this chapter we are entering into the realm of continuous-time optimal control – we are going to consider dynamical systems that evolve in continuous time, and we are going to search for control that also evolves in continuous time.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Overview of continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_indir_overview.html#continuous-time-optimal-control-problem",
    "href": "cont_indir_overview.html#continuous-time-optimal-control-problem",
    "title": "Overview of continuous-time optimal control",
    "section": "Continuous-time optimal control problem",
    "text": "Continuous-time optimal control problem\nWe start by considering a nonlinear continuous-time system modelled by the state equation \n\\dot{\\bm{x}}(t) = \\mathbf f(\\bm x(t),\\bm u(t), t),\n where\n\n\\bm x(t) \\in \\mathbb R^n is the state vector at the continuous time t\\in \\mathbb R,\n\\bm u(t) \\in \\mathbb R^m is the control vector at the continuous time t,\n\\mathbf f: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb R \\to \\mathbb{R}^n is the state transition function (in general not only nonlinear but also time-varying).\n\nA general nonlinear continuous-time optimal control problem (OCP) is then formulated as \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u(\\cdot), \\bm x(\\cdot)}&\\quad \\left(\\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} L(\\bm x(t),\\bm u(t),t) \\; \\mathrm{d}t \\right)\\\\\n\\text{subject to}  &\\quad \\dot {\\bm{x}}(t) = \\mathbf f(\\bm x(t),\\bm u(t), t),\\quad t \\in [t_\\mathrm{i},t_\\mathrm{f}],\\\\\n                    &\\quad \\bm u(t) \\in \\mathcal U(t),\\\\\n                    &\\quad \\bm x(t) \\in \\mathcal X(t),\n\\end{aligned}\n where\n\nt_\\mathrm{i} is the initial continuous time,\nt_\\mathrm{f} is the final continuous time,\n\\phi() is a terminal cost function that penalizes the state at the final time (and possibly the final time too if it is regarded as an optimization variable),\nL() is a running (also stage) cost function,\nand \\mathcal U(t) and \\mathcal X(t) are (possibly time-dependent) sets of feasible controls and states – these sets are typically expressed using equations and inequalities. Should they be constant (not changing in time), the notation is just \\mathcal U and \\mathcal X.\n\nOftentimes it is convenient to handle the constraints of the initial and final states separately: \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u(\\cdot), \\bm x(\\cdot)}&\\quad \\left(\\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} L(\\bm x(t),\\bm u(t),t) \\; \\mathrm{d}t \\right)\\\\\n\\text{subject to}  &\\quad \\dot {\\bm{x}}(t) = \\mathbf f(\\bm x(t),\\bm u(t), t),\\quad t \\in [t_\\mathrm{i},t_\\mathrm{f}],\\\\\n                    &\\quad \\bm u(t) \\in \\mathcal U(t),\\\\\n                    &\\quad \\bm x(t) \\in \\mathcal X(t),\\\\\n                    &\\quad \\bm x(t_\\mathrm{i}) \\in \\mathcal X_\\mathrm{init},\\\\\n                    &\\quad \\bm x(t_\\mathrm{f}) \\in \\mathcal X_\\mathrm{final}.\n\\end{aligned}\n\nIn particular, at the initial time just one particular state is often considered. At the final time, the state might be required to be equal to some given value, it might be required to be in some set defined through equations or inequalities, or it might be left unconstrained. Finally, the constraints on the control and states typically (but not always) come in the form of lower and upper bounds. The optimal control problem then specializes to \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u(\\cdot), \\bm x(\\cdot)}&\\quad \\left(\\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} L(\\bm x(t),\\bm u(t),t) \\; \\mathrm{d}t \\right)\\\\\n\\text{subject to}  &\\quad \\dot {\\bm{x}}(t) = \\mathbf f(\\bm x(t),\\bm u(t)),\\quad t \\in [t_\\mathrm{i},t_\\mathrm{f}],\\\\\n                    &\\quad \\bm u_{\\min} \\leq \\bm u(t) \\leq \\bm u_{\\max},\\\\\n                    &\\quad \\bm x_{\\min} \\leq \\bm x(t) \\leq \\bm x_{\\max},\\\\\n                    &\\quad \\bm x(t_\\mathrm{i}) = \\mathbf x^\\text{init},\\\\\n                    &\\quad \\left(\\bm x(t_\\mathrm{f}) = \\mathbf x^\\text{ref}, \\; \\text{or} \\; \\mathbf h_\\text{final}(\\bm x(t_\\mathrm{f})) =  \\mathbf 0, \\text{or} \\; \\mathbf g_\\text{final}(\\bm x(t_\\mathrm{f})) \\leq  \\mathbf 0\\right),\n\\end{aligned}\n where\n\nthe inequalities should be interpreted componentwise,\n\\bm u_{\\min} and \\bm u_{\\max} are lower and upper bounds on the control, respectively,\n\\bm x_{\\min} and \\bm x_{\\max} are lower and upper bounds on the state, respectively,\n\\mathbf x^\\text{init} is a fixed initial state,\n\\mathbf x^\\text{ref} is a required (reference) final state,\nand the functions \\mathbf g_\\text{final}() and \\mathbf h_\\text{final}() can be used to define the constraint set for the final state.\n\n\n\n\n\n\n\nClassification of optimal control problems: Bolza, Mayer, and Lagrange problems\n\n\n\nThe cost function in the above defined optimal control problem contains both the cost incurred at the final time and the cumulative cost (the integral of the running cost) incurred over the whole interval. An optimal control problem with this general cost function is called Bolza problem in the literature. If the cost function only penalizes the final state and the final time, the problem is called Mayer problem. If the cost function only penalizes the cumulative cost, the problem is called Lagrange problem.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Overview of continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_indir_overview.html#why-continuous-time-optimal-control",
    "href": "cont_indir_overview.html#why-continuous-time-optimal-control",
    "title": "Overview of continuous-time optimal control",
    "section": "Why continuous-time optimal control?",
    "text": "Why continuous-time optimal control?\nWhy are we interested in continuous-time optimal control when at the end of the day most if not all controllers are implemented using computers and hence in discrete time? There are several reasons:\n\nThe theory for continuous-time optimal control is highly mature and represents a pinnacle of human ingenuity. It would be a pity to ignore it. It is also much richer than the theory for discrete-time optimal control. For example, when considering the time-optimal control, we can differentiate the cost function with respect to the final time because it is a continuous (real) variable.\nAlthough the theoretical concepts needed for continuous-time optimal control are more advanced (integrals and derivatives instead of sums and differences, function spaces instead of spaces of sequences, calculus of variations instead of differential calculus), the results are often simpler than in the discrete-time case – the resulting formulas just look neater and more compact.\nWe will see later that methods for solving general continuous-time optimal control problems must use some kind of (temporal)discretization. Isn’t it then enough to study discretization and discrete-time optimal control separately? It will turn out that discretization can be regarded as a part of the solution process.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Overview of continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_indir_overview.html#approaches-to-continuous-time-optimal-control",
    "href": "cont_indir_overview.html#approaches-to-continuous-time-optimal-control",
    "title": "Overview of continuous-time optimal control",
    "section": "Approaches to continuous-time optimal control",
    "text": "Approaches to continuous-time optimal control\nThere are essentially the same three approaches to continuous-time optimal control as we have seen when studying discrete-time optimal control:\n\nindirect approaches,\ndirect approaches,\ndynamic programming.\n\n\n\n\n\n\n\n\n\nG\n\n\ndiscrete_time_optimal_control\n\nApproaches to continuous-time optimal control\n\n\n\ndirect_approach\n\nIndirect approach\n\n\n\ndiscrete_time_optimal_control-&gt;direct_approach\n\n\n\n\n\nindirect_approach\n\nDirect approach\n\n\n\ndiscrete_time_optimal_control-&gt;indirect_approach\n\n\n\n\n\ndynamic_programming\n\nDynamic programming\n\n\n\ndiscrete_time_optimal_control-&gt;dynamic_programming\n\n\n\n\n\n\n\n\nFigure 1: Three approaches to continuous-time optimal control\n\n\n\n\n\nUnlike in the chapters on optimal control of discrete-time systems, here we start with the indirect approach. We would like to mimic the methodology we introduced for discrete-time system, but note that here we are not optimizing over n-tuples of real numbers (aka vectors or finite or even infinite sequences) but we are optimizing over functions (trajectories). Instances of such optimization “variables” are shown in Figure 2.\n\n\n\n\n\n\nFigure 2: Two instances of state and input trajectories x(\\cdot) and u(\\cdot), respectively\n\n\n\nIn order to be able to analyze such optimization problem, we need to introduce the framework of calculus of variations, which is what we do next.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Overview of continuous-time optimal control"
    ]
  },
  {
    "objectID": "dynamic_programming_references.html",
    "href": "dynamic_programming_references.html",
    "title": "References",
    "section": "",
    "text": "Dynamic programming (DP) is a fairly powerful and yet general framework that finds its use in many disciplines. Optimal control is not the only one. But in this overview of the literature we deliberately focus on the DP references with optimal control flavour.\nOur introductory treatment was based almost exclusively on the (also just introductory) Chapter 6 in [1]. Electronic version of the book is freely available on the author’s webpage.\nComparable introduction is provided in [2]. Although it does not appear to be legally available for free in an electronic form, its reprint by a low-cost publisher makes it an affordable (and recommendable) classic reference. Another classic [3] actually uses dynamic programming as the key technique to derive all those LQ-optimal regulation and tracking results. A few copies of this book are available in the faculty library at NTK. The authors also made an electronic version available for free on their website.\nFairly comprehensive treatment of control-oriented DP is in the two-volume monograph [4] and [5]. It is not available online for free, but the book webpage contains links to other supporting materials including lecture notes. Furthermore, the latest book by the same author [6], which is available for free download, contains a decent introduction to dynamic programming.\nHaving just referenced a book on reinforcement learning (RL), indeed, this popular concept — or at least some of its flavours — is closely related to dynamic programming. In fact, it offers a way to overcome some of the limitations of dynamic programming. In our introductory lecture we are not covering RL, but an interested student can take advantage of availability of high-quality resources such as the the RL-related books and other resources by D. Bertsekas and another recommendable introduction to RL from control systems perspective [7], which is also available for free download.\nThe book [8] often regarded as the bible of RL is nice (and freely available for download) but may be rather difficult to read for a control engineer because of major differences in terminology and notation.\n\n\n\n\n Back to topReferences\n\n[1] F. L. Lewis, D. Vrabie, and V. L. Syrmo, Optimal Control, 3rd ed. John Wiley & Sons, 2012. Accessed: Mar. 09, 2022. [Online]. Available: https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf\n\n\n[2] D. E. Kirk, Optimal Control Theory: An Introduction, Reprint of the 1970 edition. Dover Publications, 2004.\n\n\n[3] B. D. O. Anderson and J. B. Moore, Optimal Control: Linear Quadratic Methods, Reprint of the 1989 edition. Dover Publications, 2007. Available: http://users.cecs.anu.edu.au/~john/papers/BOOK/B03.PDF\n\n\n[4] D. P. Bertsekas, Dynamic Programming and Optimal Control, 4th ed., vol. I. Belmont, Massachusetts: Athena Scientific, 2017. Available: http://athenasc.com/dpbook.html\n\n\n[5] D. P. Bertsekas, Dynamic Programming and Optimal Control, 4th ed., vol. II. Belmont, Massachusetts: Athena Scientific, 2012. Available: http://athenasc.com/dpbook.html\n\n\n[6] D. P. Bertsekas, A Course in Reinforcement Learning. Belmont, Massachusetts: Athena Scientific, 2023. Accessed: Sep. 15, 2023. [Online]. Available: https://web.mit.edu/dimitrib/www/RLCOURSECOMPLETE.pdf\n\n\n[7] S. Meyn, Control Systems and Reinforcement Learning. Cambridge University Press, 2022. Accessed: Aug. 25, 2021. [Online]. Available: https://meyn.ece.ufl.edu/control-systems-and-reinforcement-learning/\n\n\n[8] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, Massachusetts: A Bradford Book, 2018. Available: http://incompleteideas.net/book/the-book-2nd.html",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "References"
    ]
  },
  {
    "objectID": "reduction_order_model.html",
    "href": "reduction_order_model.html",
    "title": "Model order reduction",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Model order reduction"
    ]
  },
  {
    "objectID": "roban_hw.html",
    "href": "roban_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Homework"
    ]
  },
  {
    "objectID": "cont_numerical_indirect.html",
    "href": "cont_numerical_indirect.html",
    "title": "Numerical methods for indirect approach",
    "section": "",
    "text": "The indirect approach to optimal control reformulates the optimal control problem as a system of differential and algebraic equations (DAE) with the values of some variables specified at both ends of the time interval – the two-point boundary value problem (TP–BVP). It is only in special cases that we are able to reformulate the TP–BVP as an initial value problem (IVP), the prominent example of which is the LQR problem and the associate differential Riccati equation solved backwards in time. However, generally we need to solve the TP–BVP DAE and the only way to do it is by numerical methods. Here we discuss them in some detail.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for indirect approach"
    ]
  },
  {
    "objectID": "cont_numerical_indirect.html#gradient-method-for-the-tp-bvp-dae-for-free-final-state",
    "href": "cont_numerical_indirect.html#gradient-method-for-the-tp-bvp-dae-for-free-final-state",
    "title": "Numerical methods for indirect approach",
    "section": "Gradient method for the TP-BVP DAE for free final state",
    "text": "Gradient method for the TP-BVP DAE for free final state\nRecall that with the Hamiltonian defined as H(\\bm x, \\bm u, \\bm \\lambda) = L(\\bm x, \\bm u) + \\bm \\lambda^\\top \\mathbf f(\\bm x, \\bm u), the necessary conditions of optimality for the fixed final time and free final state are are given by the following system of differential and algebraic equations (DAE) \n\\begin{aligned}\n\\dot{\\bm{x}} &= \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\bm \\lambda) \\\\\n\\dot{\\bm{\\lambda}} &= -\\nabla_{\\bm x} H(\\bm x,\\bm u,\\bm \\lambda) \\\\\n\\mathbf 0 &= \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)\\qquad (\\text{or} \\qquad \\bm u^\\star = \\text{argmax } H(\\bm x^\\star,\\bm u, \\bm\\lambda^\\star),\\quad \\bm u \\in\\mathcal{U})\\\\\n\\bm x(t_\\mathrm{i}) &=\\mathbf x_\\mathrm{i}\\\\\n\\bm \\lambda(t_\\mathrm{f}) &= \\nabla\\phi(\\bm{x}(t_\\mathrm{f})).\n\\end{aligned}\n\nOne idea to solve this is to guess at the trajectory \\bm u(t) on a grid of the time interval, use it to solve the state and costate equations, and then with all the three variables \\bm x, \\bm u, and \\bm u evaluate how much the stationarity equation is actually not satisfied. Based on the this, modify \\bm u and go for another iteration. Formally this is expressed as the algorithm:\n\nSet some initial trajectory \\bm u(t),\\; t\\in[t_\\mathrm{i},t_\\mathrm{f}] on a grid of points in [t_\\mathrm{i},t_\\mathrm{f}].\nWith the chosen \\bm u(\\cdot) and the initial state \\bm x(t_\\mathrm{i}), solve the state equation \n\\dot{\\bm{x}} = \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\bm \\lambda) = \\mathbf f(\\bm x, \\bm u)\n for \\bm x(t) using a solver for initial value problem ODE, that is, on a grid of t\\in[t_\\mathrm{i},t_\\mathrm{f}].\nHaving the control and state trajectories, \\bm u(\\cdot) and \\bm x(\\cdot), solve the costate equation \n\\dot{\\bm{\\lambda}} = -\\nabla_{\\bm x} H(\\bm x,\\bm u,\\bm \\lambda)\n\nfor the costates \\bm \\lambda(t), starting at the final time t_\\mathrm{f}, invoking the boundary condition \\bm \\lambda(t_\\mathrm{f}) = \\nabla\\phi(\\bm{x}(t_\\mathrm{f})).\nEvaluate \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda) for all t\\in[t_\\mathrm{i}, t_\\mathrm{f}].\nIf \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda) \\approx  \\mathbf 0 for all t\\in[t_\\mathrm{i}, t_\\mathrm{f}], quit, otherwise modify \\bm u(\\cdot) and go to the step 2.\n\nThe question is, of course, how to modify \\bm u(t) for all t \\in [t_\\mathrm{i}, t_\\mathrm{f}] in the step 4. Recall that the variation of the (augmented) cost functional is \n\\begin{aligned}\n\\delta  J^\\text{aug} &= [\\nabla \\phi(\\bm x(t_\\mathrm{f})) - \\bm\\lambda(t_\\mathrm{f})]^\\top \\delta \\bm{x}(t_\\mathrm{f})\\\\\n& \\qquad + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\dot{\\bm{\\lambda}} +\\nabla_{\\bm x} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm x(t)\\mathrm{d}t + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\dot{\\bm{x}} - \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm\\lambda(t)  \\mathrm{d}t\\\\\n& \\qquad + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm u(t) \\mathrm{d}t\n\\end{aligned},\n and for state and costate variables satisfying the state and costate equations this variation simplifies to \n\\delta  J^\\text{aug} = \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm u(t) \\mathrm{d}t.\n\nSince our goal is to minimize J^\\text{aug}, we need to make \\Delta  J^\\text{aug}\\leq0. Provided the increment \n\\delta \\mathbf u(t)=\\bm u^{(i+1)}(t)-\\bm u^{(i)}(t)\n is small, we can consider the linear approximation \\delta  J^\\text{aug} instead. We choose \n\\delta \\bm u(t)  = -\\alpha \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)\n for \\alpha&gt;0, which means that the control trajectory in the next iteration is \\boxed\n{\\bm u^{(i+1)}(t)  = \\bm u^{(i)}(t) -\\alpha \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda),}\n and the variation of the augmented cost funtion is\n\n\\delta  J^\\text{aug} = -\\alpha\\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)]^2 \\mathrm{d}t \\leq 0,\n and it is zero only for \\nabla_{\\bm u} H(t,\\bm x,\\bm u,\\bm \\lambda) = \\mathbf 0 for all t\\in[t_\\mathrm{i}, t_\\mathrm{f}].",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for indirect approach"
    ]
  },
  {
    "objectID": "cont_numerical_indirect.html#methods-for-solving-tp-bvp-ode",
    "href": "cont_numerical_indirect.html#methods-for-solving-tp-bvp-ode",
    "title": "Numerical methods for indirect approach",
    "section": "Methods for solving TP-BVP ODE",
    "text": "Methods for solving TP-BVP ODE\nHere we assume that from the stationarity equation \n\\mathbf 0 = \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)\n we can express \\bm u(t) as a function of the the state and costate variables, \\bm x(t) and \\bm \\lambda(t), respectively. In fact, Pontryagin’s principles gives this expression as \\bm u^\\star(t) = \\text{arg} \\min_{\\bm u(t) \\in\\mathcal{U}} H(\\bm x^\\star(t),\\bm u(t), \\bm\\lambda^\\star(t)). And we substitute for \\bm u(t) into the state and costate equations. This way we eliminate \\bm u(t) from the system of DAEs and we are left with a system of ODEs for \\bm x(t) and \\bm \\lambda(t) only. Formally, the resulting Hamiltonian is a different function as it is now a function of two variables only.\n\n\\begin{aligned}\n\\dot{\\bm{x}} &= \\nabla_{\\bm\\lambda} \\mathcal H(\\bm x,\\bm \\lambda) \\\\\n\\dot{\\bm{\\lambda}} &= -\\nabla_{\\bm x} \\mathcal H(\\bm x,\\bm \\lambda) \\\\\n\\bm x(t_\\mathrm{i}) &=\\mathbf x_\\mathrm{i}\\\\\n\\bm x(t_\\mathrm{f}) &= \\mathbf x_\\mathrm{f} \\qquad \\text{or} \\qquad \\bm \\lambda(t_\\mathrm{f}) = \\nabla\\phi(\\bm{x}(t_\\mathrm{f})).\n\\end{aligned}\n\nAlthough we now have an ODE system, it is still a BVP. Strictly speaking, from now on, arbitrary reference on numerical solution of boundary value problems can be consulted to get some overview – we no longer need to restrict ourselves to the optimal control literature and software. On the other hand, the right sides are not quite arbitrary – these are Hamiltonian equations – and this property could and perhaps even should be exploited by the solution methods.\nThe methods for solving general BVPs are generally divided into\n\nshooting and multiple shooting methods,\ndiscretization methods,\ncollocation methods.\n\n\nShooting methods\n\nShooting method outside optimal control\nHaving made the diclaimer that boundary value problems constitute a topic independent of the optimal control theory, we start their investigation within a control-unrelated setup. We consider a system of two ordinary differential equations in two variables with the value of the first variable specified at both ends while the value of the other variable is left unspecified \n\\begin{aligned}\n\\begin{bmatrix}\n  \\dot y_1(t)\\\\\n  \\dot y_2(t)\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nf_1(\\bm y,t)\\\\\nf_2(\\bm y,t)\n\\end{bmatrix}\\\\\ny_1(t_\\mathrm{i}) &= \\mathrm y_{1\\mathrm{i}},\\\\\ny_1(t_\\mathrm{f}) &= \\mathrm y_{1\\mathrm{f}}.\n\\end{aligned}\n\nAn idea for a solution method is this:\n\nGuess at the missing (unspecified) value y_{2\\mathrm{i}} of y_2 at the initial time t_\\mathrm{i},\nUse an IVP solver (for example ode45 in Matlab) to find the values of both variables over the whole interval [t_\\mathrm{i},t_\\mathrm{f}].\nCompare the simulated value of the state variable y_1 at the final time t_\\mathrm{f} and compare it with the boundary value .\nBased on the error e = y_1(t_\\mathrm{f})-\\mathrm y_{1\\mathrm{f}}, update y_{2\\mathrm{i}} and go back to step 2.\n\nHow shall the update in the step 4 be realized? The value of y_1 at the final time t_\\mathrm{f} and therefore the error e are functions of the value y_{2\\mathrm{i}} of y_2 at the initial time t_\\mathrm{i}. We can formally express this upon introducing a map F such that e = F(y_{2\\mathrm{i}}). The problem now boils down to solving the nonlinear equation \\boxed\n{F(y_{2\\mathrm{i}}) = 0.}\n\nIf Newton’s method is to be used for solving this equation, the derivative of F is needed. Most often than not, numerical solvers for IVP ODE have to be called in order to evaluate the function F, in which case the derivative cannot be determined analytically. Finite difference (FD) and algorithmic/automatic differentiation (AD) methods are available.\nIn this example we only considered y_1 and y_2 as scalar variables, but in general these could be vector variables, in which case a system of equations in the vector variable has to be solved. Instead of a single scalar derivative, its matrix version – Jacobian matrix – must be determined.\nBy now the reason for calling this method shooting is perhaps obvious. Indeed, the analogy with aiming and shooting a cannon is illustrative.\nAs another example, we consider the BVP for a pendulum.\n\nExample 1 (BVP for pendulum) For an ideal pendulum described by the second-order model \\ddot \\theta + \\frac{b}{ml^2}\\dot \\theta + \\frac{g}{l} \\sin(\\theta) = 0 and for a final time t_\\mathrm{f}, at which some prescribed value of \\theta(t_\\mathrm{f}) must be achieved, compute by the shooting method the needed value of the initial angle \\theta_\\mathrm{i}, while assuming the initial angular rate \\omega_\\mathrm{i} is zero.\n\n\nShow the code\nusing DifferentialEquations\nusing Roots\nusing Plots\n\nfunction demo_shoot_pendulum()\n    θfinal = -0.2;\n    tfinal = 3.5;\n    tspan = (0.0,tfinal)\n    tol = 1e-5\n    function pendulum!(dx,x,p,t)\n        g = 9.81\n        l = 1.0;\n        m = 1.0;\n        b = 0.1;\n        a₁ = g/l\n        a₂ = b/(m*l^2)\n        θ,ω = x\n        dx[1] = ω\n        dx[2] = -a₁*sin(θ) - a₂*ω\n    end\n    prob = ODEProblem(pendulum!,zeros(Float64,2),tspan)\n    function F(θ₀::Float64)\n        xinitial = [θ₀,0.0]\n        prob = remake(prob,u0=xinitial)\n        sol = solve(prob,Tsit5(),reltol=tol/10,abstol=tol/10)\n        return θfinal-sol[end][1]\n    end\n    θinitial = find_zero(F,(-pi,pi))    # Solving the equation F(θ)=0 using Roots package. In general can find more solutions.\n    xinitial = [θinitial,0.0]\n    prob = remake(prob,u0=xinitial)     # Already solved in F(), but we solve it again for plotting.\n    sol = solve(prob,Tsit5())\n    p1 = plot(sol,lw=2,xlabel=\"Time\",ylabel=\"Angle\",label=\"θ\",idxs=(1))\n    scatter!([tfinal],[θfinal],label=\"Required terminal θ\")\n    p2 = plot(sol,lw=2,xlabel=\"Time\",ylabel=\"Angular rate\",label=\"ω\",idxs=(2))\n    display(plot(p1,p2,layout=(2,1)))\nend\n\ndemo_shoot_pendulum()\n\n\nGKS: cannot open display - headless operation mode active\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: State responses for a pendulum on a given time interval, with zero initial angular rate and the initial angle solved for numerically so that the final angle attains a give value\n\n\n\n\n\nA few general comments to the above code:\n\nThe function F(\\theta_\\mathrm{i}) that defines the nonlinear equation F(\\theta_\\mathrm{i})=0 calls a numerical solver for an IVP ODE. The latter solver then should have the numerical tolerances set more stringent than the former.\nThe ODE problem should only be defined once and then in each iteration its parameters should be updated. In Julia, this is done by the remake function, but it may be similar for other languages.\n\n\n\nShooting method for indirect approach to optimal control\nWe finally bring the method into the realm of indirect approach to optimal control – it is the initial value \\lambda_\\mathrm{i} of the costate variable that serves as an optimization variable, while the initial value x_\\mathrm{i} of the state variable is known and fixed. The final values of both the state and costate variables are the outcomes of numerical simulation obtained using a numerical solver for an IVP ODE. Based on these, the residual is computed. Either as e = x(t_\\mathrm{f})-x_\\mathrm{f} if the final state is fixed, or as e = \\lambda(t_\\mathrm{f}) - \\nabla \\phi(x(t_\\mathrm{f})) if the final state is free. Based on this residual, the initial value of the costate is updated and another iteration of the algorithm is entered.\n\n\n\n\n\n\nFigure 2: Indirect shooting\n\n\n\n\nExample 2 (Shooting for indirect approach to LQR) Standard LQR optimal control for a second-order system on a fixed finite interval with a fixed final state.\n\n\nShow the code\nusing LinearAlgebra\nusing DifferentialEquations\nusing NLsolve\nusing Random\nRandom.seed!(1234)                                       # For reproducibility.\n\nfunction shoot_lq_fixed(A,B,Q,R,xinitial,xfinal,tfinal)\n    n = size(A)[1]\n    function statecostateeq!(dw,w,p,t)\n        x = w[1:n]\n        λ = w[(n+1):end]\n        dw[1:n] = A*x - B*(R\\B'*λ)\n        dw[(n+1):end] = -Q*x - A'*λ\n    end\n    λinitial = zeros(n)\n    tspan = (0.0,tfinal)\n    tol = 1e-5\n    function F(λinitial)\n        winitial = vcat(xinitial,λinitial)\n        prob = ODEProblem(statecostateeq!,winitial,tspan)\n        dsol = solve(prob,Tsit5(),abstol=tol/10,reltol=tol/10)\n        xfinalsolved = dsol[end][1:n]\n        return (xfinal-xfinalsolved)\n    end\n    nsol = nlsolve(F,λinitial,xtol=tol)                 # Could add autodiff=:forward.\n    λinitial = nsol.zero                                # Solving once again for plotting.\n    winitial = vcat(xinitial,λinitial)\n    prob = ODEProblem(statecostateeq!,winitial,tspan)\n    dsol = solve(prob,Tsit5(),abstol=tol/10,reltol=tol/10)\n    return dsol\nend\n\nfunction demo_shoot_lq_fixed()\n    n = 2                                               # Order of the system.\n    m = 1                                               # Number of inputs.\n    A = rand(n,n)                                       # Matrices modeling the system.\n    B = rand(n,m)\n        \n    Q = diagm(0=&gt;rand(n))                               # Weighting matrices for the quadratic cost function.\n    R = rand(1,1)\n\n    xinitial = [1.0, 2.0]\n    xfinal = [3.0, 4.0]\n    tfinal = 5.0 \n\n    dsol = shoot_lq_fixed(A,B,Q,R,xinitial,xfinal,tfinal)\n\n    p1 = plot(dsol,idxs=(1:2),lw=2,legend=false,xlabel=\"Time\",ylabel=\"State\")\n    p2 = plot(dsol,idxs=(3:4),lw=2,legend=false,xlabel=\"Time\",ylabel=\"Costate\")\n    display(plot(p1,p2,layout=(2,1)))\nend\n\ndemo_shoot_lq_fixed()\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Shooting method applied to the indirect approach to LQR problem\n\n\n\n\n\n\n\n\nMultiple shooting methods\nThe key deficiency of the shooting method is that the only source of the error is the error in the initial condition, this error then amplifies as it propagates over the whole time interval as the numerical integration proceeds, and consequently the residual is very sensitive to tiny changes in the initial value. The multiple shooting method is a remedy for this. The idea is to divide the interval [t_\\mathrm{i},t_\\mathrm{f}] into N subintervals [t_k,t_{k+1}] and to introduce the values of the state and co-state variable at the beginning of each subinterval as additional variables. Additional equations are then introduced that enforce the continuity of the variable at the end of one subinterval and at the beginning of the next subinterval.\n\n\n\n\n\n\nFigure 4: Indirect multiple shooting\n\n\n\n\n\nDiscretization methods\nShooting methods take advantage of availability of solvers for IVP ODEs. These solvers produce discret(ized) trajectories, proceeding (integration) step by step, forward in time. But they do this in a way hidden from users. We just have to set the initial conditions (possibly through numerical optimization) and the solver does the rest.\nAlternatively, the formulas for the discrete-time updates are not evaluated one by one, step by step, running forward in time, but are assembled to form a system of equations, in general nolinear ones. Appropriate boundary conditions are then added to these nonlinear equations and the whole system is then solved numerically, yielding a discrete approximation of the trajectories satisfying the BVP.\nSince all those equatins are solved simultaneously (as a system of equations), there is no advantage in using explicit methods for solving ODEs, and implicit methods are used instead.\nIt is now time to recall some crucial results from the numerical methods for solving ODEs. First, we start with the popular single-step methods known as the Runge-Kutta (RK) methods.\nWe consider the standard ODE \\dot x(t) = f(x(t),t).\nand we define the Butcher tableau as \n  \\begin{array}{ l | c c c c }\n    c_1 & a_{11} & a_{12} & \\ldots & a_{1s}\\\\\n    c_2 & a_{21} & a_{22} & \\ldots & a_{2s}\\\\\n    \\vdots & \\vdots\\\\\n    c_s & a_{s1} & a_{s2} & \\ldots & a_{ss}\\\\\n    \\hline\n      & b_{1} & b_{2} & \\ldots & b_{s}\n  \\end{array}.\n such that c_i = \\sum_{j=1}^s a_{ij}, and 1 = \\sum_{j=1}^s b_{j}.\nReffering to the particular Butcher table, a single step of the method is \n  \\begin{aligned}\n  f_{k1} &= f(x_k + h_k \\left(a_{11}f_{k1}+a_{12}f_{k2} + \\ldots + a_{1s}f_{ks}),t_k+c_1h_k\\right)\\\\\n  f_{k2} &= f(x_k + h_k \\left(a_{21}f_{k1}+a_{22}f_{k2} + \\ldots + a_{2s}f_{ks}),t_k+c_2h_k\\right)\\\\\n  \\vdots\\\\\n  f_{ks} &= f(x_k + h_k \\left(a_{s1}f_{k1}+a_{s2}f_{k2} + \\ldots + a_{ss}f_{ks}),t_k+c_sh_k\\right)\\\\\n  x_{k+1} &= x_k + h_k \\left(b_1 f_{k1}+b_2f_{k2} + \\ldots + b_sf_{ks}\\right).\n  \\end{aligned}\n\nIf the matrix A is strictly lower triangular, that is, if a_{ij} = 0 for all i&lt;j , the method belongs to explicit Runge-Kutta methods, otherwise it belongs to implicit Runge-Kutta methods.\nA prominent example of explicit RK methods is the 4-stage RK method (oftentimes referred to as RK4).\n\nExplicit RK4 method\nThe Buttcher table for the method is \n      \\begin{array}{ l | c c c c }\n        0 & 0 & 0 & 0 & 0\\\\\n        1/2 & 1/2 & 0 & 0 & 0\\\\\n        1/2 & 0 & 1/2 & 0 & 0\\\\\n        1 & 0 & 0 & 1 & 0\\\\\n        \\hline\n          & 1/6 & 1/3 & 1/3 & 1/6\n      \\end{array}.\n\nFollowing the Butcher table, a single step of this method is \n      \\begin{aligned}\n      f_{k1} &= f(x_k,t_k)\\\\\n      f_{k2} &= f\\left(x_k + \\frac{h_k}{2}f_{k1},t_k+\\frac{h_k}{2}\\right)\\\\\n      f_{k3} &= f\\left(x_k + \\frac{h_k}{2}f_{k2},t_k+\\frac{h_k}{2}\\right)\\\\\n      f_{k4} &= f\\left(x_k + h_k f_{k3},t_k+h_k\\right)\\\\\n      x_{k+1} &= x_k + h_k \\left(\\frac{1}{6} f_{k1}+\\frac{1}{3}f_{k2} + \\frac{1}{3}f_{k3} + \\frac{1}{6}f_{k4}\\right)\n      \\end{aligned}.\n\nBut as we have just mentions, explicit methods are not particularly useful for solving BVPs. We prefer implicit methods. One of the simplest is the implicit midpoint method.\n\n\nImplicit midpoint method\nThe Butcher tableau is \n  \\begin{array}{ l | c r }\n    1/2 & 1/2 \\\\\n    \\hline\n      & 1\n  \\end{array}\n\nA single step is then \\begin{aligned}\n  f_{k1} &= f\\left(x_k+\\frac{1}{2}f_{k1} h_k, t_k+\\frac{1}{2}h_k\\right)\\\\\n  x_{k+1} &= x_k + h_k f_{k1}.\n  \\end{aligned}\nBut adding to the last equation x_k we get x_{k+1} + x_k = 2x_k + h_k f_{k1}.\nDividing by two we get \\frac{1}{2}(x_{k+1} + x_k) = x_k + \\frac{1}{2}h_k f_{k1} and then it follows that \\boxed{x_{k+1} = x_k + h_k f\\left(\\frac{1}{2}(x_k+x_{k+1}),t_k+\\frac{1}{2}h_k\\right).}\nThe right hand side of the last equation explains the “midpoint” in the name. It can be viewed as a rectangular approximation to the integral in x_{k+1} = x_k + \\int_{t_k}^{t_{k+1}} f(x(t),t)\\mathrm{d}t as the integral is computed as an area of a rectangle with the height determined by f() evaluated in the middle point.\nAlthough we do not explain the details here, let’s just note that it is the simplest of the collocation methods. In particular it belongs to Gauss (also Gauss-Legandre) methods.\n\n\nImplicit trapezoidal method\nThe method can be viewed both as a single-step (RK) method and a multi-step method. When viewed as an RK method, its Butcher table is \n  \\begin{array}{ l | c r }\n    0 & 0 & 0 \\\\\n    1 & 1/2 & 1/2 \\\\\n    \\hline\n      & 1/2 & 1/2 \\\\\n  \\end{array}\n   Following the Butcher table, a single step of the method is then \n  \\begin{aligned}\n  f_{k1} &= f(x_k,t_k)\\\\\n  f_{k2} &= f(x_k + h_k \\frac{f_{k1}+f_{k2}}{2},t_k+h_k)\\\\\n  x_{k+1} &= x_k + h_k \\left(\\frac{1}{2} f_{k1}+\\frac{1}{2} f_{k2}\\right).\n  \\end{aligned}\n\nBut since the collocation points are identical with the nodes (grid/mesh points), we can relabel to \\begin{aligned}\n  f_{k} &= f(x_k,t_k)\\\\\n  f_{k+1} &= f(x_{k+1},t_{k+1})\\\\\n  x_{k+1} &= x_k + h_k \\left(\\frac{1}{2} f_{k}+\\frac{1}{2} f_{k+1}\\right).\n  \\end{aligned}\n\nThis possibility is a particular advantage of Lobatto and Radau methods, which contain both end points of the interval or just one of them among the collocation points. The two symbols f_k and f_{k+1} are really just shorthands for values of the function f at the beginning and the end of the integration interval, so the first two equations of the triple above are not really equations to be solved but rather definitions. And we can assemble it all into just one equation \\boxed{\n      x_{k+1} = x_k + h_k \\frac{f(x_k,t_k)+f(x_{k+1},t_{k+1})}{2}.\n      }\n\nThe right hand side of the last equation explains the “trapezoidal” in the name. It can be viewed as a trapezoidal approximation to the integral in x_{k+1} = x_k + \\int_{t_k}^{t_{k+1}} f(x(t),t)\\mathrm{d}t as the integral is computed as an area of a trapezoid.\nWhen it comes to building a system of equations within transcription methods, we typically move all the terms just on one side to obtain the defect equations x_{k+1} - x_k - h_k \\left(\\frac{1}{2} f(x_k,t_k)+\\frac{1}{2} f(x_{k+1},t_{k+1})\\right) = 0.\n\n\nHermite-Simpson method\nIt belongs to the family of Lobatto III methods, namely it is a 3-stage Lobatto IIIA method. Butcher tableau \n  \\begin{array}{ l | c c c c }\n    0 & 0 &0 & 0\\\\\n    1/2 & 5/24 & 1/3 & -1/24\\\\\n    1 & 1/6 & 2/3 & 1/6\\\\\n    \\hline\n      & 1/6 & 2/3 & 1/6\n  \\end{array}\n\nHermite-Simpson method can actually come in three forms (this is from [1]):\n\nPrimary form\nThere are two equations for the given integration interval [t_k,t_{k+1}] x_{k+1} = x_k + h_k \\left(\\frac{1}{6}f_k + \\frac{2}{3}f_{k2} + \\frac{1}{6}f_{k+1}\\right), x_{k2} = x_k + h_k \\left(\\frac{5}{24}f_k + \\frac{1}{3}f_{k2} - \\frac{1}{24}f_{k+1}\\right), where the f symbols are just shorthand notations for values of the function at a certain point f_k = f(x_k,u(t_k),t_k), f_{k2} = f(x_{k2},u(t_{k2}),t_{k2}), f_{k+1} = f(x_{k+1},u(t_{k+1}),t_{k+1}), and the off-grid time t_{k2} is given by t_{k2} = t_k + \\frac{1}{2}h_k.\nThe first of the two equations can be recognized as Simpson’s rule for computing a definite integral. Note that while considering the right hand sides as functions of the control inputs, we also correctly express at which time (the collocation time) we consider the value of the control variable.\nBeing this general allows considering general control inputs and not only piecewise constant control inputs. For example, if we consider piecewise linear control inputs, then u(t_{k2}) = \\frac{u_k + u_{k+1}}{2}. But if we stick to the (more common) piecewise constant controls, not surprisingly u(t_{k2}) = u_k. Typically we format the equations as defect equations, that is, with zero on the right hand side \n\\begin{aligned}\nx_{k+1} - x_k - h_k \\left(\\frac{1}{6}f_k + \\frac{2}{3}f_{k2} + \\frac{1}{6}f_{k+1}\\right) &= 0,\\\\\nx_{k2} - x_k - h_k \\left(\\frac{5}{24}f_k + \\frac{1}{3}f_{k2} - \\frac{1}{24}f_{k+1}\\right) &= 0.\n\\end{aligned}\n\nThe optimization variables for every integration interval are x_k,u_k,x_{k2}, u_{k2}.\n\n\nHermite-Simpson Separated (HSS) method\nAlternatively, we can express f_{k2} in the first equation as a function of the remaining terms and then substitute to the second equation. This will transform the second equation such that only the terms indexed with k and k+1 are present. \n\\begin{aligned}\nx_{k+1} - x_k - h_k \\left(\\frac{1}{6}f_k + \\frac{2}{3}f_{k2} + \\frac{1}{6}f_{k+1}\\right) &= 0,\\\\\nx_{k2} - \\frac{x_k + x_{k+1}}{2} - \\frac{h_k}{8} \\left(f_k - f_{k+1}\\right) &= 0.\n\\end{aligned}\n\nWhile we already know (from some paragraph above) that the first equation is Simpson’s rule, the second equation is an outcome of Hermite intepolation. Hence the name. The optimization variables for every integration interval are the same as before, that is, x_k,u_k,x_{k2}, u_{k2} .\n\n\nHermite-Simpson Condensed (HSC) method\nYet some more simplification can be obtained from HSS. Namely, the second equation can be actually used to directly prescribe x_{k2} x_{k2} = \\frac{x_k + x_{k+1}}{2} + \\frac{h_k}{8} \\left(f_k - f_{k+1}\\right), which is used in the first equation as an argument for the f() function (represented by the f_{k2} symbol), by which the second equation and the term x_{k2} are eliminated from the set of defect equations. The optimization variables for every integration interval still need to contain u_{k2} even though x_{k2} was eliminated, because it is needed to parameterize f_{k2} . That is, the optimization variables then are x_k,u_k, u_{k2} . Reportedly (by Betts) this has been widely used and historically one of the first methods. When it comes to using it in optimal control, it turns out, however, that the sparsity pattern is better for the HSS.\n\n\n\n\nCollocation methods\nYet another family of methods for solving BVP ODE \\dot x(t) = f(x(t),t) are collocation methods. They are also based on discretization of independent variable – the time t. That is, on the interval [t_\\mathrm{i}, t_\\mathrm{f}], discretization points (or grid points or nodes or knots) are chosen, say, t_0, t_1, \\ldots, t_N, where t_0 = t_\\mathrm{i} and t_N = t_\\mathrm{f}. The solution x(t) is then approximated by a polynomial p_k(t) of a certain degree s on each interval [t_k,t_{k+1}] of length h_k=t_{k+1}-t_k\np_k(t) = p_{k0} + p_{k1}(t-t_k) + p_{k2}(t-t_k)^2+\\ldots + p_{ks}(t-t_k)^s.\nThe degree of the polynomial is low, say s=3 or so, certainly well below 10. With N subintervals, the total number of coefficients to parameterize the (approximation of the) solution x(t) over the whole interval is then N(s+1). For example, for s=3 and N=10, we have 40 coefficients: p_{00}, p_{01}, p_{02}, p_{03}, p_{10}, p_{11}, p_{12}, p_{13},\\ldots, p_{90}, p_{91}, p_{92}, p_{93}.\n\n\n\n\n\n\nFigure 5: Indirect collocation\n\n\n\nFinding a solution amounts to determining all those coefficients. Once we have them, the (approximate) solution is given by a piecewise polynomial.\nHow to determine the coefficients? By interpolation. But we will see in a moment that two types of interpolation are needed – interpolation of the value of the solution and interpolation of the derivative of the solution.\nThe former is only performed at the beginning of each interval, that is, at every discretization point (or grid point or node or knot). The condition reads that the polynomial p_{k-1}(t) approximating the solution x(t) on the (k-1)th interval should attain the same value at the end of that interval, that is, at t_{k-1} + h_{k-1}, as the polynomial p_k(t) approximating the solution x(t) on the kth interval attains at the same point, which from its perspective is the beginning of the kth interval, that is, t_k. We express this condition formally as \\boxed{p_{k-1}(\\underbrace{t_{k-1}+h_{k-1}}_{t_{k}}) = p_k(t_k).}\nExpanding the two polynomials, we get p_{k-1,0} + p_{k-1,1}h_{k-1} + p_{k-1,2}h_{k-1}^2+\\ldots + p_{k-1,s}h_{k-1}^s = p_{k0}.\n\n\n\n\n\n\nSubscripts in the coefficients\n\n\n\nWe adopt the notational convention that a coefficient of a polynomial is indexed by two indices, the first one indicating the interval and the second one indicating the power of the corresponding term. For example, p_{k-1,2} is the coefficient of the quadratic term in the polynomial approximating the solution on the (k-1)th interval. For the sake of brevity, we omit the comma between the two subscripts in the cases such as p_{k1} (instead of writing p_{k,1}). But we do write p_{k-1,0}, because here omiting the comma would introduce ambiguity.\n\n\nGood, we have one condition (one equation) for each subinterval. But we need more, if polynomials of degree at least one are considered (we then parameterize them by two parameters, in which case one more equation is needed for each subinterval). Here comes the opportunity for the other kind of interpolation – interpolation of the derivative of the solution. At a given point (or points) that we call collocation points, the polynomial p_k(t) approximating the solution x(t) on the kth interval should satisfy the same differential equation \\dot x(t) = f(x(t),t) as the solution does. That is, we require that at\nt_{kj} = t_k + h_k c_{j}, \\quad j=1,\\ldots, s, which we call collocation points, the polynomial satisfies \\boxed{\\dot p_k(t_{kj}) = f(p_k(t_{kj}),t_{kj}), \\quad j=1,\\ldots, s.}\nExpressing the derivative of the polynomial on the left and expanding the polynomial itself on the right, we get \n\\begin{aligned}\np_{k1} + &2p_{k2}(t_{kj}-t_k)+\\ldots + s p_{ks}(t_{kj}-t_k)^{s-1} = \\\\ &f(p_{k0} + p_{k1}(t_{kj}-t_k) + p_{ks}(t_{kj}-t_k)^2 + \\ldots + p_{ks}(t_{kj}-t_k)^s), \\quad j=1,\\ldots, s.\n\\end{aligned}\n\nThis gives us the complete set of equations for each interval. For the considered example of a cubic polynomial, we have one interpolation condition at the beginning of the interval and then three collocation conditions at the collocation points. In total, we have four equations for each interval. The number of equations is equal to the number of coefficients of the polynomial. Before the system of equations can solved for the coefficients, we must specifies the collocation points. Based on these, the collocation methods split into three families:\n\nGauss or Gauss-Legendre methods – the collocation points are strictly inside each interval.\nLobatto methods – the collocation points include also both ends of each interval.\nRadau methods – the collocation points include just one end of the interval.\n\n\n\n\n\n\n\nFigure 6: Single (sub)interval in indirect collocation – a cubic polynomial calls for three collocation points, two of which coincide with the discretization points (discrete-times); the continuity is enforced at the discretization point at the beginning of the interval\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAlthough in principle the collocation points could be arbitrary (but distinct), within a given family of methods, and for a given number of collocation points, some clever options are known that maximize accuracy.\n\n\n\nLinear polynomials\nBesides the piecewise constant approximation, which is too crude, not to speak of the discontinuity it introduces, the next simplest approximation of a solution x(t) on the interval [t_k,t_{k+1}] of length h_k=t_{k+1}-t_k is a linear (actually affine) polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k).\nOn the given kth interval it is parameterized by two parameters p_{k0} and p_{k1}, hence two equations are needed. The first equation enforces the continuity at the beginning of the interval \\boxed\n{p_{k-1,0} + p_{k-1,1}h_{k-1} = p_{k0}.}\n\nThe remaining single equation is the collocation condition at a single collocation point t_{k1} = t_k + h_k c_1, which remains to be chosen. One possible choice is c_1 = 1/2, that is \nt_{k1} = t_k + \\frac{h_k}{2}\n\nIn words, the collocation point is chosen in the middle of the interval. The collocation condition then reads \\boxed\n{p_{k1} = f\\left(p_{k0} + p_{k1}\\frac{h_k}{2}\\right).}\n\n\n\nQuadratic polynomials\nIf a quadratic polynomial is used to approximate the solution, the condition at the beginning of the interval is \\boxed\n{p_{k-1,0} + p_{k-1,1}h_{k-1} + p_{k-1,2}h_{k-1}^2 = p_{k0}.}\n\nTwo more equations – collocation conditions – are needed to specify all the three coefficients that parameterize the aproximating polynomial on a given interval [t_k,t_{k+1}]. One intuitive (and actually clever) choice is to place the collocation points at the beginning and the end of the interval, that is, at t_k and t_{k+1}. The coefficient that parameterize the relative position of the collocation points with respect to the interval are c_1=0 and c_2=1 The collocation conditions then read \\boxed\n{\\begin{aligned}\np_{k1}  &= f(p_{k0}),\\\\\np_{k1} + 2p_{k2}h_{k} &= f(p_{k0} + p_{k1}h_k + p_{k2}h_k^2).\n\\end{aligned}}\n\n\n\nCubic polynomials\nWhen a cubic polynomial is used, the condition at the beginning of the kth interval is \\boxed\n{p_{k-1,0} + p_{k-1,1}h_{k-1} + p_{k-1,2}h_{k-1}^2+p_{k-1,3}h_{k-1}^3 = p_{k0}.}\n\nThree more equations are needed to determine all the four coefficients of the polynomial. Where to place the collocations points? One intuitive (and clever too) option is to place them at the beginning, in the middle, and at the end of the interval. The relative positions of the collocation points are then given by c_1=0, c_2=1/2, and c_3=1. The collocation conditions then read \\boxed\n{\\begin{aligned}\np_{k1} &= f\\left(p_{k0} + p_{k1}(t_{k1}-t_k) + p_{k2}(t_{k1}-t_k)^2 + p_{k3}(t_{k1}-t_k)^3\\right),\\\\\np_{k1} + 2p_{k2}\\frac{h_k}{2} + 3 p_{k3}\\left(\\frac{h_k}{2}\\right)^{2} &= f\\left(p_{k0} + p_{k1}\\frac{h_k}{2} + p_{k2}\\left(\\frac{h_k}{2}\\right)^2 + p_{k3}\\left(\\frac{h_k}{2} \\right)^3\\right),\\\\\np_{k1} + 2p_{k2}h_k + 3 p_{k3}h_k^{2} &= f\\left(p_{k0} + p_{k1}h_k + p_{k2}h_k^2 + p_{k3}h_k^3\\right).\n\\end{aligned}}\n\n\n\n\nCollocation methods are implicit Runge-Kutta methods\nAn important observation that we are goint to make is that collocation methods can be viewed as implicit Runge-Kutta methods. But not all IRK methods can be viewed as collocation methods. In this section we show that the three implicit RK methods that we covered above are indeed (equivalent to) collocation methods. By the equivalence we mean that there is a linear relationship between the coefficients of the polynomials that approximate the solution on a given (sub)interval and the solution at the discretization point together with the derivative of the solution at the collocation points.\n\nImplicit midpoint method as a Radau collocation method\nFor the given integration interval [t_k,t_{k+1}], we write down two equations that relate the two coefficients of the linear polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k) and an approximation x_k of x(t) at the beginning of the interval t_k, as well as an approximation of \\dot x(t) at the (single) collocation point t_{k1} = t_{k} + \\frac{h_k}{2}.\nIn particular, the first interpolation condition is p_k(t_k) = \\textcolor{red}{p_{k0} = x_k} \\approx x(t_k).\nThe second interpolation condition, the one on the derivative in the middle of the interval is \\dot p_k\\left(t_k + \\frac{h_k}{2}\\right) = \\textcolor{red}{p_{k1} = f(x_{k1},t_{k1})} \\approx f(x(t_{k1}),t_{k1}).\nNote that here we introduced yet another unknown – the approximation x_{k1} of x(t_{k1}) at the collocation point t_{k1}. We can write it using the polynomial p_k(t) as \nx_{k1} = p_k\\left(t_k + \\frac{h_k}{2}\\right) = p_{k0} + p_{k1}\\frac{h_k}{2}.\n\nSubstituting for p_{k0} and p_{k1}, we get \nx_{k1} = x_k + f(x_{k1},t_{k1})\\frac{h_k}{2}.\n\nWe also introduce the notation f_{k1} for f(x_{k1},t_{k1}) and we can write an equation \nf_{k1} = f\\left(x_k + f_{k1}\\frac{h_k}{2}\\right).\n\nBut we want to find x_{k+1}, which we can accomplish by evaluating the polynomial p_k(t) at t_{k+1} = t_k+h_k \nx_{k+1} = x_k + f_{k1}h_k.\n\nCollecting the last two equations, we rederived the good old friend – the implicit midpoint method.\n\n\nImplicit trapezoidal method as a Lobatto collocation method\nFor the given integration interval [t_k,t_{k+1}], we write down three equations that relate the three coefficients of the quadratic polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k) + p_{k2}(t-t_k)^2 and an approximation x_k of x(t) at the beginning of the interval t_k, as well as approximations to \\dot x(t) at the two collocations points t_k and t_{k+1}.\nIn particular, the first interpolation condition is p_k(t_k) = \\textcolor{red}{p_{k0} = x_k} \\approx x(t_k).\nThe second interpolation condition, the one on the derivative at the beginning of the interval, the first collocation point, is \\dot p_k(t_k) = \\textcolor{red}{p_{k1} = f(x_k,t_k)} \\approx f(x(t_k),t_k).\nThe third interpolation condition, the one on the derivative at the second collocation point \\dot p_k(t_k+h_k) = \\textcolor{red}{p_{k1} + 2p_{k2} h_k = f(x_{k+1},t_{k+1})} \\approx f(x(t_{k+1}),t_{k+1}).\nAll the three conditions (emphasized in color above) can be written together as \n      \\begin{bmatrix}\n      1 & 0 & 0\\\\\n      0 & 1 & 0\\\\\n      0 & 1 & 2 h_k\\\\\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k+1},t_{k+1})\n      \\end{bmatrix}.\n\nThe above system of linear equations can be solved by inverting the matrix \n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      1 & 0 & 0\\\\\n      0 & 1 & 0\\\\\n      0 & -\\frac{1}{2h_k} & \\frac{1}{2h_k}\\\\\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k+1},t_{k+1})\n      \\end{bmatrix}.\n\nWe can now write down the interpolating/approximating polynomial p_k(t) = x_{k} + f(x_{k},t_{k})(t-t_k) +\\left[-\\frac{1}{2h_k}f(x_{k},t_{k}) + \\frac{1}{2h_k}f(x_{k+1},t_{k+1})\\right](t-t_k)^2.\nThis polynomial can now be used to find an (approximation of the) value of the solution at the end of the interval x_{k+1} = p_k(t_k+h_k) = x_{k} + f(x_{k},t_{k})h_k +\\left[-\\frac{1}{2h_k}f(x_{k},t_{k}) + \\frac{1}{2h_k}f(x_{k+1},t_{k+1})\\right]h_k^2, which can be simplified nearly upon inspection to x_{k+1} = x_{k} + \\frac{f(x_{k},t_{k}) + f(x_{k+1},t_{k+1})}{2} h_k, but this is our good old friend, isn’t it? We have shown that the collocation method with a quadratic polynomial with the collocation points chosen at the beginning and the end of the interval is (equivalent to) the implicit trapezoidal method. The method belongs to the family of Lobatto IIIA methods, which are all known to be collocation methods.\n\n\nHermite-Simpson method as a Lobatto collocation method\nHere we show that Hermite-Simpson method also qualifies as a collocation method. In particular, it belongs to the family of Lobatto IIIA methods, similarly as implicit trapezoidal method. The first condition, the one on the value of the cubic polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k) + p_{k2}(t-t_k)^2+ p_{k3}(t-t_k)^3 at the beginning of the interval is p_k(t_k) = \\textcolor{red}{p_{k0} = x_k} \\approx x(t_k).\nThe three remaining conditions are imposed at the collocation points, which for the integration interval [t_k,t_{k+1}] are t_{k1} = t_k , t_{k2} = \\frac{t_k+t_{k+1}}{2} , and t_{k3} = t_{k+1}. With the first derivative of the polynomial given by \\dot p_k(t) = p_{k1} + 2p_{k2}(t-t_k) + 3p_{k3}(t-t_k)^2, the first collocation condition \\dot p_k(t_k) = \\textcolor{red}{p_{k1} = f(x_k,t_k)} \\approx f(x(t_k),t_k).\nThe second collocation condition – the one on the derivative in the middle of the interval – is \\dot p_k\\left(t_k+\\frac{1}{2}h_k\\right) = \\textcolor{red}{p_{k1} + 2p_{k2} \\frac{h_k}{2} + 3p_{k3} \\left(\\frac{h_k}{2}\\right)^2 = f(x_{k2},t_{k2})} \\approx f\\left(x\\left(t_{k}+\\frac{h_k}{2}\\right),t_{k}+\\frac{h_k}{2}\\right).\nThe color-emphasized part can be simplified to \\textcolor{red}{p_{k1} + p_{k2} h_k + \\frac{3}{4}p_{k3} h_k^2 = f(x_{k2},t_{k2})}.\nFinally, the third collocation condition – the one imposed at the end of the interval – is \\dot p_k(t_k+h_k) = \\textcolor{red}{p_{k1} + 2p_{k2} h_k + 3p_{k3} h_k^2 = f(x_{k+1},t_{k+1})} \\approx f(x(t_{k+1}),t_{k+1}).\nAll the four conditions (emphasized in color above) can be written together as \n      \\begin{bmatrix}\n      1 & 0 & 0 & 0\\\\\n      0 & 1 & 0 & 0\\\\\n      0 & 1 & h_k & \\frac{3}{4} h_k^2\\\\\n      0 & 1 & 2 h_k & 3h_k^2\\\\\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2} \\\\p_{k3}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k2},t_{k2}) \\\\ f(x_{k+1},t_{k+1}).\n      \\end{bmatrix}\n\nInverting the matrix analytically, we get \n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2}\\\\ p_{k3}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      1 & 0 & 0 & 0\\\\\n      0 & 1 & 0 & 0\\\\\n      0 & -\\frac{3}{2h_k} & \\frac{2}{h_k} & -\\frac{1}{2h_k}\\\\\n      0 & \\frac{2}{3h_k^2} & -\\frac{4}{3h_k^2} & \\frac{2}{3h_k^2}\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k2},t_{k2})\\\\ f(x_{k+1},t_{k+1}).\n      \\end{bmatrix}.\n\nWe can now write down the interpolating/approximating polynomial \n      \\begin{aligned}\n      p_k(t) &= x_{k} + f(x_{k},t_{k})(t-t_k) +\\left[-\\frac{3}{2h_k}f(x_{k},t_{k}) + \\frac{2}{h_k}f(x_{k2},t_{k2}) -\\frac{1}{2h_k}f(x_{k+1},t_{k+1}) \\right](t-t_k)^2\\\\\n      & +\\left[\\frac{2}{3h_k^2}f(x_{k},t_{k}) - \\frac{4}{3h_k^2}f(x_{k2},t_{k2}) +\\frac{2}{3h_k^2}f(x_{k+1},t_{k+1}) \\right](t-t_k)^3.\n      \\end{aligned}\n\nWe can use this prescription of the polynomial p_k(t) to compute the (approximation of the) value of the solution at the end of the kth interval \n      \\begin{aligned}\n      x_{k+1} = p_k(t_k+h_k) &= x_{k} + f(x_{k},t_{k})h_k +\\left[-\\frac{3}{2h_k}f(x_{k},t_{k}) + \\frac{2}{h_k}f(x_{k2},t_{k2}) -\\frac{1}{2h_k}f(x_{k+1},t_{k+1}) \\right]h_k^2\\\\\n      & +\\left[\\frac{2}{3h_k^2}f(x_{k},t_{k}) - \\frac{4}{3h_k^2}f(x_{k2},t_{k2}) +\\frac{2}{3h_k^2}f(x_{k+1},t_{k+1}) \\right]h_k^3,\n      \\end{aligned}\n which can be simplified to \n      \\begin{aligned}\n      x_{k+1} &= x_{k} + f(x_{k},t_{k})h_k +\\left[-\\frac{3}{2}f(x_{k},t_{k}) + \\frac{2}{1}f(x_{k2},t_{k2}) -\\frac{1}{2}f(x_{k+1},t_{k+1}) \\right]h_k\\\\\n      & +\\left[\\frac{2}{3}f(x_{k},t_{k}) - \\frac{4}{3}f(x_{k2},t_{k2}) +\\frac{2}{3}f(x_{k+1},t_{k+1}) \\right]h_k,\n      \\end{aligned}\n which further simplifies to \n      x_{k+1}  = x_{k} + h_k\\left[\\frac{1}{6}f(x_{k},t_{k}) + \\frac{2}{3}f(x_{k2},t_{k2}) + \\frac{1}{6}f(x_{k+1},t_{k+1}) \\right],\n which can be recognized as the Simpson integration that we have already seen in implicit Runge-Kutta method described above.\nObviously f_{k2} needs to be further elaborated on, namely, x_{k2} needs some prescription too. We know that it was introduced as an approximation to the solution x in the middle of the interval. Since the value of the polynomial in the middle is such an approximation too, we can set x_{k2} equal to the value of the polynomial in the middle. \n      \\begin{aligned}\n      x_{k2} = p_k\\left(t_k+\\frac{1}{2}h_k\\right) &= x_{k} + f(x_{k},t_{k})\\frac{h_k}{2} +\\left[-\\frac{3}{2h_k}f(x_{k},t_{k}) + \\frac{2}{h_k}f(x_{k2},t_{k2}) -\\frac{1}{2h_k}f(x_{k+1},t_{k+1}) \\right]\\left(\\frac{h_k}{2}\\right)^2\\\\\n      & +\\left[\\frac{2}{3h_k^2}f(x_{k},t_{k}) - \\frac{4}{3h_k^2}f(x_{k2},t_{k2}) +\\frac{2}{3h_k^2}f(x_{k+1},t_{k+1}) \\right]\\left(\\frac{h_k}{2}\\right)^3,\n      \\end{aligned}\n which without further ado simplifies to \n      x_{k2} = x_{k} + h_k\\left( \\frac{5}{24}f(x_{k},t_{k}) +\\frac{1}{3}f(x_{k2},t_{k2}) -\\frac{1}{24}f(x_{k+1},t_{k+1}) \\right),\n which can be recognized as the other equation in the primary formulation of Implicit trapezoidal method described above.\n\n\n\nPseudospectral collocation methods\nThey only consider a single polynomial over the whole interval. The degree of such polynomial, in contrast with classical collocation methods, rather high, therefore also the number of collocation points is high, but their location is crucial.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for indirect approach"
    ]
  }
]