[
  {
    "objectID": "opt_theory_problems.html",
    "href": "opt_theory_problems.html",
    "title": "Optimization problems",
    "section": "",
    "text": "We formulate a general optimization problem (also a mathematical program) as \n\\begin{aligned}\n\\operatorname*{minimize} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\bm x \\in \\mathcal{X},\n\\end{aligned}\n where f is a scalar function, \\bm x can be a scalar, a vector, a matrix or perhaps even a variable of yet another type, and \\mathcal{X} is a set of values that \\bm x can take, also called the feasible set.\n\n\n\n\n\n\nNote\n\n\n\nThe term “program” here has nothing to do with a computer program. Instead, it was used by the US military during the WWII to refer to plans or schedules in training and logistics.\n\n\nIf maximization of the objective function f() is desired, we can simply multiply the objective function by -1 and minimize the resulting function.\nTypically there are two types of constraints that can be imposed on the optimization variable \\bm x:\n\nexplicit characterization of the set such as \\bm x \\in \\mathbb{R}^n or \\bm x \\in \\mathbb{Z}^n, possibly even a direct enumeration such as \\bm x \\in \\{0,1\\}^n in the case of binary variables,\nimplicit characterization of the set using equations and inequalities such as g_i(\\bm x) \\leq 0 and h_i(\\bm x) = 0 for i = 1, \\ldots, m and j = 1, \\ldots, p.\n\nAn example of a more structured and yet sufficiently general optimization problems over several real and integer variables is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^{n_x}, \\, \\bm y \\in \\mathbb{Z}^{n_y}} \\quad & f(\\bm x, \\bm y) \\\\\n\\text{subject to} \\quad & g_i(\\bm x, \\bm y) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_j(\\bm x, \\bm y) = 0, \\quad j = 1, \\ldots, p.\n\\end{aligned}\n\nIndeed, for named sets such as \\mathbb R or \\mathbb Z, it is common to place the set constraints directly underneath the word “minimize”. But it is just one convention, and these constraints could be listed in the “subject to” section as well.\n\n\n\n\n\n\nInteger optimization not included in this course\n\n\n\nIn this course we are only going to consider optimization problems with real-valued variables. This decision does not suggest that optimization with integer variables is less relevant for optimal control, quite the opposite! It is just that the theory and algorithms for integer or mixed integer optimization are based on different principles than those for real variables. And they can hardly fit into a single course. Good news for the interested students is that a graduate course named Combinatorial algorithms (B3B35KOA) covering integer optimization in detail is offered by our Cybernetics and Robotics study program at CTU FEE. Additionally, applications of integer optimization to optimal control are partly discussed in the course on Hybrid systems (B3B39HYS).\n\n\nThe form of an optimization problem that we are going to use in our course most often than not is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & g_i(\\bm x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_i(\\bm x) = 0, \\quad i = 1, \\ldots, p,\n\\end{aligned}\n which can also be written using vector-valued functions (reflected in the use of the bold face for their names) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#optimization-problem-formulation",
    "href": "opt_theory_problems.html#optimization-problem-formulation",
    "title": "Optimization problems",
    "section": "",
    "text": "We formulate a general optimization problem (also a mathematical program) as \n\\begin{aligned}\n\\operatorname*{minimize} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\bm x \\in \\mathcal{X},\n\\end{aligned}\n where f is a scalar function, \\bm x can be a scalar, a vector, a matrix or perhaps even a variable of yet another type, and \\mathcal{X} is a set of values that \\bm x can take, also called the feasible set.\n\n\n\n\n\n\nNote\n\n\n\nThe term “program” here has nothing to do with a computer program. Instead, it was used by the US military during the WWII to refer to plans or schedules in training and logistics.\n\n\nIf maximization of the objective function f() is desired, we can simply multiply the objective function by -1 and minimize the resulting function.\nTypically there are two types of constraints that can be imposed on the optimization variable \\bm x:\n\nexplicit characterization of the set such as \\bm x \\in \\mathbb{R}^n or \\bm x \\in \\mathbb{Z}^n, possibly even a direct enumeration such as \\bm x \\in \\{0,1\\}^n in the case of binary variables,\nimplicit characterization of the set using equations and inequalities such as g_i(\\bm x) \\leq 0 and h_i(\\bm x) = 0 for i = 1, \\ldots, m and j = 1, \\ldots, p.\n\nAn example of a more structured and yet sufficiently general optimization problems over several real and integer variables is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^{n_x}, \\, \\bm y \\in \\mathbb{Z}^{n_y}} \\quad & f(\\bm x, \\bm y) \\\\\n\\text{subject to} \\quad & g_i(\\bm x, \\bm y) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_j(\\bm x, \\bm y) = 0, \\quad j = 1, \\ldots, p.\n\\end{aligned}\n\nIndeed, for named sets such as \\mathbb R or \\mathbb Z, it is common to place the set constraints directly underneath the word “minimize”. But it is just one convention, and these constraints could be listed in the “subject to” section as well.\n\n\n\n\n\n\nInteger optimization not included in this course\n\n\n\nIn this course we are only going to consider optimization problems with real-valued variables. This decision does not suggest that optimization with integer variables is less relevant for optimal control, quite the opposite! It is just that the theory and algorithms for integer or mixed integer optimization are based on different principles than those for real variables. And they can hardly fit into a single course. Good news for the interested students is that a graduate course named Combinatorial algorithms (B3B35KOA) covering integer optimization in detail is offered by our Cybernetics and Robotics study program at CTU FEE. Additionally, applications of integer optimization to optimal control are partly discussed in the course on Hybrid systems (B3B39HYS).\n\n\nThe form of an optimization problem that we are going to use in our course most often than not is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & g_i(\\bm x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_i(\\bm x) = 0, \\quad i = 1, \\ldots, p,\n\\end{aligned}\n which can also be written using vector-valued functions (reflected in the use of the bold face for their names) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#properties-of-optimization-problems",
    "href": "opt_theory_problems.html#properties-of-optimization-problems",
    "title": "Optimization problems",
    "section": "Properties of optimization problems",
    "text": "Properties of optimization problems\nIt is now the presence/absence and the properties of individual components in the optimization problem defined above that characterize classes of optimization problems. In particular, we can identify the following properties:\n\nUnconstrained vs constrained\n\nPractically relevant problems are almost always constrained. But still there are good reasons to study unconstrained problems too, as many theoretical results and algorithms for constrained problems are based on transformations to unconstrained problems.\n\nLinear vs nonlinear\n\nBy linear problems we mean problems where the objective function and all the functions defining the constraints are linear (or actually affine) functions of the optimization variable \\bm x. Such problems constitute the simplest class of optimization problems, are very well understood, and there are efficient algorithms for solving them. In contrast, nonlinear problems are typically more difficult to solve (but see the discussion of the role of convexity below).\n\nSmooth vs nonsmooth\n\nEfficient algorithms for optimization over real variables benefit heavily from knowledge of the derivatives of the objective and constraint functions. If the functions are differentiable (aka smooth), we say that the whole optimization problem is smooth. Nonsmooth problems are typically more difficult to analyze and solve (but again, see the discussion of the role of convexity below).\n\nConvex vs nonconvex\n\nIf the objective function and the feasible set are convex (the latter holds when the functions defining the inequality constraints are convex and the functions defining the equality constrains are affine), the whole optimization problem is convex. Convex optimization problems are very well understood and there are efficient algorithms for solving them. In contrast, nonconvex problems are typically more difficult to solve. It is not becomming a common knowledge that convexity is a lot more important property than linearity and smoothness when it comes to solving optimization problems efficiently.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#classes-of-optimization-problems",
    "href": "opt_theory_problems.html#classes-of-optimization-problems",
    "title": "Optimization problems",
    "section": "Classes of optimization problems",
    "text": "Classes of optimization problems\nBased on the properties discussed above, we can identify the following distinct classes of optimization problems:\n\nLinear program (LP)\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A_\\mathrm{ineq}\\bm x \\leq \\mathbf b_\\mathrm{ineq},\\\\\n& \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq}.\n\\end{aligned}\n\nAn LP is obviously linear, hence it is also smooth and convex.\nSome theoretical results and numerical algorithms require a linear program in a specific form, called the standard form: \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A\\bm x = \\mathbf b,\\\\\n& \\bm x \\geq \\mathbf 0,\n\\end{aligned}\n where the inequality \\bm x \\geq \\mathbf 0 is understood componentwise, that is, x_i \\geq 0 for all i = 1, \\ldots, n.\n\n\nQuadratic program (QP)\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\bm x^\\top \\mathbf Q \\bm x +  \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A_\\mathrm{ineq}\\bm x \\leq \\mathbf b_\\mathrm{ineq},\\\\\n& \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq}.\n\\end{aligned}\n\nEven though the QP is nonlinear, it is smooth and if the matrix \\mathbf Q is positive semidefinite, it is convex. Its analysis and numerical solution are not much more difficult than those of an LP problem.\n\nQuadratically constrained quadratic program (QCQP)\nIt is worth emphasizing that for the standard QP the constraints are still given by a system of linear equations and inequalities. Sometimes we can encounter problems in which not only the cost function but also the functions defining the constraints are quadratic as in \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\bm x^\\top \\mathbf Q \\bm x +  \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\bm x^\\top \\mathbf A_i\\bm x + \\mathbf b_i \\bm x + c_i \\leq \\mathbf 0, \\quad i=1, \\ldots, m.\n\\end{aligned}\n\nA QCQP problem is convex if and only if the the constraints define a convex feasible set, which is the case when all the matrices \\mathbf A_i are positive semidefinite.\n\n\n\nConic program (CP)\nFirst, what is a cone? It is a set such that if something is in the cone, then a multiple of it by a nonnegative number is still in the set. We are going to restrict ourselves to regular cones, which are are pointed, closed and convex. An example of such regular cone in a plane is in Fig. 1 below.\n\n\n\n\n\n\nFigure 1: Regular (pointed, convex, closed) cone in a plane\n\n\n\nNow, what is the role of cones in optimization? Reformulation of nonlinear optimization problems using cones constitutes a systematic way to identify what these (conic) optimization problems have in common with linear programs, for which powerful theory and efficient algorithms exist.\nNote that an LP in the standard form can be written as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathbb{R}_+^n,\n\\end{aligned}\n where \\mathbb R_+^n is a positive orthant. Now, the positive orthant is a convex cone! We can then see the LP as an instance of a general conic optimization problem (conic program)\n\n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathcal{K},\n\\end{aligned}\n where \\mathcal{K} is a cone in \\mathbb R^n.\n\n\n\n\n\n\nInequality interpreted as belonging to a cone\n\n\n\nA fundamental idea unrolled here: the inequality \\bm x\\geq 0 can be interpreted as \\bm x belonging to a componentwise nonegative cone, that is \\bm x \\in \\mathbb R_+^n. What if some other cone \\mathcal K is considered? What would be the interpretation of the inequality then?\n\n\nSometimes in order to emphasize that the inequality is induced by the cone \\mathcal K, we write it as \\geq_\\mathcal{K}. Another convention – the one we actually adopt here – is to use another symbol for the inequality \\succeq to distinguish it from the componentwise meaning, assuming that the cone is understood from the context. We then interpret conic inequalities such as \n\\mathbf A_\\mathrm{ineq}\\bm x \\succeq \\mathbf b_\\mathrm{ineq}\n in the sense that \n\\mathbf A_\\mathrm{ineq}\\bm x - \\mathbf b_\\mathrm{ineq} \\in \\mathcal{K}.\n\nIt is high time to explore some concrete cones (other than the positive orthant). We consider just two, but there are a few more, see the literature.\n\nSecond-order cone program (SOCP)\nThe most immediate cone in \\mathbb R^n is the second-order cone, also called the Lorentz cone or even the ice cream cone. We explain it in \\mathbb R^3 for the ease of visualization, but generalization to \\mathbb R^n is straightforward. The second-order cone in \\mathbb R^3 is defined as \n\\mathcal{K}_\\mathrm{SOC}^3 = \\left\\{ \\bm x \\in \\mathbb R^3 \\mid \\sqrt{x_1^2 + x_2^2} \\leq x_3 \\right\\}.\n\nand is visualized in Fig. 2 below.\n\n\n\n\n\n\n\n\nFigure 2: A second-order cone in 3D\n\n\n\n\n\nWhich of the three axes plays the role of the axis of symmetry for the cone must be agreed beforehand. Singling this direction out, the SOC in \\mathbb R^n can also be formulated as \n\\mathcal{K}_\\mathrm{SOC}^n = \\left\\{ (\\bm x, t) \\in \\mathbb R^{n-1} \\times \\mathbb R \\mid \\|\\bm x\\|_2 \\leq t \\right\\}.\n\nA second-order conic program in standard form is then \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathcal{K}_\\mathrm{SOC}^n,\n\\end{aligned}\n\nwhich can be written explicitly as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad x_1^2 + \\cdots + x_{n-1}^2 - x_n^2 \\leq 0.\n\\end{aligned}\n\nA second-order conic program can also come in non-standard form such as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A_\\mathrm{ineq}\\bm x \\succeq  \\mathbf b_\\mathrm{ineq}.\n\\end{aligned}\n\nAssuming the data is structured as \n\\begin{bmatrix}\n\\mathbf A\\\\\n\\mathbf r^\\top\n\\end{bmatrix}\n\\bm x \\succeq\n\\begin{bmatrix}\n\\mathbf b\\\\\nq\n\\end{bmatrix},\n the inequality can be rewritten as \n\\begin{bmatrix}\n\\mathbf A\\\\\n\\mathbf r^\\top\n\\end{bmatrix}\n\\bm x -\n\\begin{bmatrix}\n\\mathbf b\\\\\nq\n\\end{bmatrix} \\in \\mathcal{K}_\\mathrm{SOC}^n,\n which finally gives \n\\|\\mathbf A \\bm x - \\mathbf b\\|_2 \\leq \\mathbf r^\\top \\bm x + q.\n\nTo summarize, another form of a second-order cone program (SOCP) is\n\n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq},\\\\\n&\\quad \\|\\mathbf A \\bm x - \\mathbf b\\|_2 \\leq \\mathbf r^\\top \\bm x + q.\n\\end{aligned}\n\nWe can see that the SOCP contains both linear and quadratic constraints, hence it generalizes LP and QP, including convex QCQP. To see the latter, expand the square of \\|\\mathbf A \\bm x - \\mathbf b\\|_2 into (\\bm x^\\top \\mathbf A^\\top  - \\mathbf b^\\top)(\\mathbf A \\bm x - \\mathbf b) = \\bm x^\\top \\mathbf A^\\top \\mathbf A \\bm x + \\ldots\n\n\nSemidefinite program (SDP)\nAnother cone of great importance the control theory is the cone of positive semidefinite matrices. It is commonly denoted as \\mathcal S_+^n and is defined as \n\\mathcal S_+^n = \\left\\{ \\bm X \\in \\mathbb R^{n \\times n} \\mid \\bm X = \\bm X^\\top, \\, \\bm z^\\top \\bm X \\bm z \\geq 0\\; \\forall \\bm z\\in \\mathbb R^n \\right\\},\n and with this cone the inequality \\mathbf X \\succeq 0 is a common way to express that \\mathbf X is positive semidefinite.\nUnlike the previous classes of optimization problems, this one is formulated with matrix variables instead of vector ones. But nothing prevents us from collecting the components of a symmetric matrix into a vector and proceed with vectors as usual, if needed:\n\n\\bm X = \\begin{bmatrix} x_1 & x_2 & x_3 \\\\ x_2 & x_4 & x_5\\\\ x_3 & x_5 & x_6 \\end{bmatrix},\n\\quad\n\\bm x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_6 \\end{bmatrix}.\n\nAn optimization problem with matrix variables constrained to be in the cone of semidefinite matrices (or their vector representations) is called a semidefinite program (SDP). As usual, we start with the standard form, in which the cost function is linear and the optimization is subject to an affine constraint and a conic constraint. In the following, in place of the inner products of two vectors \\mathbf c^\\top x we are going to use inner products of matrices defined as \n\\langle \\mathbf C, \\bm X\\rangle = \\operatorname{Tr} \\mathbf C \\bm X,\n where \\operatorname{Tr} is a trace of a matrix defined as the sum of the diagonal elements.\nThe SDP program in the standard form is then \n\\begin{aligned}\n\\operatorname{minimize}_{\\bm X} &\\quad \\operatorname{Tr} \\mathbf C \\bm X\\\\\n\\text{subject to} &\\quad \\operatorname{Tr} \\mathbf A_i \\bm X = \\mathbf b_i, \\quad i=1, \\ldots, m,\\\\\n&\\quad \\bm X \\in \\mathcal S_+^n,\n\\end{aligned}\n where the latter constraint is more often than not written as \\bm X \\succeq 0, understanding from the context that the cone of positive definite matrices is assumed.\n\n\nOther conic programs\nWe are not going to cover them here, but we only enumerate a few other cones useful in optimization: rotated second-order cone, exponential cone, power cone, … A concise overview is in [1]\n\n\n\nGeometric program (GP)\n#TODO: although of little immediate use in our course.\n\n\nNonlinear program (NLP)\nFor completeness we include here once again the general nonlinear programming problem:\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}\n\nSmoothness of the problem can easily be determined based on the differentiability of the functions. Convexity can also be determined by inspecting the functions, but this is not necessarily easy. One way to check convexity of a function is to view it as a composition of simple functions and exploit the knowledge about convexity of these simple functions. See [2, Sec. 3.2]",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "discr_dir_general.html",
    "href": "discr_dir_general.html",
    "title": "General finite-horizon nonlinear discrete-time optimal control as a nonlinear program",
    "section": "",
    "text": "In this section we formulate a finite-horizon optimal control problem (OCP) for a discrete-time dynamical system as a mathematical optimization problem (also mathematical program), which can then be solved numerically by a suitable numerical solver for nonlinear programming (NLP), or possibly quadratic programming (QP). The outcome of such numerical optimization is an optimal control trajectory (a sequence of controls), which is why this approach is called direct – we optimize directly over the trajectories.\nIn the following chapter we then present an alternative – indirect – approach, wherein the conditions of optimality are formulated first. These come in the form of a set of equations, some of them recurrent/recursive, some just algebraic. The indirect approach thus amounts to solving such equations.\nAnd then in another chapter we present the third approach – dynamic programming.\nThe three approaches form the backbone of the theory of optimal control for discrete-time systems, but later we are going to recognize the same triplet in the context of continuous-time systems.\n\n\n\n\n\n\n\n\nG\n\n\ndiscrete_time_optimal_control\n\nApproaches to discrete-time optimal control\n\n\n\ndirect_approach\n\nDirect approach\n\n\n\ndiscrete_time_optimal_control-&gt;direct_approach\n\n\n\n\n\nindirect_approach\n\nIndirect approach\n\n\n\ndiscrete_time_optimal_control-&gt;indirect_approach\n\n\n\n\n\ndynamic_programming\n\nDynamic programming\n\n\n\ndiscrete_time_optimal_control-&gt;dynamic_programming\n\n\n\n\n\n\n\n\nFigure 1: Three approaches to discrete-time optimal control\n\n\n\n\n\nBut now back to the direct approach. We will start with a general nonlinear discrete-time optimal control problem in this section, and then specialize to the linear quadratic regulation (LQR) problem in the next section. Finally, since the computed control trajectory constitutes an open-loop control scheme, something must be done about it if a feedback scheme is preferred – we introduce the concept of a receding horizon control (RHC), perhaps better known as model predictive control (MPC), which turns the direct approach into a feedback control scheme.\nWe start by considering a nonlinear discrete-time system modelled by the state equation \n\\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\n where\n\n\\bm x_k\\in \\mathbb R^n is the state at the discrete time k\\in \\mathbb Z,\n\\bm u_k\\in \\mathbb R^m is the control at the discrete time k,\n\\mathbf f_k: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb Z \\to \\mathbb{R}^n is a state transition function (in general not only nonlinear but also time-varying, with the convention that the dependence on k is expressed through the lower index).\n\nA general nonlinear discrete-time optimal control problem (OCP) is then formulated as \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm u_k \\in \\mathcal U_k,\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_k \\in \\mathcal X_k,\\quad k=i, \\ldots, N,\n\\end{aligned}\n where\n\ni is the initial discrete time,\nN is the final discrete time,\n\\phi() is a terminal cost function that penalizes the state at the final time,\nL_k() is a running (also stage) cost function,\nand \\mathcal U_k and \\mathcal X_k are sets of feasible controls and states – these sets are typically expressed using equations and inequalities. Should they be constant, the notation is just \\mathcal U and \\mathcal X.\n\nOftentimes it is convenient to handle the constraints of the initial and final states separately: \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm u_k \\in \\mathcal U_k,\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_k \\in \\mathcal X_k,\\quad k=i+1, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_i \\in \\mathcal X_\\mathrm{init},\\\\\n                    &\\quad \\bm x_N \\in \\mathcal X_\\mathrm{final}.\n\\end{aligned}\n\nIn particular, at the initial time just one particular state is often considered. At the final time, the state might be required to be equal to some given value, it might be required to be in some set defined through equations or inequalities, or it might be left unconstrained. Finally, the constraints on the control and states typically (but not always) come in the form of lower and upper bounds. The optimal control problem then specializes to \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_k \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\bm x_k \\leq \\mathbf x^{\\max},\\\\\n                    &\\quad\\bm x_i = \\mathbf x^\\text{init},\\\\\n                    &\\quad \\left(\\bm x_N = \\mathbf x^\\text{ref}, \\; \\text{or} \\; \\mathbf h^\\text{final}(\\bm x_N) =  \\mathbf 0, \\text{or} \\; \\mathbf g^\\text{final}(\\bm x_N) \\leq  \\mathbf 0\\right),                    \n\\end{aligned}\n where\n\nthe inequalities should be interpreted componentwise,\n\\mathbf u^{\\min} and \\mathbf u^{\\max} are lower and upper bounds on the control, respectively,\n\\mathbf x^{\\min} and \\mathbf x^{\\max} are lower and upper bounds on the state, respectively,\n\\mathbf x^\\text{init} is a fixed initial state,\n\\mathbf x^\\text{ref} is a required (reference) final state,\nand the functions \\mathbf g^\\text{final}() and \\mathbf h^\\text{final}() can be used to define the constraint set for the final state.\n\nThis optimal control problem is an instance of a general nonlinear programming (NLP) problem \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm x}\\in\\mathbb{R}^{n(N-i)},\\bar{\\bm u}\\in\\mathbb{R}^{m(N-i)}} &\\quad J(\\bar{\\bm x},\\bar{\\bm u})\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bar{\\bm x},\\bar{\\bm u}) =0,\\\\\n&\\quad \\mathbf g(\\bar{\\bm x},\\bar{\\bm u}) \\leq \\mathbf 0,\n\\end{aligned}\n where \\bar{\\bm u} and \\bar{\\bm x} are vectors obtained by stacking control and state vectors for individual times\n\n\\begin{aligned}\n\\bar{\\bm u} &= \\operatorname*{vec}(\\bm u_i,\\ldots, \\bm u_{N-1}) = \\begin{bmatrix}\\bm u_i\\\\ \\bm u_{i+1}\\\\ \\vdots \\\\ \\bm u_{N-1} \\end{bmatrix},\\\\\n\\bar{\\bm x} &= \\operatorname*{vec}(\\bm x_{i+1},\\ldots, \\bm x_N) = \\begin{bmatrix}\\bm x_{i+1}\\\\ \\bm x_{i+2}\\\\ \\vdots \\\\ \\bm x_{N} \\end{bmatrix}.\n\\end{aligned}\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlthought there may be applications where it is desirable to optimize over the initial state \\bm x_i as well, mostly the initial state \\bm x_i is fixed, and it does not have to be considered as an optimization variable. This can be even emphasized through the notation J(\\bar{\\bm x},\\bar{\\bm u}; \\bm x_i), where the semicolon separates the variables from (fixed) parameters.\nThe last control that affects the state trajectory on the interval [i,N] is \\bm u_{N-1}.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "General finite-horizon nonlinear optimal control as an NLP"
    ]
  },
  {
    "objectID": "dynamic_programming_references.html",
    "href": "dynamic_programming_references.html",
    "title": "References",
    "section": "",
    "text": "Dynamic programming (DP) is a fairly powerful and yet general framework that finds its use in many disciplines. Optimal control is not the only one. But in this overview of the literature we deliberately focus on the DP references with optimal control flavour.\nOur introductory treatment was based almost exclusively on the (also just introductory) Chapter 6 in [1]. Electronic version of the book is freely available on the author’s webpage.\nComparable introduction is provided in [2]. Although it does not appear to be legally available for free in an electronic form, its reprint by a low-cost publisher makes it an affordable (and recommendable) classic reference. Another classic [3] actually uses dynamic programming as the key technique to derive all those LQ-optimal regulation and tracking results. A few copies of this book are available in the faculty library at NTK. The authors also made an electronic version available for free on their website.\nFairly comprehensive treatment of control-oriented DP is in the two-volume monograph [4] and [5]. It is not available online for free, but the book webpage contains links to other supporting materials including lecture notes. Furthermore, the latest book by the same author [6], which is available for free download, contains a decent introduction to dynamic programming.\nHaving just referenced a book on reinforcement learning (RL), indeed, this popular concept — or at least some of its flavours — is closely related to dynamic programming. In fact, it offers a way to overcome some of the limitations of dynamic programming. In our introductory lecture we are not covering RL, but an interested student can take advantage of availability of high-quality resources such as the the RL-related books and other resources by D. Bertsekas) and another recommendable introduction to RL from control systems perspective [7], which is also available for free download.\nThe book [8] often regarded as the bible of RL is nice (and freely available for download) but may be rather difficult to read for a control engineer because of major differences in terminology.\n\n\n\n\n Back to topReferences\n\n[1] F. L. Lewis, D. Vrabie, and V. L. Syrmo, Optimal Control, 3rd ed. John Wiley & Sons, 2012. Accessed: Mar. 09, 2022. [Online]. Available: https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf\n\n\n[2] D. E. Kirk, Optimal Control Theory: An Introduction, Reprint of the 1970 edition. Dover Publications, 2004.\n\n\n[3] B. D. O. Anderson and J. B. Moore, Optimal Control: Linear Quadratic Methods, Reprint of the 1989 edition. Dover Publications, 2007. Available: http://users.cecs.anu.edu.au/~john/papers/BOOK/B03.PDF\n\n\n[4] D. P. Bertsekas, Dynamic Programming and Optimal Control, 4th ed., vol. I. Belmont, Massachusetts: Athena Scientific, 2017. Available: http://athenasc.com/dpbook.html\n\n\n[5] D. P. Bertsekas, Dynamic Programming and Optimal Control, 4th ed., vol. II. Belmont, Massachusetts: Athena Scientific, 2012. Available: http://athenasc.com/dpbook.html\n\n\n[6] D. P. Bertsekas, A Course in Reinforcement Learning. Belmont, Massachusetts: Athena Scientific, 2023. Accessed: Sep. 15, 2023. [Online]. Available: https://web.mit.edu/dimitrib/www/RLCOURSECOMPLETE.pdf\n\n\n[7] S. Meyn, Control Systems and Reinforcement Learning. Cambridge University Press, 2022. Accessed: Aug. 25, 2021. [Online]. Available: https://meyn.ece.ufl.edu/control-systems-and-reinforcement-learning/\n\n\n[8] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, Massachusetts: A Bradford Book, 2018. Available: http://incompleteideas.net/book/the-book-2nd.html",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "References"
    ]
  },
  {
    "objectID": "rocond_hw.html",
    "href": "rocond_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "Homework"
    ]
  },
  {
    "objectID": "ext_LQG.html",
    "href": "ext_LQG.html",
    "title": "LQG control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQG control"
    ]
  },
  {
    "objectID": "ext_stochastic_LQR.html",
    "href": "ext_stochastic_LQR.html",
    "title": "LQR for stochastic systems",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQR for stochastic systems"
    ]
  },
  {
    "objectID": "rocond_mu_synthesis.html",
    "href": "rocond_mu_synthesis.html",
    "title": "Mu synthesis",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "Mu synthesis"
    ]
  },
  {
    "objectID": "dynamic_programming_goals.html",
    "href": "dynamic_programming_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Understand and explain the Bellman’s principle of optimality.\nShow how dynamic programming and Bellman’s principle of optimality can be used to give analytical solution to a discrete-time LQ-optimal control on a finite control interval.\nGive the Hamilton-Jacobi-Bellman (HJB) equation and explain it as a reformulation of the principle of optimality for continuous-time systems. Give also its version featuring the Hamiltonian function.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Learning goals"
    ]
  },
  {
    "objectID": "dynamic_programming_goals.html#knowledge-remember-and-understand",
    "href": "dynamic_programming_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Understand and explain the Bellman’s principle of optimality.\nShow how dynamic programming and Bellman’s principle of optimality can be used to give analytical solution to a discrete-time LQ-optimal control on a finite control interval.\nGive the Hamilton-Jacobi-Bellman (HJB) equation and explain it as a reformulation of the principle of optimality for continuous-time systems. Give also its version featuring the Hamiltonian function.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Learning goals"
    ]
  },
  {
    "objectID": "dynamic_programming_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "dynamic_programming_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nUse dynamic programming to design an optimal feedback controller in the form of a lookup table for a general (possibly nonlinear) discrete-time dynamical system.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Learning goals"
    ]
  },
  {
    "objectID": "limitations_hw.html",
    "href": "limitations_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Homework"
    ]
  },
  {
    "objectID": "roban_references.html",
    "href": "roban_references.html",
    "title": "References",
    "section": "",
    "text": "Even when restricted to control systems, the concept of robustness is quite broad and can be approached from many different angles. In our course we are restricting the focus to the approaches formulated in frequency domain. The main reference for this part of the course is the book [1]. The concepts and techniques introduced in our lecture are covered in Chapters 7 and 8 (up to 8.5) of the book.\nWhat we typically do not cover in the lecture, but only due to time constraints, is the topic of structured uncertainties and their analysis using structured singular value (SSV, 𝜇). These are treated in the section 8.6 through 8.11 of the book. It is warmly recommended to have a look at it.\nAlthough the book is not freely available online (only the first three pages are downloadable on the authors’ web page), it is available in a decent number of copies in the university library.\n\n\n\n\n\n\nGet the second edition of Skogestad’s book\n\n\n\nIn case you are interested in getting the book in one way or another (perhaps even by purchasing it), make sure you get the second edition published 2005. The book contains some useful snippets of Matlab code and the first edition relies on some ancient version of Matlab toolboxes, which makes it useless these days.\n\n\nThe topic of modeling uncertainty in frequency domain using weighting filters plugged into additive or multiplicative structures is fairly classical now and as such can be found in numerous textbooks on robust control such as [2], [3], [4], [5]. Although these are fine texts, frankly speaking they offer nearly no guidance for applying the highly advanced concepts to practical problems – they mostly focus on building up the theoretical framework. In this regard, Skogestad’s book is truly unique.\nThere are some newer texts that appear to be a bit more on the application side such as [6], [7] or [8], but I am not familiar them, to be honest.\n\n\n\n\n Back to topReferences\n\n[1] S. Skogestad and I. Postlethwaite, Multivariable Feedback Control: Analysis and Design, 2nd ed. Wiley, 2005. Available: https://folk.ntnu.no/skoge/book/\n\n\n[2] J. C. Doyle, B. A. Francis, and A. R. Tannenbaum, Feedback Control Theory, Reprint of the 1990 edition by Macmillan Publishing. Dover Publications, 2009. Available: https://www.control.utoronto.ca/people/profs/francis/dft.pdf\n\n\n[3] K. Zhou, J. C. Doyle, and K. Glover, Robust and Optimal Control, 1st ed. Prentice Hall, 1995.\n\n\n[4] G. E. Dullerud and F. Paganini, A Course in Robust Control Theory: A Convex Approach. in Texts in Applied Mathematics. New York: Springer-Verlag, 2000. doi: 10.1007/978-1-4757-3290-0.\n\n\n[5] R. S. Sánchez-Peña and M. Sznaier, Robust Systems Theory and Applications, 1st ed. Wiley-Interscience, 1998.\n\n\n[6] E. Lavretsky and K. Wise, Robust and Adaptive Control: With Aerospace Applications, 2nd ed. in Advanced Textbooks in Control and Signal Processing (C&SP). Cham: Springer, 2024. Available: https://doi.org/10.1007/978-3-031-38314-4\n\n\n[7] R. K. Yedavalli, Robust Control of Uncertain Dynamic Systems: A Linear State Space Approach. New York: Springer, 2014. Available: https://doi.org/10.1007/978-1-4614-9132-3\n\n\n[8] D.-W. Gu, P. H. Petkov, and M. M. Konstantinov, Robust Control Design with MATLAB, 2nd ed. in Advanced Textbooks in Control and Signal Processing. New York: Springer, 2013. Available: https://doi.org/10.1007/978-1-4471-4682-7",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "References"
    ]
  },
  {
    "objectID": "reduction_order_controller.html",
    "href": "reduction_order_controller.html",
    "title": "Controller order reduction",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Controller order reduction"
    ]
  },
  {
    "objectID": "roban_unstructured.html",
    "href": "roban_unstructured.html",
    "title": "Robustness analysis for unstructured uncertainty",
    "section": "",
    "text": "When we introduced the concept of robustness, we only vaguely hinted that it is always related to some property of interest. Now comes the time to specify these two properties:\n\nDefinition 1 (Robust stability) Guaranteed stability of the closed feedback loop with a given controller for all admissible (=considered apriori) deviations of the model from the reality.\n\n\nDefinition 2 (Robust performance) Robustness of some performance characteristics such as steady-state regulation error, attenuation of some specified disturbance, insensitivity to measurement noise, fast response, ….\n\n\nInternal stability\nBefore we start discussing robust stability, we need to discuss one fine issue related to stability of a nominal system. We do it through the following example.\n\nExample 1 (Internal stability) Consider the following feedback system with a nominal plant G(s) and a nominal controller K(s).\n\n\n\n\n\n\nFigure 1\n\n\n\nThe question is: is this closed-loop system stable? We determine stability by looking at the denominator of a closed-loop transfer function. But which one? There are several. Perhaps the most immediate one is the transfer function from the reference r to the plant output y. With the open-loop transfer function L(s) = G(s)K(s) = \\frac{s-1}{s+1} \\frac{k(s+1)}{s(s-1)} = \\frac{k}{s}, the closed-loop transfer function is \nT(s) = \\frac{\\frac{k}{s}}{1+\\frac{k}{s}} = \\frac{k}{s+k},  \n which is perfectly stable. But note that for practical purposes, all possible closed-loop transfer functions must be stable. How about the one from the output disturbance d to the plant output y? \nS(s) = \\frac{1}{1+\\frac{k}{s}} = \\frac{s}{s+k},\n which is stable too. Isn’t this a signal that we can stop worrying? Not yet. Consider now the closed-loop transfer function from the reference r to the control u. The closed-loop transfer function is \nK(s)S(s) = \\frac{\\frac{k(s+1)}{s(s-1)}}{1+\\frac{k}{s}} = \\frac{k(s+1)}{{\\color{red}(s-1)}(s+k)}.\n\nOops! This closed-loop transfer function is not stable. Obviously the culprit here is our cancelling the zero in the RHP with an unstable pole in the controller. But let’s emphasize that the trouble is not in imperfectness of this cancellation due to numerical errors. The trouble is in the very cancelling the zero in the RHP by the controller. Identical problem would arise if an unstable pole of the plant is cancelled by the RHP zero of the controller as we can see by modifying the assignment accordingly.\n\nThe example taught (or perhaps reminded) us that in order to guarantee stability of all closed-loop transfer functions, no cancellation of poles and zeros in the right half plane is allowed. The resulting closed-loop system is then called internally stable. Checking just (arbitrary) one closed-loop transfer function for stability is then enough to conclude that all of them are stable too.\n\n\nRobust stability for a multiplicative uncertainty\nWe consider a feedback system with a plant G(s) and a controller K(s), where the uncertainty in the plant modelled as multiplicative uncertainty, that is, G(s) = (1+W(s)\\Delta(s))\\,G_0(s).\nThe technique for analyzing closed-loop stability is based on Nyquist criterion. Instead of analyzing the Nyquist plot for the nominal plant G_0(s), we analyze the Nyquist plot for the uncertain plant G(s). The corresponding open-loop transfer function is \nL(s) = G(s)K(s) = (1+W(s)\\Delta(s))\\,G_0(s)K(s) = L_0(s) + W(s)L_0(s)\\Delta(s).\n\nWhen trying to figure out the conditions, under which this family of Nyquist curves avoids the point -1, it is useful to interpret the last equation at a given frequency \\omega as a disc with the center at L_0(j\\omega) and the radius W(j\\omega)L_0(j\\omega). To see this, note that \\Delta(j\\omega) represents a complex number with a magnitude up to one, and with an arbitrary angle.\n\n\n\n\n\n\nFigure 2: Robust stability for multiplicative uncertainty\n\n\n\nThe geometric formulation of the condition is then that the distance from -1 to the nominal Nyquist plot of L_0(j\\omega) is greater than the radius W(j\\omega)L_0(j\\omega) of the disc centered at the nominal Nyquist curve With the distance from the point -1 to the nominal Nyquist plot of L_0(s) evaluated at a particular frequency \\omega a |-1-L_0(j\\omega)| = |1+L_0(j\\omega)|, the condition can be written as\n\n|W(j\\omega)L_0(j\\omega)| &lt; |1+L_0(j\\omega)|, \\;\\forall \\omega.\n\nDividing both sides by 1+L_0(j\\omega) we get \n\\frac{W(j\\omega)L_0(j\\omega)}{1+L_0(j\\omega)} &lt; 1, \\;\\forall \\omega.\n\nBut recalling the definition of the complementary sensitivity function, and dividing both sides by W, we can rewrite the condition as \\boxed\n{|T_0(j\\omega)| &lt; 1/|W(j\\omega)|, \\;\\; \\forall \\omega.}\n\nThis condition has clear interpretation in terms of the magnitude of the complementary sensitivity function – it must be smaller than the reciprocal of the magnitude of the uncertainty weight at all frequencies.\nFinally, we can also invoke the definition of the \\mathcal H_\\infty norm and reformulate the condition as \\boxed\n{\\|WT\\|_{\\infty}&lt; 1.}\n\nTo appreciate usefulness of the this format of the robust stability condition beyond mere notational compactness, we mention that \\mathcal H_\\infty norm of an LTI system can be reliably computed. Robust stability can then be then checked by computing a single number.\nIn fact, it is even better than that – there are methods for computing a feedback controller that minimizise the \\mathcal H_\\infty norm of a specified closed-loop transfer function, which suggests an optimization-based approach to design of robustly stabilizing controllers. We are going to build on this in the next chapter. But let’s stick to the analysis for now.\n\n\nRobust stability for an LFT – small gain theorem\nWe consider the upper LFT as in Figure 3.\n\n\n\n\n\n\nFigure 3: Upper LFT with the \\mathbf N term corresponding to the nominal closed-loop system structured into blocks\n\n\n\nThe term corresponding to the nominal closed-loop system is structured into blocks. It is only the N_{11} block that captures the interaction with the uncertainty in the model. For convenience we rename this block as \nM \\coloneqq N_{11}.\n\nThe open-loop transfer function is then M \\Delta. Following the same Nyquist criterion based reasoning as before, that is, asking for the conditions under which this open-loop transfer function does not touch the point -1, while the \\Delta term can introduce an arbitrary phase, we arrive at the robust stability condition for the LFT as \\boxed\n{|M(j\\omega)|&lt;1,\\;\\;\\forall \\omega.}\n\nOnce again, invoking the definition of the \\mathcal H_\\infty norm, we can rewrite the condition compactly as \\boxed\n{\\|M\\|_{\\infty}&lt;1.}\n\nOnce again, the formulation as an inequality over all frequencies can be useful for visualization and interpretation, while the inequality with the \\mathcal H_\\infty norm can be used for computation and optimization.\nThis condition of robust stability belongs to the most fundamental results in control theory. It is known as the small gain theorem.\n\n\n\n\n\n\nSmall gain theorem works for MIMO too\n\n\n\nSmall gain theorem works for a MIMO uncertainty \\boldsymbol \\Delta and a block \\mathbf N_{11} (or \\mathbf M) too \n\\|\\mathbf M\\|_{\\infty}&lt;1.\n\nBut we discuss in the next section that it is typically too conservative as the \\boldsymbol \\Delta block has typically some structure (block diagonal) and it should be exploited. More on this in the section dedicated to structured uncertainty.\n\n\n\n\nNominal performance\nHaving discussed stability (and its robustness), it is now time to turn to performance (and its robustness). Performance can mean difference things for different people, and it can be specified in a number of ways, but we would like to formulate performance requirements in the same frequency domain setting as we did for (robust) stability. Namely, we would like to specify the performance requirements in terms of the frequency response of some closed-loop transfer function. The sensitivity function seems to be a natural choice for this purpose. It turns out that by imposing upper bound constraints on |S(\\omega)| (actually |S_0(\\omega)| as we now focus on the nominal case with no uncertainty) we can specify a number of performance requirements:\n\nUp to which frequency the feedback controller attenuates the disturbance, that is, the bandwidth \\omega_\\mathrm{BW} of the system.\nHow much the feedback controller attenuates the disturbances over the bandwidth.\nHow does it behave at very low frequencies, that is, how well it regulates the steady-state error.\nWhat is the maximum amplification of the disturbance, that is, the resonance peak.\n\nThese four types of performance requirements can be pointed at in Figure 4 below.\n\n\n\n\n\n\nFigure 4: Performance specifications through the shape of the magnitude frequency response of the sensitivity function\n\n\n\nBut these requirements can also be compactly expressed throug the performance weighting filter W_\\mathrm{p}(s) as \\boxed\n{|S_0(j\\omega)| &lt; 1/|W_\\mathrm{p}(j\\omega)|,\\;\\;\\forall \\omega,}\n\\tag{1}\nwhere S_0 = \\frac{1}{1+L_0} is the sensitivity function of the nominal closed-loop system. which can again be compactly written as \\boxed\n{\\|W_\\mathrm{p}S_0\\|_{\\infty}&lt;1.}\n\nIt lends some insight if we visualize this condition in the complex plane. First, recall that S_0 = \\frac{1}{1+L_0}. Equation 1 then translates to \n|W_\\mathrm{p}(j\\omega)|&lt;|1+L_0(j\\omega)|\\;\\;\\forall \\omega,\n which can be visualized as in\n\n\n\n\n\n\nFigure 5: Nominal performance condition\n\n\n\n\n\nRobust performance for a multiplicative uncertainty\nSo far we have the condition of robust stability and the condition of nominal performance. Simultaneous satisfaction of both gives… just robust stability and nominal performance. Robust performance obviously needs a stricter condition.\n\n\n\n\n\n\nFigure 6: Robust performance condition\n\n\n\n\n\\boxed{\n|W_\\mathrm{p}(j\\omega)S_0(j\\omega)| + |W(j\\omega)T_0(j\\omega)| &lt; 1\\;\\;\\forall \\omega.}\n\nIn the SISO case, this is equivalent to \\boxed\n{\\left\\|\n\\begin{bmatrix}\nW_\\mathrm{p}S_0\\\\\nWT_0\n\\end{bmatrix}\n\\right\\|_{\\infty}\n&lt;\\frac{1}{\\sqrt{2}},}\n where the augmented closed-loop system \\begin{bmatrix} W_\\mathrm{p}S\\\\ WT_0 \\end{bmatrix} is called mixed sensitivity function.\nIn the MIMO case we do not have a useful upper bound, but at least we have received a hint that it may be useful to minimize the \\mathcal H_\\infty norm of the mixed sensitivity function. This observation will directly lead to a control design method.\n\n\n\n\n Back to top",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for unstructured uncertainty"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html",
    "href": "opt_algo_derivatives.html",
    "title": "Computing the derivatives",
    "section": "",
    "text": "We have already argued that using derivatives gives optimization algorithms a boost. There are three methods to compute derivatives (and gradients, Jacobians, Hessians):",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing the derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#symbolic-methods",
    "href": "opt_algo_derivatives.html#symbolic-methods",
    "title": "Computing the derivatives",
    "section": "Symbolic methods",
    "text": "Symbolic methods\nThese are essentially the methods that we have all learnt to apply using a pen and paper. A bunch of rules. The input for the procedure is a function and the output from the procedure is another function. For example, for f(x) = x^2, the derivative is f'(x) = 2x.\nAlthough straightforward and automatable, symbolic methods are not always the best choice. When does this happen?\n\nThe function to be differentiated is already rather complicated, and the derivative will typically be even more complicated. Its evaluation then may be computationally expensive. We will see that in the example.\nThe function to be differentiated is not available in the closed form (as a formula), but only as a software implementation, however open-source.\n\n\nExample 1 (Symbolic computation of the gradient of a function of the simulated trajectory) Consider the state-space model of a pendulum \n\\underbrace{\n\\begin{bmatrix}\n\\dot \\theta\\\\ \\dot \\omega\n\\end{bmatrix}}_{\\dot{\\bm x}}\n=\n\\underbrace{\n\\begin{bmatrix}\n\\omega\\\\ -\\frac{g}{l}\\sin\\theta\n\\end{bmatrix}}_{\\mathbf f(\\bm x)},\n where l=1\\,\\mathrm{m} is the length of the pendulum, g=9.81\\,\\mathrm{m}/\\mathrm{s}^2 is the acceleration due to gravity, \\theta and \\omega are the angle and angular velocity of the pendulum, respectively. We are going to simulate the trajectory of the pendulum that is initially at some nonzero angle, say, \\theta(0) = \\pi/4 = \\theta_0, and zero velocity, that is, \\omega(0) = 0 = \\omega_0. And we are going to consider the 2-norm (actually its square for convenience) of the state vector at the end of the simulation interval as the cost function to be minimized, for which we need to evaluate the gradient at the initial state.\nFirst, we write an ODE solver that obtains an approximation of the final point \\bm x(t_\\mathrm{f}) of the state trajectory on the interval [0,t_\\mathrm{f}], and a function that computes the cost as a function of the initial state J(\\bm x_0).\n\n\n\n\n\n\nAlternative (and still imperfect) notation\n\n\n\nThe state at the final time is a function of the state at the initial time, hence we could also write it as \\bm x(t_\\mathrm{f};\\bm x_0), in which case the cost cold be written as J(\\bm x(t_\\mathrm{f};\\bm x_0)). The dependence of the cost on the initial state is still visible, but the notation is a bit more clumsy (and abusive anyway).\n\n\n\n\nShow the code\nfunction solve_for_final_state_fd(f, x₀, tspan, h)\n    t0, tf = tspan\n    t = t0\n    x = x₀\n    while t &lt; tf\n        x = x + h * f(x)\n        t = t + h\n    end\n    return x\nend\n\nfunction J(x₀) \n    x_final = solve_for_final_state_fd(f, x₀, tspan, h)\n    return x_final[1]^2 + x_final[2]^2\nend\n\n\nAnd now we use the solver to compute the trajectory and the cost\n\n\nShow the code\ng = 9.81\nl = 1.0\nf(x) = [x[2], -g/l*sin(x[1])]  \nθ₀ = π/4\nω₀ = 0.0   \ntspan = (0.0, 1.0)\nh = 0.1\n\nJ([θ₀, ω₀])\n\n\n1.6184460563562304\n\n\nWe now use the Symbolics.jl package to compute the gradient of the cost function at the initial state. We first define symbolic state variables and and then obtain the symbolic expression for the cost function just by evaluating the function we already have at these symbolic state variables.\n\n\nShow the code\nusing Symbolics\n@variables θ₀ ω₀\nJ_sym = J([θ₀, ω₀])\n\n\n(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀)^2 + (-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(0.1(-0.9810000000000001sin(0.1(-0.9810000000000001sin(θ₀) + ω₀) + θ₀ + 0.1ω₀) - 0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) + ω₀) + 0.1(-0.9810000000000001sin(θ₀) - 0.9810000000000001sin(θ₀ + 0.1ω₀) + ω₀) + θ₀ + 0.1ω₀) + ω₀)^2\n\n\nIf the shortcomings of symbolic methods have not yet started surfacing, scroll to the right in the output field. Rather long, n’est-ce pas? And we have just started, because we now must differentiate this long expression symbolically (and then we convert it from a symbolic expression back to a standard function in Julia):\n\n\nShow the code\n∇J_sym = Symbolics.gradient(J_sym,[θ₀, ω₀])\n∇J_sym_expr = build_function(∇J_sym, [θ₀, ω₀])\n∇J_sym_fun = eval(∇J_sym_expr[1])\n\n\nFinally, let’s do some benchmarking of evaluation of the gradient at the given (initial) state:\n\n\nShow the code\nusing BenchmarkTools\n@btime ∇J_sym_fun([π/4,0.0])\n\n\n  271.405 μs (4 allocations: 160 bytes)\n\n\n2-element Vector{Float64}:\n 3.9096539447898264\n 0.020083627236371047\n\n\nAs a teaser for what what is to come, we also benchmark the solution based on AD:\n\n\nShow the code\nusing ForwardDiff\n\n@btime ForwardDiff.gradient(J, [π/4,0.0])\n\n\n  3.224 μs (133 allocations: 5.28 KiB)\n\n\n2-element Vector{Float64}:\n 3.909653944789824\n 0.020083627236369115\n\n\nNote that while for the former quite some work must have been done before the timing analysis was started (namely, the formula for the derivative had to be found), for the latter we started only with the function definition. And yet the latter approach wins hands down. But before we start exploring the AD methods, we give a brief overview of numerical methods based on finite-difference (FD) approximations.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing the derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#numerical-finite-difference-fd-methods",
    "href": "opt_algo_derivatives.html#numerical-finite-difference-fd-methods",
    "title": "Computing the derivatives",
    "section": "Numerical finite-difference (FD) methods",
    "text": "Numerical finite-difference (FD) methods\nThese methods approximate the derivative by computing differences between the function values at different points, hence the name finite-difference (FD) methods. The simplest FD methods follow from the definition of the derivative after omiting the limit:\n\n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x+h)-f(x)}{h}\\qquad\\qquad \\text{forward difference}\n or \n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x)-f(x-h)}{h}\\qquad\\qquad \\text{backward difference}\n or \n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x+\\frac{h}{2})-f(x-\\frac{h}{2})}{h}\\qquad\\qquad \\text{central difference}\n\nFor functions of vector variables, the same idea applies, but now we have to compute the difference for each component of the vector.\n\nDependence of the error on the step size\nThe finite-difference methods only approximate the derivatives. The smaller the h in the above formulas, the smaller the approximation error. Really? Not quite. Let’s explore it through an example.\n\nExample 2 (Dependence of the error on the step size) Consider a scalar function of a vector argument f(\\bm x) = \\sum_{i=1}^n x_i^2.\n\n\nShow the code\nf(x) = sum(xᵢ^2 for xᵢ in x)\n\n\nf (generic function with 1 method)\n\n\nNow, in order to compute the gradient \\nabla f, we need to compute all the individual partial derivatives, the individual components of the vector. Let’s now restrict ourselves just to one component, say, the first one, that is, let’s compute \\frac{\\partial f(\\mathbf x)}{\\partial x_1}.\nIn this simple example, a formula can be written down upon inspection: \\frac{\\partial f(\\mathbf x)}{\\partial x_1} = 2x_1:\n\n\nShow the code\nx₀ = rand(10)\n∇f_1_exact = 2*x₀[1]\n\n\n1.3128754092225619\n\n\nbut let’s pretend that this answer is not available to us (we will only use it for evaluation of approximation errors of of chosen FD methods).\nWe now give a function for computing the first entry in the gradient (vector) by using the forward FD method. Note that in defining the function we exploit the multiple dispatch functionality of Julia, thanks to which the function will handled the floating point model of the input appropriately. That is, the input vector could be IEEE-754 double-precision floating-point format, or IEEE-754 single-precision floating-point format (or perhaps even something else).\n\n\nShow the code\nfunction ∇f_1(f,x₀::Vector{T},h::T) where T&lt;:Real\n    (f(x₀[1]+h)-f(x₀[1]))/h\nend\n\n\n∇f_1 (generic function with 1 method)\n\n\nWe can now compute the first entry of the gradient for the particular vector given in the IEEE double format (default for Julia)\n\n\nShow the code\n∇f_1(f,x₀,h)\n\n\n1.4128754092225615\n\n\nWe can also compute the same quantity for the same vector represented in the IEEE single format:\n\n\nShow the code\n∇f_1(f,Vector{Float32}(x₀),Float32(h))\n\n\n1.4128759f0\n\n\nObviously, both answers differ from the accurate one computed above.\nNow, let’s examine the error as a function of the size of the interval h:\n\n\nShow the code\nh_range = exp10.(range(-13, stop=-1, length=1000));\nabs_err_64 = [abs((∇f_1_exact - ∇f_1(f,x₀,h))) for h in h_range];\nabs_err_32 = [abs((∇f_1_exact - ∇f_1(f,Vector{Float32}(x₀),Float32(h)))) for h in h_range];\n\nusing Plots\nplot(h_range, abs_err_64,xaxis=:log, yaxis=:log, xlabel=\"h\", ylabel = \"|e|\", label = \"IEEE double\")\nplot!(h_range, abs_err_32,xaxis=:log, yaxis=:log, xlabel=\"h\", ylabel = \"|e|\", label = \"IEEE single\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we read the graph from right to left – as h is getting smaller –, we observe that initially the error decreases for both 64- and 32-bit floating-point format, and it decreases at the same rate. The reason for this decrease is that the trunction error (essentially what we commit here by doing FD approximation is that we truncate the Taylor series) dominates here, and this error goes down with h.\nThe major intended takeaway from this example is that this reduction of the error only takes place down to some h below which the error no longer decreases; in fact, it actally increases as h gets smaller. The reason is that for h this small, the rounding errors dominate. Apparently, they start exhibitting themselves for smaller values with the 64-bit format than with 32-bit format. The rounding errors are getting dominant here as we are subtracting two numbers that are getting more and more similar as h decreaces. This is known as the phenomenon of catastrophic cancellation.\nIt is known from rigorous numerical analysis that the error in the case of the simple backward or forward finite difference approximation to a scalar derivative scales with \\sqrt{\\epsilon}, where \\epsilon is the machine epsilon. Here we can expect even worse outcome as the dimension n of the vector grows. Note that \\epsilon for double precision IEEE is\n\n\nShow the code\n2^(-53)\n\n\n1.1102230246251565e-16\n\n\nwhich is available in Julia through eps() function with the type as the input argument (if no argument is given, it assumes Float64):\n\n\nShow the code\nsqrt(eps())\n\n\n1.4901161193847656e-8\n\n\nSimilarly, the 32-bit version is\n\n\nShow the code\nsqrt(eps(Float32))\n\n\n0.00034526698f0\n\n\nApparently, these are roughly the “cutt-off” values of h.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing the derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#automatic-also-algorithmic-differentiation-ad-methods",
    "href": "opt_algo_derivatives.html#automatic-also-algorithmic-differentiation-ad-methods",
    "title": "Computing the derivatives",
    "section": "Automatic (also Algorithmic) differentiation (AD) methods",
    "text": "Automatic (also Algorithmic) differentiation (AD) methods\n#TODO: in the meantime, have a look at [1, Ch. 5], or [2, Sec. 6.6], or [3, Sec. 2].\n\nForward AD\n#TODO\n\nImplementation of Forward AD by dual numbers\nSimilar to a complex number, a dual number has two components, one corresponding to the value, the other corresponding to the derivative:\n\nx = v + d\\epsilon,\n where the special property of \\epsilon is \n\\epsilon^2=0.\n\n(Compare it with the property of the imaginary unit: i^2=-1.)\nMultiplication of two dual numbers y = x_1\\cdot x_2 is the defined naturally as \n\\begin{aligned}\nx &= (v_1 + d_1\\epsilon)\\cdot (v_2 + d_2\\epsilon)\\\\\n  &= v_1v_2 + (v_1d_2+d_1v_2) \\epsilon.\n\\end{aligned}\n\nSimilarly for other functions. We illustrate this using the following example.\n\nExample 3 Consider a function y(x) = \\cos(x^2). Its derivative is trivially \\frac{\\mathrm d y}{\\mathrm d x} = -2x\\sin(x^2).\nLet’s now evaluate this result at a particular value of x, say\n\nx = 2\n\n2\n\n\nFirst, we are going to develop our own data class (actually struct) in Julia for dual numbers. The code below is inspired by a code from [3] (they even provide Jupyter Notebooks).\nThen we show the usage of a functionality already provided in ForwardDiff.jl package implementing the forward mode AD.\n\n\nShow the code\nstruct Dual\n    v        # the VALUE part\n    d        # the DERIVATIVE part\nend\n\n\nNow we need to overload the involved basic operations such as addition and multiplication of two dual numbers, multiplication by a scalar, squaring and finding the value of cosine function.\n\n\nShow the code\nBase.:+(a::Dual,b::Dual) = Dual(a.v+b.v,a.d+b.d)\nBase.:*(a::Dual,b::Dual) = Dual(a.v*b.v,a.d*b.v+b.d*a.v)\nBase.:*(a::Number,b::Dual) = Dual(a*b.v,a*b.d)\nBase.:^(a::Dual,b::Int) = Dual(a.v^b,b*a.v^(b-1))\nBase.:cos(a::Dual) = Dual(cos(a.v),-sin(a.v)*a.d)\n\n\nLet’s now check the functionality of the individual functions\n\nX = Dual(x,1)\n\nDual(2, 1)\n\n\n\nY = Dual(3,0)\n\nDual(3, 0)\n\n\n\n3*X\n\nDual(6, 3)\n\n\n\nY*X\n\nDual(6, 3)\n\n\n\nX^2\n\nDual(4, 4)\n\n\n\ncos(X)\n\nDual(-0.4161468365471424, -0.9092974268256817)\n\n\nFinally, let’s use the new functionality to compute the derivative of the assigned function \\cos(x^2)\n\ncos(X^2)\n\nDual(-0.6536436208636119, 3.027209981231713)\n\n\nIn practice, you will hardly feel a need to implement your own library for algorithmic differentiation. Instead, you may want to use one of those avaialable ones, such as ForwardDiff.jl.\n\nusing ForwardDiff\nX = ForwardDiff.Dual(x,1)\nY = cos(X^2)\nY.value\n\n-0.6536436208636119\n\n\n\nY.partials\n\n1-element ForwardDiff.Partials{1, Float64}:\n 3.027209981231713\n\n\nCompare with the “exact”\n\n-2*x*sin(x^2)\n\n3.027209981231713\n\n\n\n\n\n\nReverse AD\n#TODO",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing the derivatives"
    ]
  },
  {
    "objectID": "cont_numerical_references.html",
    "href": "cont_numerical_references.html",
    "title": "References",
    "section": "",
    "text": "The indirect approach to the continuous-time optimal control problem (OCP) formulates the necessary conditions of optimality as a two-point boundary value problem (TP-BVP), which generally requires numerical methods. The direct approach to the continuous-time OCP relies heavily on numerical methods too, namely the methods for solving nonlinear programs (NLP) and methods for solving ordinary differential equations (ODE). Numerical methods for both approaches share a lot of common principles and tools, and these are collectively presented in the literature as called numerical optimal control. A recommendable (and freely online available) introduction to these methods is [1]. Shorter version of this is in chapter 8 of [2], which is also available online. A more comprehensive treatment is in [3].\nSome survey papers such as [4] and [5] can also be useful, although now primarily as historical accounts. Similarly with the classics [6] and [7], which cover the indirect approach only.\nAnother name under which the numerical methods for the direct approach are presented is trajectory optimization. There are quite a few tutorials and surveys such as [8] and [9].\n\n\n\n\n Back to topReferences\n\n[1] S. Gros and M. Diehl, “Numerical Optimal Control (Draft).” Systems Control; Optimization Laboratory IMTEK, Faculty of Engineering, University of Freiburg, Apr. 2022. Available: https://www.syscop.de/files/2020ss/NOC/book-NOCSE.pdf\n\n\n[2] J. B. Rawlings, D. Q. Mayne, and M. M. Diehl, Model Predictive Control: Theory, Computation, and Design, 2nd ed. Madison, Wisconsin: Nob Hill Publishing, LLC, 2017. Available: http://www.nobhillpublishing.com/mpc-paperback/index-mpc.html\n\n\n[3] J. T. Betts, Practical Methods for Optimal Control Using Nonlinear Programming, 3rd ed. in Advances in Design and Control. Society for Industrial and Applied Mathematics, 2020. doi: 10.1137/1.9781611976199.\n\n\n[4] A. V. Rao, “A survey of numerical methods for optimal control,” Advances in the Astronautical Sciences, vol. 135, no. 1, pp. 497–528, 2009, Accessed: Jun. 09, 2016. [Online]. Available: http://vdol.mae.ufl.edu/ConferencePublications/trajectorySurveyAAS.pdf\n\n\n[5] O. von Stryk and R. Bulirsch, “Direct and indirect methods for trajectory optimization,” Annals of Operations Research, vol. 37, no. 1, pp. 357–373, Dec. 1992, doi: 10.1007/BF02071065.\n\n\n[6] D. E. Kirk, Optimal Control Theory: An Introduction, Reprint of the 1970 edition. Dover Publications, 2004.\n\n\n[7] A. E. Bryson Jr. and Y.-C. Ho, Applied Optimal Control: Optimization, Estimation and Control, Revised edition. CRC Press, 1975.\n\n\n[8] M. Kelly, “An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation,” SIAM Review, vol. 59, no. 4, pp. 849–904, Jan. 2017, doi: 10.1137/16M1062569.\n\n\n[9] M. P. Kelly, “Transcription Methods for Trajectory Optimization: A beginners tutorial,” arXiv:1707.00284 [math], Jul. 2017, Accessed: Apr. 06, 2021. [Online]. Available: http://arxiv.org/abs/1707.00284",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "References"
    ]
  },
  {
    "objectID": "discr_dir_mpc_hw.html",
    "href": "discr_dir_mpc_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "Homework"
    ]
  },
  {
    "objectID": "discr_dir_goals.html",
    "href": "discr_dir_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Give some examples of practically useful optimal control cost functions (aka performance indices).\nExplain the challenges in designing controllers with a fixed structure (e.g. PID controllers) by optimizing over their coefficients.\nFormulate the general (nonlinear) problem of optimal control design for a discrete-time system as a numerical optimization over (finite) control sequences. Discuss the two possible variants: simultaneous and sequential.\nFormulate the problem of optimal regulator design for a discrete-time linear time-invariant (LTI) system over a finite time horizon and with a quadratic performance index as a quadratic program (QP). Develop fully both the simultaneous and sequential forms of the optimization problem and consider also including the inequality constraints. What is the major disadvantage of the control strategy based on the offline optimization over a control sequence?\nExplain the essence of receding horizon control also known as model predictive control (MPC). What are the major advantages and disadvantages?\nFormulate the MPC regulation for a linear system and a quadratic cost as a quadratic program. Give both the simultaneous and sequential versions.\nFormulate the MPC tracking for a linear system and a quadratic cost as a quadratic program. Explain the need for replacement of the control signals by their increments in the optimization problem. Give both the simultaneous and sequential versions.\nDiscuss the anticipatory reference tracking (aka preview control) and show how this could be achieved using MPC.\nShow how soft constraints can be included in the MPC optimization problem and discuss the motivation for their introduction.\nExplain the difference between the prediction horizon and control horizon.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "discr_dir_goals.html#knowledge-remember-and-understand",
    "href": "discr_dir_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Give some examples of practically useful optimal control cost functions (aka performance indices).\nExplain the challenges in designing controllers with a fixed structure (e.g. PID controllers) by optimizing over their coefficients.\nFormulate the general (nonlinear) problem of optimal control design for a discrete-time system as a numerical optimization over (finite) control sequences. Discuss the two possible variants: simultaneous and sequential.\nFormulate the problem of optimal regulator design for a discrete-time linear time-invariant (LTI) system over a finite time horizon and with a quadratic performance index as a quadratic program (QP). Develop fully both the simultaneous and sequential forms of the optimization problem and consider also including the inequality constraints. What is the major disadvantage of the control strategy based on the offline optimization over a control sequence?\nExplain the essence of receding horizon control also known as model predictive control (MPC). What are the major advantages and disadvantages?\nFormulate the MPC regulation for a linear system and a quadratic cost as a quadratic program. Give both the simultaneous and sequential versions.\nFormulate the MPC tracking for a linear system and a quadratic cost as a quadratic program. Explain the need for replacement of the control signals by their increments in the optimization problem. Give both the simultaneous and sequential versions.\nDiscuss the anticipatory reference tracking (aka preview control) and show how this could be achieved using MPC.\nShow how soft constraints can be included in the MPC optimization problem and discuss the motivation for their introduction.\nExplain the difference between the prediction horizon and control horizon.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "discr_dir_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "discr_dir_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\nImplement a simple model predictive controller (MPC) in both the simultaneous and sequential formats using Matlab. Rely on the availability of numerical solvers for quadratic programming, that is, you do not have to write your own optimization solver.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html",
    "href": "opt_theory_modellers.html",
    "title": "Optimization modelling languages",
    "section": "",
    "text": "Realistically complex optimization problems cannot be solved with just a pen and a paper – computer programs (often called optimization solvers) are needed to solve them. And now comes the challenge: as various solvers for even the same class of problems differ in the algorithms they implement, so do their interfaces – every solver expects the inputs (the data defining the optimization problem) in a specific format. This makes it difficult to switch between solvers, as the problem data has to be reformatted every time.\n\nExample 1 (Data formatting for different solvers) Consider the following optimization problem: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} \\leq \\begin{bmatrix} 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix} \\bm x \\leq  \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7\\end{bmatrix}\n\\end{aligned}\n\nThere are dozens of solvers that can be used to solve this problem. Here we demonstrate a usage of these two: OSQP and COSMO.jl. And we are going to call the solvers in Julia (using the the wrappers OSQP.jl for the former). First, we start with OSQP (in fact, this is their example):\n\n\nShow the code\nusing OSQP\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nproblem_OSQP = OSQP.Model()\nOSQP.setup!(problem_OSQP; P=P, q=q, A=A, l=l, u=u, alpha=1, verbose=false)\n\n# Solve the optimization problem and show the results\nresults_OSQP = OSQP.solve!(problem_OSQP)\nresults_OSQP.x\n\n\n2-element Vector{Float64}:\n 0.300000191114193\n 0.6999998090276102\n\n\nNow we do the same with COSMO. First, we must take into account that COSMO cannot accept two-sided inequalities, so we have to reformulate the problem so that the constraints are only in the form of \\mathbf A\\bm x + b \\geq \\bm 0: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix} -1 & -1\\\\ -1 & 0\\\\ 0 & -1\\\\ 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix}\\bm x + \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7 \\\\ -1 \\\\ 0 \\\\ 0\\end{bmatrix} \\geq  \\mathbf 0.\n\\end{aligned}\n\n\n\nShow the code\nusing COSMO\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nAa = [-A; A]\nba = [u; -l]\n\nproblem_COSMO = COSMO.Model()\nconstraint = COSMO.Constraint(Aa, ba, COSMO.Nonnegatives)\nsettings = COSMO.Settings(verbose=false)\nassemble!(problem_COSMO, P, q, constraint, settings = settings)\n\n# Solve the optimization problem and show the results\nresults_COSMO = COSMO.optimize!(problem_COSMO)\nresults_COSMO.x\n\n\n2-element Vector{Float64}:\n 0.30000000000000143\n 0.6999999999999995\n\n\nAlthough the two solvers are solving the same problem, the data has to be formatted differently for each of them (and the difference in syntax is not negligible either).\nWhat if we could formulate the same problem without considering the pecualiarities of each solver? It turns out that it is possible. In Julia we can use JuMP.jl:\n\n\nShow the code\nusing JuMP\nusing SparseArrays\nusing OSQP, COSMO\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nmodel_JuMP = Model()\n@variable(model_JuMP, x[1:2])\n@objective(model_JuMP, Min, 0.5*x'*P*x + q'*x)\n@constraint(model_JuMP, A*x .&lt;= u)\n@constraint(model_JuMP, A*x .&gt;= l)\n\n# Solve the optimization problem using OSQP and show the results\nset_silent(model_JuMP)\nset_optimizer(model_JuMP, OSQP.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_OSQP = value.(x)\n\n# Now solve the problem using COSMO and show the results\nset_optimizer(model_JuMP, COSMO.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_COSMO = value.(x)\n\n\n2-element Vector{Float64}:\n 0.2999999999999993\n 0.7000000000000002\n\n\n\nNotice how the optimization problem is defined just once in the last code and then different solvers can be chosen to solve it. The code represents an instance of a so-called optimization modelling language (OML), or actually its major class called algebraic modelling language (AML).\nThe key motivation for using an OML/AML is to separate the process of formulating the problem from the process of solving it (using a particular solver). Furthermore, such solver-independent problem description (called optimization model) better mimics the way we formulate these problems using a pen and a paper, making it (perhaps) a bit more convenient to write our own and read someone else’s models.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#why-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#why-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "",
    "text": "Realistically complex optimization problems cannot be solved with just a pen and a paper – computer programs (often called optimization solvers) are needed to solve them. And now comes the challenge: as various solvers for even the same class of problems differ in the algorithms they implement, so do their interfaces – every solver expects the inputs (the data defining the optimization problem) in a specific format. This makes it difficult to switch between solvers, as the problem data has to be reformatted every time.\n\nExample 1 (Data formatting for different solvers) Consider the following optimization problem: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} \\leq \\begin{bmatrix} 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix} \\bm x \\leq  \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7\\end{bmatrix}\n\\end{aligned}\n\nThere are dozens of solvers that can be used to solve this problem. Here we demonstrate a usage of these two: OSQP and COSMO.jl. And we are going to call the solvers in Julia (using the the wrappers OSQP.jl for the former). First, we start with OSQP (in fact, this is their example):\n\n\nShow the code\nusing OSQP\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nproblem_OSQP = OSQP.Model()\nOSQP.setup!(problem_OSQP; P=P, q=q, A=A, l=l, u=u, alpha=1, verbose=false)\n\n# Solve the optimization problem and show the results\nresults_OSQP = OSQP.solve!(problem_OSQP)\nresults_OSQP.x\n\n\n2-element Vector{Float64}:\n 0.300000191114193\n 0.6999998090276102\n\n\nNow we do the same with COSMO. First, we must take into account that COSMO cannot accept two-sided inequalities, so we have to reformulate the problem so that the constraints are only in the form of \\mathbf A\\bm x + b \\geq \\bm 0: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix} -1 & -1\\\\ -1 & 0\\\\ 0 & -1\\\\ 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix}\\bm x + \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7 \\\\ -1 \\\\ 0 \\\\ 0\\end{bmatrix} \\geq  \\mathbf 0.\n\\end{aligned}\n\n\n\nShow the code\nusing COSMO\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nAa = [-A; A]\nba = [u; -l]\n\nproblem_COSMO = COSMO.Model()\nconstraint = COSMO.Constraint(Aa, ba, COSMO.Nonnegatives)\nsettings = COSMO.Settings(verbose=false)\nassemble!(problem_COSMO, P, q, constraint, settings = settings)\n\n# Solve the optimization problem and show the results\nresults_COSMO = COSMO.optimize!(problem_COSMO)\nresults_COSMO.x\n\n\n2-element Vector{Float64}:\n 0.30000000000000143\n 0.6999999999999995\n\n\nAlthough the two solvers are solving the same problem, the data has to be formatted differently for each of them (and the difference in syntax is not negligible either).\nWhat if we could formulate the same problem without considering the pecualiarities of each solver? It turns out that it is possible. In Julia we can use JuMP.jl:\n\n\nShow the code\nusing JuMP\nusing SparseArrays\nusing OSQP, COSMO\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nmodel_JuMP = Model()\n@variable(model_JuMP, x[1:2])\n@objective(model_JuMP, Min, 0.5*x'*P*x + q'*x)\n@constraint(model_JuMP, A*x .&lt;= u)\n@constraint(model_JuMP, A*x .&gt;= l)\n\n# Solve the optimization problem using OSQP and show the results\nset_silent(model_JuMP)\nset_optimizer(model_JuMP, OSQP.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_OSQP = value.(x)\n\n# Now solve the problem using COSMO and show the results\nset_optimizer(model_JuMP, COSMO.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_COSMO = value.(x)\n\n\n2-element Vector{Float64}:\n 0.2999999999999993\n 0.7000000000000002\n\n\n\nNotice how the optimization problem is defined just once in the last code and then different solvers can be chosen to solve it. The code represents an instance of a so-called optimization modelling language (OML), or actually its major class called algebraic modelling language (AML).\nThe key motivation for using an OML/AML is to separate the process of formulating the problem from the process of solving it (using a particular solver). Furthermore, such solver-independent problem description (called optimization model) better mimics the way we formulate these problems using a pen and a paper, making it (perhaps) a bit more convenient to write our own and read someone else’s models.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#why-not-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#why-not-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "Why not optimization modelling languages?",
    "text": "Why not optimization modelling languages?\nAs a matter of fact, some optimization experts even keep avoiding OML/AML altogether. For example, if a company pays for a (not really cheap) license of Gurobi Optimizer – a powerful optimization library for (MI)LP/QP/QCQP –, it may be the case that for a particular very large-scale optimization problem their optimization specialist will have hard time to find a third-party solver of comparable performance. If then its Python API makes definition of optimization problems convenient too (see the code below), maybe there is little regret that such problem definitions cannot be reused with a third-party solver. The more so that since it is tailored to Gurobi solver, it will offer control over the finest details.\nimport gurobipy as gp\nimport numpy as np\n\n# Define the data for the model\nP = np.array([[4.0, 1.0], [1.0, 2.0]])\nq = np.array([1.0, 1.0])\nA = np.array([[1.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\nl = np.array([1.0, 0.0, 0.0])\nu = np.array([1.0, 0.7, 0.7])\n\n# Create a new model\nm = gp.Model(\"qp\")\n\n# Create a vector variable\nx = m.addMVar((2,))\n\n# Set the objective\nobj = 1/2*(x@P@x + q@x)\nm.setObjective(obj)\n\n# Add the constraints\nm.addConstr(A@x &gt;= l, \"c1\")\nm.addConstr(A@x &lt;= u, \"c2\")\n\n# Run the solver\nm.optimize()\n\n# Print the results\nfor v in m.getVars():\n    print(f\"{v.VarName} {v.X:g}\")\n\nprint(f\"Obj: {m.ObjVal:g}\")\nSimilar and yet different is the story of the IBM ILOG CPLEX, another top-notch solvers addressing the same problems as Gurobi. They do have their own modeling language called Optimization Modelling Language (OPL), but it is also only interfacing with their solver(s). We can only guess that their motivation for developing their own optimization modelling language was that at the time of its developments (in 1990s) Python was still in pre-2.0 stage and formulating optimization problems in programming languages like C/C++ or Fortran was nowhere close to being convenient. Gurobi, in turn, started in 2008, when Python was already a popular language.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#language-independent-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#language-independent-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "Language-independent optimization modelling languages",
    "text": "Language-independent optimization modelling languages\nOptimization/algebraic modelling languages were originally developed outside programming languages, essentially as standalone tools. Examples are AMPL, GAMS, and, say, GLPK/GMPL (MathProg). We listed these main names here since they can be bumped across (they are still actively developed), but we are not going to discuss them in our course any further. The reason is that there are now alternatives that are implemented as packages/toolboxes in programming languages such as Julia, Matlab, and Python, which offer a more fluent workflow – a user can use the same programming language to acquire the data, preprocess them, formulate the optimization problem, configure and call a solver, and finally do some postprocessing including a visualization and whatever reporting, all without leaving the language of their choice.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-julia",
    "href": "opt_theory_modellers.html#optimization-modelling-in-julia",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Julia",
    "text": "Optimization modelling in Julia\nMy obvious (personal) bias towards Julia programming language is partly due to the terrific support for optimization modelling in Julia:\n\nJuMP.jl not only constitutes one of the flagship packages of the Julia ecosystem but it is on par with the state of the art optimization modelling languages. Furthermore, being a free and open source software, it enjoys a vibrant community of developers and users. They even meet annually at JuMP-dev conference (in 2023 in Boston, MA).\nConvex.jl is an implementation of the concept of Disciplined Convex Programming (DCP) in Julia (below we also list its implementations in Matlab and Python). Even though it is now registered as a part of the JuMP.jl project, it is still a separate concept. Interesting, convenient, but it seems to be in a maintanence mode now.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-matlab",
    "href": "opt_theory_modellers.html#optimization-modelling-in-matlab",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Matlab",
    "text": "Optimization modelling in Matlab\nPopularity of Matlab as a language and an ecosystem for control-related computations is undeniable. Therefore, let’s have a look at what is available for modelling optimization problems in Matlab:\n\nOptimization Toolbox for Matlab is one of the commercial toolboxes produced by Matlab and Simulink creators. Since the R2017b release the toolbox supports Problem-based optimization workflow (besides the more traditional Solver-based optimization workflow supported since the beginning), which can be regarded as a kind of an optimization/algebraic modelling language, albeit restricted to their own solvers.\nYalmip started as Yet Another LMI Parser quite some time ago (which reveals its control theoretic roots), but these days it serves as fairly complete algebraic modelling language (within Matlab), interfacing to perhaps any optimization solver, both commercial and free&open-source. It is free and open-source. Is is still actively developed and maintained and it abounds with tutorials and examples.\nCVX is a Matlab counterpart of Convex.jl (or the other way around, if you like, since it has been here longer). The name stipulates that it only allows convex optimization probles (unlike Yalmip) – it follows the Disciplined Convex Programming (DCP) paradigm. Unfortunately, the development seems to have stalled – the last update is from 2020.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-python",
    "href": "opt_theory_modellers.html#optimization-modelling-in-python",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Python",
    "text": "Optimization modelling in Python\nPython is a very popular language for scientific computing. Although it is arguable if it is actually suitable for implementation of numerical algoritms, when it comes to building optimization models, it does its job fairly well (and the numerical solvers it calls can be developed in different language). Several packages implementing OML/AML are available:\n\ncvxpy is yet another instantiation of Disciplined Convex Programming that we alredy mention when introducing Convex.jl and CVX. And it turns out that this one exhibits the greatest momentum. The team of developers seems to be have exceeded a critical mass, hence the tools seems like a safe bet already.\nPyomo is a popular open-source optimization modelling language within Python.\nAPMonitor and GEKKO are relatively young projects, primarily motivated by applications of machine learning and optimization in chemical process engineering.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "ext_hw.html",
    "href": "ext_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Homework"
    ]
  },
  {
    "objectID": "discr_indir_LQR_tracking.html",
    "href": "discr_indir_LQR_tracking.html",
    "title": "Reference tracking using LQR",
    "section": "",
    "text": "#TODO\n\n\n\n Back to top",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Reference tracking using LQR"
    ]
  },
  {
    "objectID": "cont_dp_references.html",
    "href": "cont_dp_references.html",
    "title": "References",
    "section": "",
    "text": "Dynamic programming for continuous-time systems is typically not covered in standard texts on dynamic programming, because those mainly focus on discrete-time systems. But there is no shortage of discussions of HJB equation in control theory texts. Our introductory treatment here is based on Section 6.3 in [1].\nThe classical [2] uses HJB equation as the main tool for solving various version of the LQR problem.\n[3] discusses the HJB equation in Chapter 5. In the section 5.2 they also discuss the connection with Pontryagin’s principle.\n\n\n\n\n Back to topReferences\n\n[1] F. L. Lewis, D. Vrabie, and V. L. Syrmo, Optimal Control, 3rd ed. John Wiley & Sons, 2012. Accessed: Mar. 09, 2022. [Online]. Available: https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf\n\n\n[2] B. D. O. Anderson and J. B. Moore, Optimal Control: Linear Quadratic Methods, Reprint of the 1989 edition. Dover Publications, 2007. Available: http://users.cecs.anu.edu.au/~john/papers/BOOK/B03.PDF\n\n\n[3] D. Liberzon, Calculus of Variations and Optimal Control Theory: A Concise Introduction. Princeton University Press, 2011. Available: http://liberzon.csl.illinois.edu/teaching/cvoc/cvoc.html",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "References"
    ]
  },
  {
    "objectID": "rocond_references.html",
    "href": "rocond_references.html",
    "title": "References",
    "section": "",
    "text": "The literature for \\mathcal H_\\infty control is essentially identical to the one we gave in the previous chapter on analysis of robustness. In particular, we stick to our primary textbook [1], in which the material is discussed in the forty-page Chapter 9.\nWhile discussing the analysis of robustness in the previous chapter, we omited discussion of structured uncertainties using the structured singular values (SSU, mu, \\mu). Similarly here we did not delve into the extension of that framework towards control synthesis. Purely because of time constraints. But should you find some time, have a look at section 8.12, which discusses the methodology called \\mu synthesis.\n\n\n\n\n Back to topReferences\n\n[1] S. Skogestad and I. Postlethwaite, Multivariable Feedback Control: Analysis and Design, 2nd ed. Wiley, 2005. Available: https://folk.ntnu.no/skoge/book/",
    "crumbs": [
      "12. Robust control",
      "References"
    ]
  },
  {
    "objectID": "discr_dir_mpc_explicit.html",
    "href": "discr_dir_mpc_explicit.html",
    "title": "Explicit MPC",
    "section": "",
    "text": "Model predictive control (MPC) is not computationally cheap (compared to, say, PID or LQG control) as it requires solving optimization problem – typically a quadratic program (QP) - online. The optimization solver needs to be a part of the controller.\nThere is an alternative, though, at least in some cases. It is called explicit MPC. The computationally heavy optimization is only perfomed only during the design process and the MPC controller is then implemented just as an affine state feedback\n\\boxed{\n\\bm u_k(\\bm x_k) = \\mathbf F^i \\bm x_k + \\mathbf g^i,\\; \\text{if}\\; \\bm x_k\\in \\mathcal R^i,}\n where \\mathcal R^i, \\; i=1, 2, \\ldots, p are polyhedral regions, into which the state space \\mathbb R^n is partitioned, and \\mathbf F^i and \\mathbf g^i are the coefficient matrices and vectors that parameterize the affine state feedback controller in that region.\nThe regions \\mathcal R^i and the corresponding coefficients \\mathbf F^i and \\mathbf g^i are determined during the offline design process. Which set of coefficients is chosen is determined online (in real time) based on which region \\mathcal R^i the state \\bm x_k is located in.",
    "crumbs": [
      "6. More on MPC",
      "Explicit MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_explicit.html#multiparametric-programming",
    "href": "discr_dir_mpc_explicit.html#multiparametric-programming",
    "title": "Explicit MPC",
    "section": "Multiparametric programming",
    "text": "Multiparametric programming\nThe key technique for explicit MPC is multi-parametric programming. Consider the following optimization problem\n\nJ^\\star(\\bm x) = \\inf_{\\bm z\\in \\mathbb R^m} J(\\bm z;\\bm x),\n where \\bm z\\in \\mathbb R^m is an optimization (vector) variable, while \\bm x\\in \\mathbb R^n is a vector of parameters (it is a common notational convention to separate variables and parameters). For a given parameter \\bm x, the cost function J is to be minimized. We now want to study how the optimal cost J^\\star depends on the parameter. For a scalar parameter, this task is called parametric programming, for vector parameters, the name of the problem changes to multiparametric programming.\n\nExample 1 (Parametric programming) Consider the following cost function J(z;x) in z\\in\\mathbb R, parameterized by x\\in \\mathbb R. The optimization variable z is subject to an inequality constraint, and this constraint is also parameterized by x. \n\\begin{aligned}\nJ(z;x) &= \\frac{1}{2} z^2 + 2zx + 2x^2 \\\\\n\\text{subject to} &\\quad  z \\leq 1 + x.\n\\end{aligned}\n\nIn this simple case we can aim at analytical solution. We proceed in the standard way – we introduce a Lagrange multiplicator \\lambda and form the augmented cost function \nL(z,\\lambda; x) = \\frac{1}{2} z^2 + 2zx + 2x^2 + \\lambda (z-1-x).\n\nThe necessary conditions of optimality for the inequality-constrained problem come in the form of KKT conditions \n\\begin{aligned}\nz + 2x + \\lambda &= 0,\\\\\nz - 1 - x &\\leq  0,\\\\\n\\lambda & \\geq 0,\\\\\n\\lambda (z - 1 - x) &= 0.\n\\end{aligned}\n\nThe last condition – the complementarity condition – gives rise to two scenarios: one corresponding to \\lambda = 0, and the other corresponding to z - 1 - x = 0. We consider them separately below.\nAfter substituting \\lambda = 0 into the KKT conditions, we get \n\\begin{aligned}\nz + 2x &= 0,\\\\\nz - 1 - x & \\leq  0.\n\\end{aligned}\n\nFrom the first equation we get how z depends on x, and from the second we obtain a bound on x. Finally, we can also substitute the expression for z into the cost function J to get the optimal cost J^\\star as a function of x. All these are summarized here \n\\begin{aligned}\nz &= -2x,\\\\\nx & \\geq -\\frac{1}{3},\\\\\nJ^\\star(x) &= 0.\n\\end{aligned}\n\nNow, the other scenario. Upon substituting z - 1 - x = 0 into the KKT conditions we get \n\\begin{aligned}\nz + 2x + \\lambda &= 0,\\\\\nz - 1 - x &=  0,\\\\\n\\lambda & \\geq 0.\n\\end{aligned}\n\nFrom the second equation we get the expression for z in terms of x, substituting into the first equation and invoking the condition on nonnegativity of \\lambda we get the bound on x (not suprisingly it complements the one obtained in the previous scenario). Finally, substituting for z in the cost function J we get a formula for the cost J^\\star as a function of x.\n\n\\begin{aligned}\nz &= 1 + x,\\\\\n\\lambda &= -z - 2x \\geq 0 \\quad \\implies \\quad x \\leq -\\frac{1}{3},\\\\\nJ^\\star(x) &= \\frac{9}{2}x^2 + 3x + \\frac{1}{2}.\n\\end{aligned}\n\nThe two scenarios can now be combined into a single piecewise affine function z^\\star(x) \nz^\\star(x) = \\begin{cases}\n1+x & \\text{if } x \\leq -\\frac{1}{3},\\\\\n-2x & \\text{if } x &gt; -\\frac{1}{3}.\n\\end{cases}\n\n\n\nShow the code\nx = range(-1, 1, length=100)\nz(x) = x &lt;= -1/3 ? 1 + x : -2x\nJstar(x) = x &lt;= -1/3 ? 9/2*x^2 + 3x + 1/2 : 0\n\nusing Plots\nplot(x, z.(x), label=\"\",lw=2)\nvline!([-1/3],line=:dash, label=\"\")\nxlabel!(\"x\")\nylabel!(\"z⋆(x)\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Piecewise affine dependence of the minimizer on the parameter\n\n\n\n\nand a piecewise quadratic cost function J^\\star(x) \nJ^\\star(x) = \\begin{cases}\n\\frac{9}{2}x^2 + 3x + \\frac{1}{2} & \\text{if } x \\leq -\\frac{1}{3},\\\\\n0 & \\text{if } x &gt; -\\frac{1}{3}.\n\\end{cases}\n\n\n\nShow the code\nplot(x, Jstar.(x), label=\"\",lw=2)\nvline!([-1/3],line=:dash, label=\"\")\nxlabel!(\"x\")\nylabel!(\"J⋆(x)\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Piecewise quadratic dependence of the optimal cost on the parameter\n\n\n\n\n\n\nExample 2 (Multiparametric programming) #TODO",
    "crumbs": [
      "6. More on MPC",
      "Explicit MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_explicit.html#explicit-mpc",
    "href": "discr_dir_mpc_explicit.html#explicit-mpc",
    "title": "Explicit MPC",
    "section": "Explicit MPC",
    "text": "Explicit MPC\nAs we have discussed a few times, the cost of a finite-horizon optimal control problem is a function of the control trajectory \\bm u_0, \\bm u_1, \\ldots, \\bm u_{N-1}, while the initial state \\bm x_0 is a parameter (not subject to optimization)\n\nJ\\left(\\begin{bmatrix}\\bm u_0\\\\ \\bm u_1\\\\ \\vdots \\\\ \\bm u_{N-1}\\end{bmatrix}; \\bm x_0\\right) = \\phi(\\bm x_N) + \\sum_{k=0}^{N-1} L(\\bm x_k, \\bm u_k).\n\nYou may now perhaps appreciate our choice of notation in the previous paragraphs in that we used \\bm x as the parameter. It fits here perfectly. The state at the beginning of the time horizon over which the optimal control is computed is a parameter.\nConsidering that the cost function is quadratic, and parameterized by a vector, we can apply the multiparametric programming techniquest to find the optimal control law as a function of the state (here playing the role of a vector parameter).\n#TODO\n[1], [2], [3], [4]",
    "crumbs": [
      "6. More on MPC",
      "Explicit MPC"
    ]
  },
  {
    "objectID": "opt_algo_solvers.html",
    "href": "opt_algo_solvers.html",
    "title": "Numerical solvers",
    "section": "",
    "text": "The number of numerical solvers is huge. First, we give a short biased list of solvers which we may use within this course.\n\nOptimization Toolbox for Matlab: fmincon, fminunc, linprog, quadpro, … Available within the all-university Matlab license for all students and employees at CTU.\nGurobi Optimizer: LP, QP, SOCP, MIP, commercial (but free academic license available).\nIBM ILOG CPLEX: LP, QP, SOCP, MIP, commercial (but free academic license available).\nMOSEK: LP, QP, MIP, SOCP, SDP, commercial (but free academic license available).\nHIGHS: LP, QP, MIP, open source.\nKnitro: NLP, commercial.\nIpopt: NLP, open source.\nSEDUMI: SOCP, SDP, open source.\n…\n\nSecond, for a reasonably comprehensive and well maintained list of solvers, consult the NEOS Guide to Optimization web page (in particular the link at the bottom of that page). Similar list is maintained within Hans Mittelman’s Decision Tree for Optimization Software web page.\nWorking in Matlab and using Yalmip for defining and solving optimization problems, the list of optimization solvers supported by Yalmip shows what is available.\nSimilarly, users of Julia and JuMP will find the list of solvers supported by JuMP useful. The list is worth consulting even if Julia is not the tool of choice, as many solvers are indepdenent of Julia.\n\n\n\n Back to top",
    "crumbs": [
      "2. Optimization – algorithms",
      "Numerical solvers"
    ]
  },
  {
    "objectID": "discr_dir_mpc_recursive_feasibility.html",
    "href": "discr_dir_mpc_recursive_feasibility.html",
    "title": "Recursive feasibility",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "Recursive feasibility"
    ]
  },
  {
    "objectID": "rocond_mixed_sensitivity.html",
    "href": "rocond_mixed_sensitivity.html",
    "title": "Mixed sensitivity design",
    "section": "",
    "text": "In the previous chapter we mentioned that there are several ways to capture uncertainty in the model and analyze robustness with respect to the uncertainty. We have chosen the worst-case approach based on the small gain theorem for analysis of robust stability, which in the case of linear systems has an intuitive frequency-domain interpretation.\nThis chosen framework has two benefits. First, having being formulated in frequency domain, it offers us to take advantage of the insight developed in introductory courses on automatic control, that typically invest quite some effort into developing frequency doman concepts such as magnitude and phase Bode plots, Nyquist plot, and sensitivity and complementary sensitivity functions. Generations of control engineers have contributed to the collective know-how carried by these classical concepts and techniques.\nSecond, by formulating the requirements on robust stability, nominal stability and robust performance as constraints on \\mathcal H_\\infty norms of some closed-loop systems, an immediate extension from analysis to automated synthesis (control design) is enabled by availability of numerical methods for \\mathcal H_\\infty optimization. This enhances the classical frequency-domain control design techniques in that while the classical methods require that we know what we want and we also know how to achieve it, the \\mathcal H_\\infty optimization based methods require that we only know what we want (and express our requirements in frequency domain). We don’t have to bother with how to achieve it because there are numerical solvers that will do the job for us.\nLet’s introduce the first instance of such methodology. We have learnt that the robust performance condition in presence of multiplicative uncertainty is formulated as a bound on the \\mathcal H_\\infty norm of the mixed sensitivity function \\begin{bmatrix}W_pS\\\\WT\\end{bmatrix}, namely \n\\left\\|\n\\begin{bmatrix}\nW_pS\\\\WT\n\\end{bmatrix}\n\\right\\|_{\\infty}\n&lt; \\frac{1}{\\sqrt{2}}.\n\nEvaluating this condition can be done in a straightforward way, either at a grid of frequencies (inefficient) or by invoking a method for computing the norm.\nBut the major god news of this chapter is that we can also turn this into an optimization problem \n\\operatorname*{minimize}_{K \\text{ stabilizing}}\n\\left\\|\n\\begin{bmatrix}\nW_pS\\\\WT\n\\end{bmatrix}\n\\right\\|_{\\infty}.\n\nIn words, we are looking for a controller K that guaranees stability of the closed-loop system and it also minimizes the \\mathcal H_\\infty norm of the mixed sensitivity function.\nSuch optimization solvers are indeed available.\n\n\n\n\n\n\nMixed sensitivity minimization as a special case of the general \\mathcal H_\\infty optimization\n\n\n\nIn anticipation of what is to come, we note here that the above minimization of the \\mathcal H_\\infty norm of the mixed sensitivity function is a special case of the more general \\mathcal H_\\infty optimization problem (minimization of the norm of a general closed-loop transfer function). Therefore, even if your software tools does not have a specific function for mixed sensitivity optimization, chances are that a solver for the general \\mathcal H_\\infty optimization function is available. And we will soon see how to reformulate the mixed sensitivity minimization as the general \\mathcal H_\\infty optimization problem.\n\n\nHaving derived the bound on the norm of the mixed sensitivity function (equal to 1/\\sqrt{2} in the SISO case), it may now be tempting to conclude that the only goal of the optimization is to find a controller that satisfies this bound. However, it turns out that the optimization has another useful property – it is called self-equalizing property. We are not going to prove it, we will be happy just to interpret it: it means that with the optimal controller the frequency response of the considered (weighted and possibly mixed) sensitivity function is flat (constant over all frequencies).\nIn order to understand the impact of this property, let us consider the problem of minimizing just \\|WT\\|_\\infty. We choose this problem even though practically it is not really useful to require just (robust) stability. For \\gamma = \\min_{K}\\|WT\\|_\\infty, the flatness of the frequency response |W(j\\omega)T(j\\omega)| means that the magnitude frequency response |T(j\\omega)| is proportional to 1/|W(j\\omega)|, that is,\n\n|T(j\\omega)| = \\frac{\\gamma}{|W(j\\omega)|},\\qquad \\gamma \\in \\mathbb R, \\gamma &gt; 0.\n\nThis gives another motivation for our \\mathcal{H}_\\infty optimization endeavor – through minimization we shape the closed-loop magnitude frequency responses. This automatic/automated loopshaping is the second benefit promised at the beginning of this section. But we emphasize that for practical pursposes it is only useful to minimize the norm of the mixed sensitivity function, in which case more than just simultaneous shaping of W_\\mathrm{p}S and WT must be achieved.\nWith this new interpretation, we can feel free to include other terms in the optimization criterion. In particular, the criterion can be extended to include the control effort as in (after reindexing the weighting filters) \\boxed\n{\\operatorname*{minimize}_{K \\text{ stabilizing}}  \n\\left\\|\n\\begin{bmatrix}\nW_1S\\\\W_2KS\\\\W_3T\n\\end{bmatrix}\n\\right\\|_{\\infty}.}\n\nThe middle term penalizes control (similarly as R term in LQ optimality criterion \\int(x^TQx+u^TRu)dt). Typically it is sufficient to set it equal to a nonnegative constant.\nAn important property of this method is that it extends to the multiple-input-multiple-output (MIMO) case. Nothing needs to be changes in the formal problem statement as the \\mathcal H_\\infty norm is defined for MIMO systems as well.\n\n\n\n Back to top",
    "crumbs": [
      "12. Robust control",
      "Mixed sensitivity design"
    ]
  },
  {
    "objectID": "cont_indir_via_calculus_of_variations.html",
    "href": "cont_indir_via_calculus_of_variations.html",
    "title": "Indirect approach to optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Indirect approach to optimal control"
    ]
  },
  {
    "objectID": "rocond_software.html",
    "href": "rocond_software.html",
    "title": "Software",
    "section": "",
    "text": "mixsyn: control design by minimnizing the \\mathcal{H}_\\infty norm the the mixed-sensitivity function.\nhinfsyn: control design by minimizing the \\mathcal{H}_\\infty norm of a closed-loop transfer function formulated using an LFT.\nncfsyn: another control design based on \\mathcal{H}_\\infty optimization, but this one considers a different uncertainty model not covered in our course. Although this uncertainty model does not have as intuitive an interpretation as the multiplicative uncertainty model use in mixed sensitivity synthesis, it captures a broad class of uncertainties. Furtheremore, the resulting controller enjoys the same decomposition into a state feedback and an observer as the popular LQG controller, which can be an advantage from an implementation viewpoint. Highly recommended method.\nmusyn: similar general setup as the hinfsyn method, but it considers a structure in the \\Delta term. It is regarded by some as the culmination of the \\mathcal{H}_\\infty control design methods. The disadvantage is that it is the most computationaly intensive of the methods we covered, and the resulting controller is typically of rather high order.",
    "crumbs": [
      "12. Robust control",
      "Software"
    ]
  },
  {
    "objectID": "rocond_software.html#matlab",
    "href": "rocond_software.html#matlab",
    "title": "Software",
    "section": "",
    "text": "mixsyn: control design by minimnizing the \\mathcal{H}_\\infty norm the the mixed-sensitivity function.\nhinfsyn: control design by minimizing the \\mathcal{H}_\\infty norm of a closed-loop transfer function formulated using an LFT.\nncfsyn: another control design based on \\mathcal{H}_\\infty optimization, but this one considers a different uncertainty model not covered in our course. Although this uncertainty model does not have as intuitive an interpretation as the multiplicative uncertainty model use in mixed sensitivity synthesis, it captures a broad class of uncertainties. Furtheremore, the resulting controller enjoys the same decomposition into a state feedback and an observer as the popular LQG controller, which can be an advantage from an implementation viewpoint. Highly recommended method.\nmusyn: similar general setup as the hinfsyn method, but it considers a structure in the \\Delta term. It is regarded by some as the culmination of the \\mathcal{H}_\\infty control design methods. The disadvantage is that it is the most computationaly intensive of the methods we covered, and the resulting controller is typically of rather high order.",
    "crumbs": [
      "12. Robust control",
      "Software"
    ]
  },
  {
    "objectID": "rocond_software.html#julia",
    "href": "rocond_software.html#julia",
    "title": "Software",
    "section": "Julia",
    "text": "Julia\n\nRobustAndOptimalControl.jl",
    "crumbs": [
      "12. Robust control",
      "Software"
    ]
  },
  {
    "objectID": "rocond_software.html#python",
    "href": "rocond_software.html#python",
    "title": "Software",
    "section": "Python",
    "text": "Python\n\nPython Control Systems Library",
    "crumbs": [
      "12. Robust control",
      "Software"
    ]
  },
  {
    "objectID": "intro_software.html",
    "href": "intro_software.html",
    "title": "Software for the course",
    "section": "",
    "text": "Although our course is heavily based on mathematical methods, it is a course designed for (control) engineers, which is to say that our main shared goal is to learn to solve engineering problems by using the mathematical methods. And except for the simplest textbook problems, the needed methods are numerical, which inevitably means that our course must have a strong software component.\nOn the other hand, our graduate course is not just a vocational training in using a dedicated software tool. Instead, we are going to implement the methods introduced in our course by ourselves, at least to some degree, and even if at a prototype level. We believe that this is the best way to learn the potentials and limitations of the methods, even if ultimately – when solving real industrial problems – we may want (or be forced) to use use software tools developed by specialists.",
    "crumbs": [
      "0. Introduction",
      "Software for the course"
    ]
  },
  {
    "objectID": "intro_software.html#julia",
    "href": "intro_software.html#julia",
    "title": "Software for the course",
    "section": "Julia",
    "text": "Julia\nIn our course we are going to use the Julia programming language for implementation of the methods and numerical experimentation. The language and its compiler and standard libraries are free (and open source), ane a rich ecosystem of packages is available. Julia is a modern language, similarly high-level as Matlab or Python, but with a performance comparable to C or Fortran.\nJulia is going to be used not only by the course lecturer(s) when demonstrating the methods in the lectures, these online lecture notes and exercises, but also by the students when programming the solutions to assigned homework problems. In order to reduce the load on students, incomplete code snippets will be provided, which students will have to complete only in the core algorithmic sections.\nBelow we give some basic information about Julia, including links to learning resources.\n\nInstall Julia\nInstructions for all platforms are at https://docs.julialang.org/en/v1/manual/installation/.\n\n\nInstall VS Code\nThe most recommendable IDE is VS Code https://code.visualstudio.com. Julia Extension https://code.visualstudio.com/docs/languages/julia can be installed within VS Code.\n\n\nDocumentation\nThe official documentation is available at https://docs.julialang.org/en/v1/. It can also be downloaded as a PDF file. Programmers fluent with Matlab or Python will find this overview of differences https://docs.julialang.org/en/v1/manual/noteworthy-differences/#Noteworthy-Differences-from-other-Languages useful.\n\n\nDiscussion forum\nAn active discussion forum exists https://discourse.julialang.org. But you may perhaps start by asking questions on our own course forum within the Teams platform.",
    "crumbs": [
      "0. Introduction",
      "Software for the course"
    ]
  },
  {
    "objectID": "intro_software.html#matlab-and-simulink",
    "href": "intro_software.html#matlab-and-simulink",
    "title": "Software for the course",
    "section": "Matlab and Simulink",
    "text": "Matlab and Simulink\nAs a course on computational methods for control design, we certainly cannot ignore the existence of Matlab and Simulink due their wide adoption both in academia and industry. In particular, when it comes to Simulink and its code generation capabilities, there is hardly any alternative. Therefore we will list in the respective sections of these lecture notes the appropropriate Matlab and Simulink functions and tools. After all, those students who decide to work on a semestral project in our automatic control lab will have to use Matlab and Simulink.",
    "crumbs": [
      "0. Introduction",
      "Software for the course"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html",
    "href": "opt_theory_reformulations.html",
    "title": "Problem reformulations",
    "section": "",
    "text": "There is bag of trick that can be used to reformulate an optimization problem into an equivalent a form that is more suitable for a theoretical analysis or a particular numerical solver. Here we only pick a few.",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#maximization-into-minimization",
    "href": "opt_theory_reformulations.html#maximization-into-minimization",
    "title": "Problem reformulations",
    "section": "Maximization into minimization",
    "text": "Maximization into minimization\nGiven a function f(\\bm x), we can maximize it by minimizing -f(\\bm x).",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#equality-into-inequality-constraints",
    "href": "opt_theory_reformulations.html#equality-into-inequality-constraints",
    "title": "Problem reformulations",
    "section": "Equality into inequality constraints",
    "text": "Equality into inequality constraints\nAs a matter of fact, we can declare an NLP problem only inequality constraints as the most general one. This is because we can always transform an equality constraint into two inequality constraints. Given an equality constraint h(\\bm x) = 0, we can write it as h(\\bm x) \\leq 0 and -h(\\bm x) \\leq 0, that is,\n\n\\underbrace{\\begin{bmatrix}\nh(\\bm x) \\\\\n-h(\\bm x)\n\\end{bmatrix}}_{\\mathbf g(\\bm x)} \\leq \\mathbf 0.\n\nOn the other hand, it is typically useful to keep the equality constraints explicit in the problem formulation for the benefit of theoretical analysis, numerical methods and convenience of the user/modeller.",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#inequality-into-sort-of-equality-constraints",
    "href": "opt_theory_reformulations.html#inequality-into-sort-of-equality-constraints",
    "title": "Problem reformulations",
    "section": "Inequality into “sort-of” equality constraints",
    "text": "Inequality into “sort-of” equality constraints\nConsider the inequality constraint g(\\bm x) \\leq 0. By introducing a slack variable s and imposing the nonnegativity condition, we can turn the inequality into the equality g(\\bm x) + s = 0. Well, we have not completely discarded an inequality because now we have s \\geq 0. But this new problem may be better suited for some theoretical analysis or numerical methods.\nIt is also possible to express the nonnegativity constraint implicitly by considering an unrestricted variable s and using it within the inequality through its square s^2:\n\ng(\\bm x) + s^2 = 0.",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#linear-cost-function-always-possible",
    "href": "opt_theory_reformulations.html#linear-cost-function-always-possible",
    "title": "Problem reformulations",
    "section": "Linear cost function always possible",
    "text": "Linear cost function always possible\nGiven a cost function f(\\bm x) to be minimized, we can always upper-bound it by a new variable \\gamma accompanied by a new constraint f(\\bm x) \\leq \\gamma and then minimize just \\gamma \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm{x}\\in\\mathbb R^n, \\gamma\\in\\mathbb R} & \\quad \\gamma \\\\\n\\text{subject to} & \\quad f(\\bm x) \\leq \\gamma.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#absolute-value",
    "href": "opt_theory_reformulations.html#absolute-value",
    "title": "Problem reformulations",
    "section": "Absolute value",
    "text": "Absolute value\nConsider an optimization problem in which the cost function contains the absolute value of a variable \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\sum_i c_i|x_i|\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x \\geq \\mathbf b.\n\\end{aligned}\n\nWe also impose the restriction that all the coefficients c_i are nonnegative. The cost function is then a sum of piecewise linear convex function, which can be shown to be convex.\nThe trouble with the absolute value function is that it is not linear, it is not even smooth. And yet, as we will see below, this optimization with the absolute value can be reformulated as a linear program.\nOne possible reformulation introduces two new nonnegative (vector) variables \\bm x^+\\geq 0 and \\bm x^-\\geq 0, with which the original variables can be expressed as x_i = x_i^+ - x_i^-, \\; i=1, \\ldots, n. The cost function can then be written as \\sum c_i|x_i| = \\sum_i c_i (x_i^+ + x_i^-).\nThis may look surprising (and incorrect) at first, but we argue that at an optimum, x_i^+ or x_i^- must be zero. Otherwise we could subtract (in case c_i&gt;0) the same amount from/to both, which would not change the satisfaction of the constraints (this modification cancels in x_i = x_i^+ - x_i^-), and the cost would be further reduced.\nThe LP in the standard form then changes to\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x^+\\in \\mathbb R^n, \\bm x^-\\in \\mathbb R^n} &\\quad \\mathbf c^\\top (\\bm x^+ + \\bm x^-)\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x^+ - \\mathbf A \\bm x^- \\geq \\mathbf b,\\\\\n&\\quad \\bm x^+ \\geq \\mathbf 0,\\\\\n&\\quad \\bm x^- \\geq \\mathbf 0.\n\\end{aligned}\n\nAnother possibility is to exploit the reformulation of z_i = |x_i| as x_i\\leq z and -x_i\\leq z. The original problem then transforms into\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm z\\in \\mathbb R^n, \\bm x\\in \\mathbb R^n} &\\quad \\mathbf c^\\top \\bm z\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x \\geq \\mathbf b,\\\\\n&\\qquad \\bm x \\leq \\bm z,\\\\\n&\\quad -\\bm x \\leq \\bm z.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#piecewise-linear",
    "href": "opt_theory_reformulations.html#piecewise-linear",
    "title": "Problem reformulations",
    "section": "Piecewise linear",
    "text": "Piecewise linear\n#TODO",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#quadratic",
    "href": "opt_theory_reformulations.html#quadratic",
    "title": "Problem reformulations",
    "section": "Quadratic",
    "text": "Quadratic\n#TODO",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "reduction_order_benchmarks.html",
    "href": "reduction_order_benchmarks.html",
    "title": "Benchmarks",
    "section": "",
    "text": "Model-Order-Reduction (MOR) Wiki\nSLICOT Benchmark Examples for Model Reduction\n\n\n\n\n Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Benchmarks"
    ]
  },
  {
    "objectID": "dynamic_programming_DDP.html",
    "href": "dynamic_programming_DDP.html",
    "title": "Differential dynamic programming (DDP)",
    "section": "",
    "text": "We consider the standard discrete-time optimal control problem:\n\nWe are given a dynamical system modelled by \\bm x_{k+1} = \\mathbf f(\\bm x_k, \\bm u_k). For simplicity of notation we only consider a time invariant system, extension to the time-varying is straightforward.\nDue to time invariance we consider the time k=0 as the initial time. Specification of the initial state is thus \\bm x_0 = \\mathbf x_\\mathrm{init}.\nOn the time horizon [0,N] we now search for the optimal control trajectory (\\bm u_k)_{k=0}^{N-1} such that the cost function J_0\\left(\\bm x_0, (\\bm u_k)_{k=0}^{N-1}\\right) = \\phi(\\bm x_N) + \\sum_{k=0}^{N-1} L(\\bm x_k, \\bm u_k) is minimized.\nConstraints on the state at the final time can be added later, we do not consider them in this initial study.\n\nWe are already aware of the curse of dimensionality of the standard dynamic programming as it calls for evaluating the optimal cost (to go) J_k^\\star(\\bm x_k) at a grid of the state space and the time interval. Differential dynamic programming (DDP) is a way to relieve the computational burden by restricting the considered set of states to a neighborhood of some given state and control trajectories. It is only in this neighbourhood that an improving trajectories are search for. The key assumption for the method is that the optimal cost can be approximated by a quadratic function, for which such minimization is trivial.\n\n\n\n\n\n\nSimilarity to iterative algorithms for optimization such as Newton’s method\n\n\n\nWhen searching for the \\min_{\\bm x\\in\\mathbb R^n} f(\\bm x), Newton’s method starts with an initial guess of the solution \\bm x^0, assumes that in its neighbourhood the cost function f(\\bm x_0 + \\bm d) can be approximated by a quadratic function q(\\bm d) = f(\\bm x_0) + \\left(\\nabla f(\\bm x_0)\\right)^\\top \\bm d + \\frac{1}{2}\\bm d^\\top \\nabla^2 f(\\bm x_0) \\bm d, minimizes this quadratic function, which leads to the update of the guess \\bm x^1 = x^0 + \\underbrace{\\arg \\min_{\\bm d\\in\\mathbb R^n} q(\\bm d)}_{\\bm d_0}, that is, \\bm x^1 = \\bm x^0 + \\bm d^0, and repeats.\n\n\nWe start with some initial (guess of the optimal) control trajectory (\\bm u_0^0, \\bm u_1^0, \\ldots, \\bm u_{N-1}^0) \\eqqcolon (\\bm u_k^0)_{k=0}^{N-1}.\nThe system (initially at the fixed \\bm x_0) responds to this control trajectory with the state trajectory (\\bm x_1^0, \\bm x_2^0, \\ldots, \\bm x_{N}^0) \\eqqcolon (\\bm x_k^0)_{k=1}^{N}.\nCorrespondingly, at time k\\in[0,N-1] the cost to go from the state x_k^0 is \nJ_k^0(\\bm x_k^0) \\coloneqq J_k(\\bm x_k^0, (\\bm u_i^0)_{i=k}^{N-1}).\n\nRecall that if the optimal control trajectory (\\bm u_0^\\star, \\bm u_1^\\star, \\ldots, \\bm u_{N-1}^\\star) \\eqqcolon (\\bm u_k^\\star)_{k=0}^{N-1} is applied, the optimal state trajectory (\\bm x_1^\\star, \\bm x_2^\\star, \\ldots, \\bm x_{N}^\\star) \\eqqcolon (\\bm x_k^\\star)_{k=1}^{N} is a consequence. Furthermove, at every time k the cost to go is \nJ_k^\\star(\\bm x_k^\\star) \\eqqcolon J_k(\\bm x_k^\\star, (\\bm u_i^\\star)_{i=k}^{N-1}),\n and it is optimal, of course, by the very principle of dynamic programming.\n\n\n\n\n\n\nUpper index denotes the control used from the given state and time on\n\n\n\nIn the following we will heavily use the upper right index. For example, when writing J_k^\\heartsuit(\\bm x_k), we mean that the control sequence (\\bm u_k^\\heartsuit, \\bm u_{k+1}^\\heartsuit, \\ldots, \\bm u_{N-1}^\\heartsuit) is applied when the system is at state \\bm x_k at time k, that is\nJ_k^\\heartsuit(\\bm x_k) \\coloneqq J_k(\\bm x_k, (\\bm u_k^\\heartsuit, \\bm u_{k+1}^\\heartsuit, \\ldots, \\bm u_{N-1}^\\heartsuit)).\n\n\nWith the nominal control trajectory is applied, it obviously holds that \nJ_k^0(\\bm x_k^0) = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0).\n\\tag{1}\nAssume now a tiny perturbation \\delta \\bm x_k^0 from the state \\bm x_k^0 and \\delta \\bm u_k^0 from the control \\bm u_k^0 at the time k. The motivation for introduction of such perturbation is the same as we had in Newton’s method – trying to decrease the cost function a bit.\nThe previous equation also holds for the perturbed state and control, but note that since the control sequence is different, the optimal cost to go is different too. \nJ_k^1(\\underbrace{\\bm x_k^0+\\delta \\bm x_k^0}_{\\bm x_k^1}) = L(\\bm x_k^0+\\delta \\bm x_k^0,\\underbrace{\\bm u_k^0+\\delta \\bm u_k^0}_{\\bm u_k^1}) + J_{k+1}^1(\\underbrace{\\bm x_{k+1}^0+\\delta \\bm x_{k+1}^0}_{\\bm x_{k+1}^1}),\n\\tag{2} where \n\\delta \\bm x_{k+1}^0 = \\bm f(\\bm x_k^0+\\delta \\bm x_k^0,\\bm u_k^0+\\delta \\bm u_k^0) - \\bm x_{k+1}^0.\n\nWe now aim at approximating the cost to go by a quadratic function using the first three terms of Taylor’s series. We will need the first and second derivatives of the terms in the above equation. Let’s start with the first derivatives. When differentiating J_{k+1}^1(\\bm x_{k+1}^0) with respect to \\bm x_k^0, we need to invoke the chain rule as \\bm x_{k+1}^0 = \\bm f(\\bm x_k^0, \\bm u_k^0).\n\n\n\n\n\n\nChain rule for the first derivative of composed functions\n\n\n\nConsider the composed function h(\\bm x)\\coloneqq f(\\mathbf g(\\bm x)). Its first derivative is \n\\mathrm{D} h(\\bm x) = \\left.\\mathrm{D}g(\\bm y)\\right|_{\\bm y=\\mathbf f(\\bm x)}\\mathrm{D} \\mathbf f(\\bm x),\n where \\mathrm{D}g(\\bm y) is a row vector of derivatives, and \\mathrm{D} \\mathbf f(\\bm x) is a (Jacobian) matrix \n\\mathrm D\\mathbf f(\\bm x) =\n\\begin{bmatrix}\n\\frac{\\partial f_1(\\bm x)}{\\partial x_1} & \\frac{\\partial f_1(\\bm x)}{\\partial x_2} & \\ldots & \\frac{\\partial f_1(\\bm x)}{\\partial x_n}\\\\\n\\frac{\\partial f_2(\\bm x)}{\\partial x_1} & \\frac{\\partial f_2(\\bm x)}{\\partial x_2} & \\ldots & \\frac{\\partial f_2(\\bm x)}{\\partial x_n}\\\\\n\\vdots\\\\\n\\frac{\\partial f_m(\\bm x)}{\\partial x_1} & \\frac{\\partial f_m(\\bm x)}{\\partial x_2} & \\ldots & \\frac{\\partial f_m(\\bm x)}{\\partial x_n}\n\\end{bmatrix}.\n\nOr, if formatted as the gradient, which we prefer in our course, we can write \\boxed\n{\\nabla h(\\bm x) = \\nabla \\mathbf f(x) \\left.\\nabla g(\\bm y)\\right|_{\\bm y = \\mathbf f(\\bm x)},}\n where \n\\nabla \\mathbf f(x) = \\begin{bmatrix} \\nabla f_1(\\bm x) & \\nabla f_2(\\bm x) & \\ldots & \\nabla f_m(\\bm x)\\end{bmatrix}.\n\n\n\nApplying this to obtain derivatives of J_{k+1}^1\\left(\\bm f(\\bm x_k^0, \\bm u_k^0)\\right) with respect to \\bm x_k gives \n\\mathrm{D}_{\\bm x_{k}}J_{k+1}^1\\left(\\bm f(\\bm x_k^0, \\bm u_k^0)\\right) = \\left.\\mathrm{D}_{\\bm x_{k+1}}J_{k+1}^1\\left(\\bm x_{k+1}^0\\right)\\right|_{\\bm x_{k+1}^0 = \\mathbf f(\\bm x_k^0,\\bm u_k^0)} \\mathrm{D}_{\\bm x_k} \\bm f(\\bm x_k^0, \\bm u_k^0),\n or, using gradients \\boxed\n{\\nabla_{\\bm x_{k}}J_{k+1}^1\\left(\\bm f(\\bm x_k^0, \\bm u_k^0)\\right) = \\nabla_{\\bm x_k} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\left.\\nabla_{\\bm x_{k+1}}J_{k+1}^1\\left(\\bm x_{k+1}^0\\right)\\right|_{\\bm x_{k+1}^0 = \\mathbf f(\\bm x_k^0,\\bm u_k^0)}.}\n\nAnd similarly for the derivatives with respect to \\bm u_k.\n\n\n\n\n\n\nChain rule for the second derivative of composed functions\n\n\n\nWe consider (again) the composed function h(\\bm x)\\coloneqq g(\\mathbf f(\\bm x)). Referring to the previous box with the expressions for the first derivative, the second derivative must obviously invoke the rule for differentiation of a product. The Hessian is \n  \\mathrm H h(\\bm x) = \\left[\\mathrm{D}\\mathbf f(\\bm x)\\right]^\\top \\left.\\mathrm H g(\\bm y)\\right|_{\\bm y=\\mathbf f(\\bm x)}\\mathrm{D}\\mathbf f(\\bm x) + \\sum_{k=1}^m \\left.\\frac{\\partial g(\\bm y)}{\\partial y_k}\\right|_{\\bm y = \\mathbf f(\\bm x)} \\mathrm{H} f_k(\\bm x).\n\nThis is based on Magnus and Neudecker (2019), Theorem 6.8 on page 122. It is worth emphasizing that in the second term we form the Hessians (that is, matrices) for each component f_i(\\bm x) of the vector function \\mathbf f(\\bm x). This way we avoid the need for anything like a three-dimensional (tensor) version of Hessians and we can stick to matrices and vectors.\nSince in our course we prefer working with gradients, we present the result in the gradient form \\boxed\n{  \\nabla^2 h(\\bm x) = \\nabla \\mathbf f(\\bm x) \\left.\\nabla^2 g(\\bm y)\\right|_{\\bm y=\\mathbf f(\\bm x)}\\left[\\nabla\\mathbf f(\\bm x)\\right]^\\top + \\sum_{k=1}^m \\left.\\frac{\\partial g(\\bm y)}{\\partial y_k}\\right|_{\\bm y = \\mathbf f(\\bm x)} \\nabla^2 f_k(\\bm x).}\n\n\n\nApplying this to obtain the second derivatives of J_{k+1}^1\\left(\\bm f(\\bm x_k^0, \\bm u_k^0)\\right) with respect to \\bm x_k gives \n\\begin{aligned}\n\\nabla_{\\bm{xx}_k}^2 J_{k+1}^1\\left(\\bm f(\\bm x_k^0, \\bm u_k^0)\\right) &= \\nabla_{\\bm x_k} \\mathbf f(\\bm x_k^0, \\bm u_k^0) \\left.\\nabla_{\\bm{xx}_{k+1}}^2 J_{k+1}^1 (\\bm x_{k+1}^0) \\right|_{\\bm x_{k+1}^0 = \\mathbf f(\\bm x_k^0,\\bm u_k^0)}\\left[\\nabla_{\\bm x_k} \\mathbf f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top\\\\\n&+ \\sum_{i=1}^m \\left.\\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}}\\right|_{\\bm x_{k+1}^0 = \\mathbf f(\\bm x_k^0,\\bm u_k^0)} \\nabla_{\\bm{xx}_k}^2 f_i(\\bm x_k^0, \\bm u_k^0).\n\\end{aligned}\n\nWith this preparation, we can go for expanding the Equation 2. Strictly speaking, when keeping only the first three terms of the Taylor series on both sides, the quality no longer holds. But we will enforce it as a means of approximation.\n\n\\begin{aligned}\nJ_k^1(\\bm x_k^1) + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^1(\\bm x_{k+1}^0)\\\\\n+\\left[\\nabla_{\\bm x} L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right]^\\top \\,\\delta \\bm x_k^0 \\\\\n+\\left[\\nabla_{\\bm u} L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right]^\\top \\,\\delta \\bm u_k^0 \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\left[\\nabla_{\\bm x\\bm x}^2 L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{xx}}^2 f_i(\\bm x_k^0, \\bm u_k^0) \\right] \\,\\delta \\bm x_k^0\\\\\n+ \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\left[\\nabla_{\\bm u\\bm u}^2 L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{uu}}^2 f_i(\\bm x_k^0, \\bm u_k^0) \\right]  \\,\\delta \\bm u_k^0\\\\\n+ [\\delta \\bm x_k^0]^\\top \\left[\\nabla_{\\bm x\\bm u}^2 L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{xu}}^2 f_i(\\bm x_k^0, \\bm u_k^0) \\right] \\,\\delta \\bm u_k^0\n\\end{aligned}\n\nIn order to tame the complexity of the expressions a bit we introduce the auxiliary function – the discrete-time Hamiltonian \\boxed\n{H(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}) = L(\\bm x_k, \\bm u_k) + \\bm \\lambda_{k+1}^\\top \\bm f(\\bm x_k, \\bm u_k).}\n\nThe gradient of the Hamiltonian with respect to \\bm x_k is \\boxed\n{\\nabla_{\\bm x} H(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}) = \\nabla_{\\bm x} L(\\bm x_k, \\bm u_k) + \\nabla_{\\bm x} \\bm f(\\bm x_k, \\bm u_k) \\, \\bm \\lambda_{k+1}.}\n\nIf we evaluate \\bm \\lambda_{k+1} at \\nabla J_{k+1}^1(\\bm x_{k+1}^0), and the state and the control at \\bm x_k^0 and \\bm u_k^0, respectively, we get \n\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) = \\nabla_{\\bm x} L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla J_{k+1}^1(\\bm x_{k+1}^0),\n which can be used to simplify the second row in the above equation. Similarly, \n\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) = \\nabla_{\\bm u} L(\\bm x_k^0, \\bm u_k^0) + \\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0) \\, \\nabla J_{k+1}^1(\\bm x_{k+1}^0),\n which can be used to simplify the third row in the above equation.\nThe matrix of second derivatives – the Hessian – of the Hamiltonian is \\boxed\n{\\nabla_{\\bm x\\bm x}^2 H(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}) = \\nabla_{\\bm x\\bm x}^2 L(\\bm x_k, \\bm u_k) + \\sum_{i=1}^m \\lambda_{k+1,i} \\nabla_{\\bm{xx}}^2 f_i(\\bm x_k^0, \\bm u_k^0).}\n\nEvaluating \\bm \\lambda_{k+1} at \\nabla J_{k+1}^1(\\bm x_{k+1}^0) and the state and the control at \\bm x_k^0 and \\bm u_k^0, respectively, we get \n\\nabla_{\\bm x \\bm x}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) = \\nabla_{\\bm x \\bm x}^2 L(\\bm x_k^0, \\bm u_k^0) + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{xx}}^2 f_i(\\bm x_k^0, \\bm u_k^0),\n and similarly \n\\nabla_{\\bm u \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) = \\nabla_{\\bm u \\bm u}^2 L(\\bm x_k^0, \\bm u_k^0) + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{uu}}^2 f_i(\\bm x_k^0, \\bm u_k^0),\n and \n\\nabla_{\\bm x \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) = \\nabla_{\\bm x \\bm u}^2 L(\\bm x_k^0, \\bm u_k^0) + \\sum_{i=1}^m \\frac{\\partial J_{k+1}^1 (\\bm x_{k+1}^0)}{\\partial x_{k+1,i}} \\nabla_{\\bm{xu}}^2 f_i(\\bm x_k^0, \\bm u_k^0).\n\nAll these can be used to simplify the above set of equations to \\boxed\n{\\begin{aligned}\nJ_k^1(\\bm x_k^0) + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^1(\\bm x_{k+1}^0)\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm x_k^0 \\\\\n+\\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm u_k^0 \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\left[\\nabla_{\\bm x \\bm x}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right] \\,\\delta \\bm x_k^0\\\\\n+ \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\left[\\nabla_{\\bm u \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right]  \\,\\delta \\bm u_k^0\\\\\n+ [\\delta \\bm x_k^0]^\\top \\left[\\nabla_{\\bm x \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right] \\,\\delta \\bm u_k^0\n\\end{aligned}}\n\nOne more modification is needed. When \\delta \\bm x_k = 0, \\bm x_k^0 = \\bm x_k^1, and yet J_k^0(\\bm x_k^0) \\neq J_k^1(\\bm x_k^1) in general, because the two assume different control from the time k onward. We denote the offset as \na_k = J_k^1(\\bm x_k^0) - J_k^0(\\bm x_k^0).\n\nBut then \nJ_k^1(\\bm x_k^0) = J_k^0(\\bm x_k^0) + a_k,\n which we substitute into the above boxed multiline equation. And while we are at it, let’s also label the long terms with shorter labels: \n\\begin{aligned}\nJ_k^{\\color{red}0}(\\bm x_k^0) + {\\color{red}a_k} + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^{\\color{red}0}(\\bm x_{k+1}^0) + {\\color{red}a_{k+1}}\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm x_k^0 \\\\\n+\\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm u_k^0 \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\underbrace{\\left[\\nabla_{\\bm x \\bm x}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right]}_{\\mathbf A_k} \\,\\delta \\bm x_k^0\\\\\n+ \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\underbrace{\\left[\\nabla_{\\bm u \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right]}_{\\mathbf C_k}  \\,\\delta \\bm u_k^0\\\\\n+ [\\delta \\bm x_k^0]^\\top \\underbrace{\\left[\\nabla_{\\bm x \\bm u}^2 H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\nabla_{\\bm x} \\bm f(\\bm x_k^0, \\bm u_k^0) \\nabla^2 J_{k+1}^1(\\bm x_{k+1}^0) \\left[\\nabla_{\\bm u} \\bm f(\\bm x_k^0, \\bm u_k^0)\\right]^\\top \\right]}_{\\mathbf B_k} \\,\\delta \\bm u_k^0\n\\end{aligned}\n\nThe fact that we do not know a_k and a_{k+1} does not have to worry us at this moment. We will figure it out later.\nFor convenience, we rewrite the above equation in the shortened form with the new symbols \\boxed\n{\\begin{aligned}\nJ_k^{0}(\\bm x_k^0) + a_k + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0) + a_{k+1}\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm x_k^0 + \\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm u_k^0 \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\mathbf A_k \\,\\delta \\bm x_k^0 + [\\delta \\bm x_k^0]^\\top \\mathbf B_k \\,\\delta \\bm u_k^0 + \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\mathbf C_k  \\,\\delta \\bm u_k^0.\n\\end{aligned}}\n\\tag{3}\nWe now apply the principle of optimality (the principle of dynamic programming) to the approximation of Equation 2 \nJ_k^1(\\bm x_k^1) = \\min_{\\delta u_k^0} \\left[L(\\bm x_k^0+\\delta \\bm x_k^0,\\bm u_k^0+\\delta \\bm u_k^0) + J_{k+1}^1(\\bm x_{k+1}^1)\\right]\n by the just derived quadratic model \n\\begin{aligned}\nJ_k^{0}(\\bm x_k^0) + a_k + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = {\\color{red}\\min_{\\delta \\bm u_k^0}}\\left[L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0) + a_{k+1}\\right.\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm x_k^0 + \\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm u_k^0 \\\\\n\\left.+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\mathbf A_k \\,\\delta \\bm x_k^0 + [\\delta \\bm x_k^0]^\\top \\mathbf B_k \\,\\delta \\bm u_k^0 + \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\mathbf C_k  \\,\\delta \\bm u_k^0\\right].\n\\end{aligned}\n\nSimplifying the minimized term on the right hand side by dropping the terms that do not depend on \\delta \\bm u_k^0 gives \n\\delta \\bm u_k^0 = \\arg \\min_{\\delta \\bm u_k^0} \\left[\\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm u_k^0 + [\\delta \\bm x_k^0]^\\top \\mathbf B_k \\,\\delta \\bm u_k^0 + \\frac{1}{2}[\\delta \\bm u_k^0]^\\top \\mathbf C_k  \\,\\delta \\bm u_k^0. \\right]\n\n\n\n\n\n\n\nWarning\n\n\n\nThe notation is a bit awkward. So far \\bm u_k^0 was understood as some perturbation, but now on the left hand side we consider a particular one – the one that is optimal. Shall we add some other superscript to distinguish these? Perhaps something like \\delta \\bm u_k^{0\\star}? We prefer not to clutter the notation and just declare that \\delta \\bm u_k^0 is the optimal control perturbation.\n\n\n\nNo constraints on the control\nIf no constraints are imposed on the control, the optimal control perturbation can be found analytically by setting the gradient of the above expression to zero, that is\n\n\\nabla_{\\bm u_k}H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\mathbf B_k^\\top \\delta \\bm x_k^0 + \\mathbf C_k  \\,\\delta \\bm u_k^0 = 0,\n from which it follows that the optimizing control perturbation is \n\\delta \\bm u_k^0 = \\underbrace{-\\mathbf C_k^{-1} \\mathbf B_k^\\top}_{\\color{blue}\\mathbf K_k} \\delta \\bm x_k^0 \\underbrace{- \\mathbf C_k^{-1}\\nabla_{\\bm u_k}H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)}_{\\color{blue}\\mathbf k_k},\n where \n\\delta \\bm x_k^0 = \\bm x_k^1 - \\bm x_k^0.\n\nThe major conclusion now is that the perturbation control is generated in a feedback manner using an affine control law (or policy) as \\boxed\n{\\delta \\bm u_k^0 = {\\color{blue}\\mathbf K_k} \\delta \\bm x_k^0 + {\\color{blue}\\mathbf k_k}.}\n\\tag{4}\nThe full control sequence \\left(\\bm u_k^1\\right)_{k=0}^{N-1} is then\n\n\\bm u_k^1 = \\bm u_k^0 + \\delta \\bm u_k^0.\n\nNow we substitute the affine feedback control law given by Equation 4 into Equation 3 for the optimal cost-to-go approximation. We get \n\\begin{aligned}\nJ_k^{0}(\\bm x_k^0) + a_k + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0) + a_{k+1}\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,\\delta \\bm x_k^0 + \\left[\\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\,[\\mathbf K_k \\delta \\bm x_k^0 + \\mathbf k_k] \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\mathbf A_k \\,\\delta \\bm x_k^0 + [\\delta \\bm x_k^0]^\\top \\mathbf B_k \\,[\\mathbf K_k \\delta \\bm x_k^0 + \\mathbf k_k] + \\frac{1}{2}[\\mathbf K_k \\delta \\bm x_k^0 + \\mathbf k_k]^\\top \\mathbf C_k  \\,[\\mathbf K_k \\delta \\bm x_k^0 + \\mathbf k_k].\n\\end{aligned},\n\nwhich after combining the terms with the equal powers of \\delta \\bm x_k^0 transforms to \n\\begin{aligned}\nJ_k^{0}(\\bm x_k^0) + a_k + \\left[\\nabla J_k^1(\\bm x_k^0)\\right]^\\top \\delta \\bm x_k^0 + \\frac{1}{2} \\left[\\delta \\bm x_k^0\\right]^\\top \\nabla^2 J_k^1(\\bm x_k^0) \\delta \\bm x_k^0 \\\\\n= L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0) + a_{k+1} + \\left[H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\mathbf k_k + \\frac{1}{2} \\mathbf k_k^\\top \\mathbf C_k \\,\\mathbf k_k\\\\\n+\\left[\\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\mathbf K_k^\\top \\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\mathbf k_k^\\top\\mathbf B_k^\\top + \\mathbf K_k^\\top \\mathbf C_k \\mathbf k_k\\right]^\\top \\,\\delta \\bm x_k^0 \\\\\n+ \\frac{1}{2}[\\delta \\bm x_k^0]^\\top \\left[\\mathbf A_k + 2\\mathbf B_k \\,\\mathbf K_k + \\mathbf K_k^\\top \\mathbf C_k \\mathbf K_k\\right]\\,\\delta \\bm x_k^0.\n\\end{aligned}.\n\nSetting equal the terms corresponding to the same powers of \\delta \\bm x_k^0 on both sides, while also recalling Equation 1, which we restate here \nJ_k^{0}(\\bm x_k^0) = L(\\bm x_k^0,\\bm u_k^0) + J_{k+1}^0(\\bm x_{k+1}^0),\n gives the following equations \\boxed\n{\\begin{aligned}\na_k &= a_{k+1} + \\left[H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right)\\right]^\\top \\mathbf k_k + \\frac{1}{2} \\mathbf k_k^\\top \\mathbf C_k \\,\\mathbf k_k,\\\\\n\\nabla J_k^1(\\bm x_k^0) &= \\nabla_{\\bm x} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\mathbf K_k^\\top \\nabla_{\\bm u} H\\left(\\bm x_k^0, \\bm u_k^0, \\nabla J_{k+1}^1(\\bm x_{k+1}^0)\\right) + \\mathbf k_k^\\top\\mathbf B_k^\\top + \\mathbf K_k^\\top \\mathbf C_k \\mathbf k_k,\\\\\n\\nabla^2 J_k^1(\\bm x_k^0) &= \\mathbf A_k + \\mathbf B_k\\,\\mathbf K_k + \\mathbf K_k^\\top \\mathbf B_k^\\top + \\mathbf K_k^\\top \\mathbf C_k \\mathbf K_k,\n\\end{aligned}}\n where in the last line we symmetrized by 2\\mathbf B_k\\,\\mathbf K_k = \\mathbf B_k\\,\\mathbf K_k + \\mathbf K_k^\\top \\mathbf B_k^\\top.\nThese are three backwards running difference equations. They are initialized with the boundary conditions at the final time \\boxed\n{\\begin{aligned}\na_N &= 0,\\\\\n\\nabla J_N^1(\\bm x_N^0) &= \\nabla \\phi(\\bm x_N^0), \\\\\n\\nabla^2 J_N^1(\\bm x_N^0) &= \\nabla^2 \\phi(\\bm x_N^0).\n\\end{aligned}}\n\n\n\n\n\n\n Back to topReferences\n\nMagnus, Jan R., and Heinz Neudecker. 2019. Matrix Differential Calculus with Applications in Statistics and Econometrics. 3rd ed. Wiley Series in Probability and Statistics. Hoboken, NJ: Wiley. https://onlinelibrary.wiley.com/doi/book/10.1002/9781119541219.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Differential dynamic programming (DDP)"
    ]
  },
  {
    "objectID": "limitations_SISO.html",
    "href": "limitations_SISO.html",
    "title": "Limitations for SISO systems",
    "section": "",
    "text": "For a given system, there may be some inherent limitations of achievable performance. However hard we try to design/tune a feedback controller, certain closed-loop performance indicators such as bandwidth, steady-state accuracy, or resonant peaks may have inherent limits. We are going to explore these. The motivation is that once we know what is achievable, we do not have to waste time by trying to achieve the impossible.\nAt first it may look confusing that we are only formulating this problem of learning the limits towards the end of our course, since one view of the whole optimal control theory is that is that it provides a systematic methodology for learning what is possible to achieve. Shall we need to know the shortest possible time in which the drone can be brought from one position and orientation to another, we just formulate the minimum-time optimal control problem and solve it. Even if at the end of the day we intend to use a different controller – perhaps one supplied commercially with a fixed structure like a PID controller – at least we can assess the suboptimality of such controller by comparing its performance with the optimal one.\nIn this section we are going to restrict ourselves to SISO systems. Then in the next section we will extend the results to MIMO systems.\nS+T = 1",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#clarification-of-the-definition-of-bandwidth",
    "href": "limitations_SISO.html#clarification-of-the-definition-of-bandwidth",
    "title": "Limitations for SISO systems",
    "section": "Clarification of the definition of bandwidth",
    "text": "Clarification of the definition of bandwidth",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#interpolation-conditions-of-internal-stability",
    "href": "limitations_SISO.html#interpolation-conditions-of-internal-stability",
    "title": "Limitations for SISO systems",
    "section": "Interpolation conditions of internal stability",
    "text": "Interpolation conditions of internal stability\nConsider that the plant modelled by the transfer function G(s) has a zero in the right half-plane (RHP), that is,\n\nG(z) = 0, \\; z\\in \\text{RHP}.\n\nIt can be shown that the closed-loop transfer functions S(s) and t(s) satisfy the interpolation conditions \\boxed{\nS(z)=1,\\;\\;\\;T(z)=0\n}\n\n\nProof. Showing this is straightforward and insightful: since no unstable pole-zero cancellation is allowed if internal stability is to be guaranteed, the open-loop transfer function L=KG must inherit the RHP zero of G, that is,\n\nL(z) = K(z)G(z) = 0, \\; z\\in \\text{RHP}.\n\nBut then the sensitivity function S=1/(1+L) must satisfy \nS(z) = \\frac{1}{1+L(z)} = 1.\n\nConsequently, the complementary sensitivity function T=1-S must satisfy the interpolation condition T(z)=0.\n\nSimilarly, assuming that the plant transfer function G(s) has a pole in the RHP, that is,\n\nG(p) = \\infty, \\; p\\in \\text{RHP},\n which can also be formulated in a cleaner way (avoiding the infinity in the definition) as \n\\frac{1}{G(p)} = 0, \\; p\\in \\text{RHP},\n the closed-loop transfer functions S(s) and T(s) satisfy the interpolation conditions \\boxed\n{T(p) = 1,\\;\\;\\;S(p) = 0.}\n\nThe interpolation conditions that we have just derived constitute the basis on which we are going to derive the limitations of achievable closed-loop magnitude frequency responses. But we need one more technical results before we can proceed. Most probably you have already encountered it in some course on complex analysis - maximum modulus principle. We state this result in the jargon of control theory.\n\nTheorem 1 (Maximum modulus principle) For a stable transfer function F(s), that is, for a function with no pole in the closed right half-plane (RHP) it holds that\n\n\\sup_{\\omega}|F(j\\omega)|\\geq |F(s_0)|\\;\\;\\; \\forall s_0\\in \\text{RHP}.\n\nThis can also be expressed compactly as \n\\|F(s)\\|_\\infty \\geq |F(s_0)|\\;\\;\\; \\forall s_0\\in \\text{RHP}.\n\n\nNow instead of some general F(s) we consider the weighted sensitivity function W_\\mathrm{p}(s)S(s). And the complex number s in the RHP equals to a zero z of the plant transfer function G(s), that is, G(z)=0, \\; z\\in\\mathbb C, \\; \\Re(z)\\geq 0. Then the maximum modulus principle together with the interpolation condition S(z)=1 implies that\n\n\\|W_\\mathrm{p}S\\|_{\\infty}\\geq |W_\\mathrm{p}(z)|.\n\nSimilar result holds for the weighted complementary sensitivity function W(s)T(s) and an unstable pole p of the plant transfer function G(s), when combining the maximum modulus principle with the interpolation condition T(p)=1\n\n\\|WT\\|_{\\infty}\\geq |W(p)|.\n\nThese two simple results can be further generalized to the situations in which the plant transfer function G(s) has multiple zeros and poles in the RHP. Namely, if G(s) has N_p unstable poles p_i and N_z unstable zeros z_j,\n\n\\|W_\\mathrm{p}S\\|_{\\infty}\\geq c_{1j}|W_\\mathrm{p}(z_j)|, \\;\\;\\;c_{1j}=\\prod_{i=1}^{N_p}\\frac{|z_j+\\bar{p}_i|}{|z_j-p_i|}\\geq 1,\n \n\\|WT\\|_{\\infty}\\geq c_{2i}|W(p_i)|, \\;\\;\\;c_{2i}=\\prod_{j=1}^{N_z}\\frac{|\\bar{z}_j+p_i|}{|z_j-p_i|}\\geq 1.\n\nAs a special case, consider the no-weight cases W_\\mathrm{p}(s)=1 and W(s)=1 with just a single unstable pole and zero. Then the limitations on the achievable closed-loop magnitude frequency responses can be formulated as \n\\|S\\|_{\\infty} &gt; c, \\;\\; \\|T\\|_{\\infty} &gt; c, \\;\\;\\;c=\\frac{|z+p|}{|z-p|}.\n\n\nExample 1 For G(s) = \\frac{s-4}{(s-1)(0.1s+1)}, the limitations are \n\\|S\\|_{\\infty}&gt;1.67, \\quad \\|T\\|_{\\infty}&gt;1.67.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#limitations-of-the-achievable-bandwidth-due-to-zeros-in-the-right-half-plane",
    "href": "limitations_SISO.html#limitations-of-the-achievable-bandwidth-due-to-zeros-in-the-right-half-plane",
    "title": "Limitations for SISO systems",
    "section": "Limitations of the achievable bandwidth due to zeros in the right half-plane",
    "text": "Limitations of the achievable bandwidth due to zeros in the right half-plane\nThere are now two requirements on the weighted sensitivity function that must be reconciled. First, the performance requirements \n|S(j\\omega)|&lt;\\frac{1}{|W_\\mathrm{p}(j\\omega)|}\\;\\;\\forall\\omega\\;\\;\\;\\Longleftrightarrow \\|W_\\mathrm{p}S\\|_{\\infty}&lt;1\n and second, the just derived consequence of the interpolation condition \n\\|W_\\mathrm{p}S\\|_{\\infty}\\geq |W_\\mathrm{p}(z)|.\n\nThe only way to satisfy both is to guarantee that \n|W_\\mathrm{p}(z)|&lt;1.\n\nNow, consider the popular first-order weight \nW_\\mathrm{p}(z)=\\frac{s/M+\\omega_\\mathrm{B}}{s+\\omega_\\mathrm{B} A}.\n\nFor one real zero in the RHP, the inequality |W_\\mathrm{p}(z)|&lt;1 can be written as \n\\omega_\\mathrm{B}(1-A) &lt; z\\left(1-\\frac{1}{M}\\right).\n\nSetting A=0 a M=2, the upper bound on the bandwidth follows\n\\boxed\n{\\omega_\\mathrm{B}&lt;0.5z.}\n\nFor complex conjugate pair \n\\omega_\\mathrm{B}=|z|\\sqrt{1-\\frac{1}{M^2}}\n M=2: \\omega_\\mathrm{B}&lt;0.86|z|.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#limitation-of-the-achievable-bandwidth-due-to-poles-in-the-right-half-plane",
    "href": "limitations_SISO.html#limitation-of-the-achievable-bandwidth-due-to-poles-in-the-right-half-plane",
    "title": "Limitations for SISO systems",
    "section": "Limitation of the achievable bandwidth due to poles in the right half-plane",
    "text": "Limitation of the achievable bandwidth due to poles in the right half-plane\nUsing \n|T(j\\omega)|&lt;\\frac{1}{|W(j\\omega)|}\\;\\;\\;\\forall\\omega\\;\\;\\;\\Longleftrightarrow \\|WT\\|_{\\infty}&lt;1\n and the interpolation condition \\|WT\\|_{\\infty}\\geq |W(p)|: \n|W(p)|&lt;1\n With weight \nW(s)= \\frac{s}{\\omega_{BT}^*}+\\frac{1}{M_T}\n we get a lower bound on the bandwidth \n\\omega_{BT}^* &gt; p\\frac{M_T}{M_T-1}\n M_T=2: {\\omega_{BT}^*&gt;2p}\\ For complex conjugate pair: \\omega_{BT}^*&gt;1.15|p|.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#limitations-due-to-time-delay",
    "href": "limitations_SISO.html#limitations-due-to-time-delay",
    "title": "Limitations for SISO systems",
    "section": "Limitations due to time delay",
    "text": "Limitations due to time delay\nConsider the problem of designing a feedback controller for reference tracking. An ideal closed-loop transfer function T(s) from the reference to the output satisfies T(s)=1. If the plant has a time delay, the best achievable closed-loop transfer function T(s) is given by \nT(s) = e^{-\\theta s},\n that is, the reference is perfectly tracked, albeit with some delay. The best achievable sensitivity function S(s) is then given by \nS(s) = 1-e^{-\\theta s}.\n\nIn order to make the analysis simpler, we approximate the sensitivity function by the first-order Taylor expansion \nS(s) \\approx \\theta s,\n from which we can see that the magnitude frequency response of the sensitivity function is approximated by a linear function of frequency. Unit gain is achieved at about\n\n\\omega_{c}=1/\\theta.\n From this approximation, we can see that the bandwidth of the system is limited by the time delay \\theta as\n\\boxed{\n\\omega_c &lt; \\frac{1}{\\theta}.\n}",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#limitations-due-presence-of-disturbance",
    "href": "limitations_SISO.html#limitations-due-presence-of-disturbance",
    "title": "Limitations for SISO systems",
    "section": "Limitations due presence of disturbance",
    "text": "Limitations due presence of disturbance",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "limitations_SISO.html#limitations-due-to-saturation-of-actuators",
    "href": "limitations_SISO.html#limitations-due-to-saturation-of-actuators",
    "title": "Limitations for SISO systems",
    "section": "Limitations due to saturation of actuators",
    "text": "Limitations due to saturation of actuators",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for SISO systems"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html",
    "href": "opt_theory_unconstrained.html",
    "title": "Theory for unconstrained optimization",
    "section": "",
    "text": "We are going to analyze the optimization problem with no constraints \n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad  f(\\bm x).\nWhy are we considering such an unrealistic problem? After all, every engineering problem is subject to some constraints.\nBesides the standard teacher’s answer that we should always start with easier problems, there is another answer: it is common for analysis and algorithms for constrained optimization problems to reformulate them as unconstrained ones and then apply tools for unconstrained problems.",
    "crumbs": [
      "1. Optimization – theory",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#local-vs-global-optimality",
    "href": "opt_theory_unconstrained.html#local-vs-global-optimality",
    "title": "Theory for unconstrained optimization",
    "section": "Local vs global optimality",
    "text": "Local vs global optimality\nFirst, let’s define carefully what we mean by a minimum in the unconstrained problem.\n\n\n\n\n\n\nCaution\n\n\n\nFor those whose mother tongue does not use articles such as the and a/an in English, it is worth emphasizing that there is a difference between “the minimum” and “a minimum”. In the former we assume that there is just one minimum, in the latter we make no such assumption.\n\n\nConsider a (scalar) function of a scalar variable for simplicity. Something like the function whose graph is shown in Fig. 1 below.\n\n\n\n\n\n\nFigure 1: An example of a scalar function of a scalar real argument that exhibits several local minima and maxima\n\n\n\nWe say, that the function has a local minimum at x^\\star if f(x)\\geq f(x^\\star) in an \\varepsilon neighbourhood. All the red dots in the above figure are local minima. Similarly, of course, the function has a local maximum at x^\\star if f(x)\\leq f(x^\\star) in an \\varepsilon neighbourhood. Such local maxima are the green dots in the figure. The smallest and the largest of these are global minima and maxima, respectively.",
    "crumbs": [
      "1. Optimization – theory",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#conditions-of-optimality",
    "href": "opt_theory_unconstrained.html#conditions-of-optimality",
    "title": "Theory for unconstrained optimization",
    "section": "Conditions of optimality",
    "text": "Conditions of optimality\nHere we consider two types of conditions of optimality for unconstrained minimization problems: necessary and sufficient conditions. Necessary conditions must be satisfied at the minimum, but even when they are, the optimality is not guaranteed. On the other hand, the sufficient conditions need not be satisfied at the minimum, but if they are, the optimality is guaranteed. We show the necessary conditions of the first and second order, while the sufficient condition only of the second order.\n\nScalar optimization variable\nYou may want to have a look at the video, but below we continue with the text that covers (and polishes and sometimes even extends a bit) the content of the video.\n\nWe recall here the fundamental assumption made at the beginning of our introduction to optimization – we only consider optimization variables that are real-valued (first, just scalar x \\in \\mathbb R, later vectors \\bm x \\in \\mathbb R^n), and objective functions f() that are sufficiently smooth – all the derivatives exist. Then the conditions of optimality can be derived upon inspecting the Taylor series approximation of the cost function around the minimum.\n\nTaylor series approximation around the optimum\nDenote x^\\star as the (local) minimum of the function f(x). The Taylor series expansion of f(x) around x^\\star is\n\nf(x^\\star+\\alpha) = f(x^\\star)+\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star}\\alpha + \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2 + {\\color{blue}\\mathcal{O}(\\alpha^3)},\n where \\mathcal{O}() is called Big O and has the property that \n\\lim_{\\alpha\\rightarrow 0}\\frac{\\mathcal{O}(\\alpha^3)}{\\alpha^3} \\leq M&lt;\\infty.\n\nAlternatively, we can write the Taylor series expansion as \nf(x^\\star+\\alpha) = f(x^\\star)+\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star}\\alpha + \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2 + {\\color{red}o(\\alpha^2)},\n using the little o with the property that \n\\lim_{\\alpha\\rightarrow 0}\\frac{o(\\alpha^2)}{\\alpha^2} = 0.\n\nWhether \\mathcal{O}() or \\mathcal{o}() concepts are used, it is just a matter of personal preference. They both express that the higher-order terms in the expansion tend to be negligible compare to the first- and second-order term as \\alpha is getting smaller. \\mathcal O(\\alpha^3) goes to zero at least as fast as a cubic function, while o(\\alpha^2) goes to zero faster than a quadratic function.\nIt is indeed important to understand that this negligibility of the higher-order terms is only valid asymptotically – for a particular \\alpha it may easily happend that, say, the third-order term is still dominating.\n\n\nFirst-order necessary conditions of optimality\nFor \\alpha sufficiently small, the first-order Taylor series expansion is a good approximation of the function f(x) around the minimum. Since \\alpha enters this expansion linearly, the cost function can increase or decrease with \\alpha, depending on the sign of the first derivative. The only way to ensure that the function as a (local) minimu at x^\\star is to have the first derivative equal to zero, that is \\boxed{\n\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star} = 0.}\n\n\n\nSecond-order necessary conditions of optimality\nOnce the first-order necessary condition of optimality is satisfied, the dominating term (as \\alpha is getting smalle) is the second-order term \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2. Since \\alpha is squared, it is the sign of the second derivative that determines the contribution of the whole second-order term to the cost function value. For the minimum, the second derivative must be nonnegative, that is\n\n\\boxed{\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star} \\geq 0.}\n\nFor completeness we state that the sign must be nonpositive for the maximum.\n\n\nSecond-order sufficient condition of optimality\nFollowing the same line of reasoning as above, the if the second derivative is positive, the miniumum is guaranteed, that is, the sufficient condition of optimality is \n\\boxed{\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star} &gt; 0.}\n\nIf the second derivative fails to be positive and is just zero (thus still satisfying the necessary condition), does it mean that the point is not a minimum? No. We must examine higher order terms.\n\n\n\nVector optimization variable\nOnce again, should you prefer watching a video, here it is, but below we continue with the text that covers the content of the video.\n\n\nFirst-order necessary conditions of optimality\nOne way to handle the vector variables is to convert the vector problem into a scalar one by fixing a direction to an arbitrary vector \\bm d and then considering the scalar function of the form f(\\bm x^\\star + \\alpha \\bm d). For convenience we define a new function \ng(\\alpha) \\coloneqq f(\\bm x^\\star + \\alpha \\bm d)\n and from now on we can invoke the results for scalar functions. Namely, we expand the g() function around zero as \ng(\\alpha) = g(0) + \\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0}\\alpha + \\frac{1}{2}\\frac{\\mathrm{d}^2 g(\\alpha)}{\\mathrm{d}\\alpha^2}\\bigg|_{\\alpha=0}\\alpha^2 + \\mathcal{O}(\\alpha^3),\n and argue that the first-order necessary condition of optimality is \n\\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0} = 0.\n\nNow, invoking the chain rule, we go back from g() to f() \n\\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0} = \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} \\frac{\\partial(\\bm x^\\star + \\alpha \\bm d)}{\\partial\\alpha}\\bigg|_{\\alpha=0} = \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star}\\,\\bm d = 0,\n where \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} is a row vector of partial derivatives of f() evaluated at \\bm x^\\star. Since the vector \\bm d is arbitrary, the necessary condition is that\n\n\\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} = \\mathbf 0,  \n\nMore often than not we use the column vector to store partial derivatives. We call it the gradient of the function f() and denoted it as \n\\nabla f(\\bm x) \\coloneqq \\begin{bmatrix}\\frac{\\partial f(\\bm x)}{\\partial x_1} \\\\ \\frac{\\partial f(\\bm x)}{\\partial x_n} \\\\ \\vdots \\\\ \\frac{\\partial f(\\bm x)}{\\partial x_n}\\end{bmatrix}.  \n\nThe first-order necessary condition of optimality using gradients is then \n\\boxed{\\left.\\nabla f(\\bm x)\\right|_{x=x^\\star} = \\mathbf 0.  }\n\n\n\n\n\n\n\nGradient is a column vector\n\n\n\nIn some literature the gradient \\nabla f(\\bm x) is defined as a row vector. For the condition of optimality it does not matteer since all we require is that all partial derivatives vanish. But for other purposes in our text we regard the gradient as a vector living in the same vector space \\mathbb R^n as the optimization variable. The row vector is sometimes denoted as \\mathrm Df(\\bm x).\n\n\n\nComputing the gradient of a scalar function of a vector variable\nA convenient way is to compute the differential fist and then to identify the derivative in it. Recall that the differential is the first-order approximation to the increment of the function due to a change in the variable\n\n\\Delta f \\approx \\mathrm{d}f = \\nabla f(x)^\\top \\mathrm d \\bm x.\n\nFinding the differential of a function is conceptually easier than finding the derivative since it is a scalar quantity. When searching for the differential of a composed function, we follow the same rules as for the derivative (such as that the one for finding the differential of a product). Let’s illustrate it using an example.\n\nExample 1 For the function \nf(\\mathbf x) = \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x},\n where \\mathbf Q is symetric, the differential is \n\\mathrm{d}f = \\frac{1}{2}\\mathrm d\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\mathrm d\\bm{x} + \\mathbf{r}^\\top\\mathrm{d}\\bm{x},\n in which the first two terms can be combined thanks to the fact that they are scalars \n\\mathrm{d}f = \\left(\\bm{x}^\\top\\frac{\\mathbf{Q} + \\mathbf{Q}^\\top}{2} + \\mathbf{r}^\\top\\right)\\mathrm{d}\\bm{x},\n and finally, since we assumed that \\mathbf Q is a symmetric matrix, we get \n\\mathrm{d}f = \\left(\\mathbf{Q}\\bm{x} + \\mathbf{r}\\right)^\\top\\mathrm{d}\\bm{x},\n from which we can identify the gradient as \n\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\mathbf{x} + \\mathbf{r}.\n\nThe first-order condition of optimality is then \n\\boxed{\\mathbf{Q}\\mathbf{x} = -\\mathbf{r}.}\n\nAlthough this was just an example, it is actually a very useful one. Keep this result in mind – necessary condition of optimality of a quadratic function comes in the form of a set of linear equations.\n\n\n\n\nSecond-order necessary conditions of optimality\nAs before, we fix the direction \\bm d and consider the function g(\\alpha) = f(\\bm x^\\star + \\alpha \\bm d). We expand the expression for the first derivative as \n\\frac{\\mathrm d g(\\alpha)}{\\mathrm d \\alpha} = \\sum_{i=1}^{n}\\frac{\\partial f(\\bm x)}{\\partial x_i}\\bigg|_{\\bm x = \\bm x^\\star} d_i,\n and differentiating this once again, we get the second derivative \n\\frac{\\mathrm d^2 g(\\alpha)}{\\mathrm d \\alpha^2} = \\sum_{i,j=1}^{n}\\frac{\\partial^2 f(\\bm x)}{\\partial x_ix_j}\\bigg|_{\\bm x = \\bm x^\\star}d_id_j\n\n\n\\begin{aligned}\n\\frac{\\mathrm d^2 g(\\alpha)}{\\mathrm d \\alpha^2}\\bigg|_{\\alpha=0} &= \\sum_{i,j=1}^{n}\\frac{\\text{d}^2}{\\text{d}x_ix_j}f(\\bm x)\\bigg|_{\\bm x = \\bm x^\\star}d_id_j\\\\\n&= \\mathbf d^\\text{T} \\underbrace{\\nabla^\\text{2}f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star}}_\\text{Hessian} \\mathbf d.\n\\end{aligned}\n where \\nabla^2 f(\\mathbf x) is the Hessian (the symmetrix matrix of the second-order mixed partial derivatives) \n\\nabla^2 f(x) = \\begin{bmatrix}\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1^2} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1\\partial x_2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1\\partial x_n}\\\\\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2\\partial x_1} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2^2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2\\partial x_n}\\\\\n                 \\vdots\\\\\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_1} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_n}.\n                \\end{bmatrix}\n\nSince \\bm d is arbitrary, the second-order necessary condition of optimality is then \n\\boxed{\\nabla^\\text{2}f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star} \\succeq 0,}\n where, once again, the inequality \\succeq reads that the matrix is positive semidefinite.\n\n\nSecond-order sufficient condition of optimality\n\n\\boxed{\\nabla^2 f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star} \\succ 0,}\n where, once again, the inequality \\succ reads that the matrix is positive definite.\n\nExample 2 For the quadratic function f(\\mathbf x) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{Q}\\mathbf{x} + \\mathbf{r}^\\mathrm{T}\\mathbf{x}, the Hessian is \n\\nabla^2 f(\\mathbf{x}) = \\mathbf{Q}\n and the second-order necessary condition of optimality is \n\\boxed{\\mathbf{Q} \\succeq 0.}\n\nSecond-order sufficient condition of optimality is then \n\\boxed{\\mathbf{Q} \\succ 0.}\n\nOnce again, this was more than just an example – quadratic functions are so important for us that it is worth remembering this result.",
    "crumbs": [
      "1. Optimization – theory",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#classification-of-stationary-points",
    "href": "opt_theory_unconstrained.html#classification-of-stationary-points",
    "title": "Theory for unconstrained optimization",
    "section": "Classification of stationary points",
    "text": "Classification of stationary points\nFor a stationary (also critical) point \\bm x^\\star, that is, one that satisfies the first-order necessary condition \n\\nabla f(\\bm x^\\star) = 0,\n\nwe can classify it as\n\nMinimum: if \\nabla^2 f(\\bm x^\\star)\\succ 0,\nMaximum: if \\nabla^2 f(\\bm x^\\star)\\prec 0,\nSaddle point: if \\nabla^2 f(\\bm x^\\star) is indefinite,\nSingular point: if \\nabla^2 f(\\bm x^\\star) is singular and \\nabla^2 f(\\bm x^\\star)\\succeq 0 (or \\nabla^2 f(\\bm x^\\star)\\prec 0).\n\n\n\nExample 3 (Minimum of a quadratic function) We consider a quadratic function f(\\mathbf x) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{Q}\\mathbf{x} + \\mathbf{r}^\\mathrm{T}\\mathbf{x} for a particular \\mathbf{Q} and \\mathbf{r}.\n\n\nShow the code\nQ = [1 1; 1 2]\nr = [0, 1]\n\nusing LinearAlgebra\nx_stationary = -Q\\r\neigvals(Q)\n\n\n2-element Vector{Float64}:\n 0.38196601125010515\n 2.618033988749895\n\n\nThe matrix is positive definite, so the stationary point is a minimum. In fact, the minimum. Surface and contour plots of the function are shown below.\n\nShow the code\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([x_stationary[1]], [x_stationary[2]], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\nGKS: cannot open display - headless operation mode active\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 2: Minimum of a quadratic function\n\n\n\n\n\nExample 4 (Saddle point of a quadratic function)  \n\n\nShow the code\nQ = [-1 1; 1 2]\nr = [0, 1]\n\nusing LinearAlgebra\nx_stationary = -Q\\r\neigvals(Q)\n\n\n2-element Vector{Float64}:\n -1.3027756377319946\n  2.302775637731995\n\n\nThe matrix is indefinite, so the stationary point is a saddle point. Surface and contour plots of the function are shown below.\n\nShow the code\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([x_stationary[1]], [x_stationary[2]], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 3: Saddle point of a quadratic function\n\n\n\n\n\nExample 5 (Singular point of a quadratic function)  \n\n\nShow the code\nQ = [2 0; 0 0]\nr = [3, 0]\n\nusing LinearAlgebra\neigvals(Q)\n\n\n2-element Vector{Float64}:\n 0.0\n 2.0\n\n\nThe matrix Q is singular, which has two consequences:\n\nWe cannot compute the stationary point since Q is not invertible. In fact, there is a whole line (a subspace) of stationary points.\nThe matrix Q is positive semidefinite, which generally means that optimality cannot be concluded. But in this particular case of a quadratic function, there are no higher-order terms in the Taylor series expansion, so the stationary point is a minimum.\n\nSurface and contour plots of the function are shown below.\n\nShow the code\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ndisplay(contour(x1_data,x2_data,f_data))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 4: Singular point of a quadratic function\n\n\n\n\n\nExample 6 (Singular point of a non-quadratic function) Consider the function f(\\bm x) = x_1^2 + x_2^4. Its gradient is \\nabla f(\\bm x) = \\begin{bmatrix}2x_1\\\\ 4x_2^3\\end{bmatrix} and it vanishes at \\bm x^\\star = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}. The Hessian is \\nabla^2 f(\\bm x) = \\begin{bmatrix}2 & 0\\\\ 0 & 12x_2^2\\end{bmatrix}, which when evaluated at the stationary point is \\nabla^2 f(\\bm x)\\bigg|_{\\bm x=\\mathbf 0} = \\begin{bmatrix}2 & 0\\\\ 0 & 0\\end{bmatrix}, which is positive semidefinite. We cannot conclude if the function attains a minimum at \\bm x^\\star.\nWe need to examine higher-order terms in the Taylor series expansion. The third derivatives are \n\\frac{\\partial^3 f}{\\partial x_1^3} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_1^2\\partial x_2} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_1\\partial x_2^2} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_2^3} = 24x_2,\n and when evaluated at zero, they all vanish.\nAll but one fourth derivatives also vanish. The one that does not is \n\\frac{\\partial^4 f}{\\partial x_2^4} = 24,\n which is positive, and since the associated derivative is of the even order, the power si also even, hence the function attains a minimum at \\bm x^\\star = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}.\nThis can also be visually confirmed by the surface and contour plots of the function.\n\nShow the code\nf(x) = x[1]^2 + x[2]^4\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([0.0], [0.0], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 5: Singular point of a non-quadratic function that turrns out to be a minimum",
    "crumbs": [
      "1. Optimization – theory",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "reduction_order_hw.html",
    "href": "reduction_order_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Homework"
    ]
  },
  {
    "objectID": "cont_indir_overview.html",
    "href": "cont_indir_overview.html",
    "title": "Overview of continuous-time optimal control",
    "section": "",
    "text": "Through this chapter we are entering into the realm of continuous-time optimal control – we are going to consider dynamical systems that evolve in continuous time, and we are going to search for control that also evolves in continuous time.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Overview of continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_indir_overview.html#continuous-time-optimal-control-problem",
    "href": "cont_indir_overview.html#continuous-time-optimal-control-problem",
    "title": "Overview of continuous-time optimal control",
    "section": "Continuous-time optimal control problem",
    "text": "Continuous-time optimal control problem\nWe start by considering a nonlinear continuous-time system modelled by the state equation \n\\dot{\\bm{x}}(t) = \\mathbf f(\\bm x(t),\\bm u(t), t),\n where\n\n\\bm x(t) \\in \\mathbb R^n is the state vector at the continuous time t\\in \\mathbb R,\n\\bm u(t) \\in \\mathbb R^m is the control vector at the continuous time t,\n\\mathbf f: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb R \\to \\mathbb{R}^n is the state transition function (in general not only nonlinear but also time-varying).\n\nA general nonlinear continuous-time optimal control problem (OCP) is then formulated as \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u(\\cdot), \\bm x(\\cdot)}&\\quad \\left(\\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} L(\\bm x(t),\\bm u(t),t) \\; \\mathrm{d}t \\right)\\\\\n\\text{subject to}  &\\quad \\dot {\\bm{x}}(t) = \\mathbf f(\\bm x(t),\\bm u(t)),\\quad t \\in [t_\\mathrm{i},t_\\mathrm{f}],\\\\\n                    &\\quad \\bm u(t) \\in \\mathcal U(t),\\\\\n                    &\\quad \\bm x(t) \\in \\mathcal X(t),\n\\end{aligned}\n where\n\nt_\\mathrm{i} is the initial continuous time,\nt_\\mathrm{f} is the final continuous time,\n\\phi() is a terminal cost function that penalizes the state at the final time (and possibly the final time too if it is regarded as an optimization variable),\nL() is a running (also stage) cost function,\nand \\mathcal U(t) and \\mathcal X(t) are (possibly time-dependent) sets of feasible controls and states – these sets are typically expressed using equations and inequalities. Should they be constant (not changing in time), the notation is just \\mathcal U and \\mathcal X.\n\nOftentimes it is convenient to handle the constraints of the initial and final states separately: \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u(\\cdot), \\bm x(\\cdot)}&\\quad \\left(\\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} L(\\bm x(t),\\bm u(t),t) \\; \\mathrm{d}t \\right)\\\\\n\\text{subject to}  &\\quad \\dot {\\bm{x}}(t) = \\mathbf f(\\bm x(t),\\bm u(t)),\\quad t \\in [t_\\mathrm{i},t_\\mathrm{f}],\\\\\n                    &\\quad \\bm u(t) \\in \\mathcal U(t),\\\\\n                    &\\quad \\bm x(t) \\in \\mathcal X(t),\\\\\n                    &\\quad \\bm x(t_\\mathrm{i}) \\in \\mathcal X_\\mathrm{init},\\\\\n                    &\\quad \\bm x(t_\\mathrm{f}) \\in \\mathcal X_\\mathrm{final}.\n\\end{aligned}\n\nIn particular, at the initial time just one particular state is often considered. At the final time, the state might be required to be equal to some given value, it might be required to be in some set defined through equations or inequalities, or it might be left unconstrained. Finally, the constraints on the control and states typically (but not always) come in the form of lower and upper bounds. The optimal control problem then specializes to \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u(\\cdot), \\bm x(\\cdot)}&\\quad \\left(\\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} L(\\bm x(t),\\bm u(t),t) \\; \\mathrm{d}t \\right)\\\\\n\\text{subject to}  &\\quad \\dot {\\bm{x}}(t) = \\mathbf f(\\bm x(t),\\bm u(t)),\\quad t \\in [t_\\mathrm{i},t_\\mathrm{f}],\\\\\n                    &\\quad \\bm u_{\\min} \\leq \\bm u(t) \\leq \\bm u_{\\max},\\\\\n                    &\\quad \\bm x_{\\min} \\leq \\bm x(t) \\leq \\bm x_{\\max},\\\\\n                    &\\quad \\bm x(t_\\mathrm{i}) = \\mathbf x^\\text{init},\\\\\n                    &\\quad \\left(\\bm x(t_\\mathrm{f}) = \\mathbf x^\\text{ref}, \\; \\text{or} \\; \\mathbf h_\\text{final}(\\bm x(t_\\mathrm{f})) =  \\mathbf 0, \\text{or} \\; \\mathbf g_\\text{final}(\\bm x(t_\\mathrm{f})) \\leq  \\mathbf 0\\right),\n\\end{aligned}\n where\n\nthe inequalities should be interpreted componentwise,\n\\bm u_{\\min} and \\bm u_{\\max} are lower and upper bounds on the control, respectively,\n\\bm x_{\\min} and \\bm x_{\\max} are lower and upper bounds on the state, respectively,\n\\mathbf x^\\text{init} is a fixed initial state,\n\\mathbf x^\\text{ref} is a required (reference) final state,\nand the functions \\mathbf g_\\text{final}() and \\mathbf h_\\text{final}() can be used to define the constraint set for the final state.\n\n\n\n\n\n\n\nClassification of optimal control problems: Bolza, Mayer, and Lagrange problems\n\n\n\nThe cost function in the above defined optimal control problem contains both the cost incurred at the final time and the cumulative cost (the integral of the running cost) incurred over the whole interval. An optimal control problem with this general cost function is called Bolza problem in the literature. If the cost function only penalizes the final state and the final time, the problem is called Mayer problem. If the cost function only penalizes the cumulative cost, the problem is called Lagrange problem.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Overview of continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_indir_overview.html#why-continuous-time-optimal-control",
    "href": "cont_indir_overview.html#why-continuous-time-optimal-control",
    "title": "Overview of continuous-time optimal control",
    "section": "Why continuous-time optimal control?",
    "text": "Why continuous-time optimal control?\nWhy are we interested in continuous-time optimal control when at the end of the day most if not all controllers are implemented using computers and hence in discrete time? There are several reasons:\n\nThe theory for continuous-time optimal control is highly mature and represents a pinnacle of human ingenuity. It would be a pity to ignore it. It is also much richer than the theory for discrete-time optimal control. For example, when considering the time-optimal control, we can differentiate the cost function with respect to the final time because it is a continuous (real) variable.\nAlthough the theoretical concepts needed for continuous-time optimal control are more advanced (integrals and derivatives instead of sums and differences, function spaces instead of spaces of sequences, calculus of variations instead of differential calculus), the results are often simpler than in the discrete-time case – the resulting formulas just look neater and more compact.\nWe will see later that methods for solving general continuous-time optimal control problems must use some kind of (temporal)discretization. Isn’t it then enough to study discretization and discrete-time optimal control separately? It will turn out that discretization can be regarded as a part of the solution process.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Overview of continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_indir_overview.html#approaches-to-continuous-time-optimal-control",
    "href": "cont_indir_overview.html#approaches-to-continuous-time-optimal-control",
    "title": "Overview of continuous-time optimal control",
    "section": "Approaches to continuous-time optimal control",
    "text": "Approaches to continuous-time optimal control\nThere are essentially the same three approaches to continuous-time optimal control as we have seen when studying discrete-time optimal control:\n\nindirect approaches,\ndirect approaches,\ndynamic programming.\n\n\n\n\n\n\n\n\n\nG\n\n\ndiscrete_time_optimal_control\n\nApproaches to continuous-time optimal control\n\n\n\ndirect_approach\n\nIndirect approach\n\n\n\ndiscrete_time_optimal_control-&gt;direct_approach\n\n\n\n\n\nindirect_approach\n\nDirect approach\n\n\n\ndiscrete_time_optimal_control-&gt;indirect_approach\n\n\n\n\n\ndynamic_programming\n\nDynamic programming\n\n\n\ndiscrete_time_optimal_control-&gt;dynamic_programming\n\n\n\n\n\n\n\n\nFigure 1: Three approaches to continuous-time optimal control\n\n\n\n\n\nIn this chapter we start with the indirect approach. But first we need to introduce the framework of calculus of variations, which is what we do in the next section.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Overview of continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_indir_LQR_fin_horizon.html",
    "href": "cont_indir_LQR_fin_horizon.html",
    "title": "Indirect approach to LQR on a finite horizon",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Indirect approach to LQR on a finite horizon"
    ]
  },
  {
    "objectID": "discr_dir_mpc_stability.html",
    "href": "discr_dir_mpc_stability.html",
    "title": "Stability of MPC",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "Stability of MPC"
    ]
  },
  {
    "objectID": "intro_outline.html",
    "href": "intro_outline.html",
    "title": "Course outline",
    "section": "",
    "text": "The course is structured into 14 topics, each of them corresponding to one lecture. The topics are as follows:\n\nOptimization\n\nTheory: formulations, conditions, types of problems, optimization modellers, …\nAlgorithms: computing derivatives (symbolic, finite difference, autdiff), gradient, Newton, …, solvers\n\nDiscrete-time optimal control\n\nDirect approach (via optimization): on finite horizon, MPC\nIndirect approach (via Hamilton equations): finite and infinite horizon, LQR, Riccati equations, …\nDynamic programming: Bellman’s principle, …\nMore on MPC: combining direct and indirect approaches and dynamic programming\n\nContinuous-time optimal control\n\nIndirect approach (via calculus of variations): boundary value problem, Riccati equations, LQR\nIndirect approach (via Pontryagin’s principle of maximum): time-optimal constrained control\nNumerical methods for both direct and indirect approaches: shooting, multiple shooting, collocation\nSome extensions of LQ-optimal control: stochastic LQR, LQG, LTR, \\(\\mathcal{H}_2\\)\n\nRobust control\n\nModeling of uncertainty, robustness analysis: small gain theorem, structured singular values\nRobust control design: \\(\\mathcal{H}_\\infty\\)-optimal control, \\(\\mu\\)-synthesis\nAnalysis of achievable performance\n\nOther topics\n\nModel order reduction, controller order reduction\n\n\n\n\n\n Back to top",
    "crumbs": [
      "0. Introduction",
      "Course outline"
    ]
  },
  {
    "objectID": "cont_dp_hw.html",
    "href": "cont_dp_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Homework"
    ]
  },
  {
    "objectID": "cont_indir_hw.html",
    "href": "cont_indir_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Homework"
    ]
  },
  {
    "objectID": "cont_indir_references.html",
    "href": "cont_indir_references.html",
    "title": "References",
    "section": "",
    "text": "Indirect approach to optimal control is based on calculus of variations (and its later extension in the form of Pontryagin’s principle of maximum). Calculus of variations is an advanced mathematical discipline that requires non-trivial foundations and effort to master. In our course, however, we take the liberty of aiming for intuitive understanding rather than mathematical rigor. At roughly the same level, the calculus of variations is introduced in books on optimal control, such as the classic and affordable [1], the popular and online available [2], or very accessible and also freely available online [3].\nWith anticipation, we provide here a reference to the paper [4], which shows how the celebrated Pontryagin’s principle of maximum extends the calculus of variations significantly. But we will only discuss this in the next chapter.\nFor those interested in a having a standard reference for the calculus of variations, the classic [5] is recommendable, the more so that it is fairly slim.\n\n\n\n\n Back to topReferences\n\n[1] D. E. Kirk, Optimal Control Theory: An Introduction, Reprint of the 1970 edition. Dover Publications, 2004.\n\n\n[2] F. L. Lewis, D. Vrabie, and V. L. Syrmo, Optimal Control, 3rd ed. John Wiley & Sons, 2012. Accessed: Mar. 09, 2022. [Online]. Available: https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf\n\n\n[3] D. Liberzon, Calculus of Variations and Optimal Control Theory: A Concise Introduction. Princeton University Press, 2011. Available: http://liberzon.csl.illinois.edu/teaching/cvoc/cvoc.html\n\n\n[4] H. J. Sussmann and J. C. Willems, “300 years of optimal control: From the brachystochrone to the maximum principle,” IEEE Control Systems, vol. 17, no. 3, pp. 32–44, Jun. 1997, doi: 10.1109/37.588098.\n\n\n[5] I. M. Gelfand and S. V. Fomin, Calculus of Variations, Reprint of the 1963 edition. Mineola, N.Y: Dover Publications, 2020.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "References"
    ]
  },
  {
    "objectID": "dynamic_programming_tabular.html",
    "href": "dynamic_programming_tabular.html",
    "title": "Tables as outcomes of dynamic programming",
    "section": "",
    "text": "Based on what we have seen so far, it turns out that the key to solving the discrete-time optimal control problem is to find some… functions. Either the optimal cost function J_k^\\star(\\bm x_k) or the optimal Q-factor Q_k^\\star(\\bm x_k,\\bm u_k). Once we have them, we can easily find the optimal control \\bm u_k^\\star(\\bm x_k). The question is, however, how to find these functions. We have seen some recursions for both of them, but it is not clear how to turn these into practical algorithms. We do it here.\nWe are going to use \nJ_k^\\star(\\bm x_k) = \\min_{\\bm u_k}\\left(L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1})\\right)\n backwards in (discrete) time at a grid of states. Indeed, gridding the state space is the key technique in dynamic programming, because DP assumes a finite state space. If it is not finite, we must grid it.\nWe start with the final time N. We evaluate the terminal cost function \\phi(\\bm x_N) at a grid of states, which directly yields the optimal costs J_N^\\star(\\bm x_N).\nWe then proceed to the time N-1. Evaluating the optimal cost function J^\\star_{N-1} at each grid point in the state space calls for some optimization, namely \n\\min_{u_{N-1}} \\left(L_{N-1}(\\bm x_{N-1},\\bm u_{N-1}) + J_{N}^\\star(\\mathbf f_{N-1}(\\bm x_{N-1}, \\bm u_{N-1}))\\right).\n\nWe save the optimal costs and the corresponding controls at the given grid points (giving two arrays of values), decrement the time to N-2, and repeat. All the way down to the initial time i.\nLet’s summarize that as an outcome of this whole procedure we have two tables – one for the optimal cost, the other for the optimal control.\n\n\n\n Back to top",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Tabular outcomes of DP"
    ]
  },
  {
    "objectID": "cont_indir_references_2.html",
    "href": "cont_indir_references_2.html",
    "title": "References",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "References"
    ]
  },
  {
    "objectID": "discr_dir_mpc_references.html",
    "href": "discr_dir_mpc_references.html",
    "title": "References",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "References"
    ]
  },
  {
    "objectID": "intro_references.html",
    "href": "intro_references.html",
    "title": "Literature for the course",
    "section": "",
    "text": "The two subdomains of control theory - optimal control and robust control -, which we explore in this course, are both quite mature, and are covered by a wealth of books, introductory and advanced. The same holds for the doman of numerical optimization, which provides important components for both control subdomains.\nIn our course we do not follow any single textbook chapter by chapter.\nInstead, besides our own lecture notes we are going to refer to various study resources once we study the individual (weekly) topics. In doing that we prefer freely available online resources and books available in the university library, but we also give some recommendations for relevant and helpful books that are not freely available.\nBelow we give a general recommendation on literature categorized into the three domains. Students are not required to obtain those books but perhaps such curated and commented lists might do some service to interested students.",
    "crumbs": [
      "0. Introduction",
      "Literature for the course"
    ]
  },
  {
    "objectID": "intro_references.html#numerical-optimization",
    "href": "intro_references.html#numerical-optimization",
    "title": "Literature for the course",
    "section": "Numerical optimization",
    "text": "Numerical optimization\nThe popular [1] offers deep but clear explanations, and [2] is commonly regarded as a comprehensive reference, and yet it is readable. The broad scope of the recently published [3] fits our course nearly perfectly, even including the optimal control part. Very didactic and also accompanied by Matlab and Python codes is [4]. [5] is also recommendable. Unfortunately, none of these books is freely available online.\nHowever, there are a few recently published textbooks that are also legally available online and that are fairly comprehensive and readable: [6], [7] and [8], the latter including code in Julia language. The freely available “convex optimization bible” [9] contains perfect explanations of many advanced optimization concepts such as duality.",
    "crumbs": [
      "0. Introduction",
      "Literature for the course"
    ]
  },
  {
    "objectID": "intro_references.html#optimal-control",
    "href": "intro_references.html#optimal-control",
    "title": "Literature for the course",
    "section": "Optimal control",
    "text": "Optimal control\nOne of a few optimal control textbooks that covers also discrete-time problems is the classical textboook [10], which has been lately made available online on the author’s web page.\nMajority of classical optimal control textbooks and monographs focus on continuous-time systems, for which we need to invoke calculus of variations. While it is not a trivial subject, an accessible treatment tailored to continuous-time optimal control is provided by [11] (a draft available online). The monograph [12] is regarded a classic, but [13] is perhaps a bit more readable (and affordable as it was reprinted by a low-cost publisher). A few copies of another reprinted classic [14] are available in the library, but freely downloadable electronic version are also available.\nWhile all these classical monographs provide fundamental theoretical insight, computational tools they provide (mostly in one way or another reformulating the optimal control problem into a Riccati equation) are mostly restricted to linear systems. For nonlinear continuous-time systems, hardly anything is as practical as numerical methods as covered for example by [15]. When restricted to discrete-time systems, model predictive control is covered by [16], which is also available online.",
    "crumbs": [
      "0. Introduction",
      "Literature for the course"
    ]
  },
  {
    "objectID": "intro_references.html#robust-control",
    "href": "intro_references.html#robust-control",
    "title": "Literature for the course",
    "section": "Robust control",
    "text": "Robust control\nThe topic of robust control is also described in a wealth of books. But unlike in the area of optimization and general optimal control, here we do have a strong preference for a single book, and our choice is fairly confident: [17]. We are going to use it in the last third of the course. A few copies of this book are available in the university library (reserved for the students of this course). In addition, the first three chapters are freely downloadable on the author’s web page. But we strongly recommend considering purchasing the book. It might turn out very useful as a reference even after passing an exam in this course.\nOther classical references are [18], [19], [20], [21], [22], but we are not going to used them in our course.",
    "crumbs": [
      "0. Introduction",
      "Literature for the course"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html",
    "href": "opt_theory_constrained.html",
    "title": "Theory for constrained optimization",
    "section": "",
    "text": "We consider the following optimization problem with equality constraints \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\n\\end{aligned}\n where \\mathbf h(\\bm x) \\in \\mathbb R^m defines a set of m equations \n\\begin{aligned}\nh_1(\\bm x) &= 0\\\\\nh_2(\\bm x) &= 0\\\\\n\\vdots\\\\\nh_m(\\bm x) &= 0.\n\\end{aligned}\n\nAugmenting the original cost function f with the constraint functions h_i scaled by Lagrange variables \\lambda_i gives the Lagrangian function \n\\mathcal{L}(\\bm x,\\boldsymbol\\lambda) \\coloneqq f(\\bm x) + \\sum_{i=1}^m \\lambda_i h_i(\\bm x) = f(\\bm x) + \\boldsymbol \\lambda^\\top \\mathbf h(\\bm x).\n\n\n\n\n\nThe first-order necessary condition of optimality is \n\\nabla \\mathcal{L}(\\bm x,\\boldsymbol\\lambda) = \\mathbf 0,\n which amounts to two (generally vector) equations \n\\boxed{\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nDefining a matrix \\nabla \\mathbf h(\\bm x) \\in \\mathbb R^{n\\times m} as horizontally stacked gradients of the constraint functions \n\\nabla \\mathbf h(\\bm x) \\coloneqq \\begin{bmatrix}\n                                 \\nabla h_1(\\bm x) && \\nabla h_2(\\bm x) && \\ldots && \\nabla h_m(\\bm x)\n                            \\end{bmatrix},\n in fact, a transpose of the Jacobian matrix, the necessary condition can be rewritten in a vector form as \\boxed\n{\\begin{aligned}\n\\nabla f(\\bm x) + \\nabla \\mathbf h(\\bm x)\\boldsymbol \\lambda &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nBeware of the nonregularity issue! The (\\nabla \\mathbf h(\\bm x))^\\mathrm T is regular at a given \\bm x (the \\bm x is a regular point) if it has a full column rank. Rank-deficiency reveals a defect in formulation.\n\nExample 1 (Equality-constrained quadratic program) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} &\\quad \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x}\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x + \\mathbf b = \\mathbf 0.\n\\end{aligned}\n\nThe first-order necessary condition of optimality is\n\n\\begin{bmatrix}\n  \\mathbf Q & \\mathbf A^\\top\\\\\\mathbf A & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\bm x \\\\ \\boldsymbol \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  -\\mathbf r\\\\\\mathbf b\n\\end{bmatrix}.\n\n\n\n\n\n\nUsing the unconstrained Hessian \\nabla^2_{\\mathbf{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda) is too conservative. Instead, use projected Hessian\n\n\\mathbf{Z}^\\mathrm{T}\\;\\nabla^2_{\\bm{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda)\\;\\mathbf Z &gt; 0,\n where \\mathbf Z is an (orthonormal) basis of the nullspace of the Jacobian (\\nabla \\mathbf h(\\bm x))^\\top.",
    "crumbs": [
      "1. Optimization – theory",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#equality-constraints",
    "href": "opt_theory_constrained.html#equality-constraints",
    "title": "Theory for constrained optimization",
    "section": "",
    "text": "We consider the following optimization problem with equality constraints \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\n\\end{aligned}\n where \\mathbf h(\\bm x) \\in \\mathbb R^m defines a set of m equations \n\\begin{aligned}\nh_1(\\bm x) &= 0\\\\\nh_2(\\bm x) &= 0\\\\\n\\vdots\\\\\nh_m(\\bm x) &= 0.\n\\end{aligned}\n\nAugmenting the original cost function f with the constraint functions h_i scaled by Lagrange variables \\lambda_i gives the Lagrangian function \n\\mathcal{L}(\\bm x,\\boldsymbol\\lambda) \\coloneqq f(\\bm x) + \\sum_{i=1}^m \\lambda_i h_i(\\bm x) = f(\\bm x) + \\boldsymbol \\lambda^\\top \\mathbf h(\\bm x).\n\n\n\n\n\nThe first-order necessary condition of optimality is \n\\nabla \\mathcal{L}(\\bm x,\\boldsymbol\\lambda) = \\mathbf 0,\n which amounts to two (generally vector) equations \n\\boxed{\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nDefining a matrix \\nabla \\mathbf h(\\bm x) \\in \\mathbb R^{n\\times m} as horizontally stacked gradients of the constraint functions \n\\nabla \\mathbf h(\\bm x) \\coloneqq \\begin{bmatrix}\n                                 \\nabla h_1(\\bm x) && \\nabla h_2(\\bm x) && \\ldots && \\nabla h_m(\\bm x)\n                            \\end{bmatrix},\n in fact, a transpose of the Jacobian matrix, the necessary condition can be rewritten in a vector form as \\boxed\n{\\begin{aligned}\n\\nabla f(\\bm x) + \\nabla \\mathbf h(\\bm x)\\boldsymbol \\lambda &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nBeware of the nonregularity issue! The (\\nabla \\mathbf h(\\bm x))^\\mathrm T is regular at a given \\bm x (the \\bm x is a regular point) if it has a full column rank. Rank-deficiency reveals a defect in formulation.\n\nExample 1 (Equality-constrained quadratic program) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} &\\quad \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x}\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x + \\mathbf b = \\mathbf 0.\n\\end{aligned}\n\nThe first-order necessary condition of optimality is\n\n\\begin{bmatrix}\n  \\mathbf Q & \\mathbf A^\\top\\\\\\mathbf A & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\bm x \\\\ \\boldsymbol \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  -\\mathbf r\\\\\\mathbf b\n\\end{bmatrix}.\n\n\n\n\n\n\nUsing the unconstrained Hessian \\nabla^2_{\\mathbf{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda) is too conservative. Instead, use projected Hessian\n\n\\mathbf{Z}^\\mathrm{T}\\;\\nabla^2_{\\bm{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda)\\;\\mathbf Z &gt; 0,\n where \\mathbf Z is an (orthonormal) basis of the nullspace of the Jacobian (\\nabla \\mathbf h(\\bm x))^\\top.",
    "crumbs": [
      "1. Optimization – theory",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#inequality-constraints",
    "href": "opt_theory_constrained.html#inequality-constraints",
    "title": "Theory for constrained optimization",
    "section": "Inequality constraints",
    "text": "Inequality constraints\n\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf g(\\bm x) \\leq \\mathbf 0,\n\\end{aligned}\n where \\mathbf g(\\bm x) \\in \\mathbb R^p defines a set of p inequalities.\n\nFirst-order necessary condition of optimality\nKarush-Kuhn-Tucker (KKT) conditions of optimality are then composed of these four (sets of) conditions \n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^p \\mu_i \\nabla g_i(\\bm x) &= \\mathbf 0,\\\\\n\\mathbf{g}(\\bm{x}) &\\leq \\mathbf 0,\\\\\n\\mu_i g_i(\\bm x) &= 0,\\quad i = 1,2,\\ldots, m\\\\\n\\mu_i &\\geq 0,\\quad   i = 1,2,\\ldots, m.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#equality-and-inequality-constraints",
    "href": "opt_theory_constrained.html#equality-and-inequality-constraints",
    "title": "Theory for constrained optimization",
    "section": "Equality and inequality constraints",
    "text": "Equality and inequality constraints\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\\\\\n                    &\\quad \\mathbf g(\\bm x) \\leq \\mathbf 0.\n\\end{aligned}\n\n\nFirst-order necessary condition of optimality\nThe KKT conditions\n\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) + \\sum_{i=1}^p \\mu_i \\nabla g_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\mathbf{x}) &= \\mathbf 0\\\\\n\\mathbf{g}(\\mathbf{x}) &\\leq \\mathbf 0\\\\\n\\mu_i g_i(\\bm x) &= 0,\\quad i = 1,\\ldots, m\\\\\n\\mu_i &\\geq 0,\\quad   i = 1,\\ldots, m.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#duality",
    "href": "opt_theory_constrained.html#duality",
    "title": "Theory for constrained optimization",
    "section": "Duality",
    "text": "Duality\nDuality theory offers another view of the original optimization problem by bringing in another but related one.\nCorresponding to the general optimization problem \n  \\begin{aligned}\n  \\operatorname*{minimize}\\;&f(\\bm x)\\\\\n  \\text{subject to}\\; & \\mathbf g(\\bm x)\\leq \\mathbf 0\\\\\n  & \\mathbf h(\\bm x) = \\mathbf 0,\n  \\end{aligned}\n\nwe form the Lagrangian function \\mathcal L(\\bm x,\\bm \\lambda,\\bm \\mu) = f(\\bm x) + \\bm \\lambda^\\top \\mathbf h(\\bm x) + \\bm \\mu^\\top \\mathbf g(\\bm x)\nFor any (fixed) values of (\\bm \\lambda,\\bm \\mu) such that \\bm \\mu\\geq 0, we define the Lagrange dual function through the following unconstrained optimization problem \nq(\\bm\\lambda,\\bm\\mu) = \\inf_{\\bm x}\\mathcal L(\\bm x,\\bm \\lambda,\\bm \\mu).\n\nObviously, it is alway possible to pick a feasible solution \\bm x, in which case the value of the Lagrangian and the original function coincide, and so the result of this minimization is no worse (larger) than the minimum for the original optimization problem. It can thus serve as a lower bound q(\\bm \\lambda,\\bm \\mu) \\leq f(\\bm x^\\star).\nThis result is called weak duality. A natural idea is to find the values of \\bm \\lambda and \\bm \\mu such that this lower bound is tightest, that is, \n\\begin{aligned}\n  \\operatorname*{maximize}_{\\bm\\lambda, \\bm\\mu}\\; & q(\\bm\\lambda,\\bm\\mu)\\\\\n  \\text{subject to}\\;& \\bm\\mu \\geq \\mathbf 0.\n\\end{aligned}\n\nUnder some circumstances the result can be tight, which leads to strong duality, which means that the minimum of the original (primal) problem and the maximum of the dual problem coincide. \nq(\\bm \\lambda^\\star,\\bm \\mu^\\star) = f(\\bm x^\\star).\n\nThis related dual optimization problem can have some advantages for development of both theory and algorithms.",
    "crumbs": [
      "1. Optimization – theory",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "B(E)3M35ORR – Optimal and Robust Control",
    "section": "",
    "text": "This website constitutes the (work-in-progress) lecture notes for the graduate course Optimal and Robust Control (B3M35ORR, BE3M35ORR) taught within Cybernetics and Robotics graduate program at Faculty of Electrical Engineering, Czech Technical University in Prague, Czechia.\nEach chapter corresponds to a single weekly block (covered by a lecture, a seminar/exercise, and some homework), hence 14 chapters in total.\nAlthough most of the material needed for the course is here, occassionally we may refer the officially enrolled students to the course page within the FEL Moodle system for some other material.\n\n\n\n Back to top"
  },
  {
    "objectID": "dynamic_programming_LQR.html",
    "href": "dynamic_programming_LQR.html",
    "title": "Solving LQR via dynamic programming",
    "section": "",
    "text": "In the previous section we have used dynamic programming as a numerical algorithm for solving a general discrete-time optimal control problem. We now show how to use dynamic programming to solve the discrete-time LQR problem. We consider a linear discrete-time system modelled by \n\\bm x_{k+1} = \\mathbf A\\bm x_k + \\mathbf B\\bm u_k,\n for which we want to minimize the quadratic cost given by \nJ_0(\\bm x_0, \\bm u_0, \\bm u_1, \\ldots, \\bm u_{N-1}) = \\frac{1}{2}\\bm x_N^\\top \\mathbf S_N \\bm x_N + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left(\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k\\right),\n with \\mathbf S_N\\succeq 0, \\mathbf Q\\succeq 0, \\mathbf R\\succ 0, as usual.\nWe now invoke the principle of optimality, that is, we start at the end of the time interval and find the optimal cost \nJ_N^\\star(\\bm x_N) = \\frac{1}{2}\\bm x_N^\\top \\mathbf S_N \\bm x_N.\n\nActually, we have nothing to minimize here, we just evaluate the cost. We then proceed backwards in time, that is, we decrease the time to k=N-1. Here we do have something to minimize: \nJ^\\star_{N-1}(\\bm x_{N-1}) = \\min_{\\bm u_{N-1}\\in\\mathbb R^m} J_{N-1}(\\bm x_{N-1},\\bm u_{N-1}) = \\min_{\\bm u_{N-1}\\in\\mathbb R^m} \\left[L(\\bm x_{N-1},\\bm u_{N-1}) + J^\\star_{N}(\\bm x_{N}) \\right].\n\nWe now expand the expression for the cost\n\n\\begin{aligned}\nJ_{N-1}(\\bm x_{N-1},\\bm u_{N-1}) &= \\frac{1}{2} \\left(\\bm x_{N-1}^\\top \\mathbf Q \\bm x_{N-1} + \\bm u_{N-1}^\\top \\mathbf R \\bm u_{N-1} \\right) + J^\\star_{N}(\\bm x_{N}) \\\\\n&= \\frac{1}{2} \\left(\\bm x_{N-1}^\\top \\mathbf Q \\bm x_{N-1} + \\bm u_{N-1}^\\top \\mathbf R \\bm u_{N-1} \\right) + \\frac{1}{2}\\mathbf x_N^\\top \\mathbf S_N \\mathbf x_N\\\\\n&= \\frac{1}{2} \\left( \\bm x_{N-1}^\\top \\mathbf Q \\bm x_{N-1} + \\bm u_{N-1}^\\top \\mathbf R \\bm u_{N-1} + \\mathbf x_N^\\top \\mathbf S_N \\mathbf x_N \\right)\\\\\n&= \\frac{1}{2} \\left[ \\bm x_{N-1}^\\top \\mathbf Q \\bm x_{N-1} + \\bm u_{N-1}^\\top \\mathbf R \\bm u_{N-1} + (\\bm x_{N-1}^\\top \\mathbf A^\\top + \\bm u_{N-1}^\\top \\mathbf B^\\top) \\mathbf S_N (\\mathbf A\\bm x_{N-1} + \\mathbf B\\bm u_{N-1}) \\right]\\\\\n&= \\frac{1}{2} \\left[\\bm x_{N-1}^\\top (\\mathbf Q  + \\mathbf A^\\top\\mathbf S_N \\mathbf A)\\bm x_{N-1} + 2\\mathbf x^\\top_{N-1}\\mathbf A ^\\top \\mathbf S_N \\mathbf B  \\bm u_{N-1} + \\mathbf u^\\top_{N-1}(\\mathbf R + \\mathbf B^\\top \\mathbf S_n \\mathbf B)\\bm u_{N-1} \\right].\n\\end{aligned}\n\nSince we assumed no constraint on \\bm u_{N-1}, finding the minimum of J_{N-1} is as easy as setting its gradient to zero \n\\mathbf 0 = \\nabla_{\\bm u_{N-1}} J_{N-1} = (\\mathbf R + \\mathbf B^\\top \\mathbf S_n \\mathbf B)\\bm u_{N-1} + \\mathbf B^\\top \\mathbf S_N\\mathbf A\\bm x_{N-1},\n which leads to \n\\bm u_{N-1}^\\star = -\\underbrace{(\\mathbf B^\\top \\mathbf S_N\\mathbf B + \\mathbf R)^{-1}\\mathbf B^\\top \\mathbf S_N \\mathbf A}_{\\mathbf K_{N-1}} \\bm x_{N-1},\n which amounts to solving a system of linear equations. We can also recognize the Kalman gain matrix \\mathbf K_{N-1}, which we derived using the indirect approach in the previous chapter.\nThe optimal cost J^\\star_{N-1} can be obtained by substituting \\bm u_{N-1}^\\star into J_{N-1} \nJ_{N-1}^\\star = \\frac{1}{2}\\bm x_{N-1}^\\top \\underbrace{\\left[(\\mathbf A-\\mathbf B\\mathbf K_{N-1})^\\top \\mathbf S_N(\\mathbf A-\\mathbf B\\mathbf K_{N-1}) + \\mathbf K_{N-1}^\\top \\mathbf R \\mathbf K_{N-1} + \\mathbf Q\\right]}_{\\mathbf S_{N-1}} \\bm x_{N-1}.\n\nNote that the optimal cost J^\\star_{N-1} is also a quadratic function of the state as is the cost J^\\star_{N}. We denote the matrix that defines this quadratic function as \\mathbf S_{N-1}. We do this in anticipation of continuation of this procedure to k = N-2, N-3, \\ldots, which will give \\mathbf S_{N-2}, \\mathbf S_{N-3}, \\ldots. The rest of the story is quite predictable, isn’t it? Applying the Bellman’s principle of optimality we (re)discovered the discrete-time Riccati equation in the Joseph stabilized form \n\\mathbf S_k = (\\mathbf A-\\mathbf B\\mathbf K_{N-1})^\\top \\mathbf S_N(\\mathbf A-\\mathbf B\\mathbf K_{N-1}) + \\mathbf K_{N-1}^\\top \\mathbf R \\mathbf K_{N-1} + \\mathbf Q,\n together with the prescription for the state feedback (Kalman) gain \n\\mathbf K_{k} = (\\mathbf B^\\top \\mathbf S_N\\mathbf B + \\mathbf R)^{-1}\\mathbf B^\\top \\mathbf S_N \\mathbf A.\n\n\n\n\n Back to top",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Solving LQR via DP"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html",
    "href": "opt_algo_constrained.html",
    "title": "Algorithms for constrained optimization",
    "section": "",
    "text": "We keep adhering to our previous decision to focus on the algorithms that use derivatives. But even then the number of derivative-based algorithms for constrained optimization – considering both equality and inequality constraints – is huge. They can be classified in many ways. Here we choose the very pragmatic point of view of the immediate use within our course, and within the discipline of optimal control in general. It is certainly a bit narrow point of view, but it will get us going… In this viewpoint we admit inspiration by the overview paper [1]. And there is a wealth of literature providing a more rigorous classification, which we give references to.\nThere are essentially two types of optimization problems (aka mathematical programms) that dominate the discipline of optimal control:\nWe will therefore focus our discussion of methods to these two.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#quadratic-programming",
    "href": "opt_algo_constrained.html#quadratic-programming",
    "title": "Algorithms for constrained optimization",
    "section": "Quadratic programming",
    "text": "Quadratic programming\nWe consider the problem\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} &\\quad \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{c}^\\top\\bm{x}\\\\\n\\text{subject to} &\\quad \\mathbf A_\\text{eq} \\bm x = \\mathbf b_\\text{eq},\\\\\n&\\quad \\mathbf A_\\text{ineq} \\bm x \\leq \\mathbf b_\\text{ineq}.\n\\end{aligned}\n\n\nActive set methods\n#TODO\n\n\nInterior point methods\n#TODO\n\n\nFirst-order methods\nThe first-order methods (the methods using only the first derivatives) seem to be going through some rennaissance in the last two decades or so. Being computationally simpler than their higher-order counterparts (such as the Newton’s and Quasi-Newton methods), and enhanced with some clever acceleration modifications, they are the methods of choice in machine learning applications, where the size of the problems (the number of variables and the number of equations and inequalities) can easily reach millions and more. Although in optimal control we are typically not encountering optimization problems of this size, we can still benefit from the trend. While the size of the optimization problem can be medium or even small (a few dozens of variables and constraints), the time budget for its solution can be extremely small (easily bellow a millisecond, or even down to a few tens of microseconds). Furthermore, such optimization may be performed on some embedded hardware with limited resource. Computational simplicity of first-order methods (they do not rely on more advance linear algebra computations such as matrix decompositions) makes them particularly suited for these applications.\n\nProjected gradient method\nOne way to extend the standard gradient method to constraint optimization is to combine it with a suitable projection operator. The idea behind the algorithm is that after the standard gradient descent update, a projection onto the feasible set is performed.\nCommonly, an orthogonal projection is used, which is defined as \nP_\\mathcal{C}(x) \\coloneqq \\arg\\min_{\\bm y\\in\\mathcal{C}} \\|\\bm y - \\bm x\\|_2.\n\nFor a general set \\mathcal C, the projection can be computationaly expensive. But for some simple yet useful sets, the projection is trivial. The prominent example is a box (a multidimensional interval):\n\nfunction projection_on_box!(x,xₗ,xᵤ)\n    for i=1:length(x)\n        if x[i] &lt; xₗ[i]\n            x[i] = xₗ[i]\n        elseif x[i] &gt; xᵤ[i]\n            x[i] = xᵤ[i]\n        end\n    end\nend\n\nprojection_on_box! (generic function with 1 method)\n\n\nIn our implementation of the algorithm we use a fixed step lenght based on the maximum curvature of the Hessian.\n\n\nShow the code\nusing LinearAlgebra\n\nfunction projected_gradient_quadratic(Q,c,xₗ,xᵤ,x₀,ϵ,N)\n    x = x₀                           # initializing x\n    f(x) = 1/2*dot(x,Q,x)+dot(x,c)\n    ∇f(x) = Q*x+c                    # defining the gradient\n    L = maximum(diag(Q,0))           # maximum curvature (here we assume just a diagonal Q, otherwise max(eigvals))\n    α = 1/L                          # step length\n    k = 0\n    d = 1+ϵ                          # initial value of the distance between two solutions (epsilon plus whatever)\n    while (norm(d) &gt; ϵ/L)\n        xold = x\n        x = x - α*∇f(x)              # the step in the descent direction\n        projection_on_box!(x,xₗ,xᵤ)  # the projection of the descent step on the box\n        d = x-xold                   # the current step (after the projection)\n        k = k+1\n        if k &gt;= N\n         return f(x),x\n        end\n    end\n    return f(x),x\nend\n\n\nprojected_gradient_quadratic (generic function with 1 method)\n\n\n\n\nShow the code\nx₀ = [1.5, 1.5]     # the initial vector\n\nxₗ = [0.0, 0.0]     # the lower bound\nxᵤ = [2.0, 2.0]     # the upper bound\n\nQ = [1 0; 0 3]      # the positive definite matrix defining the quadratic form\nc = [1; 2]          # the vector defining the linear part\n\nϵ  = 1e-5           # the tolerance\nN  = 100;           # the maximum number of steps\n\n\n\n\nShow the code\nf_opt,x_opt = projected_gradient_quadratic(Q,c,xₗ,xᵤ,x₀,ϵ,N)\n\n\n(0.0, [0.0, 0.0])\n\n\nBelow we also give a bit more “decorated” version that produces the sequence of solutions that we can also plot.\n\n\nShow the code\nusing Printf\n\nfunction projected_gradient_quadratic(Q,c,xₗ,xᵤ,x₀,ϵ,N)\n    x = x₀                    # initializing x\n    X = x                     # the vector of vectors that will be output\n    f(x) = 1/2*dot(x,Q,x)+dot(x,c)\n    fx = f(x)\n    F = [fx,]\n    ∇f(x) = Q*x+c              # building the gradient\n    gx = ∇f(x)\n    L = maximum(diag(Q,0))    # maximum curvature (here I assume just diagonal Q, otherwise max(eigvals))\n    α = 1/L                   # step length\n    #α = 1/5                  # just to explore the behaviour when the step is longer or shorter than to the boundary\n    k = 0\n    d = 1\n    while (norm(d) &gt; ϵ/L)\n        k = k+1\n        xold = x\n        x = x - α*gx          # step in the descent direction\n        projection_on_box!(x,xₗ,xᵤ)\n        d = x-xold\n        @printf(\"iter = %3d   ||∇f(x)|| = %6.4e   f(x) = %6.4e\\n\",k,norm(gx),fx)\n        gx = ∇f(x)\n        fx = f(x)\n        X = hcat(X,x)\n        push!(F,fx)\n        if k &gt;= N\n         return F,X\n        end\n    end\n    return F,X\nend\n\n\nprojected_gradient_quadratic (generic function with 1 method)\n\n\n\nF,X = projected_gradient_quadratic(Q,c,xₗ,xᵤ,x₀,ϵ,N)\n\niter =   1   ||∇f(x)|| = 6.9642e+00   f(x) = 9.0000e+00\niter =   2   ||∇f(x)|| = 2.6034e+00   f(x) = 8.8889e-01\niter =   3   ||∇f(x)|| = 2.2879e+00   f(x) = 1.1728e-01\niter =   4   ||∇f(x)|| = 2.2361e+00   f(x) = 0.0000e+00\n\n\n([9.0, 0.8888888888888891, 0.117283950617284, 0.0, 0.0], [1.5 0.6666666666666667 … 0.0 0.0; 1.5 0.0 … 0.0 0.0])\n\n\n\n\nShow the code\nx1_grid = x2_grid = -2:0.01:4;\nf(x) = 1/2*dot(x,Q,x)+dot(x,c)\nz_grid = [f([x1,x2]) for x2=x2_grid, x1=x1_grid];\n\nxs = -Q\\c           # the stationary point of the unconstrained problem \n\nusing Plots\nplot(Shape([(2,2),(2,0),(0,0),(0,2),(2,2)]),opacity=0.2,label=\"bounds\")\ncontour!(x1_grid,x2_grid,z_grid)\nplot!(X[1,:],X[2,:],label=\"xₖ\",marker=:diamond,aspect_ratio=1)\nscatter!([x₀[1],],[x₀[2],],label=\"x₀\")\nscatter!([xs[1],],[xs[2],],label=\"x⋆ unconstrained\")\nxlabel!(\"x₁\");ylabel!(\"x₂\")\n#xlims!(-4,4); ylims!(-4,4)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSplitting methods\n#TODO",
    "crumbs": [
      "2. Optimization – algorithms",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#nonlinear-programming",
    "href": "opt_algo_constrained.html#nonlinear-programming",
    "title": "Algorithms for constrained optimization",
    "section": "Nonlinear programming",
    "text": "Nonlinear programming\n\nSequential quadratic programming (SQP)\nKKT conditions for a nonlinear program with equality constraints solved by Newton’s method. Interpretation: at each iteration, we solve a quadratic program (QP) with linear constraints.\n\n\nInterior point methods\n#TODO",
    "crumbs": [
      "2. Optimization – algorithms",
      "Constrained optimization"
    ]
  },
  {
    "objectID": "discr_indir_general.html",
    "href": "discr_indir_general.html",
    "title": "General nonlinear discrete-time optimal control as a two-point boundary value problem",
    "section": "",
    "text": "While in the previous chapter we formulated an optimal control problem (OCP) directly as a mathematical programming (general NLP or even QP) problem over the control (and possibly state) trajectories, in this chapter we introduce an alternative – indirect – approach. The essence of the approach is that we formulate first-order necessary conditions of optimality for the OCP in the form of equations, and then solve these. Although less straightforward to extend with additional constraints than the direct approach, the indirect approach also exhibits some advantages. In particular, in some cases (such as a quadratic cost and a linear system) it yields a feedback control law and not just a control trajectory.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "discr_indir_general.html#optimization-constrains-given-only-by-the-state-equations",
    "href": "discr_indir_general.html#optimization-constrains-given-only-by-the-state-equations",
    "title": "General nonlinear discrete-time optimal control as a two-point boundary value problem",
    "section": "Optimization constrains given only by the state equations",
    "text": "Optimization constrains given only by the state equations\nAs in the chapter on the direct approach, here we also start by considering a general nonlinear and possibly time-varying discrete-time dynamical system characterized by the state vector \\bm x_k\\in\\mathbb R^n whose evolution in discrete time k is uniquely determined by the state equation \n\\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\n accompanied by the initial state (vector) \\bm x_i\\in\\mathbb R^n and a sequence of control inputs \\bm u_i, \\bm u_{i+1}, \\ldots, \\bm u_{k-1}, where the control variable can also be a vector, that is, \\bm u_k \\in \\mathbb R^m.\nThese state equations will constitute the only constraints of the optimization problem. Unlike in the direct approach, here in our introductory treatment we do not impose any inequality constraints such as bounds on the control inputs, because the theory to be presented is not able to handle them.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "discr_indir_general.html#general-additive-cost-function",
    "href": "discr_indir_general.html#general-additive-cost-function",
    "title": "General nonlinear discrete-time optimal control as a two-point boundary value problem",
    "section": "General additive cost function",
    "text": "General additive cost function\nFor the above described dynamical system we want to find a control sequence \\bm u_k that minimizes a suitable optimization criterion over a finite horizon k\\in[i,N]. Namely, we will look for a control that minimizes a criterion of the following kind \nJ_i^N(\\underbrace{\\bm x_{i+1}, \\bm x_{i+2}, \\ldots, \\bm x_{N}}_{\\bar{\\bm x}}, \\underbrace{\\bm u_{i}, \\ldots, \\bm u_{N-1}}_{\\bar{\\bm u}};\\bm x_i) = \\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k).\n\\tag{1}\n\n\n\n\n\n\nNote\n\n\n\nRegarding the notation J_i^N(\\cdot) for the cost, if the initial and final times are understood from the context, they do not have to be displayed. But we will soon need to indicate the initial time explicitly in our derivations.\n\n\nThe property of the presented cost function that will turn out crucial in our subsequent work is that is additive over the time horizon. Although this restricts the class of cost functions a bit, it is still general enough to encompass a wide range of problems, such as minimizing the total (financial) cost to be paid, the total energy to be used, the total distance to be travelled, the cumulative error to be minimized, etc.\nHere is a list of a few popular cost functions.\n\nMinimum-time (or time-optimal) problem\n\nSetting \\phi=1 and L_k=1 gives J=N-i, that is, the length of the time horizon, the duration of control. Altough in this course we do not introduce concepts and tools for optimization over integer variables, in this simple case of just a single integer variable even a simple search over the length of control interval will be computationally tractable. Furthermore, as we will see in one of the next chapters once we switch from discrete-time to continuous-time systems, this time-optimal control design problem will turn out tractable using the tools presented in this course.\n\nMinimum-fuel problem\n\nSetting \\phi=0 and L_k=|u_k|, which gives J=\\sum_{k=i}^{N-1}|u_k|.\n\nMinimum-energy problem\n\nSetting \\phi=0 and L_k=\\frac{1}{2} u_k^2, which gives J=\\frac{1}{2} \\sum_{k=i}^{N-1} u_k^2. It is fair to admit that this sum of squared inputs cannot always be interpretted as the energy – for instance, what if the control input is a degree of openning of a valve? Sum of angles over time can hardly be interpreted as energy. Instead, it should be interpretted in the mathematical way as the (squared) norm, that is, a “size” of the input. Note that the same objection can be given to the previous case of a minimum-fuel problem.\n\nMixed quadratic problem (also LQ-optimal control problem)\n\nSetting \\phi=\\frac{1}{2}s_N x_N^2 and L_k=\\frac{1}{2} (qx_k^2+ru_k^2),\\, q,r\\geq 0, which gives J=\\frac{1}{2}s_Nx_N^2+\\frac{1}{2} \\sum_{k=i}^{N-1} (r x_k^2+q u_k^2). Or in the case of vector state and control variables J=\\frac{1}{2}\\bm x_N^\\top \\mathbf S_N \\bm x_N+\\frac{1}{2} \\sum_{k=i}^{N-1} (\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k), \\, \\mathbf Q, \\mathbf R \\succeq 0. This type of an optimization cost is particularly popular. Both for the mathematical reasons (we all now appreciate the nice properties of quadratic functions) and for practical engineering reasons as it allows us to capture a trade-off between the control performance (penalty on \\bm x_k) and control effort (penalty on \\bm u_k). Note also that the state at the terminal time N is penalized separately just in order to allow another trade-off between the transient and terminal behavior. The cost function can also be modified to penalize deviation of the state from some nonzero desired (aka reference) state trajectory, that is J=\\frac{1}{2}(\\bm x_N - \\bm x_N^\\text{ref})^\\top \\mathbf S_N (\\bm x_N - \\bm x_N^\\text{ref}) +\\frac{1}{2} \\sum_{k=i}^{N-1} \\left((\\bm x_k - \\bm x_k^\\text{ref})^\\top \\mathbf Q (\\bm x_k - \\bm x_k^\\text{ref}) + \\bm u_k^\\top \\mathbf R \\bm u_k\\right).\n\n\nNote that in none of these cost function did we include \\bm u_{N} as an optimization variables as it has no influence over the interval [i,N].\nNeedless to emphasize, as in some other applications maximizing may seem more appropriate (such as maximizing the yield, bandwidth or robustness), we can always reformulate the maximization into minimization. Therefore in our course we always formulate the optimal control problems as minimization problems.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "discr_indir_general.html#derivation-of-the-first-order-necessary-conditions-of-optimality",
    "href": "discr_indir_general.html#derivation-of-the-first-order-necessary-conditions-of-optimality",
    "title": "General nonlinear discrete-time optimal control as a two-point boundary value problem",
    "section": "Derivation of the first-order necessary conditions of optimality",
    "text": "Derivation of the first-order necessary conditions of optimality\nHaving formulated a finite-dimensional constrained nonlinear optimization problem, we avoid the temptation to call an NLP solver to solve it numerically, and proceed instead with our own analysis of the problem. Let’s see how far we can get. By introducing Lagrange multipliers {\\color{blue}\\bm\\lambda_k}, we turn the constrained problem into an unconstrained one. The new cost function (we use the prime to distinguish it from the original cost) is \n\\begin{aligned}\n& {J'}_i^N(\\bm x_i, \\ldots, \\bm x_N, \\bm u_i, \\ldots, \\bm u_{N-1},{\\color{blue}\\bm \\lambda_i, \\ldots, \\bm \\lambda_{N-1}}) \\\\\n&\\qquad\\qquad\\qquad = \\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1}\\left[L_{k}(\\bm x_k,\\bm u_k)+\\bm {\\color{blue}\\lambda^\\top_{k}}\\;\\left[\\mathbf f_k(\\bm x_k,\\bm u_k)-\\bm x_{k+1}\\right]\\right].\n\\end{aligned}\n\nFrom now on, in principle, we do not need any guidance here, do we? We are given an unconstrained optimization problem and its solution is just a few moments away. In particular, stationary point(s) must be found (and then we are going to argue if these qualify as minimizers or not). This calls for differentiating the above expression with respect to all the variables and setting these derivatives equal to zeros.\nAlthough the principles are clear, some hindsight might be shared here if compact formulas are to be found. First such advice is to rename the variable(s) {\\color{blue}\\boldsymbol \\lambda_k} to {\\color{red}\\boldsymbol \\lambda_{k+1}} \n\\begin{aligned}\n& {J'}_i^N(\\bm x_i, \\ldots, \\bm x_N, \\bm u_i, \\ldots, \\bm u_{N-1},{\\color{red}\\bm \\lambda_{i+1}, \\ldots, \\bm \\lambda_{N}}) \\\\\n& \\qquad\\qquad\\qquad = \\phi(N,\\bm x_N) + \\sum_{k=i}^{N-1}\\left[L_{k}(\\bm x_k,\\bm u_k)+\\boldsymbol {\\color{red}\\boldsymbol \\lambda^\\top_{k+1}}\\; \\left[\\mathbf f_k(\\bm x_k,\\bm u_k)-\\mathbf x_{k+1}\\right]\\right].\n\\end{aligned}\n\nThis is really just a notational decision but thanks to it our resulting formulas will enjoy some symmetry.\n\n\n\n\n\n\nNote\n\n\n\nMaybe it would be more didactic to leave you to go on without this advice on notation, and only then to nudge you to figure out this remedy by yourself. But admittedly this is not the kind of competence that we aim at in this course. Let’s spend time with more rewarding things.\n\n\nAnother notational advice – but this one is more systematic and fundamental — is to make the above expression a bit shorter by introducing a new variable defined as \\boxed{H_k(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1}) = L_{k}(\\bm x_k,\\bm u_k)+\\boldsymbol \\lambda_{k+1}^\\top \\; \\mathbf f_k(\\bm x_k,\\bm u_k).}\n\nWe will call this new function Hamiltonian. Indeed, the choice of this name is motivated by the analogy with the equally named concept used in physics and theoretical mechanics, but we will only make more references to this analogy later in the course once we transition to continuous-time systems modelled by differential equations.\nIntroducing the Hamiltonian reformulates the cost function (and we omit the explicit dependence on all its input arguments) as \n{J'}_i^N = \\phi(N,\\bm x_N) + \\sum_{k=i}^{N-1}\\left[H_{k}(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1})-\\boldsymbol\\lambda^\\top_{k+1}\\;\\mathbf x_{k+1}\\right].\n\nThe final polishing of the expression before starting to compute the derivatives consists in bringing together the terms that contain related variables: the state \\bm x_N at the final time, the state \\bm x_i at the initial time, and the states, controls and Lagrange multipliers in the transient period\n\n{J'}_i^N = \\underbrace{\\phi(N,\\bm x_N) -\\boldsymbol\\lambda^\\top_{N}\\;\\mathbf x_{N}}_\\text{at terminal time} + \\underbrace{H_i(\\bm x_i,\\mathbf u_i,\\boldsymbol\\lambda_{i+1})}_\\text{at initial time} + \\sum_{k=i+1}^{N-1}\\left[H_{k}(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1})-\\boldsymbol\\lambda^\\top_{k}\\;\\mathbf x_{k}\\right].\n\nAlthough this step was not necessary, it will make things a bit more convenient once we start looking for the derivatives. And the time for it has just come.\nRecall now the recommended procedure for finding derivatives of functions of vectors: find the differential instead, and identify the derivative within the result. The gradient is then (by convention) obtained as the transpose of the derivative. Following this derivative-identification procedure, we anticipate the differential of the augmented cost function in the following form \n\\begin{split}\n\\text{d}{J'}_i^N &= (\\qquad)^\\top \\; \\text{d}\\bm x_N + (\\qquad)^\\top \\; \\text{d}\\bm x_i \\\\&+ \\sum_{k=i+1}^{N-1}(\\qquad)^\\top \\; \\text{d}\\bm x_k + \\sum_{k=i}^{N-1}(\\qquad)^\\top \\; \\text{d}\\bm u_k + \\sum_{k=i+1}^{N}(\\qquad)^\\top \\; \\text{d}\\boldsymbol \\lambda_k.\n\\end{split}\n\nIdentifying the gradients amounts to filling in the empty brackets. It straightforward if tedious (in particular the lower and upper bounds on the summation indices must be carefuly checked). The solution is \n\\begin{split}\n\\text{d}{J'}_i^N &= \\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\top \\; \\text{d}\\bm x_N + \\left(\\nabla_{\\bm x_i}H_i\\right)^\\top \\; \\text{d}\\bm x_i \\\\&+ \\sum_{k=i+1}^{N-1}\\left(\\nabla_{\\bm x_k}H_k-\\boldsymbol\\lambda_k\\right)^\\top \\; \\text{d}\\bm x_k + \\sum_{k=i}^{N-1}\\left(\\nabla_{\\bm u_k}H_k\\right)^\\top \\; \\text{d}\\bm u_k + \\sum_{k=i+1}^{N}\\left(\\nabla_{\\boldsymbol \\lambda_k}H_{k-1}-\\bm x_k\\right)^\\top \\; \\text{d}\\boldsymbol \\lambda_k.\n\\end{split}\n\nThe ultimate goal of this derivation was to find stationary points for the augmented cost function, that is, to find conditions under which \\text{d}{J'}_i^N=0. In typical optimization problems, the optimization is conducted with respect to all the participating variables, which means that the corresponding differentials may be arbitrary and the only way to guarantee that the total differential of J_i' is zeros is to make the associated gradients (the contents of the brackets) equal to zero. There are two exceptions to this rule in our case, though:\n\nThe state at the initial time is typically fixed and not available for optimization. Then \\text{d}\\bm x_i=0 and the corresponding necessary condition is replaced by the statement that \\bm x_i is equal to some particular value, say, \\bm x_i = \\mathbf x^\\text{init}. We have already discussed this before. In fact, in these situations we might even prefer to reflect it by the notation J_i^N(\\ldots;\\bm x_i), which emphasizes that \\bm x_i is a parameter and not a variable. But in the solution below we do allow for the possibility that \\bm x_i is a variable too (hence \\text{d}\\bm x_i\\neq 0) for completeness.\nThe state at the final time may also be given/fixed, in which case the corresponding condition is replaced by the statement that \\bm x_N is equal to some particular value, say, \\bm x_N = \\mathbf x^\\text{ref}. But if it is not the case, then the final state is also subject to optimization and the corresponding necessary condition of optimality is obtained by setting the content of the corresponding brackets to zero.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "discr_indir_general.html#necessary-conditions-of-optimality-as-two-point-boundary-value-problem-tp-bvp",
    "href": "discr_indir_general.html#necessary-conditions-of-optimality-as-two-point-boundary-value-problem-tp-bvp",
    "title": "General nonlinear discrete-time optimal control as a two-point boundary value problem",
    "section": "Necessary conditions of optimality as two-point boundary value problem (TP-BVP)",
    "text": "Necessary conditions of optimality as two-point boundary value problem (TP-BVP)\nThe ultimate form of the first-order necessary conditions of optimality, which incorporates the special cases discussed above, is given by these equations \n\\boxed{\n\\begin{aligned}\n\\mathbf x_{k+1} &= \\nabla_{\\boldsymbol\\lambda_{k+1}}H_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1},\\\\\n\\boldsymbol\\lambda_k &= \\nabla_{\\bm x_k}H_k, \\;\\;\\; \\color{gray}{k=i+1,\\ldots, N-1}\\\\\n0 &=  \\nabla_{\\bm u_k}H_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1}\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\top \\mathrm{d}\\bm x_N},\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_i}H_i\\right)^\\top \\mathrm{d}\\bm x_i},\n\\end{aligned}\n}\n or more explicitly \n\\boxed{\n\\begin{aligned}\n\\mathbf x_{k+1} &= \\mathbf f_k(\\bm x_k,\\bm u_k), \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1},\\\\\n\\boldsymbol\\lambda_k &= \\nabla_{\\bm x_k}\\mathbf f_k\\;\\;   \\boldsymbol\\lambda_{\\mathbf k+1}+\\nabla_{\\bm x_k}L_k, \\;\\;\\; \\color{gray}{k=i+1,\\ldots, N-1}\\\\\n0 &=  \\nabla_{\\bm u_k}\\mathbf f_k\\;\\; \\boldsymbol\\lambda_{k+1}+\\nabla_{u_k}L_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1}\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\top \\mathrm{d}\\bm x_N},\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_i}H_i\\right)^\\top \\mathrm{d}\\bm x_i}.\n\\end{aligned}\n}\n\nRecall that since \\mathbf f is a vector function, \\nabla \\mathbf f is not just a gradient but rather a matrix whose columns are gradients of the individual components of the vector \\mathbf f — it is a transpose of Jacobian.\n\n\n\n\n\n\nNote\n\n\n\nThe first three necessary conditions above can be made completely “symmetric” by running the second one from k=i because the \\boldsymbol\\lambda_i introduced this way does not influence the rest of the problem and we could easily live with one useless variable.\n\n\nWe have just derived the (necessary) conditions of optimality in the form of five sets of (vector) equations:\n\nThe first two are recurrent (or discrete-time) equations, which means that they introduce coupling between the variables evaluated at consecutive times. In fact, the former is just the standard state equation that gives the state at one time as a function of the state (and the control) at the previous time. The latter gives a prescription for the variable \\bm \\lambda_k as a function of (among others) the same variable evaluated at the next (!) time, that is, \\bm \\lambda_{k+1}. Although from the optimization perspective these variables play the role of Lagrange multipliers, we call them co-state variables in optimal control theory because of the way they relate to the state equations. The corresponding vector equation is called a co-state equation.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is a crucial property of the co-state equation that the evolution of the co-state variable is dictated backward in time by the equation.\n\n\n\nThe third set of equations are just algebraic equations that relate the control inputs to the state and co-state variables. Sometimes it is called a stationarity equation.\nThe last two are just single (vector) equations related to the end and the beginning of the time horizon. They are both stated in the general enough form that allows the corresponding states to be treated as either fixed or subject to optimization. In particular, if the final state is to be treated as free (subject to optimization), that is, \\mathrm{d}\\bm x_N can be atritrary and the only way the corresponding equation can be satisfied is \\nabla_{\\bm x_N}\\phi=\\lambda_N. If, on the other hand, the final state is to be treated as fixed, the the corresponding equation is just replaced by \\bm x_N = \\mathbf x^\\text{ref}. Similarly for the initial state. But as we have hinted a few times, most often than not the initial state will be regarded as fixed and not subject to optimization, in which case the corresponding equation is replaced by \\bm x_i = \\mathbf x^\\text{init}.\n\nTo summarize, the equations that give the necessary conditions of optimality for a general nonlinear discrete-time optimal control problem form a discrete-time two-point boundary value problem (TP-BVP). Values of some variables are specified at the initial time, values of some (maybe the same or some other) variables are defined at the final time. The equations prescribe the evolution of some variables forward in time while for some other variables the evolution backward in time is dictated.\n\n\n\n\n\n\nNote\n\n\n\nThis is in contrast with the initial value problem (IVP) for state equations, for which we only specify the state at one end of the time horizon — the initial state — and then the state equation disctates the evolution of the (state) variable forward in time.\n\n\nBoundary value problems are more difficult to solve than the initial value problems. Typically we can only solve them numerically, in which case it is appropriate to ask if anything has been gained by this indirect procedure compared with the direct one. After all, we did not even incorporate the inequality constraints in the problem, which was a piece of case in the direct approach. But we will see that in some special cases the TP-BVP they can be solved analytically and the outcome is particularly useful and would never have been discovered, if only the direct approach had been followed. We elaborate on this in the next section.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General finite-horizon nonlinear optimal control as a TP-BVP"
    ]
  },
  {
    "objectID": "discr_indir_DARE.html",
    "href": "discr_indir_DARE.html",
    "title": "Discrete-time algebraic Riccati equation (DARE)",
    "section": "",
    "text": "We have learnt previously that the following matrix equation \n\\bm X=\\mathbf A^\\top\\left[\\bm X-\\bm X\\mathbf B(\\mathbf B^\\top\\bm X\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top\\bm X\\right]\\mathbf A+\\mathbf Q\n or, equivalently, \\boxed{\n\\mathbf A^\\top\\bm X\\mathbf A - \\bm X +\\mathbf Q - \\mathbf A^\\top\\bm X\\mathbf B(\\mathbf B^\\top\\bm X\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top\\bm X\\mathbf A = \\mathbf 0}\n\ncalled discrete-time algebraic Riccati equation (DARE) is instrumental in solving the infinite time horizon LQR problem. The equation must be solved for the matrix \\bm X in order to compute the state feedback gain.\nThe key assumptions are that \\mathbf Q \\succeq 0, \\mathbf R \\succ 0, the pair (\\mathbf A, \\mathbf B) is stabilizable, and the pair (\\mathbf A, \\sqrt{\\mathbf Q}) is detectable. The solution to the DARE is unique, symmetric and positive semidefinite. If, furthermore, the pair (\\mathbf A, \\mathbf Q) is observable, the solution is positive definite.\n\n\n\n\n\n\nThe matrix variable \\bm X\n\n\n\nNote that here we have replaced the previous name for matrix variable \\bm S_\\infty by the new \\bm X to emphasize that it is the unknown here.\n\n\nThere are several approaches to solving the DARE, and the most reliable and accurate once have already been implemented in major computational packages (see the section on software). Some overviews and detailed explanations are provided in [1], [2], and [3]. Here we only sketch one of them, that displays an important property.\nRecall the linear system that we have developed from the two-point boundary value problem under the assumption of invertability of the matrix \\mathbf A:\n\n\\begin{bmatrix}\n\\mathbf x_{k}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\n\\underbrace{\\begin{bmatrix}\n\\mathbf A^{-1} & \\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q\\mathbf A^{-1} & \\mathbf A^\\top+\\mathbf Q\\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\n\\end{bmatrix}}_{\\mathbf H}\n\\begin{bmatrix}\n\\mathbf x_{k+1} \\\\ \\boldsymbol\\lambda_{k+1}\n\\end{bmatrix}\n\nThe matrix H has a very special property. Defining the auxilliary matrix \\mathbf J \n\\mathbf J = \\begin{bmatrix}\\mathbf 0 & \\mathbf I\\\\ -\\mathbf I & \\mathbf 0\\end{bmatrix},\n the matrix \\mathbf H can be shown to satisfy the following: \n\\mathbf H^\\top\\mathbf J\\mathbf H = \\mathbf J.\n\nSuch matrices are called symplectic and they have several special properties. First, note that \n\\mathbf H^\\top\\mathbf J = \\mathbf J\\mathbf H^{-1},\n from which it follows that \n\\mathbf J^{-1}\\mathbf H^\\top\\mathbf J = \\mathbf H^{-1}.\n\nUsing the fact that \\mathbf J^{-1} = -\\mathbf J, we get the promised special property – a particularly simple way to compute the inverse of \\mathbf H: \n\\mathbf H^{-1} = -\\mathbf J\\mathbf H^\\top\\mathbf J.\n\nWe can use this to obtain the inverse of our particular \\mathbf H matrix:\n\n\\mathbf H^{-1}\n=\n\\begin{bmatrix}\n\\mathbf A + \\mathbf B \\mathbf R^{-1} \\mathbf B^\\top \\mathbf A^{-\\top}\\mathbf Q & \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\mathbf A^{-\\top} \\\\\n\\mathbf A^{-\\top} \\mathbf Q & \\mathbf A^{-\\top},\n\\end{bmatrix}\n where we used the shortand notation \\mathbf A^{-\\top} for (\\mathbf A^{-1})^\\top.\nNow, it can be shown that if \\lambda is an eigenvalue of \\mathbf H, so is 1/\\lambda. We are not going to prove it here, we refer an interested reader to [4, p. 81].\n\nExample 1 (Eigenvalues of a symplectic matrix)  \n\n\nShow the code\nusing LinearAlgebra\n\nA = [0.225384  0.166015   0.60408\n    0.920342  0.0644107  0.354692\n    0.483302  0.536062   0.718341]\nB = [0.587251  0.29765\n    0.305953  0.616242  \n    0.400612  0.201951]\n\nq = [1.0, 2.0, 3.0]\nr = [1.0, 2.0]\n\nQ = diagm(0=&gt;q)\nR = diagm(0=&gt;r)\n\nH = [inv(A) A\\B/R*B'; Q/A A'+Q/A*B/R*B']\n\nλ, V = eigen(H)\n\nusing Plots\nscatter(λ, aspect_ratios = 1, label=\"\")\nθ = range(0, 2π, length=100)\nx = cos.(θ)\ny = sin.(θ)\nplot!(x, y, label=\"\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Eigenvalues of the symplectic (discrete-time Hamiltonian) matrix \\mathbf H are symmetric with respect to the unit circle\n\n\n\n\n\nSimilarly, only without providing a proof we state here that if the eigenvectors of \\mathbf H corresponding to the unstable eigenvalue (|\\lambda|&gt;1) are denoted as \\mathbf v_1, \\ldots, \\mathbf v_n, and are stacked together to form a matrix \\mathbf V^\\mathrm{unstable} = \\begin{bmatrix}\\mathbf v_1 & \\ldots & \\mathbf v_n \\end{bmatrix}, and we name the two vertical n\\times n blocks in the matrix \\mathbf V^\\mathrm{unstable} = \\begin{bmatrix} \\mathbf X_1\\\\\\mathbf X_2\\end{bmatrix}, the solution to the DARE can be expressed as \n\\bm X = \\mathbf X_2\\mathbf X_1^{-1}.\n\nThis method of solving the DARE is referred to as the eigenvector method and is regarded as a subset of a broader family of invariant subspace methods. It is not particularly reliable when the matrix \\mathbf H has eigenvalues close to the unit circle. And it also required invertibility of the matrix \\mathbf A in the first place. Still, we wanted to present it here because it displays a fundamental phenomenon often encountered in optimal control – the need to split some eigenvalues, eigenvectors, subspaces into stable and unstable ones.\n\nExample 2 (Solving the DARE using the eigenvector method)  \n\n\nShow the code\nindices_unstable = findall(x -&gt; abs(x) &gt; 1, λ)\nVᵘ = V[:,indices_unstable]\nX₁ = Vᵘ[1:3,:]\nX₂ = Vᵘ[4:6,:]\n\nX = X₂/X₁\nX = real(X) # We get rid of the negligible (≈ 10⁻¹⁶) imaginary parts.\n\n\n3×3 Matrix{Float64}:\n 2.5197    0.499383  0.914329\n 0.499383  2.65859   0.835092\n 0.914329  0.835092  4.30357\n\n\nWe can compare against the solution provided by the specialized package MatrixEquations.jl:\n\n\nShow the code\nusing MatrixEquations\nX_ared, CLSEIG, F = ared(A,B,R,Q);\nX_ared\n\n\n3×3 Matrix{Float64}:\n 2.5197    0.499383  0.914329\n 0.499383  2.65859   0.835092\n 0.914329  0.835092  4.30357\n\n\n\nAs a demonstration of this stable-unstable decomposition phenomenon, we show that the LQR state feedback places the closed loop poles into the stable eigenvalues of the symplectic \\mathbf H matrix. Again we provide no proof here, but an interested reader can find it in [4, Sec. 2.5].\n\nExample 3  \n\n\nShow the code\nusing ControlSystems\n\nK = lqr(Discrete, A,B,Q,R)\np = eigvals(A-B*K)      # closed-loop poles\n\nscatter(λ, aspect_ratios = 1, label=\"Eigenvalues of H\")\nscatter!(p, aspect_ratios = 1, label=\"LQR closed-loop poles\")\n\nθ = range(0, 2π, length=100)\nx = cos.(θ)\ny = sin.(θ)\nplot!(x, y, label=\"\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Eigenvalues of the symplectic (discrete-time Hamiltonian) matrix \\mathbf H and the poles of the state feedback loop closed by the LQR controller\n\n\n\n\n\nThe property demonstrated through the example suggests a naive method for designing an LQR controller – form the discrete-time Hamiltonina matrix \\mathbf H, compute its eigenvalues, select the stable ones, and then use some pole-placement method to design a state feedback gains to places the closed-loop poles into the selected stable eigenvalues. The method is not recommendable, but it provides yet another insight into the LQR problem.\n\n\n\n\n Back to topReferences\n\n[1] V. Sima, Algorithms for Linear-Quadratic Optimization. New York: Chapman and Hall/CRC, 1996.\n\n\n[2] D. A. Bini, B. Iannazzo, and B. Meini, Numerical Solution of Algebraic Riccati Equations. Philadelphia: SIAM-Society for Industrial and Applied Mathematics, 2012.\n\n\n[3] B. Datta, Numerical Methods for Linear Control Systems. Amsterdam; Boston: Academic Press, 2003.\n\n\n[4] F. L. Lewis, D. Vrabie, and V. L. Syrmo, Optimal Control, 3rd ed. John Wiley & Sons, 2012. Accessed: Mar. 09, 2022. [Online]. Available: https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time algebraic Riccati equation (DARE)"
    ]
  },
  {
    "objectID": "opt_algo_references.html",
    "href": "opt_algo_references.html",
    "title": "References",
    "section": "",
    "text": "Pretty much identical to the literature recommended in the previous section on optimization theory.\nSome practical aspects are discussed in Guidelines for Numerical Issues for Gurobi Optimizer.\n\n\n\n Back to top",
    "crumbs": [
      "2. Optimization – algorithms",
      "References"
    ]
  },
  {
    "objectID": "uncertainty.html",
    "href": "uncertainty.html",
    "title": "Uncertainty modelling",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "cont_indir_CARE.html",
    "href": "cont_indir_CARE.html",
    "title": "Continuous-time Riccati equation",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Continuous-time Riccati equation"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html",
    "href": "discr_indir_LQR_inf_horizon.html",
    "title": "Discrete-time LQR on an infinite horizon",
    "section": "",
    "text": "In this section we are going to solve the LQR problem on the time horizon extended to infinity, that is, our goal is to find an infinite (vector) control sequence \\bm u_0, \\bm u_{1},\\ldots, \\bm u_{\\infty} that minimizes \nJ_0^\\infty = \\frac{1}{2}\\sum_{k=0}^{\\infty}\\left[\\bm x_k^\\top \\mathbf Q \\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right],\n where, as before \\mathbf Q = \\mathbf Q^\\top \\succeq 0 and \\mathbf R = \\mathbf R^\\top \\succ 0 and the system is modelled by \n\\bm x_{k+1} = \\mathbf A \\bm x_{k} + \\mathbf B \\bm u_k, \\qquad \\bm x_0 = \\mathbf x_0.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on an infinite horizon"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html#why-the-infinite-time-horizon",
    "href": "discr_indir_LQR_inf_horizon.html#why-the-infinite-time-horizon",
    "title": "Discrete-time LQR on an infinite horizon",
    "section": "Why the infinite time horizon?",
    "text": "Why the infinite time horizon?\nThe first question that must inevitably pop up is the one about the motivation for introducing the infinite time horizon:\n\nDoes the introduction of an infinite time horizon reflect that we do not care about when the controller accomplishes the task?\n\nNo, certainly not. The infinite time horizon is introduced to model the case when the system is expected to operate indefinitely. This is a common scenario in practice, for example, in the case of temperature control in a building.\nSimilarly, the infinite time horizon can be used in the scenarios when the final time is not known and we leave it up to the controller to take as much time as it needs to reach the desired state. But even then we can still express our desire to reach the desired state as soon as possible by choosing the weights \\mathbf Q and \\mathbf R appropriately.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on an infinite horizon"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html#steady-state-solution-to-discrete-time-riccati-equation",
    "href": "discr_indir_LQR_inf_horizon.html#steady-state-solution-to-discrete-time-riccati-equation",
    "title": "Discrete-time LQR on an infinite horizon",
    "section": "Steady-state solution to discrete-time Riccati equation",
    "text": "Steady-state solution to discrete-time Riccati equation\nWe have seen in the previous section that the solution to LQR problem with free final state and finite time horizon is given by a time-varying state feedback control law \\bm u_k = \\mathbf K_k \\bm x_k. The sequence of gains \\mathbf K_k for k=0,\\ldots, N-1, is given by the sequence of matrices \\mathbf S_k for k=0,\\ldots, N, which in turn is given as the solution to the (discrete-time) Riccati equation initialized by the penalty \\mathbf S_N and solved backwards in time. But we have also seen that, at least in our example, provided the time interval was long enough, the sequence \\mathbf K_k and \\mathbf S_k both converged to some steady state values as the time k proceeded backwards towards the beginning of the time interval.\nWhile using these steady-state values instead of the full sequences lead to a suboptimal solution on a finite time horizon, it turns out that it actually gives the optimal solution on an infinite time horizon. Although our argument here may be viewed as rather hand-wavy, it is intuitive — there is no end to the time interval, hence the steady-state values are not given a chance to change “towards the end”, as we observed in the finite time horizon case.\n\n\n\n\n\n\nNote\n\n\n\nOther approaches exist for solving the infinite time horizon LQR problem that do not make any reference to the finite time horizon problem, some of them are very elegant and concise, but here we intentionally stick to viewing it as the extension of the finite time horizon problem.\n\n\n\nNotation\nBefore we proceed with the discussion of how to find the steady-state values (the limits) of \\mathbf S_k and subsequently \\mathbf K_k, we must discuss the notation first. So, while icreasing the time horizon N and the solution to the Riccati equation settles towards the beginning of the time interval. We can thenk pick the steady-state values right at the initial time k=0, that is, \\mathbf S_0 and \\mathbf K_0. But thanks to time invariance, we can also fix the final time to some (arbitrary) N and strech the interval by moving its beginning toward -\\infty. The limits of the sequences \\mathbf S_k and \\mathbf K_k can be then considered at k goes toward -\\infty. It seems appropriate to denote these limits as \\mathbf S_{-\\infty} and \\mathbf K_{-\\infty} then. Well, the fact is that the commonly accepted notation for the limits found in the literature is just \\mathbf S_\\infty and \\mathbf K_\\infty \n\\mathbf S_\\infty \\triangleq \\lim_{k\\rightarrow -\\infty} \\mathbf S_k, \\qquad \\mathbf K_\\infty \\triangleq \\lim_{k\\rightarrow -\\infty} \\mathbf K_k.\n\n\n\nHow to compute the steady-state solution to Riccati equation?\nLeaving aside for the moment the important question whether and under which conditions such a limit \\mathbf S_\\infty exists, the immediate question is how to compute such limit. One straightforward strategy is to run the recurrent scheme (Riccati equation) and generate the sequence \\mathbf S_{N}, \\mathbf S_{N-1}, \\mathbf S_{N-2}, \\ldots so long as there is a nonnegligible improvement, that is, once \\mathbf S_{k}\\approx\\mathbf S_{k+1}, stop iterating. That is certainly doable.\nThere is, however, another idea. We apply the steady-state condition \n\\mathbf S_{\\infty} = \\mathbf S_k=\\mathbf S_{k+1}\n to the Riccati equation. The resulting equation \\boxed{\n\\mathbf S_{\\infty}=\\mathbf A^\\text{T}\\left[\\mathbf S_{\\infty}-\\mathbf S_{\\infty}\\mathbf B(\\mathbf B^\\text{T}\\mathbf S_{\\infty}\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\text{T}\\mathbf S_{\\infty}\\right]\\mathbf A+\\mathbf Q}\n is called discrete-time algebraic Riccati equation (DARE) and it is one of the most important equations in the field of computational control design.\nThe equation may look quite “messy” and offers hardly any insight. Remember the good advice to shring the problem to the scalar size while studying similar matrix-vector expressions and striving to get some insight. Our DARE simplifies to \ns_\\infty = a^2s_\\infty - \\frac{a^2b^2s_\\infty^2}{b^2s_\\infty+r} + q\n\nMultiplying both sides by the denominator we get the equivalent quadratic (in s_\\infty) equation \nb^2s_\\infty^2 + (r - ra^2 - qb^2)s_\\infty - qr = 0.\n\nVoilà! A scalar DARE is just a quadratic equation, for which the solutions can be found readily.\nThere is a caveat here, though, reflected in using plural in “solutions” above. Quadratic equation can have two (or none) real solutions. But the sequence produced by original recursive Riccati equation is determined uniquely! What’s up? How are the solutions to ARE related to the limiting solution of recursive Riccati equation?\nAnswering this question will keep us busy for most of this lecture. We will structure this broad question into several sub-questions\n\nUnder which conditions it is guaranteed that there exists a (bounded) limiting solution \\mathbf S_\\infty to the recursive Riccati equation for all initial (actually final) values \\mathbf S_N?\nUnder which conditions is the limit solution unique for arbitrary \\mathbf S_N?\nUnder which conditions is it guaranteed that the time-invariant feedback gain \\mathbf K_\\infty computed from \\mathbf S_\\infty stabilizes the system (on the infinite control interval)?",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on an infinite horizon"
    ]
  },
  {
    "objectID": "discr_dir_LQR.html",
    "href": "discr_dir_LQR.html",
    "title": "Finite-horizon LQR as a QP",
    "section": "",
    "text": "Here we specialize the general procedure from the previous section to the case of a Linear system and a Quadratic cost. We start by considering a simple problem of Regulation, wherein the goal is to bring the system either exactly or approximately to zero final state, that is, \\mathbf x^\\text{ref}=\\mathbf 0 and we want \\bm x_N=\\mathbf x^\\text{ref} or \\bm x_N\\approx\\mathbf x^\\text{ref}, respectively. The problem is known as the LQR problem. \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_0,\\ldots, \\bm u_{N-1}, \\bm x_{0},\\ldots, \\bm x_N} &\\quad  \\frac{1}{2} \\bm x_N^\\top \\mathbf S \\bm x_N + \\frac{1}{2} \\sum_{k=0}^{N-1} \\left(\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k \\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{k+1} = \\mathbf A\\bm x_k + \\mathbf B\\bm u_k,\\quad k = 0, \\ldots, N-1, \\\\\n                    &\\quad \\bm x_0 = \\mathbf x_0,\\\\\n                    &\\quad \\bm x_N = \\mathbf 0\\;  (\\text{or}\\, \\bm x_N \\approx \\mathbf 0).\n\\end{aligned}\nReferring to the two options for the last constraint,",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Finite-horizon LQR as a QP"
    ]
  },
  {
    "objectID": "discr_dir_LQR.html#simultaneous-sparse-formulation",
    "href": "discr_dir_LQR.html#simultaneous-sparse-formulation",
    "title": "Finite-horizon LQR as a QP",
    "section": "Simultaneous (sparse) formulation",
    "text": "Simultaneous (sparse) formulation\nBelow we rewrite the latter problem, that is, \\bm x_N\\approx\\mathbf 0, in the “unrolled” form, where we stack the state and control variables into “long” vectors \\bar{\\bm x} and \\bar{\\bm u}. Doing the same for the former is straightforward. \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u},\\bar{\\bm x}} & \\frac{1}{2}\\left(\\begin{bmatrix} \\bm x_1^\\top & \\bm x_2^\\top & \\ldots & \\bm x_N^\\top \\end{bmatrix}\n\\underbrace{\\begin{bmatrix}\\mathbf Q & & & \\\\ & \\mathbf Q & &\\\\ & &\\ddots & \\\\ & & & \\mathbf S \\end{bmatrix}}_{\\overline{\\mathbf Q}}\n\\underbrace{\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix}}_{\\bar{\\bm x}}\\right.\\\\\n&\\qquad +\\left.\n\\begin{bmatrix} \\bm u_0^\\top & \\bm u_1^\\top & \\ldots & \\bm u_{N-1}^\\top \\end{bmatrix}\n\\underbrace{\\begin{bmatrix}\\mathbf R & & & \\\\ & \\mathbf R & &\\\\ & &\\ddots & \\\\ & & & \\mathbf R \\end{bmatrix}}_{\\overline{\\mathbf R}}\n\\underbrace{\\begin{bmatrix} \\bm u_0 \\\\ \\bm u_1 \\\\ \\vdots \\\\ \\bm u_{N-1} \\end{bmatrix}}_{\\bar{\\bm u}}\\right)\n+ \\underbrace{\\frac{1}{2}\\mathbf x_0^\\top \\mathbf Q \\mathbf x_0}_{\\mathrm{constant}}\n\\end{aligned}\n subject to \n\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\bm x_3\\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix} = \\underbrace{\\begin{bmatrix}\\mathbf 0 & & & &\\\\\\mathbf A & \\mathbf 0 & & &\\\\ &\\mathbf A &\\mathbf 0 & & \\\\ & & &\\ddots & \\\\& & &\\mathbf A & \\mathbf 0 \\end{bmatrix}}_{\\overline{\\mathbf A}}\n\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\bm x_3\\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf B & & & & \\\\ & \\mathbf B & & & \\\\& &\\mathbf B & \\\\ & & &\\ddots \\\\ & & & & \\mathbf B \\end{bmatrix}}_{\\overline{\\mathbf B}}\\begin{bmatrix} \\bm u_0 \\\\ \\bm u_1 \\\\ \\bm u_2\\\\\\vdots \\\\ \\bm u_{N-1} \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf A\\\\\\mathbf 0\\\\\\mathbf 0\\\\\\vdots\\\\\\mathbf 0\\end{bmatrix}}_{\\overline{\\mathbf A}_0}\\mathbf x_0,    \n\nin which we have already substituted the particular \\mathbf x_0 for the variable \\bm x_0. Consequently, the last term in the cost function can be discarded because it is constant.\nThe terms with the \\bar{\\bm x} vector can be combined and we get \n\\begin{bmatrix} \\mathbf 0 \\\\ \\mathbf 0 \\\\ \\mathbf 0\\\\ \\vdots \\\\ \\mathbf 0 \\end{bmatrix} = \\underbrace{\\begin{bmatrix}-\\mathbf I & & & &\\\\\\mathbf A & -\\mathbf I & & &\\\\ &\\mathbf A &-\\mathbf I & & \\\\ & & &\\ddots & \\\\& & &\\mathbf A & -\\mathbf I \\end{bmatrix}}_{\\overline{\\mathbf A} - \\mathbf I}\n\\begin{bmatrix} \\mathbf x_1 \\\\ \\mathbf x_2 \\\\ \\mathbf x_3\\\\ \\vdots \\\\ \\mathbf x_N \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf B & & & & \\\\ & \\mathbf B & & & \\\\& &\\mathbf B & \\\\ & & &\\ddots \\\\ & & & & \\mathbf B \\end{bmatrix}}_{\\overline{\\mathbf B}}\\begin{bmatrix} \\mathbf u_0 \\\\ \\mathbf u_1 \\\\ \\mathbf u_2\\\\\\vdots \\\\ \\mathbf u_{N-1} \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf A\\\\\\mathbf 0\\\\\\mathbf 0\\\\\\vdots\\\\\\mathbf 0\\end{bmatrix}}_{\\overline{\\mathbf A}_0}\\mathbf x_0.\n\\tag{1}\nUpon stacking the two “long” vectors into \\bar{\\bm z} we reformulate the optimization problem as \n\\operatorname*{minimize}_{\\widetilde{\\mathbf z}\\in\\mathbb{R}^{2N}}\\quad \\frac{1}{2}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}^\\top &\\bar{\\bm u}^\\top\\end{bmatrix}}_{\\bar{\\bm z}^\\top} \\underbrace{\\begin{bmatrix}\\overline{\\mathbf Q} & \\\\ & \\overline{\\mathbf R} \\end{bmatrix}}_{\\widetilde{\\mathbf Q}}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}\\\\\\bar{\\bm u}\\end{bmatrix}}_{\\bar{\\bm z}}\n subject to \n\\mathbf 0 = \\underbrace{\\begin{bmatrix}(\\overline{\\mathbf A}-\\mathbf I) & \\overline{\\mathbf B}\\end{bmatrix}}_{\\widetilde{\\mathbf A}}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}\\\\\\bar{\\bm u}\\end{bmatrix}}_{\\bar{\\bm z}} + \\underbrace{\\overline{\\mathbf A}_0 \\mathbf x_0}_{\\tilde{\\mathbf b}}.\n\nTo summarize, we have reformulated the optimal control problem as a linearly constrained quadratic program \n\\boxed{\n\\begin{aligned}\n\\underset{\\bar{\\bm z}\\in\\mathbb{R}^{2N}}{\\text{minimize}} &\\quad \\frac{1}{2}\\bar{\\bm z}^\\top \\widetilde{\\mathbf Q} \\bar{\\bm z}\\\\\n\\text{subject to} &\\quad \\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\bm b} = \\mathbf 0.\n\\end{aligned}}\n\nThis constrained optimization problem can still be solved without invoking a numerical solver for solving quadratic programs (QP). We do it by introducing a vector \\boldsymbol\\lambda of Lagrange multipliers to form the Lagrangian function \n\\mathcal{L}(\\bar{\\bm z}, \\boldsymbol \\lambda) = \\frac{1}{2}\\bar{\\bm z}^\\top \\widetilde{\\mathbf Q} \\bar{\\bm z} + \\boldsymbol\\lambda^\\top(\\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\mathbf b}),\n for which the gradients with respect to \\bar{\\bm z} and \\boldsymbol\\lambda are \n\\begin{aligned}\n\\nabla_{\\tilde{\\bm{z}}} \\mathcal{L}(\\bar{\\bm z}, \\boldsymbol\\lambda) &= \\widetilde{\\mathbf Q}\\bar{\\bm z} + \\tilde{\\mathbf A}^\\top\\boldsymbol\\lambda,\\\\\n\\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L}(\\tilde{\\bm x}, \\boldsymbol\\lambda) &=\\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\mathbf b}.\n\\end{aligned}\n\nRequiring that the overall gradient vanishes leads to the following KKT set of linear equations \n\\begin{bmatrix}\n  \\widetilde{\\mathbf Q} & \\widetilde{\\mathbf A}^\\top\\\\ \\widetilde{\\mathbf A} & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bar{\\bm z}\\\\\\boldsymbol\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf 0\\\\ -\\tilde{\\mathbf b}\n\\end{bmatrix}.\n\nSolving this could be accomplished by using some general solver for linear systems or by using some more tailored solver for symmetric indefinite systems (based on LDL factorization).\n\nExample 1 (Reformulating the unconstrained LQR problem as a system of linear equations – simultaneous approach)  \n\n\nShow the code\nusing BlockArrays\nusing LinearAlgebra\nusing LinearSolve\nusing QDLDL\nusing SparseArrays\n\nfunction direct_dlqr_simultaneous(A,B,x₀,Q,R,S,N)\n    n = size(A)[1]\n    m = size(B)[2]\n    Q̄ = BlockArray(spzeros(N*n,N*n),repeat([n],N),repeat([n],N))\n    for i=1:(N-1)\n        Q̄[Block(i,i)] = Q\n    end\n    Q̄[Block(N,N)] = S\n    R̄ = BlockArray(spzeros(N*m,N*m),repeat([m],N),repeat([m],N))\n    for i=1:N\n        R̄[Block(i,i)] = R\n    end\n    Q̃ = blockdiag(sparse(Q̄),sparse(R̄))              # The matrix defining the quadratic cost.\n    B̄ = BlockArray(spzeros(N*n,N*m),repeat([n],N),repeat([m],N))\n    for i=1:N\n        B̄[Block(i,i)] = B\n    end\n    Ā = BlockArray(sparse(-1.0*I,n*N,n*N),repeat([n],N),repeat([n],N))\n    for i=2:N\n        Ā[Block(i,(i-1))] = A\n    end\n    Ã = sparse([Ā B̄])                               # The matrix defining the linear (affine) equation.\n    Ā₀ = spzeros(n*N,n)\n    Ā₀[1:n,1:n] = A\n    b̃ = Ā₀*sparse(x₀)                               # The constant offset for the linear (affine) equation.\n    K = [Q̃ Ã'; Ã spzeros(size(Ã,1),size(Ã,1))]      # Sparse KKT matrix.\n    k = [spzeros(size(Q̃,1)); -b̃]                    # Right hand side of the KKT system\n    prob = LinearProblem(K,k)                       # The KKT system as a linear problem.\n    z̃λ = LinearSolve.solve(prob)                    # Solving the KKT system. Ready for trying various solvers.\n    xopt = reshape(z̃λ[1:(n*N)],(n,:))\n    uopt = reshape(z̃λ[(n*N+1):(n+m)*N],(m,:))\n    return xopt,uopt\nend\n\nn = 2               # Number of state variables.\nm = 1               # Number of (control) input variables. \nA = rand(n,n)       # State matrix.\nB = rand(n,m)       # Input coupling matrix.\nx₀ = [1.0, 3.0]     # Initial state.\n\nN = 10              # Time horizon.\n\ns = [1.0, 2.0]      \nq = [1.0, 2.0]\nr = [1.0]\n\nS = diagm(0=&gt;s)     # Matrix defining the terminal state cost.\nQ = diagm(0=&gt;q)     # Matrix defining the running state dost.\nR = diagm(0=&gt;r)     # Matrix defining the cost of control.\n\nxopt,uopt = direct_dlqr_simultaneous(A,B,x₀,Q,R,S,N)\n\nusing Plots\np1 = plot(0:(N-1),uopt',marker=:diamond,label=\"u\",linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"u\")\n\np2 = plot(0:N,hcat(x₀,xopt)',marker=:diamond,label=[\"x₁\" \"x₂\"],linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"x\")\n\nplot(p1,p2,layout=(2,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding constraints on controls and states\nWhen solving a realistic optimal control problem, we may want to impose inequality constraints on \\bm u_k due to saturation of actuators. We may also want to add constraints on \\bm x_k as well, which may reflect some performance specifications. In both cases, we would arrive at the full KKT conditions, and rather than trying to solve these, we resort to some finetuned numerical solver for quadratic programming (QP) instead.\n\nExample 2 (Simultaneous approach to the LQR problem with constraints on control – higher-level implementation using optimization modelling language JuMP) While developing the model all the way down to the individual matrices and vectors gives an insight into the structure of the problem (we learnt that in absence of constraints it amounts to solving an indefinite system of linear equations), here we show how the use of an optimization modelling language can make the process of building the model a lot more convenient. We use JuMP for this purpose, but things would be similar with, say, cvxpy in Python or Yalmip in Matlab.\n\n\nShow the code\nusing LinearAlgebra\nusing JuMP\nusing OSQP\n\nn = 2               # Number of state variables.\nm = 1               # Number of (control) input variables. \nA = rand(n,n)       # State matrix.\nB = rand(n,m)       # Input coupling matrix.\nx₀ = [1.0, 3.0]     # Initial state.\n\nN = 10              # Time horizon.\n\ns = [1.0, 2.0]      \nq = [1.0, 2.0]\nr = [1.0]\n\nS = diagm(0=&gt;s)     # Matrix defining the terminal state cost.\nQ = diagm(0=&gt;q)     # Matrix defining the running state dost.\nR = diagm(0=&gt;r)     # Matrix defining the cost of control.\n\numin = -1.0\numax = 1.0\n\nocp = Model(OSQP.Optimizer)\nset_silent(ocp)\n\n@variable(ocp, umin &lt;= u[1:N] &lt;= umax)\n@variable(ocp, x[1:n,1:N+1])\n\nfor i in 1:N\n    @constraint(ocp, x[:,i+1] == A*x[:,i] + B*u[i])\nend\n\nfix(x[1,1], x₀[1])\nfix(x[2,1], x₀[2])\n\n@objective(ocp, Min, 1/2*dot(x[:,N],S,x[:,N]) + 1/2*sum(dot(x[:,i],Q,x[:,i]) + dot(u[i],R,u[i]) for i in 1:N-1))\n\noptimize!(ocp)\nuopt = value.(u)\nxopt = value.(x)\n\nusing Plots\n\np1 = plot(0:(N-1),uopt,marker=:diamond,label=\"u\",linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"u\")\n\np2 = plot(0:N,xopt',marker=:diamond,label=[\"x₁\" \"x₂\"],linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"x\")\n\nplot(p1,p2,layout=(2,1))",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Finite-horizon LQR as a QP"
    ]
  },
  {
    "objectID": "discr_dir_LQR.html#sequential-dense-formulation",
    "href": "discr_dir_LQR.html#sequential-dense-formulation",
    "title": "Finite-horizon LQR as a QP",
    "section": "Sequential (dense) formulation",
    "text": "Sequential (dense) formulation\nWe can express \\bar{\\bm x} as a function of \\bar{\\bm u} and \\mathbf x_0. This can be done in a straightforward way using (Eq. 1), namely, \n\\bar{\\bm x} = (\\mathbf I-\\overline{\\mathbf A})^{-1}\\overline{\\mathbf B} \\bm u + (\\mathbf I-\\overline{\\mathbf A})^{-1} \\overline{\\mathbf A}_0 \\mathbf x_0.\n\nHowever, instead of solving the sets of equations, we can do this substitution in a more insightful way. Write down the state equation for several discrete times \n\\begin{aligned}\n\\bm x_1 &= \\mathbf A\\mathbf x_0 + \\mathbf B\\bm u_0\\\\\n\\bm x_2 &= \\mathbf A\\mathbf x_0 + \\mathbf B\\bm u_0\\\\\n     &= \\mathbf A(\\mathbf A\\mathbf x_0 + \\mathbf B\\bm u_0)+ \\mathbf B\\bm u_0\\\\\n     &= \\mathbf A^2\\mathbf x_0 + \\mathbf A\\mathbf B\\bm u_0 + \\mathbf B\\bm u_0\\\\\n     &\\vdots\\\\\n\\bm x_k &= \\mathbf A^k\\mathbf x_0 + \\mathbf A^{k-1}\\mathbf B\\bm u_0 +\\mathbf A^{k-2}\\mathbf B\\bm u_1 +\\ldots \\mathbf B\\bm u_{k-1}.\n\\end{aligned}\n\nRewriting into matrix-vector form (and extending the time k up to the final time N) \n\\begin{bmatrix}\n\\bm x_1\\\\\\bm x_2\\\\\\vdots\\\\\\bm x_N\n\\end{bmatrix}\n=\n\\underbrace{\n\\begin{bmatrix}\n  \\mathbf B & & & \\\\\n  \\mathbf A\\mathbf B & \\mathbf B & & \\\\\n  \\vdots & & \\ddots &\\\\\n  \\mathbf A^{N-1}\\mathbf B & \\mathbf A^{N-2}\\mathbf B & & \\mathbf B\n\\end{bmatrix}}_{\\widehat{\\mathbf C}}\n  \\begin{bmatrix}\n\\bm u_0\\\\\\bm u_1\\\\\\vdots\\\\\\bm u_{N-1}\n\\end{bmatrix}\n+\n\\underbrace{\n  \\begin{bmatrix}\n\\mathbf A\\\\\\mathbf A^2\\\\\\vdots\\\\\\mathbf A^N\n\\end{bmatrix}}_{\\widehat{\\mathbf A}}\\mathbf x_0.\n\nFor convenience, let’s rewrite the compact relation between \\bar{\\bm x} and \\bar{\\bm u} and \\mathbf x_0 \n\\bar{\\bm x} = \\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0.\n\\tag{2}\nWe can now substitute this into the original cost, which then becomes independent of \\bar{\\bm x}, which we reflect formally by using a new name \\tilde J \n\\begin{aligned}\n\\tilde J(\\bar{\\bm u};\\mathbf x_0) &= \\frac{1}{2}(\\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0)^\\top\\overline{\\mathbf Q} (\\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0) + \\frac{1}{2}\\bar{\\bm u}^\\top\\overline{\\mathbf R} \\bar{\\bm u} + \\frac{1}{2}\\mathbf x_0^\\top\\mathbf Q\\mathbf x_0\\\\\n&= \\frac{1}{2}\\bar{\\bm u}^\\top\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\mathbf x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\frac{1}{2} \\mathbf x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf A} \\mathbf x_0 + \\frac{1}{2}\\bar{\\bm u}^\\top\\overline{\\mathbf R} \\bar{\\bm u} + \\frac{1}{2}\\mathbf x_0^\\top\\mathbf Q\\mathbf x_0\\\\\n&= \\frac{1}{2}\\bar{\\bm u}^\\top(\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} + \\overline{\\mathbf R})\\bar{\\bm u} + \\mathbf x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\frac{1}{2} \\mathbf x_0^\\top(\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf A} + \\mathbf Q)\\mathbf x_0.\n\\end{aligned}\n\nThe last term (the one independent of \\bar{\\bm u}) does not have an impact on the optimal \\bar{\\bm u} and therefore it can be discarded, but such minor modification perhaps does not justify a new name for the cost function. We write it as \n\\tilde J(\\bar{\\bm u};\\mathbf x_0) = \\frac{1}{2}\\bar{\\bm u}^\\top\\underbrace{(\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} + \\overline{\\mathbf R})}_{\\mathbf H}\\bar{\\bm u} +  \\mathbf x_0^\\top\\underbrace{\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C}}_{\\mathbf F^\\top} \\bar{\\bm u}.\n\nThis cost is a function of \\bar{\\bm u}, the initial state \\mathbf x_0 is regarded as a fixed parameter. Its gradient is \n\\nabla \\tilde J = \\mathbf H\\bar{\\bm u}+\\mathbf F\\mathbf x_0.\n\nSetting it to zero leads to the following linear system of equations \n\\mathbf H\\bar{\\bm u}=-\\mathbf F\\mathbf x_0\n that needs to be solved for \\bar{\\bm u}. Formally, we write the solution as \n\\bar{\\bm u} = -\\mathbf H^{-1} \\mathbf F \\mathbf x_0.\n\n\nExample 3 (Reformulating the unconstrained LQR problem as a system of linear equations – sequential approach)  \n\n\nShow the code\nfunction direct_dlqr_sequential(A,B,x₀,Q,R,S,N)\n    n = size(A)[1]\n    m = size(B)[2]\n    Q̄ = BlockArray(spzeros(N*n,N*n),repeat([n],N),repeat([n],N))\n    for i=1:(N-1)\n        Q̄[Block(i,i)] = Q\n    end\n    Q̄[Block(N,N)] = S\n    R̄ = BlockArray(spzeros(N*m,N*m),repeat([m],N),repeat([m],N))\n    for i=1:N\n        R̄[Block(i,i)] = R\n    end\n    Ĉ = BlockArray(spzeros(N*n,N*m),repeat([n],N),repeat([m],N))\n    Â = BlockArray(spzeros(N*n,n),repeat([n],N),[n])\n    for i=1:N\n        for j = 1:i\n            Ĉ[Block(i,j)] = A^(i-j)*B\n            Â[Block(i,1)] = A^i\n        end\n    end\n    H = Ĉ'*Q̄*Ĉ + R̄\n    H = Array(H)\n    F = Ĉ'*Q̄*Â\n    F = Array(F)\n\n    uopt = -H\\(F*x₀)\n    xopt = Ĉ*uopt + Â*x₀\n    xopt = reshape(xopt,(2,:))\n    return xopt,uopt\nend\n\nn = 2               # Number of state variables.\nm = 1               # Number of (control) input variables. \nA = rand(n,n)       # State matrix.\nB = rand(n,m)       # Input coupling matrix.\nx₀ = [1.0, 3.0]     # Initial state.\n\nN = 10              # Time horizon.\n\ns = [1.0, 2.0]      \nq = [1.0, 2.0]\nr = [1.0]\n\nS = diagm(0=&gt;s)     # Matrix defining the terminal state cost.\nQ = diagm(0=&gt;q)     # Matrix defining the running state dost.\nR = diagm(0=&gt;r)     # Matrix defining the cost of control.\n\nxopt,uopt = direct_dlqr_sequential(A,B,x₀,Q,R,S,N)\n\nusing Plots\np1 = plot(0:(N-1),uopt,marker=:diamond,label=\"u\",linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"u\")\n\np2 = plot(0:N,hcat(x₀,xopt)',marker=:diamond,label=[\"x₁\" \"x₂\"],linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"x\")\n\nplot(p1,p2,layout=(2,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding the constraints on controls\nAdding constraints on \\bar{\\bm u} is straightforward. It is just that instead of a linear system we will have a linear system with additional inequality constraints. Let’s get one \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u}} & \\quad \\frac{1}{2}\\bar{\\bm u}^T \\mathbf H \\bar{\\bm u} + \\mathbf x_0^T\\mathbf F^T \\bar{\\bm u}\\\\\n\\text{subject to} &\\quad \\bar{\\bm u} \\leq \\bar{\\mathbf u}^\\mathrm{max}\\\\\n               &\\quad \\bar{\\bm u} \\geq \\bar{\\mathbf u}^\\mathrm{min},\n\\end{aligned}\n which we can rewrite more explicitly (in the matrix-vector format) as \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u}} & \\quad \\frac{1}{2}\\bar{\\bm u}^T \\mathbf H \\bar{\\bm u} + \\mathbf x_0^T\\mathbf F^T \\bar{\\bm u}\\\\\n\\text{subject to} & \\begin{bmatrix}\n                  \\mathbf{I}  &    &         &    \\\\\n                    & \\mathbf{I}  &         &    \\\\\n                    &    & \\ddots  &    \\\\\n                    &    &         &  \\mathbf{I} \\\\\n                    -\\mathbf{I}   &   &     &    \\\\\n                    & -\\mathbf{I} &         &    \\\\\n                    &    & \\ddots  &    \\\\\n                    &    &         &  -\\mathbf{I}\n                 \\end{bmatrix}\n                 \\begin{bmatrix}\n                  \\bm u_0 \\\\ \\bm u_1 \\\\ \\vdots \\\\ \\bm u_{N-1}\n                 \\end{bmatrix}\n                 \\leq\n                 \\begin{bmatrix}\n                  \\mathbf u^\\mathbf{max} \\\\ \\mathbf u^\\mathrm{max} \\\\ \\vdots \\\\ \\mathbf u^\\mathrm{max}\\\\ -\\mathbf u^\\mathrm{min} \\\\ -\\mathbf u^\\mathrm{min} \\\\ \\vdots \\\\ -\\mathbf u^\\mathrm{min}\n                 \\end{bmatrix}.\n\\end{aligned}\n\n\nExample 4 (Reformulating the LQR problem with constraints on control as a quadratic program – sequential approach)  \n\n\nShow the code\nusing LinearAlgebra\nusing BlockArrays\nusing SparseArrays\nusing JuMP\nusing OSQP\n\nfunction direct_dlqr_sequential(A,B,x₀,Q,R,S,N,(umin,umax))\n    n = size(A)[1]\n    m = size(B)[2]\n    Q̄ = BlockArray(spzeros(N*n,N*n),repeat([n],N),repeat([n],N))\n    for i=1:(N-1)\n        Q̄[Block(i,i)] = Q\n    end\n    Q̄[Block(N,N)] = S\n    R̄ = BlockArray(spzeros(N*m,N*m),repeat([m],N),repeat([m],N))\n    for i=1:N\n        R̄[Block(i,i)] = R\n    end\n    Ĉ = BlockArray(spzeros(N*n,N*m),repeat([n],N),repeat([m],N))\n    Â = BlockArray(spzeros(N*n,n),repeat([n],N),[n])\n    for i=1:N\n        for j = 1:i\n            Ĉ[Block(i,j)] = A^(i-j)*B\n            Â[Block(i,1)] = A^i\n        end\n    end\n    H = Ĉ'*Q̄*Ĉ + R̄\n    H = Array(H)\n    F = Ĉ'*Q̄*Â\n    F = Array(F)\n    prob = Model()\n    @variable(prob, u[1:N*m])\n    @objective(prob, Min, 1/2*dot(u,H,u) + dot(F*x₀,u))\n    @constraint(prob, u .&gt;= umin)\n    @constraint(prob, u .&lt;= umax)\n    set_silent(prob)\n    set_optimizer(prob, OSQP.Optimizer)\n    optimize!(prob)\n    uopt = value.(u)\n    xopt = Ĉ*uopt + Â*x₀\n    xopt = reshape(xopt,(2,:))\n#=     u = Variable(N*m)\n    problem = minimize(1/2*quadform(u,H) + dot(F*x₀,u))\n    problem.constraints = [u &gt;= umin, u &lt;= umax]\n    Convex.solve!(problem, SCS.Optimizer; silent = true)\n    xopt = Ĉ*u.value + Â*x₀\n    xopt = reshape(xopt,(2,:))\n    uopt = u.value =#\n    return xopt,uopt\nend\n\nn = 2               # Number of state variables.\nm = 1               # Number of (control) input variables. \nA = rand(n,n)       # State matrix.\nB = rand(n,m)       # Input coupling matrix.\nx₀ = [1.0, 3.0]     # Initial state.\n\nN = 10              # Time horizon.\n\ns = [1.0, 2.0]      \nq = [1.0, 2.0]\nr = [1.0]\n\nS = diagm(0=&gt;s)     # Matrix defining the terminal state cost.\nQ = diagm(0=&gt;q)     # Matrix defining the running state dost.\nR = diagm(0=&gt;r)     # Matrix defining the cost of control.\n\numin = -1.0\numax = 1.0\n\nxopt,uopt = direct_dlqr_sequential(A,B,x₀,Q,R,S,N,(umin,umax))\n\nusing Plots\np1 = plot(0:(N-1),uopt,marker=:diamond,label=\"u\",linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"u\")\n\np2 = plot(0:N,hcat(x₀,xopt)',marker=:diamond,label=[\"x₁\" \"x₂\"],linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"x\")\n\nplot(p1,p2,layout=(2,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding the constraints on states\nWe might feel a little bit uneasy about loosing an immediate access to \\bar{\\bm x}. But the game is not lost. We just need to express \\bar{\\bm x} as a function of \\bar{\\bm u} and \\mathbf x_0 and impose the constraint on the result. But such expression is already available, see (Eq. 2). Therefore, we can formulate the constraint, say, an upper bound on the state vector \n\\bm x_k \\leq \\mathbf x_\\mathrm{max}\n as \n\\bar{\\mathbf x}^\\mathrm{min} \\leq \\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0 \\leq \\bar{\\mathbf x}^\\mathrm{max},\n where the the bars in \\bar{\\mathbf x}^\\mathrm{min} and \\bar{\\mathbf x}^\\mathrm{max} obviously indicates that these vectors were obtained by stacking the corresponding vectors for all times k=1,\\ldots,N.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Finite-horizon LQR as a QP"
    ]
  },
  {
    "objectID": "cont_indir_calculus_of_variations.html",
    "href": "cont_indir_calculus_of_variations.html",
    "title": "Calculus of variations",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Calculus of variations"
    ]
  },
  {
    "objectID": "reduction_order_model.html",
    "href": "reduction_order_model.html",
    "title": "Model order reduction",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Model order reduction"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html",
    "href": "opt_algo_unconstrained.html",
    "title": "Algorithms for unconstrained optimization",
    "section": "",
    "text": "Our motivation for studying numerical algorithms for unconstrained optimization remains the same as when we studied the conditions of optimality for such unconstrained problems – such algorithms constitute building blocks for constrained optimization problems. Indeed, many algorithms for constrained problems are based on reformulating the constrained problem into an unconstrained one and then applying the algorithms studied in this section.\nIt may be useful to recapitulate our motivation for studying optimization algorithms in general – after all, there are dozens of commercial or free&open-source software tools for solving optimization problems. Why not just use them? There are two answers beyond the traditional “at a grad school we should understand what we are using”:\nThere is certainly no shortage of algorithms for unconstrained optimization. In this crash course we can cover only a few. But the few we cover here certainly form a solid theoretical basis and provide practically usable tools.\nOne possible way to classify the algorithms is based on whether they use derivatives of the objective functions or not. In this course, we only consider the former approaches as they leads to more efficient algorithms. For the latter methods, we can refer to the literature (the prominent example is Nelder-Mead method).\nAll the relevant methods are iterative. Based on what happens within each iteration, we can classify them into two categories:",
    "crumbs": [
      "2. Optimization – algorithms",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html#descent-methods",
    "href": "opt_algo_unconstrained.html#descent-methods",
    "title": "Algorithms for unconstrained optimization",
    "section": "Descent methods",
    "text": "Descent methods\nA single iteration of a descent method consists of the following step: \n\\boxed{\n\\bm x_{k+1} = \\bm x_{k} + \\alpha_k \\bm d_k,}        \n\\tag{1} where \\bm x_k is the current solution, \\bm d_k is the search direction, and \\alpha_k is the step length.\nThe obvious quality that the search direction needs to satisfy, is that the cost function decreses along it, at least locally (for a small step length).\n\nDefinition 1 (Descent direction) At the current iterate \\bm x_k, the direction \\bm d_k is called a descent direction if \n\\nabla f(\\bm x_k)^\\top \\bm d_k &lt; 0,\n that is, the directional derivative is negative along the direction \\bm d_k.\n\nThe product above is an inner product of the two vectors \\bm d_k and \\nabla f(\\mathbf x_k). Recall that it is defined as \n\\nabla f(\\bm x_k)^\\top \\bm d_k = \\|\\nabla f(\\bm x_k)\\| \\|\\bm d_k\\| \\cos \\theta,\n where \\theta is the angle between the gradient and the search direction. This condition has a nice geometric interpretation in a contour plot for an optimization in \\mathbb R^2. Consider the line tangent to the function countour at \\bm x_k. A descent direction must be in the other half-plane generated by the tangent line than the one into which the gradient \\nabla f(\\bm x_k) points.\nBeware that it is only guaranteed that the cost function is reduced if the length of the step is sufficently small. For longer steps the higher-order terms in the Taylor’s series approximation of the cost function can dominate.\nBefore we proceed to the question of which descent direction to choose, we adress the question of how far to go along the chosen direction. This is the problem of line search.\n\nStep length determination (aka line search)\nNote that once the search direction has been fixed (whether we used the negative of the gradient or any other descent direction), the problem of finding the step length \\alpha_k is just a scalar optimization problem. It turns out, however, that besides finding the true minimum along the search directions, it is often sufficient to find the minimum only approximately, or not aiming at minimization at all and work with a fixed step length instead.\n\nFixed length of the step\nHere we give a guidance on the choice of the lenght of the step. But we need to introduce a useful concept first.\n\nDefinition 2 (L-smoothness) For a continuously differentiable function f, the gradient \\nabla f is said to be L-smooth if there exists a constant L&gt;0 such that \n\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L \\|x-y\\|.\n\n\nNot that if the second derivatives exist, L is an upper bound on the norm of the Hessian \n\\|\\nabla^2 f\\|\\leq L.\n\nFor quadratic functions, L is the largest eigenvalue of the Hessian \nL = \\max_i \\lambda_i (\\mathbf Q).\n\nThe usefulness of the concept of L-smoothness is that it provides a quadratic function that serves as an upper bound for the original function. This is formulated as the following lemma.\n\nLemma 1 (Descent lemma) Consider an L-smooth function f. Then for any \\mathbf x_k and \\mathbf x_{k+1}, the following inequality holds \nf(\\mathbf x_{k+1}) \\leq  f(\\mathbf x_{k}) + \\nabla f(\\mathbf x_k)^\\top (\\mathbf x_{k-1}-\\mathbf x_{k}) + \\frac{L}{2}\\|\\mathbf x_{k-1}-\\mathbf x_{k}\\|^2\n\n\nWhat implication does the result have on the determination of the step length?\n\n\\alpha = \\frac{1}{L}.\n\n\n\nExact line search\nA number of methods exist: bisection, golden section, Newton, … As finding the true minium in each iteration is often too computationally costly and hardly needed, we do not cover these methods here. One exception the Newton’s method, which for vector variables constitutes another descent method on its own and we cover it later.\nAnother exception is the case of a quadratic function in the following example.\n\nExample 1 Here we develop a solution for exact minimization of a quadratic functions f(\\bm x) = \\frac{1}{2} \\bm x^\\top\\mathbf Q \\bm x + \\mathbf c^\\top \\bm x along a given direction. We show that it leads to a closed-form formula. Although not particularly useful in practice (for a quadratic function we already know we can find the minimizer by solving a system of linear equations), it is a good exercise in understanding the problem of line search. Furthermore, we will use it later to demonstrate the behaviour of the steepest descent method. The problem is to \\operatorname*{minimize}_{\\alpha_k} f(\\bm x_k + \\alpha_k \\bm d_k). We express the cost as a function of the current iterate, the direction, and step length. \n\\begin{aligned}\nf(\\bm x_k + \\alpha_k \\bm d_k) &= \\frac{1}{2}(\\bm x_k + \\alpha_k\\bm d_k)^\\top\\mathbf Q (\\bm x_k + \\alpha_k\\bm d_k) +\\mathbf c^\\top(\\bm x_k + \\alpha_k\\bm d_k)\\\\\n&= \\frac{1}{2} \\bm x_k^\\top\\mathbf Q \\bm x_k + \\bm d_k^\\top\\mathbf Q\\bm x_k \\alpha_k + \\frac{1}{2} \\bm d_k^\\top\\mathbf Q\\bm d_k \\alpha_k^2+ \\mathbf c^\\top(\\bm x_k + \\alpha_k\\bm d_k).\n\\end{aligned}\n\nConsidering the current iterate and the search direction constant, by differentiating the function with respect to the length of the step, we get \n\\frac{\\mathrm{d}f(\\bm x_k + \\alpha_k\\bm d_k)}{\\mathrm{d}\\alpha_k} = \\bm d_k^\\top \\underbrace{(\\mathbf Q\\bm x_k + \\mathbf c)}_{\\nabla f(\\bm x_k)} + \\bm d_k^\\top\\mathbf Q\\bm d_k \\alpha_k.\n\nAnd now setting the derivative to zero, we find the optimal step length \n\\boxed{\n\\alpha_k = -\\frac{\\bm d_k^\\top \\nabla f(\\bm x_k)}{\\bm d_k^\\top\\mathbf Q\\bm d_k} = -\\frac{\\bm d_k^\\top (\\mathbf Q\\bm x_k + \\mathbf c)}{\\bm d_k^\\top\\mathbf Q\\bm d_k}.}\n\\tag{2}\nAs we have mentioned, this result will be useful for some benchmarking later.\n\n\n\nApproximate line search – backtracking\nThere are several methods for approximate line search. Here we describe the backtracking algorithm, which is based on the sufficient decrease condition (also known as Armijo condition), which reads \nf(\\bm x_k+\\alpha_k\\bm d_k) - f(\\bm x_k) \\leq \\gamma \\alpha_k \\mathbf d^T \\nabla f(\\bm x_k),\n where \\gamma\\in(0,1), typically \\gamma is very small, say \\gamma = 10^{-4}.\nThe term on the right can be be viewed as a linear function of \\alpha_k. Its negative slope is a bit less steep than the directional derivative of the function f at \\bm x_k. The condition of sufficient decrease thus requires that the cost function (as a function of \\alpha_k) is below the graph of this linear function.\nNow, the backtracking algorithm is parameterized by three parameters: the initial step lenght \\alpha_0&gt;0, the typically very small \\gamma\\in(0,1) that parameterizes the Armijo condition, and yet another parameter \\beta\\in(0,1).\nThe k-th iteration of the algorithm goes like this: failure of the sufficient decrease condition for a given \\alpha_k or, equivalently, satisfaction of the condition \nf(\\bm x_k) - f(\\bm x_k+\\alpha_k\\bm d_k) &lt; -\\gamma \\alpha_k \\mathbf d^T \\nabla f(\\bm x_k)\n sends the algorithm into another reduction of \\alpha_k by \\alpha_k = \\beta\\alpha_k. A reasonable choice for \\beta is 0.5, which corresponds to halving the step length upon failure to decrease sufficiently.\nThe backtracking algorithm can be implemented as follows\n\n\nShow the code\nfunction backtracking_line_search(f, ∇fₖ, xₖ, dₖ; α₀=1.0, β=0.5, γ=0.1)\n    αₖ = α₀\n    while f(xₖ)-f(xₖ+αₖ*dₖ) &lt; -γ*αₖ*dot(dₖ,∇fₖ)\n        αₖ *= β\n    end\n    return αₖ\nend\n\n\nbacktracking_line_search (generic function with 1 method)\n\n\nNow we are ready to proceed to the question of choosing a descent direction.\n\n\n\nSteepest descent (aka gradient descent) method\nA natural candidate for a descent direction is the negative of the gradient \n\\bm d_k = -\\nabla f(\\bm x_k).\n\nIn fact, among all descent directions, this is the one for which the descent is steepest (the gradient determines the direction of steepest ascent), though we will see later that this does not mean that the convergence of the method is the fastest.\nIn each iteration of the gradient method, this is the how the solution is updated\n\n\\boxed{\n\\bm x_{k+1} = \\bm x_{k} - \\alpha_k \\nabla f(\\bm x_{k}),}\n where the determination of the step length \\alpha_k has already been discussed in the previous section.\nLet’s now examine the behaviour of the method by applying it to minimization of a quadratic function. Well, for a quadratic function it is obviously an overkill, but we use it in the example because we can compute the step length exactly using Eq. 2, which then helps the methods show its best performance.\n\nExample 2 (Steepest descent method for a quadratic function with exact line search)  \n\n\nShow the code\nusing LinearAlgebra         # For dot() function.\nusing Printf                # For formatted output.\n\nx0 = [2, 3]                 # Initial vector.\nQ = [1 0; 0 3]              # Positive definite matrix defining the quadratic form.\nc = [1, 2]                   # Vector defining the linear part.\n\nxs = -Q\\c                   # Stationary point, automatically the minimizer for posdef Q. \n\nϵ  = 1e-5                   # Threshold on the norm of the gradient.\nN  = 100;                   # Maximum number of steps .\n\nfunction gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n    x = x0\n    iter = 0\n    f = 1/2*dot(x,Q*x)+dot(x,c)\n    ∇f = Q*x+c\n    while (norm(∇f) &gt; ϵ)\n        α = dot(∇f,∇f)/dot(∇f,Q*∇f)\n        x = x - α*∇f\n        iter = iter+1\n        f = 1/2*dot(x,Q*x)+dot(x,c)\n        ∇f = Q*x+c\n        @printf(\"i = %3d   ||∇f(x)|| = %6.4e   f(x) = %6.4e\\n\", iter, norm(∇f), f)\n        if iter &gt;= N\n            return f,x\n        end\n    end\n    return f,x\nend\n\nfopt,xopt = gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n\n\ni =   1   ||∇f(x)|| = 2.0229e+00   f(x) = 7.8495e-01\ni =   2   ||∇f(x)|| = 9.0210e-01   f(x) = -1.0123e+00\ni =   3   ||∇f(x)|| = 1.6005e-01   f(x) = -1.1544e+00\ni =   4   ||∇f(x)|| = 7.1374e-02   f(x) = -1.1657e+00\ni =   5   ||∇f(x)|| = 1.2663e-02   f(x) = -1.1666e+00\ni =   6   ||∇f(x)|| = 5.6470e-03   f(x) = -1.1667e+00\ni =   7   ||∇f(x)|| = 1.0019e-03   f(x) = -1.1667e+00\ni =   8   ||∇f(x)|| = 4.4679e-04   f(x) = -1.1667e+00\ni =   9   ||∇f(x)|| = 7.9269e-05   f(x) = -1.1667e+00\ni =  10   ||∇f(x)|| = 3.5350e-05   f(x) = -1.1667e+00\ni =  11   ||∇f(x)|| = 6.2718e-06   f(x) = -1.1667e+00\n\n\n(-1.1666666666479069, [-0.9999939492423319, -0.6666672167355456])\n\n\nWe can also decorate the code a bit to visualize how the iterations proceeded.\n\n\nShow the code\nfunction gradient_descent_quadratic_exact_decor(Q,c,x0,ϵ,N)\n    x = x0\n    X = x\n    f = 1/2*dot(x,Q*x)+dot(x,c)\n    F = [f,]\n    ∇f = Q*x+c\n    iter = 0\n    while (norm(∇f) &gt; ϵ)\n        α = dot(∇f,∇f)/dot(∇f,Q*∇f)\n        x = x - α*∇f\n        iter = iter+1\n        f = 1/2*dot(x,Q*x)+dot(x,c)\n        ∇f = Q*x+c\n        X = hcat(X,x)\n        push!(F,f)\n        if iter &gt;= N\n         return F,X\n        end\n    end\n    return F,X\nend\n\nF,X = gradient_descent_quadratic_exact_decor(Q,c,x0,ϵ,N)\n\nx1_grid = x2_grid = -4:0.01:4;\nf(x) = 1/2*dot(x,Q*x)+dot(x,c)\nz_grid = [f([x1,x2]) for x2=x2_grid, x1=x1_grid];\n\nusing Plots\ncontour(x1_grid,x2_grid,z_grid)\nplot!(X[1,:],X[2,:],label=\"xk\",marker=:diamond,aspect_ratio=1)\nscatter!([x0[1],],[x0[2],],label=\"x0\")\nscatter!([xs[1],],[xs[2],],label=\"xopt\")\nxlabel!(\"x1\");ylabel!(\"x2\");\nxlims!(-4,4); ylims!(-4,4)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Zigzagging of the steepest descent method for a quadratic function\n\n\n\n\n\nAltough the number of iterations in the above example is acceptable, a major characteristic of the method is visible. Its convergence is slowing down as we are approaching a local minimum, which is visually recognizable oscillations or zig-zagging. But it can be much worse for some data, as the next example shows.\n\nExample 3 (Steepest descent method for an ill-conditioned quadratic function with exact line search) Consider minimization of the following cost function f(\\bm x) = 1000x_1^2 + 40x_1x_2 + x_2^2.\n\n\nShow the code\nQ = [1000 20; 20 1]\nc = [0, 0]\nx0 = [1,1000]\n\nF,X = gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n\n\ni =   1   ||∇f(x)|| = 5.9951e+02   f(x) = 2.9939e+05\ni =   2   ||∇f(x)|| = 1.2093e+04   f(x) = 1.7221e+05\ni =   3   ||∇f(x)|| = 3.4484e+02   f(x) = 9.9052e+04\ni =   4   ||∇f(x)|| = 6.9560e+03   f(x) = 5.6974e+04\ni =   5   ||∇f(x)|| = 1.9835e+02   f(x) = 3.2771e+04\ni =   6   ||∇f(x)|| = 4.0011e+03   f(x) = 1.8850e+04\ni =   7   ||∇f(x)|| = 1.1409e+02   f(x) = 1.0842e+04\ni =   8   ||∇f(x)|| = 2.3014e+03   f(x) = 6.2364e+03\ni =   9   ||∇f(x)|| = 6.5623e+01   f(x) = 3.5872e+03\ni =  10   ||∇f(x)|| = 1.3237e+03   f(x) = 2.0633e+03\ni =  11   ||∇f(x)|| = 3.7746e+01   f(x) = 1.1868e+03\ni =  12   ||∇f(x)|| = 7.6141e+02   f(x) = 6.8264e+02\ni =  13   ||∇f(x)|| = 2.1711e+01   f(x) = 3.9265e+02\ni =  14   ||∇f(x)|| = 4.3796e+02   f(x) = 2.2585e+02\ni =  15   ||∇f(x)|| = 1.2488e+01   f(x) = 1.2991e+02\ni =  16   ||∇f(x)|| = 2.5191e+02   f(x) = 7.4722e+01\ni =  17   ||∇f(x)|| = 7.1831e+00   f(x) = 4.2980e+01\ni =  18   ||∇f(x)|| = 1.4490e+02   f(x) = 2.4722e+01\ni =  19   ||∇f(x)|| = 4.1317e+00   f(x) = 1.4220e+01\ni =  20   ||∇f(x)|| = 8.3344e+01   f(x) = 8.1791e+00\ni =  21   ||∇f(x)|| = 2.3765e+00   f(x) = 4.7046e+00\ni =  22   ||∇f(x)|| = 4.7939e+01   f(x) = 2.7061e+00\ni =  23   ||∇f(x)|| = 1.3670e+00   f(x) = 1.5565e+00\ni =  24   ||∇f(x)|| = 2.7574e+01   f(x) = 8.9529e-01\ni =  25   ||∇f(x)|| = 7.8627e-01   f(x) = 5.1497e-01\ni =  26   ||∇f(x)|| = 1.5861e+01   f(x) = 2.9621e-01\ni =  27   ||∇f(x)|| = 4.5226e-01   f(x) = 1.7038e-01\ni =  28   ||∇f(x)|| = 9.1229e+00   f(x) = 9.7999e-02\ni =  29   ||∇f(x)|| = 2.6014e-01   f(x) = 5.6369e-02\ni =  30   ||∇f(x)|| = 5.2474e+00   f(x) = 3.2423e-02\ni =  31   ||∇f(x)|| = 1.4963e-01   f(x) = 1.8649e-02\ni =  32   ||∇f(x)|| = 3.0183e+00   f(x) = 1.0727e-02\ni =  33   ||∇f(x)|| = 8.6065e-02   f(x) = 6.1701e-03\ni =  34   ||∇f(x)|| = 1.7361e+00   f(x) = 3.5490e-03\ni =  35   ||∇f(x)|| = 4.9504e-02   f(x) = 2.0414e-03\ni =  36   ||∇f(x)|| = 9.9859e-01   f(x) = 1.1742e-03\ni =  37   ||∇f(x)|| = 2.8475e-02   f(x) = 6.7539e-04\ni =  38   ||∇f(x)|| = 5.7439e-01   f(x) = 3.8848e-04\ni =  39   ||∇f(x)|| = 1.6378e-02   f(x) = 2.2345e-04\ni =  40   ||∇f(x)|| = 3.3038e-01   f(x) = 1.2853e-04\ni =  41   ||∇f(x)|| = 9.4207e-03   f(x) = 7.3928e-05\ni =  42   ||∇f(x)|| = 1.9003e-01   f(x) = 4.2523e-05\ni =  43   ||∇f(x)|| = 5.4188e-03   f(x) = 2.4459e-05\ni =  44   ||∇f(x)|| = 1.0931e-01   f(x) = 1.4069e-05\ni =  45   ||∇f(x)|| = 3.1168e-03   f(x) = 8.0922e-06\ni =  46   ||∇f(x)|| = 6.2873e-02   f(x) = 4.6546e-06\ni =  47   ||∇f(x)|| = 1.7928e-03   f(x) = 2.6773e-06\ni =  48   ||∇f(x)|| = 3.6164e-02   f(x) = 1.5400e-06\ni =  49   ||∇f(x)|| = 1.0312e-03   f(x) = 8.8578e-07\ni =  50   ||∇f(x)|| = 2.0801e-02   f(x) = 5.0949e-07\ni =  51   ||∇f(x)|| = 5.9314e-04   f(x) = 2.9306e-07\ni =  52   ||∇f(x)|| = 1.1965e-02   f(x) = 1.6856e-07\ni =  53   ||∇f(x)|| = 3.4117e-04   f(x) = 9.6958e-08\ni =  54   ||∇f(x)|| = 6.8821e-03   f(x) = 5.5769e-08\ni =  55   ||∇f(x)|| = 1.9624e-04   f(x) = 3.2078e-08\ni =  56   ||∇f(x)|| = 3.9585e-03   f(x) = 1.8451e-08\ni =  57   ||∇f(x)|| = 1.1288e-04   f(x) = 1.0613e-08\ni =  58   ||∇f(x)|| = 2.2769e-03   f(x) = 6.1045e-09\ni =  59   ||∇f(x)|| = 6.4925e-05   f(x) = 3.5113e-09\ni =  60   ||∇f(x)|| = 1.3097e-03   f(x) = 2.0197e-09\ni =  61   ||∇f(x)|| = 3.7345e-05   f(x) = 1.1617e-09\ni =  62   ||∇f(x)|| = 7.5331e-04   f(x) = 6.6821e-10\ni =  63   ||∇f(x)|| = 2.1480e-05   f(x) = 3.8435e-10\ni =  64   ||∇f(x)|| = 4.3330e-04   f(x) = 2.2107e-10\ni =  65   ||∇f(x)|| = 1.2355e-05   f(x) = 1.2716e-10\ni =  66   ||∇f(x)|| = 2.4923e-04   f(x) = 7.3142e-11\ni =  67   ||∇f(x)|| = 7.1068e-06   f(x) = 4.2071e-11\n\n\n(4.207097012988499e-11, [-2.37187654299357e-7, 1.1842143766173123e-5])\n\n\nWhile for the previous problem of the same kind and size the steepest descent method converged in just a few steps, for this particular data it takes many dozens of steps.\nThe culprit here are bad properties of the Hessian matrix Q. By “bad properties” we mean the so-called ill-conditioning, which is reflected in the very high condition number. Recall that condition number \\kappa for a given matrix \\mathbf A is defined as \n\\kappa(\\mathbf A) = \\|\\mathbf A^{-1}\\|\\cdot \\|\\mathbf A\\|,\n and it can be computed as ratio of the largest and smallest singular values, that is, \n\\kappa(\\mathbf A) = \\frac{\\sigma_{\\max}(\\mathbf A)}{\\sigma_{\\min}(\\mathbf A)}.\n\nIdeally this number should not be much larger than 1. In the example above it is\n\n\nShow the code\ncond(Q)\n\n\n1668.0010671466664\n\n\nwhich is well above 1000. Is there anything that we can do about it? The answer is yes. We can scale the original date to improve the conditioning.\n\n\nScaled gradient method for ill-conditioned problems\nUpon introducing a matrix \\mathbf S that relates the original vector variable \\bm x with a new vector variable \\bm y according to \n\\bm x = \\mathbf S \\bm y,\n the optimization cost function changes from f(\\bm x) to f(\\mathbf S \\bm y). Let’s rename the latter to g(\\bm y). And we will now examine how the steepest descent iteration changes. Straightforward application of the chain rule for finding derivatives of composite functions yields \ng'(\\bm y) = f'(\\mathbf S\\bm y) = f'(\\mathbf S\\bm y)\\mathbf S.\n\nKeeping in mind that gradients are transposes of derivatives, we can write \n\\nabla g(\\bm y) = \\mathbf S^\\top \\nabla f(\\mathbf S\\bm y).\n\nSteepest descent iterations then change accordingly\n\n\\begin{aligned}\n\\bm y_{k+1} &= \\bm y_k - \\alpha_k \\nabla g(\\bm y_k)\\\\\n\\bm y_{k+1} &= \\bm y_k - \\alpha_k \\mathbf S^T\\nabla f(\\mathbf S \\bm y_k)\\\\\n\\underbrace{\\mathbf S \\bm y_{k+1}}_{\\bm x_{k+1}} &= \\underbrace{\\mathbf S\\bm y_k}_{\\bm x_k} - \\alpha_k \\underbrace{\\mathbf S \\mathbf S^T}_{\\mathbf D}\\nabla f(\\underbrace{\\mathbf S \\bm y_k}_{\\bm x_k}).\n\\end{aligned}\n\nUpon renaming the product \\mathbf S \\mathbf S^T as a scaling matrix \\mathbf D, a single iteration changes to \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - \\alpha_k \\mathbf D\\nabla f(\\bm x_{k}).}\n\\tag{3}\nThe key question now is: how to choose the matrix \\mathbf D?\nWe would like to make the Hessian matrix \\nabla^2 f(\\mathbf S \\bm y) (which in the case of a quadratic matrix form is the matrix \\mathbf Q as we used it above) better conditioned. Ideally, \\nabla^2 f(\\mathbf S \\bm y)\\approx \\mathbf I.\nA simple way for improving the conditioning is to define the scaling matrix \\mathbf D as a diagonal matrix whose diagonal entries are given by \n\\mathbf D_{ii} = [\\nabla^2 f(\\bm x_k)]^{-1}_{ii}.\n\nIn words, the diagonal entries of the Hessian matrix are inverted and they then form the diagonal of the scaling matrix.\n\n\n\n\n\n\nHighlighting the structure of the scaled gradient method\n\n\n\nIt is worth emphasizing how the algorithm changed: the direction of steepest descent (the negative of the gradient) is premultiplied by some (scaling) matrix. We will see in a few moments that another method – Newton’s method – has a perfectly identical structure.\n\n\n\n\n\nNewton’s method\nNewton’s method is one of flagship algorithms in numerical computing. With confidence I include it in my personal Top 10 list of algorithms relevant for engineers and scientists. We may encounter the method in two settings:\n\nas a method for solving (systems of) nonlinear equations (aka rootfinding),\nand as a method for optimization.\n\nThe two are inherently related and it is useful to be able to see the connection.\n\nNewton’s method for rootfinding\nThe problem to be solved is that of finding x\\in\\mathbb R for which a given function g() vanishes. In other words, we solve the following equation \ng(x) = 0.\n\nThe above state scalar version has also its vector extension \n\\mathbf g(\\bm x) = \\mathbf 0,\n in which \\bm x stands for an n-tuple of variables, that is, \\bm x\\in \\mathbb R^n, and \\mathbf g() denotes an n-tuple of functions. Even more general version allows for different number of variables and equations.\nWe start with a scalar version. A single iteration of the method evaluates not only the value g(x_k) of the function g at the given point x_k but also its derivative g'(x_k). It then uses the two to approximate the function g() at x_k by a linear (actually affine) function and computes the intersection of this approximating function with the horizontal axis. This gives as x_{k+1}, that is, the (k+1)-th approximation to a solution (root). We can write this down as \n\\begin{aligned}\n\\underbrace{g(x_{k+1})}_{0} &= g(x_{k}) + g'(x_{k})(x_{k+1}-x_k)\\\\\n0 &= g(x_{k}) + g'(x_{k})x_{k+1}-g'(x_{k})x_k,\n\\end{aligned}\n from which the famous formula follows \n\\boxed{x_{k+1} = x_{k} - \\frac{g(x_k)}{g'(x_k)}.}\n\nIn the vector form, the formula is \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - [\\nabla \\mathbf g(\\bm x_k)^\\top]^{-1}\\mathbf g(\\bm x_k),}\n where \\nabla \\mathbf g(\\bm x_k)^\\top is the (Jacobian) matrix of the first derivatives of \\mathbf g at \\bm x_k, that is, \\nabla \\mathbf g() is a matrix with the gradient of the g_i(\\bm x) function as its i-th column.\n\n\nNewton’s method for optimization\nOnce again, we restrict ourselves to a scalar case first. The problem is \n\\operatorname*{minimize}_{x\\in\\mathbb{R}}\\quad f(x).\n\nAt the k-th iteration of the algorithm, the solution is x_k. The function to be minimized is approximated at x_k by a quadratic function m_k(). In order to find parameterization of this quadratic function, not only the function f() itself but also its first and second derivatives, f'() and f''(), respectively, must be evaluated at x_k. Using these three, a function m_k(x) approximating f(x) at some x not too far from x_k can be defined \nm_k(x) = f(x_k) + f'(x_k)(x-x_k) + \\frac{1}{2}f''(x_k)(x-x_k)^2.\n\nThe problem of minimizing this new function in the k-th iteration is then formulated, namely,\n\n\\operatorname*{minimize}_{x_{k+1}\\in\\mathbb{R}}\\quad m_k(x_{k+1}).\n\nThe way to find this solution is straightforward: find the derivative of m_k() and find the value of x_{k+1} for which this derivative vanishes. The result is \n\\boxed{x_{k+1} = x_{k} - \\frac{f'(x_k)}{f''(x_k)}.}\n\n\nExample 4 (Newton’s method for minimization in the scalar case) We consider the scalar function of a single (real) variable\n\n\nShow the code\n# f(x) = 3cos(x)*sin(2x+2)^2\nf(x) = -x^4 + 12x^3 - 47x^2 + 60x\n\n\nf (generic function with 1 method)\n\n\nLet’s have a look at its graph on some interval.\n\n\nShow the code\nx_grid = 1:0.01:5.5\nf_grid = f.(x_grid)\n\nusing Plots\nplot(x_grid,f_grid,label=\"f(x)\",xlabel=\"x\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, the essence of the method is that at a given point x_k we approximate the function locally by a quadratic function. Say, we choose\n\n\nShow the code\n#xₖ = 0.25\nxₖ = 2.4\n\n\n2.4\n\n\nWe can evaluate at this point not only the function but its first and second derivatives as well. We use one of the packages for AD.\n\n\nShow the code\nusing ForwardDiff\nDf = x -&gt; ForwardDiff.derivative(f,x)\nD²f = x -&gt; ForwardDiff.derivative(Df,x)\n\n\n#6 (generic function with 1 method)\n\n\nNow, we can write the quadratic “model function” as\n\n\nShow the code\nmₖ(x) = f(xₖ) + Df(xₖ)*(x-xₖ) + 1/2*D²f(xₖ)*(x-xₖ)^2\n\n\nmₖ (generic function with 1 method)\n\n\nWe now evaluate this model on the interval and plot it:\n\n\nShow the code\nmₖ_grid = mₖ.(x_grid)\n\nplot!(x_grid,mₖ_grid,label=\"mₖ(x)\")\nscatter!([xₖ,],[f(xₖ),],label=\"f(xₖ)\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the formula for the Newton’s step we get that\n\n\nShow the code\nxₖ₊₁ = xₖ - Df(xₖ)/D²f(xₖ)\n\n\n3.798347107438017\n\n\n\n\nShow the code\nscatter!([xₖ₊₁],[f(xₖ₊₁)],label=\"f(xₖ₊₁)\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s go for another iteration:\n\n\nShow the code\nplot(x_grid,f_grid,label=\"f(x)\",xlabel=\"x\")\n\nmₖ₊₁(x) = f(xₖ₊₁) + Df(xₖ₊₁)*(x-xₖ₊₁) + 1/2*D²f(xₖ₊₁)*(x-xₖ₊₁)^2\nmₖ₊₁_grid = mₖ₊₁.(x_grid)\nplot!(x_grid,mₖ₊₁_grid,label=\"mₖ₊₁(x)\")\nscatter!([xₖ,],[f(xₖ),],label=\"f(xₖ)\")\nscatter!([xₖ₊₁,],[f(xₖ₊₁),],label=\"f(xₖ₊₁)\")\n\nxₖ₊₂ = xₖ₊₁ - Df(xₖ₊₁)/D²f(xₖ₊₁)\nscatter!([xₖ₊₂],[f(xₖ₊₂)],label=\"f(xₖ₊₂)\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd the iterations would go on…\nBefore we leave this example, you are invited to experiment with setting different initial values of x.\n\nThe vector version of the Newton’s step is \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - [\\nabla^2 f(\\bm x_k)]^{-1} \\nabla f(\\bm x_k).}\n\nAlthough the mathematical formula contains a symbol for the inverse of a matrix, computationally it is better to formulate this computation in a way that a system of linear equations is solved. Namely, denoting the second term on the right by \\bm d_k, our symbol for the direction, that is, [\\nabla^2 f(\\bm x_k)]^{-1} \\nabla f(\\bm x_k) = \\bm d_k, we can find \\bm d_k by solving the following system of linear equations\n\n\\nabla^2 f(\\bm x_k) \\bm d_k = \\nabla f(\\bm x_k).\n\n\n\n\n\n\n\nInverse of a matrix is rarely needed in numerical computations\n\n\n\nIndeed, discussion forums for various programming languages and environments abound with questions about how to find the inverse of a matrix. The answer is almost always: are you sure you really need it? Most probably what you need is to solve a systems of linear equations, and that is a (slightly) different task.\n\n\n\nExample 5 (Newton’s method for minimization in the vector case) Consider the function\n\nf(x) = (x[1]+1)^4 + x[1]*x[2] + (x[2]+1)^4\n\nf (generic function with 1 method)\n\n\nThe graph of the function is in Fig. 2 below.\n\n\nShow the code\nx₁_grid = x₂_grid = -3:0.1:3;  \nf_grid = [f([x₁,x₂]) for x₁=x₂_grid, x₂=x₂_grid];\n\nusing Plots\nsurface(x₁_grid,x₂_grid,f_grid')   # Note the transpose.\nxlabel!(\"x₁\")\nylabel!(\"x₂\")\nxlims!(-3,3) \nylims!(-3,3)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Graph of the example function\n\n\n\n\nSome more insight can perhaps be obtained by plotting the contours as in Fig. 3.\n\n\nShow the code\ncontour(x₁_grid,x₂_grid,f_grid',levels=50)  # Note the transpose.\nxlabel!(\"x₁\")\nylabel!(\"x₂\")\nxlims!(-3,3) \nylims!(-3,3)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Contour graph of the example function\n\n\n\n\nWe choose to compute the gradient manually/symbolically:\n\n∇f(x) = [4*(1 + x[1])^3 + x[2], x[1] + 4*(1 + x[2])^3]\n\n∇f (generic function with 1 method)\n\n\nand similarly we do for the Hessian:\n\n∇²f(x) = [12*(1 + x[1])^2 1; 1 12*(1 + x[2])^2]\n\n∇²f (generic function with 1 method)\n\n\nWe now consider a particular value of the vector variable and use not only the function value but also the gradient and the Hessian to compute the local quadratic model:\n\n\nShow the code\nxₖ = [0.7,0.5]\n\nusing LinearAlgebra     # Because of the dot function\nmₖ(x) = f(xₖ) + dot(∇f(xₖ),(x-xₖ)) + 1/2*dot((x-xₖ),∇²f(xₖ),(x-xₖ))\n\n\nmₖ (generic function with 1 method)\n\n\nThe contours of the local quadratic model and the contours of the original function can be plotted together as in Fig. 4.\n\n\nShow the code\nmₖ_grid = [mₖ([x₁,x₂]) for x₁=x₁_grid, x₂=x₂_grid];\ncontour!(x₁_grid, x₂_grid, mₖ_grid',levels=10)\nscatter!([xₖ[1],],[xₖ[2],],label=\"xₖ\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Contours of a quadratic model over those for the original example function\n\n\n\n\nThe following step of the Newton’s method would constitute in finding the minimum of the quadratic function, that is, to localize the center of the contour ellipses. It can be guessed that such minimum will be located in the direction to the bottom left from the initial point, which is consistent with the contours of the original cost function.\n\n\nShow the code\nxₖ₊₁ = xₖ - ∇²f(xₖ)\\∇f(xₖ)\nscatter!([xₖ₊₁[1],],[xₖ₊₁[2],],label=\"xₖ₊₁\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now compute a whole sequence of such local minimizers of quadratic models. In other words, we demonstrate the functionality of a basic implementation of Newton’s method.\n\n\nShow the code\nusing Printf\n\nfunction newton_method(f,∇f,∇²f,x₀,ϵ,N)\n    x = x₀\n    k = 0\n    while (norm(∇f(x)) &gt; ϵ)\n        k = k+1\n        x = x - ∇²f(x)\\∇f(x)\n        @printf(\"iter = %3d     ||∇f|| = %17.6f     f = %19.6f \\n\",k,norm(∇f(x)),f(x))\n        if k &gt;= N\n         return f(x),x\n        end\n    end\n    return f(x),x\nend\n\n\nnewton_method (generic function with 1 method)\n\n\nAfter setting the termination parameters we are ready to start the iterative algorithm:\n\n\nShow the code\nϵ = 1e-8\nN = 100\nx₀ = xₖ\n\nf_opt,x_opt = newton_method(f,∇f,∇²f,x₀,ϵ,N)\n\n\niter =   1     ||∇f|| =          7.104527     f =            2.630340 \niter =   2     ||∇f|| =          1.872767     f =            0.650808 \niter =   3     ||∇f|| =          0.365832     f =            0.390200 \niter =   4     ||∇f|| =          0.034334     f =            0.375189 \niter =   5     ||∇f|| =          0.000780     f =            0.375000 \niter =   6     ||∇f|| =          0.000001     f =            0.375000 \niter =   7     ||∇f|| =          0.000000     f =            0.375000 \n\n\n(0.375, [-0.4999999999999264, -0.5000000000000205])\n\n\nFinally, plot the minimizer into the contour plots.\n\nscatter!([x_opt[1],],[x_opt[2],],label=\"x⋆\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient implementation by exploiting the symmetry and the positive definiteness of Hessian\nIn our straightforward implementation of the Newton’s method, we formulated a linear system of equations \\mathbf A\\bm x = \\mathbf b, and we used the backslash operator (introduced by K. Hensel in 1929, and adopted and popularized by Matlab, and then imported by other languages, including Juluia) to find the solution by writing x = A\\b. Although there is some decision tree behind the backslash operator (for example, depending on the shape of the matrix), more detailed analysis of the properties of the matrix is typically not performed automatically when calling x = A\\b. But the Hessian matrix \\nabla^2 f(\\bm x_k) (playing the role of the matrix \\mathbf A in the previous sentences) does possess important properties. First, it is symmetric. This can be exploited to solve the system of equations more efficiently. Second, well-behaved Hesians are positive definite in the vicinity of a local minimum. For this class of matrices, an efficient way to solve the corresponding system of equations is to use the Cholesky decomposition (factorization). (By the way, the pronounciation is ʃəˈlɛski.) A real positive-definite matrix \\mathbf A can be decomposed as\n\n\\mathbf A = \\bm L \\bm L^\\top,\n where \\bm L is a lower triangular matrix.\n\nExample 6 (Solving a system of linear equations with a symmetric positive definite matrix using Cholesky decomposition)  \n\n\nShow the code\nA = [10 1 0; 1 20 3; 0 3 30]\nb = [1, 2, 3]\n\nusing LinearAlgebra\nC = cholesky(A)\n\n\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 3.16228  0.316228  0.0\n  ⋅       4.46094   0.672504\n  ⋅        ⋅        5.43578\n\n\nC is a structure that contains the decomposition. We can access the lower triangular matrix \\bm L as\n\n\nShow the code\nL = C.L\n\n\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n 3.16228    ⋅         ⋅ \n 0.316228  4.46094    ⋅ \n 0.0       0.672504  5.43578\n\n\nHaving computed the lower triangular matrix \\bm L, we can write the original problem as \n\\bm L \\bm L^\\top \\bm x = \\mathbf b,\n which can be reformulated into solving two triangular systems by simple backsubstitution: \n\\begin{aligned}\n\\bm L \\bm y &= \\mathbf b,\\\\\n\\bm L^\\top \\bm x &= \\bm y.\n\\end{aligned}\n\n\n\nShow the code\ny = L\\b\n\n\n3-element Vector{Float64}:\n 0.31622776601683794\n 0.42591904767910926\n 0.49920457702436966\n\n\n\n\nShow the code\nx = L'\\y\n\n\n3-element Vector{Float64}:\n 0.09183673469387756\n 0.08163265306122447\n 0.09183673469387756\n\n\nBoth steps are realized by the following single line of code in Julia, but note that what is behind is really solving the two triangular systems by backsubstitution\n\n\nShow the code\nx = C\\b\n\n\n3-element Vector{Float64}:\n 0.09183673469387756\n 0.08163265306122447\n 0.09183673469387757\n\n\nYou can verify by yourself (using @btime macro from the BenchmarkTools package) that the decomposition followed by solving two triangular systems is faster than calling a general solver for linear systems of equations.\n\nBack to the Newton’s method. We can make a few observations:\n\nIf compared to the general prescription for descent direction methods (as described in Eq. 1), the Newton’s method determines the direction and the step lenght at once (both \\alpha_k and \\mathbf d_k are contained in the term - [\\nabla^2 f(\\mathbf x_k)]^{-1} \\nabla f(\\mathbf x_k)).\nIf compared with steepest descent (gradient) method, especially with its scaled version in Eq. 3, Newton’s method fits into the framework nicely because the inverse [\\nabla^2 f(\\mathbf x_k)]^{-1} of the Hessian can be regarded as a particular choice of a scaling matrix \\mathbf D. Indeed, you can find arguments in some textbooks that Newton’s method involves scaling that is optimal in some sense. We skip the details here because we only wanted to highlight the similarity in the structure of the two methods.\n\nThe great popularity of Newton’s method is mainly due to its nice convergence – quadratic. Although we skip any discussion of convergence rates here, note that for all other methods this is an ideal that is to be approached.\nThe plausible convergence rate of Newton’s method is paid for by a few disadvantages\n\nThe need to compute the Hessian. This is perhaps not quite obvious with simple problems but it can play some role with larger problems (recall our discussion of symbolic methods for finding derivatives).\nOnce the Hessian is computed, it must be inverted (actually, a linear system must by solved). Although we have already discussed the efficient method based on Cholesky decomposition, it is still quite some computational work.\nThe necessity to solve a linear system of equations requires that the Hessian be nonsingular. When can an optimization problem lose nonsingularity of its Hessian? And can anything be done, when it does so?\nAnd it is not only that the Hessian must be nonsingular, but it must also be positive (definite). Note that in the scalar case this corresponds to the situation when the second derivative is positive. Negativeness of the second derivative can send the algorithm in the opposite direction – away from the local minimum – , which would ruin the convergence of the algorithm completely.\n\nThe last two issues are handled by some modification of the standard Newton’s method.\n\n\nDamped Newton’s method\nA parameter \\alpha\\in(0,1) is introduced that shortens the step as in \n  \\bm x_{k+1} = \\bm x_{k} - \\alpha\\left[\\nabla^2 f(\\bm x_k)\\right]^{-1} \\nabla f(\\bm x_k).\n\n\n\nModifying the Hessian so that it is positive definite\n\n  \\bm x_{k+1} = \\bm x_{k} - \\left[\\nabla^2 f(\\bm x_k)+ \\lambda \\mathbf I\\right]^{-1} \\nabla f(\\bm x_k).\n\n\n\nFixed constant positive definite matrix instead of the inverse of the Hessian\nThe step is determined as \n  \\bm x_{k+1} = \\bm x_{k} - \\mathbf B \\nabla f(\\bm x_k).\n\nNote that the interpretation of the constant \\mathbf B in the position of the (inverse of the) Hessian in the rootfinding setting is that the slope of the approximating linear (affine) function is always constant.\nNow that we admitted to have something else then just the (inverse of the) Hessian in the formula for Newton’s method, we can explore further this new freedom. This will bring us into a family of methods called Quasi-Newton methods.\n\n\n\n\nQuasi-Newton’s methods\n#TODO In the meantime, have a look at [1, Sec. 4.4.4], or [2, Sec. 6.3], or [3, Ch. 13].\nSimilarly as we did when introducing the Newton’s method, we start our exposition with solving equations. Quasi-Newton methods (indeed, the plural is appropriate here because there is a whole family of methods under this name) generalize the key idea behind the (scalar) secant method for rootfinding. Let’s recall it here. The methods is based on secant approximation of the derivative: \n\\dot f(x_k) \\approx \\frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}.\n\nWe substitute this approximation into the Newton’s formula \nx_{k+1} = x_k - \\underbrace{\\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})}}_{\\approx \\dot f(x_k)}f(x_k).\n\nTransitioning from scalar rootfinding to optimization is as easy as increasing the order of the derivatives in the formula\n\n\\ddot f(x_k) \\approx \\frac{\\dot f(x_k)-\\dot f(x_{k-1})}{x_k-x_{k-1}} =: b_k,\n which can be rewritten into the secant condition \nb_k (\\underbrace{x_k-x_{k-1}}_{s_{k-1}}) = \\underbrace{\\dot f(x_k)-\\dot f(x_{k-1})}_{y_{k-1}}.\n\nThe vector version of the secant condition is\n\n\\begin{aligned}\\boxed{\n\\bm B_{k+1} \\mathbf s_k = \\mathbf y_k},\n\\end{aligned}\n where \\bm B_{k+1} is a matrix (to be determined) with Hessian-like properties\n\n\\bm B_{k+1} = \\bm B_{k+1}^\\top, \\qquad \\bm B_{k+1} \\succ \\mathbf 0.\n\nHow can we get it? Computing the matrix at every step anew is not computationally efficient. The preferred way is to compute the matrix \\bm B_{k+1} just by adding as small an update to the matrix \\bm B_{k+1} computed in the previous step as possible \n\\bm B_{k+1} = \\bm B_{k} + \\text{small update}.\n\nSeveral update schemes are documented in the literature. Particularly attractive are schemes that update not just \\bm B_{k+1} but \\bm B_{k+1}^{-1} directly. One popular update is BFGS:\n\\boxed{\n\\begin{aligned}\n\\bm H_{k+1} &= \\bm H_{k} + \\left(1+\\frac{\\mathbf y_k^\\top \\bm H_k \\mathbf y_k}{\\mathbf s_k^\\top\\mathbf y_k}\\right)\\cdot\\frac{\\mathbf s_k\\mathbf s_k^\\top}{\\mathbf s_k^\\top \\mathbf y_k} - \\frac{\\mathbf s_k \\mathbf y_k^\\top \\bm H_k + \\bm H_k\\mathbf y_k \\mathbf s_k^\\top}{\\mathbf y_k^\\top \\mathbf s_k}.\n\\end{aligned}}",
    "crumbs": [
      "2. Optimization – algorithms",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html#trust-region-methods",
    "href": "opt_algo_unconstrained.html#trust-region-methods",
    "title": "Algorithms for unconstrained optimization",
    "section": "Trust region methods",
    "text": "Trust region methods\n#TODO In the meantime, have a look at [1, Sec. 4.5], or [2, Sec. 4.4].\nThe key concept of trust region methods is that of… trust region. Trust region is a region (typically a ball or an ellipsoid) around the current point, in which we trust some approximation of the original cost function. We then find the minimum of this approximating function subject to the constraint on the norm of the step. Typically, the approximating function that is simple enough to minimize is a quadratic one: \nm_k(\\bm d) = f(\\bm x_k) + \\nabla f(\\bm x_k)^\\top \\bm d + \\frac{1}{2}\\bm d^\\top \\underbrace{\\nabla^2 f(\\bm x_k)}_{\\text{or} \\approx} \\bm d\n but trust the model only within \n\\|\\bm d\\|_2 \\leq \\delta_k.\n\nIn other words, we formulate the constrained optimization problem \\boxed{\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm d\\in\\mathbb R^n} &\\quad m_k(\\bm d)\\\\\n\\text{subject to} &\\quad \\|\\bm d\\|_2 \\leq \\delta_k.\n\\end{aligned}}\n\nFor later convenience (when differentiating the Lagrangian), we rewrite the constraint as \n\\frac{1}{2}\\left(\\|\\bm d\\|_2^2 - \\delta_k^2\\right) \\leq 0.\n\nLet’s write down the optimality conditions for this constrained problem. The Lagrangian is \nL(\\bm x_k, \\bm d) = f(\\bm x_k) + \\nabla f(\\bm x_k)^\\top \\bm d + \\frac{1}{2}\\bm d^\\top \\nabla^2 f(\\bm x_k)\\bm d + \\frac{\\mu}{2} (\\|\\bm d\\|^2-\\delta_k^2)\n\nThe necessary conditions (the KKT conditions) can be written upon inspection \n\\begin{aligned}\n\\nabla_{\\bm{d}}L(\\bm x_k, \\bm d) = \\nabla f(\\bm x_k) + \\nabla^2 f(\\bm x_k) \\bm d + \\mu \\bm d &= 0,\\\\\n\\|\\bm d\\|_2^2 - \\delta_k^2 &\\leq 0,\\\\\n\\mu &\\geq 0,\\\\\n\\mu \\left(\\|\\bm d\\|_2^2 - \\delta_k^2\\right) &= 0.\n\\end{aligned}\n\nNow, there are two scenarios: either the optimal step \\bm d keeps the updated \\bm x_{k+1} still strictly inside the trust region, or the updated \\bm x_{k+1} is at the boundary of the trust region. In the former case, since the constraint is satisfied strictly, the dual variable \\mu=0 and the optimality condition simplifies to \\nabla f(\\bm x_k) + \\nabla^2 f(\\bm x_k) \\bm d= 0, which leads to the standard Newton’s update \\bm d = -[\\nabla^2 f(\\bm x_k)]^{-1}\\nabla f(\\bm x_k). In the latter case the update is\n\n\\bm d = -[\\nabla^2 f(\\bm x_k) + \\mu \\mathbf I]^{-1}\\nabla f(\\bm x_k),\n which has the form that we have already discussed when mentioning modifications of Newton’s method.\nLet’s recall here our discussion of line search methods – we argued that there is rarely a need to compute the minimum at each step, and “good enough” reductions of the cost function typically suffice. The situation is similar for the trust region methods – approximate solution to the minimization (sub)problem is enough. However, here we are not going to discuss such methods here.\nOne issue that, however, requires discussion, is the issue of evaluationg the predictive performance of the (quadratic) model. If the model is not good enough, the trust region must be shrunk, if it is fairly good, the trust region must be expanded. In both cases, the constrained optimization (sub)problem must be solved again.\nOne metric we can use to evaluate the model is\n\n\\eta = \\frac{\\text{actual improvement}}{\\text{predicted improvement}} = \\frac{f(\\bm x_k)-f(\\bm x_{k+1})}{f(\\bm x_k)-m_k(\\bm x_{k+1})}.\n\nWe shrink the region for small \\eta (\\approx 0), and expand it for larger \\eta (\\approx 1).\nWe conclude this short discussion of trust region methods by comparing it with the descent methods. While in the descent methods we set the direction first, and the perform a line search in the chosen direction, in trust region methods this sequence is reversed. Kind of. By setting the radius of the trust region, we essentially set an upper bound on the step length. The subsequent optimization subproblem can be viewed as a search for a direction.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Unconstrained optimization"
    ]
  },
  {
    "objectID": "discr_indir_goals.html",
    "href": "discr_indir_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Give the first-order necessary conditions of optimality for a general optimal control problem for a nonlinear discrete-time system over a finite horizon. Namely, give the general two-point boundary value problem, highlighting the state equation, the co-state equation and a stationarity equation. Do not forget to include general boundary conditions.\nGive the first-order necessary conditions of optimality for a linear and time invariant (LTI) discrete-time system and a quadratic cost function over a finite horizon. Namely, give them in the format displaying the state equation, co-state equation and stationarity equation. Show and discuss also two types of boundary conditions.\nGive a qualitative characterization of the solution to the fixed final state LQ-optimal control problem over a finite horizon, that is, you do not have to give formulas but you should be able to state among the highlights that the control is open-loop and that reachability of the system is a necessary condition.\nGive a qualitative characterization of the solution to the free final state LQ-optimal control problem over a finite horizon, that is, you do not have to give formulas but you should be able to state among the highlights that the control is closed-loop, namely, a time-varying linear state feedback and that the feedback gains can be computed by solving a difference Riccati equation.\nDiscuss how solution to the free final state LQ problem changes as the horizon is extended to infinity. Emphasize that the optimal solution is given by a constant linear state feedback whose gains are computed by solving a discrete-time algebraic Riccati equation (DARE). What are the conditions under which a stabilizing solution is guaranteed to exist? What are the conditions under which it is guaranteed that there is a unique stabilizing solution of DARE?",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "discr_indir_goals.html#knowledge-remember-and-understand",
    "href": "discr_indir_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Give the first-order necessary conditions of optimality for a general optimal control problem for a nonlinear discrete-time system over a finite horizon. Namely, give the general two-point boundary value problem, highlighting the state equation, the co-state equation and a stationarity equation. Do not forget to include general boundary conditions.\nGive the first-order necessary conditions of optimality for a linear and time invariant (LTI) discrete-time system and a quadratic cost function over a finite horizon. Namely, give them in the format displaying the state equation, co-state equation and stationarity equation. Show and discuss also two types of boundary conditions.\nGive a qualitative characterization of the solution to the fixed final state LQ-optimal control problem over a finite horizon, that is, you do not have to give formulas but you should be able to state among the highlights that the control is open-loop and that reachability of the system is a necessary condition.\nGive a qualitative characterization of the solution to the free final state LQ-optimal control problem over a finite horizon, that is, you do not have to give formulas but you should be able to state among the highlights that the control is closed-loop, namely, a time-varying linear state feedback and that the feedback gains can be computed by solving a difference Riccati equation.\nDiscuss how solution to the free final state LQ problem changes as the horizon is extended to infinity. Emphasize that the optimal solution is given by a constant linear state feedback whose gains are computed by solving a discrete-time algebraic Riccati equation (DARE). What are the conditions under which a stabilizing solution is guaranteed to exist? What are the conditions under which it is guaranteed that there is a unique stabilizing solution of DARE?",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "discr_indir_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "discr_indir_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nDesign an LQ-optimal state feedback controller for a discrete-time linear system both for a finite and an infinite horizon, both for regulation and for tracking.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Learning goals"
    ]
  },
  {
    "objectID": "intro_rules.html",
    "href": "intro_rules.html",
    "title": "Rules of the course",
    "section": "",
    "text": "In this course we follow, at least partially, the concept of flipped learning. This means that you are expected to study a bit on your own even before the lecture. The study material is made available through this web site. It consists primarily of texts, codes and frequently also videos. At the lecture we will not repeat the introductory material, instead we will discuss it, and focus on the more advanced topics and examples.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#studying-starts-before-the-lecture",
    "href": "intro_rules.html#studying-starts-before-the-lecture",
    "title": "Rules of the course",
    "section": "",
    "text": "In this course we follow, at least partially, the concept of flipped learning. This means that you are expected to study a bit on your own even before the lecture. The study material is made available through this web site. It consists primarily of texts, codes and frequently also videos. At the lecture we will not repeat the introductory material, instead we will discuss it, and focus on the more advanced topics and examples.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#learning-goals",
    "href": "intro_rules.html#learning-goals",
    "title": "Rules of the course",
    "section": "Learning goals",
    "text": "Learning goals\nTo guide you in your self-study before the lecture, we always provide a set of learning goals for the given (weakly) block. These learning goals are structured into two parts – knowledge and understading, and problem-solving skills. These are formulated in a way that guides you while studying. Always start by reading this list and only then proceed to the study material. Then come back to the learning goals and check if you can tick them off.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#online-quizzes",
    "href": "intro_rules.html#online-quizzes",
    "title": "Rules of the course",
    "section": "Online quizzes",
    "text": "Online quizzes\nIn order to push you gently towards such studying before the lecture, you will be asked to fill in a short online quiz (in Brute system) even before the actual lecture.\n\n\n\n\n\n\nDeadlines for online quizzes\n\n\n\nThe deadlines are always set to the very beginning of the lectures, that is, Wednesday, 11:00 am.\n\n\nThese quizzes will not test if you achieved all the learning goals – you are not certainly not expected to learn everything by yourself. Typically only some easier learning goals of the “knowledge and understanding” type will be tested.\nThe role of such quizzes in the overall grading is described later on this page.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#howework-problems",
    "href": "intro_rules.html#howework-problems",
    "title": "Rules of the course",
    "section": "Howework problems",
    "text": "Howework problems\nIn our course, homework problems are assigned weekly. Homework problems are typically related to the “skills” part of the learning goals, and as such they will almost exclusively consist of writing some computer code.\nThe second (\\(\\approx\\)) half of the exercises session will be always dedicated to the homework problem(s). Ideally you will be able to solved the problems before leaving the classroom, but at least you will have an opportunity to get started with solving the problem, while taking advantage of having a lecturer and other students nearby.\nA challenging – but certainly rewarding as well – attribute of our homework problems is that their solutions are expected to be programmed in Julia language (see Software for the course).\nSimilarly as the online quizzes, you will be expected to submit the solutions through the Brute system, which will give an immediate feedback if the submitted solution is accepted or not. #TODO: what if the submitted solution is evaluated as incorrect? Can it be resubmitted? Immediately? How many times?\n\n\n\n\n\n\nDeadlines for submission of solutions to homework problems\n\n\n\nThe deadlines are always set to Wednesday, just before the beginning of the corresponding exercises, e.g., 12:45, 14:30, 16:15.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#semestral-project",
    "href": "intro_rules.html#semestral-project",
    "title": "Rules of the course",
    "section": "Semestral project",
    "text": "Semestral project\nStudents aiming at the final grading A must work on one project from our list during the semester (details will be made available shortly after the beginning of the semester). Students with no ambitions for the A grade do not have to work on the projects.\nAlthough students are encouraged to collaborate, no need to hide it, the submitted . #TODO: this should be made more precise.\nAt the end of the semester, a short report will have to be presented to the teacher. #TODO: students are typically finishing it during the exam period, do they have to do it before their own exam?\nAn ideal format of such report is a git repository within the FEE Gitlab containing a short text, software code, possibly data sets (from simulations or experiments), and graphs, photos, videos (from experiments).\nIts grading is binary too –⁠ either it is accepted by the teacher or not. In case the report is not accepted, details will be given and the student can correct/improve/extend the report and resubmit. Note however, that at least one week (5 working days) is needed for the teacher to evaluate the report. A report submitted a few days before the deadline stands no chance to be modified and resubmitted.",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "intro_rules.html#grading",
    "href": "intro_rules.html#grading",
    "title": "Rules of the course",
    "section": "Grading",
    "text": "Grading\n\nOnline quizzes and homework problems\nThe primary motivation for introduction of the online quizzes and homework problems is to help your learning process (you receive feedback in real time). Your performance in quizzes and homework problems does not affect the final grade. Feel free to make some mistakes while learning.\nHowever, some thresholds do have to be passed in quizzes and homework problems to earn the credit (“zápočet” in Czech), and to be allowed to take an exam. The bar is not high, but if you are neglecting the quizzes and homework assignments, your passing will be in danger. Namely, at least 70% of competences (demonstrated in quizzes and homework problems) must be provably mastered. #TODO: how about retaking the quizzes and resubmitting the hw solutions?\n\n\nFinal exam\nThe final grade will be based on the exam organized at the end of the semester (in the exam period). The final exam will have both an open-book and closed-book parts. #TODO: details will be added.\n\n\nOverall grade\nGrading of the two partial exams will be done according to the following standard table, where the percentages refer to the number of maximum points that could be obtained in the given exam.\n\n\n\nGrading\nOpen-book exam\nClosed-book exam\n\n\n\n\nA\n90 %\n90 %\n\n\nB\n80 %\n80 %\n\n\nC\n70 %\n70 %\n\n\nD\n60 %\n60 %\n\n\nE\n50 %\n50 %\n\n\nF\nbelow 50 %\nbelow 50 %\n\n\n\nUltimately, in order to arrive at the overall final grade (A through F) we take the minimum of the two partial exam grades.\n\n\nRole of the semestral project in the grading\nWhile the semestral projects are not mandatory, without them you will not be able to the the highest grade A. On the other hand, if you do the project and your report is accepted, and you pass the exam, the grade you receive in the exam will be automatically improved by one (e.g., from C do B).",
    "crumbs": [
      "0. Introduction",
      "Rules of the course"
    ]
  },
  {
    "objectID": "discr_indir_hw.html",
    "href": "discr_indir_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Specifically, you should design a discrete-time state-feedback controller that performs fast “sideways” motion of a small indoor quadrotor (four-rotor drone). Namely, the controller should bring a quadrotor from one horizontal position to another. A 2D model is in the figure below\n\n\n\nQuadrotor model\n\n\nand the corresponding motion equations are\n\\begin{align*}\n\\ddot y(t) &= -a(t)\\sin \\theta(t)\\\\\n\\ddot z(t) & = a(t)\\cos \\theta(t) - g\\\\\n\\ddot \\theta(t) &= \\alpha(t)\n\\end{align*} where a(t) and \\alpha(t) represent the control inputs to the system, namely the linear and rotational acceleration. This assumes that the innermost control loops are already implemented and closed.\nThe gravitational acceleration g is approximated by 10\\,\\text{m}\\,\\text{s}^{-2}. The position variables y(t) and z(t) have units of m, \\theta is given in rad, and the inputs a(t) and \\alpha(t) are in \\text{m}\\,\\text{s}^{-2} and \\text{rad}\\,\\text{s}^{-2}, respectively. Only concentrating on the horizontal control, the input a(t) is set to\n a(t)=\\frac{10}{\\cos\\theta(t)} resulting in \\dot z(t)=0 and the simplified dynamics\n\\begin{align*}\n\\ddot y(t)&=-10 \\tan \\theta (t),\\\\\n\\ddot \\theta(t) &= \\alpha(t).\n\\end{align*}\nThe concrete control goal is to bring the quadrotor from the initial state y(0)=1,\\,\\dot y(0)=\\theta(0)=\\dot \\theta(0)=0 to the final state y(T) = \\dot y(T)=\\theta(T)=\\dot \\theta(T)=0.\nIn addition, there are constraints on the input \\alpha(t) and on the state variable \\theta(t):\n\\begin{align*}\n|\\alpha(t)| &\\leq 100,\\\\\n|\\theta(t)| &\\leq \\frac{\\pi}{6}.\n\\end{align*}\n\n\n\nLinearize the system around the equilibrium point y=0,\\,\\dot y=0,\\,\\theta=0,\\,\\dot \\theta=0 and discretize it with a sampling time T_s=0.01\\,\\text{s}.\nIn Julia, using the linearized model, design an LQ-optimal controller that gets the quadrotor from the initial to the vicinity of the final state (\\lvert x_i(t) \\rvert \\leq 0.01 for i=1,2,3,4) in the shortest possible time you can achieve while respecting the constraints on the input and the state variable.\nFor your solution to be accepted, you need to get to the vicinity of the final state within 3 seconds.\nWe advise you to use ControlSystemsBase package for the LQ-optimal controller synthesis.\nThe controller should be implemented as a function state_feedback(x, t) that takes the current state x and the current time step t as input and returns the control input \\alpha(t).\n\n\n\n\n\n\n\nRanking\n\n\n\nAll the solutions that meet the basic requirements will be ranked according to the time it takes for the quadrotor to reach the final state. The faster the solution, the higher the ranking.\nThe top three contenders will be awarded bonus 10% (grade increase) to the practical (open-book) part of the final exam.\n\n\nYour solution should be based on the following template and should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\n\nusing OrdinaryDiffEq, ControlSystemsBase, LinearAlgebra, Plots\n\nconst Ts = 1/100\n\nfunction quadrotor!(ẋ, x, u, t)\n\n    α = max(min(u, 100), -100)\n\n    ẋ[1] = x[2]\n    ẋ[2] = -10*tan(x[3])\n    ẋ[3] = x[4]\n    ẋ[4] = α\nend\n\n## TODO Linearize and discretize the system\n\n## TODO design the LQ-optimal controller\n\nfunction state_feedback(x, t)\n\n    ## TODO implement the state-feedback controller \n\n    return 0.0\nend\n\nYou may test your implementation using the following code snippet:\n\nT0 = 0;\nTf = 3;\nts = T0:Ts:Tf;\n\nN = length(ts)\nxs = zeros(4, N)\nus = zeros(1, N-1)\nxs[:, 1] = [1.0; 0.0; 0.0; 0.0]\n\nfor i = 1:N-1\n    us[:, i] .= state_feedback(xs[:, i], i)\n    prob = ODEProblem(quadrotor!, xs[:, i], [0, Ts], us[1, i])\n    sol = solve(prob, Tsit5())\n    xs[:, i+1] = sol.u[end]\nend\n\n\np1 = plot(ts, xs[1, :], label=\"y\")\nplot!(ts, xs[2, :], label=\"ẏ\")\nplot!(ts, xs[3, :], label=\"θ\")\nplot!(ts, xs[4, :], label=\"θ̇\")\n\nplot!([T0, Tf], [-pi/6, -pi/6], label=\"θ = -π/6\", linestyle=:dash)\nplot!([T0, Tf], [pi/6, pi/6], label=\"θ = π/6\", linestyle=:dash)\n\np2 = plot(ts[1:end-1], us[1, :], label=\"u\")\nplot(p1, p2, layout=(2, 1), size=(800, 600))",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Homework"
    ]
  },
  {
    "objectID": "discr_indir_hw.html#use-the-lq-optimal-control-methodology-to-design-a-discrete-time-state-feedback-regulator-for-a-given-lti-system",
    "href": "discr_indir_hw.html#use-the-lq-optimal-control-methodology-to-design-a-discrete-time-state-feedback-regulator-for-a-given-lti-system",
    "title": "Homework",
    "section": "",
    "text": "Specifically, you should design a discrete-time state-feedback controller that performs fast “sideways” motion of a small indoor quadrotor (four-rotor drone). Namely, the controller should bring a quadrotor from one horizontal position to another. A 2D model is in the figure below\n\n\n\nQuadrotor model\n\n\nand the corresponding motion equations are\n\\begin{align*}\n\\ddot y(t) &= -a(t)\\sin \\theta(t)\\\\\n\\ddot z(t) & = a(t)\\cos \\theta(t) - g\\\\\n\\ddot \\theta(t) &= \\alpha(t)\n\\end{align*} where a(t) and \\alpha(t) represent the control inputs to the system, namely the linear and rotational acceleration. This assumes that the innermost control loops are already implemented and closed.\nThe gravitational acceleration g is approximated by 10\\,\\text{m}\\,\\text{s}^{-2}. The position variables y(t) and z(t) have units of m, \\theta is given in rad, and the inputs a(t) and \\alpha(t) are in \\text{m}\\,\\text{s}^{-2} and \\text{rad}\\,\\text{s}^{-2}, respectively. Only concentrating on the horizontal control, the input a(t) is set to\n a(t)=\\frac{10}{\\cos\\theta(t)} resulting in \\dot z(t)=0 and the simplified dynamics\n\\begin{align*}\n\\ddot y(t)&=-10 \\tan \\theta (t),\\\\\n\\ddot \\theta(t) &= \\alpha(t).\n\\end{align*}\nThe concrete control goal is to bring the quadrotor from the initial state y(0)=1,\\,\\dot y(0)=\\theta(0)=\\dot \\theta(0)=0 to the final state y(T) = \\dot y(T)=\\theta(T)=\\dot \\theta(T)=0.\nIn addition, there are constraints on the input \\alpha(t) and on the state variable \\theta(t):\n\\begin{align*}\n|\\alpha(t)| &\\leq 100,\\\\\n|\\theta(t)| &\\leq \\frac{\\pi}{6}.\n\\end{align*}\n\n\n\nLinearize the system around the equilibrium point y=0,\\,\\dot y=0,\\,\\theta=0,\\,\\dot \\theta=0 and discretize it with a sampling time T_s=0.01\\,\\text{s}.\nIn Julia, using the linearized model, design an LQ-optimal controller that gets the quadrotor from the initial to the vicinity of the final state (\\lvert x_i(t) \\rvert \\leq 0.01 for i=1,2,3,4) in the shortest possible time you can achieve while respecting the constraints on the input and the state variable.\nFor your solution to be accepted, you need to get to the vicinity of the final state within 3 seconds.\nWe advise you to use ControlSystemsBase package for the LQ-optimal controller synthesis.\nThe controller should be implemented as a function state_feedback(x, t) that takes the current state x and the current time step t as input and returns the control input \\alpha(t).\n\n\n\n\n\n\n\nRanking\n\n\n\nAll the solutions that meet the basic requirements will be ranked according to the time it takes for the quadrotor to reach the final state. The faster the solution, the higher the ranking.\nThe top three contenders will be awarded bonus 10% (grade increase) to the practical (open-book) part of the final exam.\n\n\nYour solution should be based on the following template and should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\n\nusing OrdinaryDiffEq, ControlSystemsBase, LinearAlgebra, Plots\n\nconst Ts = 1/100\n\nfunction quadrotor!(ẋ, x, u, t)\n\n    α = max(min(u, 100), -100)\n\n    ẋ[1] = x[2]\n    ẋ[2] = -10*tan(x[3])\n    ẋ[3] = x[4]\n    ẋ[4] = α\nend\n\n## TODO Linearize and discretize the system\n\n## TODO design the LQ-optimal controller\n\nfunction state_feedback(x, t)\n\n    ## TODO implement the state-feedback controller \n\n    return 0.0\nend\n\nYou may test your implementation using the following code snippet:\n\nT0 = 0;\nTf = 3;\nts = T0:Ts:Tf;\n\nN = length(ts)\nxs = zeros(4, N)\nus = zeros(1, N-1)\nxs[:, 1] = [1.0; 0.0; 0.0; 0.0]\n\nfor i = 1:N-1\n    us[:, i] .= state_feedback(xs[:, i], i)\n    prob = ODEProblem(quadrotor!, xs[:, i], [0, Ts], us[1, i])\n    sol = solve(prob, Tsit5())\n    xs[:, i+1] = sol.u[end]\nend\n\n\np1 = plot(ts, xs[1, :], label=\"y\")\nplot!(ts, xs[2, :], label=\"ẏ\")\nplot!(ts, xs[3, :], label=\"θ\")\nplot!(ts, xs[4, :], label=\"θ̇\")\n\nplot!([T0, Tf], [-pi/6, -pi/6], label=\"θ = -π/6\", linestyle=:dash)\nplot!([T0, Tf], [pi/6, pi/6], label=\"θ = π/6\", linestyle=:dash)\n\np2 = plot(ts[1:end-1], us[1, :], label=\"u\")\nplot(p1, p2, layout=(2, 1), size=(800, 600))",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Homework"
    ]
  },
  {
    "objectID": "opt_theory_hw.html",
    "href": "opt_theory_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Solve the problem of distributing a limited power to N electric vehicle chargers throughout the next K hours. Specifically, you are supposed to find an hourly-sampled optimal plan for each car that minimizes the total cost of the charging.\nYou are given a time-dependent maximum energy a[k] available for charging (in kWh), a time-dependent cost of the energy c[k] (in €/kWh), maximum allowed charging energy per hour for the ith car m_i (in kWh), total requested energy r_i for the ith car (also in kWh), and the departure (discrete) time d_i for each car.\nAll the cars are connected to chargers and can start charging from time 1. Departure time is the time when the energy charged to the ith car has reached at least r_i and charging of the car must stop.\nLet’s emphasize: the index k is a time index running from 1 to K, and the index i specifies the corresponding car, i.e. i\\in\\{1,2,\\ldots, N\\}\nFormulate this task as an optimization problem, identify the class of this optimization problem (LP, QP or NLP) and solve it by completing the following Julia script and heeding the following instructions.\n\nModel the optimization problem either trough JuMP or Convex.\nSolve it using one of the available solvers: HiGHS, SCS, Ipopt.\nUpload only a single file named hw.jl as your solution.\n\n\nusing JuMP # or Convex\n\nusing HiGHS, SCS, Ipopt # Available solvers\n\n\"\"\"\n    find_optimal_charging_plan(\n        a::Vector{Float64},\n        c::Vector{Float64},\n        m::Vector{Float64},\n        r::Vector{Float64},\n        d::Vector{Int64}\n    )\n\nComputes an optimal charging schedule for `N` electric vehicles over `K` hours.\n\n# Arguments\n- `a`: A `K`-element vector specifying the maximum available charging energy per hour (kWh).\n- `c`: A `K`-element vector representing the cost of charging per hour (€/kWh).\n- `m`: An `N`-element vector with the maximum allowed charging energy for each vehicle (kWh).\n- `r`: An `N`-element vector specifying the total energy required by each vehicle (kWh).\n- `d`: An `N`-element vector indicating the departure time (hour) of each vehicle.\n\n# Returns\nA tuple containing:\n- An `N × K` matrix representing the optimal charging schedule (kWh allocated per vehicle per hour).\n- The optimal total charging cost (€).\n- A symbol indicating the type of optimization problem solved (`:LP`, `:QP`, or `:NLP`).\n\"\"\"\nfunction find_optimal_charging_plan(\n    a::Vector{Float64},\n    c::Vector{Float64},\n    m::Vector{Float64},\n    r::Vector{Float64},\n    d::Vector{Int64}\n)\n\n    K = length(a) # Timespan (hours)\n    N = length(m) # Number of vehicles\n\n    # TODO model and solve the problem\n\n    return zeros(N, K), 0.0, :NLP # or :LP or :QP\n\nend\n\nThe data you can use to test your solution is given in the following tables.\n\nData for the three vehicles: maximum energy per hour, total requested energy, and the departure time\n\n\n\n\n\n\n\n\nCar\nm_i (kWh)\nr_i (kWh)\nd_i\n\n\n\n\n1\n6\n15\n3\n\n\n2\n6\n25\n7\n\n\n3\n4\n30\n10\n\n\n\n\nEvolution in time of the maximum available energy and the cost of the energy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\na[k]\n11.6\n11.9\n10.6\n8.8\n8.0\n8.8\n10.6\n11.9\n11.6\n10.0\n\n\nc[k]\n0.58\n0.72\n0.92\n0.68\n0.54\n0.78\n0.64\n0.57\n0.74\n0.74",
    "crumbs": [
      "1. Optimization – theory",
      "Homework"
    ]
  },
  {
    "objectID": "opt_theory_hw.html#electric-vehicle-charging",
    "href": "opt_theory_hw.html#electric-vehicle-charging",
    "title": "Homework",
    "section": "",
    "text": "Solve the problem of distributing a limited power to N electric vehicle chargers throughout the next K hours. Specifically, you are supposed to find an hourly-sampled optimal plan for each car that minimizes the total cost of the charging.\nYou are given a time-dependent maximum energy a[k] available for charging (in kWh), a time-dependent cost of the energy c[k] (in €/kWh), maximum allowed charging energy per hour for the ith car m_i (in kWh), total requested energy r_i for the ith car (also in kWh), and the departure (discrete) time d_i for each car.\nAll the cars are connected to chargers and can start charging from time 1. Departure time is the time when the energy charged to the ith car has reached at least r_i and charging of the car must stop.\nLet’s emphasize: the index k is a time index running from 1 to K, and the index i specifies the corresponding car, i.e. i\\in\\{1,2,\\ldots, N\\}\nFormulate this task as an optimization problem, identify the class of this optimization problem (LP, QP or NLP) and solve it by completing the following Julia script and heeding the following instructions.\n\nModel the optimization problem either trough JuMP or Convex.\nSolve it using one of the available solvers: HiGHS, SCS, Ipopt.\nUpload only a single file named hw.jl as your solution.\n\n\nusing JuMP # or Convex\n\nusing HiGHS, SCS, Ipopt # Available solvers\n\n\"\"\"\n    find_optimal_charging_plan(\n        a::Vector{Float64},\n        c::Vector{Float64},\n        m::Vector{Float64},\n        r::Vector{Float64},\n        d::Vector{Int64}\n    )\n\nComputes an optimal charging schedule for `N` electric vehicles over `K` hours.\n\n# Arguments\n- `a`: A `K`-element vector specifying the maximum available charging energy per hour (kWh).\n- `c`: A `K`-element vector representing the cost of charging per hour (€/kWh).\n- `m`: An `N`-element vector with the maximum allowed charging energy for each vehicle (kWh).\n- `r`: An `N`-element vector specifying the total energy required by each vehicle (kWh).\n- `d`: An `N`-element vector indicating the departure time (hour) of each vehicle.\n\n# Returns\nA tuple containing:\n- An `N × K` matrix representing the optimal charging schedule (kWh allocated per vehicle per hour).\n- The optimal total charging cost (€).\n- A symbol indicating the type of optimization problem solved (`:LP`, `:QP`, or `:NLP`).\n\"\"\"\nfunction find_optimal_charging_plan(\n    a::Vector{Float64},\n    c::Vector{Float64},\n    m::Vector{Float64},\n    r::Vector{Float64},\n    d::Vector{Int64}\n)\n\n    K = length(a) # Timespan (hours)\n    N = length(m) # Number of vehicles\n\n    # TODO model and solve the problem\n\n    return zeros(N, K), 0.0, :NLP # or :LP or :QP\n\nend\n\nThe data you can use to test your solution is given in the following tables.\n\nData for the three vehicles: maximum energy per hour, total requested energy, and the departure time\n\n\n\n\n\n\n\n\nCar\nm_i (kWh)\nr_i (kWh)\nd_i\n\n\n\n\n1\n6\n15\n3\n\n\n2\n6\n25\n7\n\n\n3\n4\n30\n10\n\n\n\n\nEvolution in time of the maximum available energy and the cost of the energy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\na[k]\n11.6\n11.9\n10.6\n8.8\n8.0\n8.8\n10.6\n11.9\n11.6\n10.0\n\n\nc[k]\n0.58\n0.72\n0.92\n0.68\n0.54\n0.78\n0.64\n0.57\n0.74\n0.74",
    "crumbs": [
      "1. Optimization – theory",
      "Homework"
    ]
  },
  {
    "objectID": "discr_dir_mpc_economic.html",
    "href": "discr_dir_mpc_economic.html",
    "title": "Economic MPC",
    "section": "",
    "text": "(Ellis, Liu, and Christofides 2017), (Ellis, Durand, and Christofides 2014), (Faulwasser, Grüne, and Müller 2018), (Rawlings, Angeli, and Bates 2012)",
    "crumbs": [
      "6. More on MPC",
      "Economic MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_economic.html#literature",
    "href": "discr_dir_mpc_economic.html#literature",
    "title": "Economic MPC",
    "section": "",
    "text": "(Ellis, Liu, and Christofides 2017), (Ellis, Durand, and Christofides 2014), (Faulwasser, Grüne, and Müller 2018), (Rawlings, Angeli, and Bates 2012)",
    "crumbs": [
      "6. More on MPC",
      "Economic MPC"
    ]
  },
  {
    "objectID": "cont_indir_constrained.html",
    "href": "cont_indir_constrained.html",
    "title": "Constrained optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Constrained optimal control"
    ]
  },
  {
    "objectID": "cont_indir_time_optimal.html",
    "href": "cont_indir_time_optimal.html",
    "title": "Time-optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Time-optimal control"
    ]
  },
  {
    "objectID": "opt_algo_hw.html",
    "href": "opt_algo_hw.html",
    "title": "Homework",
    "section": "",
    "text": "In this assignment, you will implement the BFGS (Broyden–Fletcher–Goldfarb–Shanno) algorithm—a popular quasi-Newton method for unconstrained optimization—in the Julia programming language. Specifically, you will be implementing the inverse Hessian update variant, which can be, e.g., found in the Broyden–Fletcher–Goldfarb–Shanno algorithm Wikipedia page.\nThe instructions for the assignment are as follows\n\nYour solution should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\nYou should use Automatic Differentiation (AD) to compute the gradients of the objective function. You can use the Zygote package for this purpose.\nYou should use backtracking line search with the Armijo condition to find the step size, see Approximate line search – backtracking.\nThe implementation should be based on the provided template below.\n\n\nusing LinearAlgebra\nusing Zygote # for Automatic Differentiation (AD)\n\n\"\"\"\n    BFGS(J::Function, x₀::Vector{T}, ε::T=sqrt(eps(T)), maxiter::Int=50) where T &lt;: AbstractFloat\n\nPerforms unconstrained optimization using the Broyden–Fletcher–Goldfarb–Shanno (BFGS) quasi-Newton method.\n\n# Arguments\n- `f`: The objective function to be minimized. It should take a vector `x` and return a scalar.\n- `x₀`: A vector representing the initial guess for the optimization. The element type `T` must be a subtype of `AbstractFloat`.\n- `ε`: The convergence tolerance for the gradient norm (default: `√eps(T)`, where `T` is the element type of `x₀`).\n- `maxiter`: The maximum number of iterations allowed (default: 50).\n- `verbose`: A boolean indicating whether to print the optimization progress (default: `false`).\n\n# Returns\nA tuple containing:\n- A vector of type `Vector{T}` representing the optimized solution.\n- The optimal function value at the solution.\n- The number of iterations performed.\n- A symbol indicating the termination status (`:converged` or `:maxiter_reached`)\n\nThis implementation supports arbitrary precision arithmetic if `x₀` is of a higher precision type (e.g., `BigFloat`).\n\"\"\"\nfunction BFGS(f::Function, x₀::Vector{T}, ε::T=√(eps(T)), maxiter::Int=50; verbose::Bool=false) where T &lt;: AbstractFloat\n\n    xₖ = x₀ # Initial guess\n\n    # Preallocate memory - all the variables that will be used in the loop\n    xₖ₊₁ = similar(xₖ)\n    ∇fₖ = zeros(T, length(xₖ))\n    ∇fₖ₊₁ = similar(∇fₖ)\n    Hₖ = Matrix{T}(I, length(x₀), length(x₀)) \n    Hₖ₊₁ = similar(Hₖ)\n    yₖ = similar(∇fₖ)\n    sₖ = similar(xₖ)\n    pₖ = similar(xₖ)\n\n    # TODO Compute ∇f(xₖ) using AD\n    # ∇fₖ =\n\n    if norm(∇fₖ, Inf) &lt; ε # Convergence check - Inital guess is already optimal\n        return xₖ, f(xₖ), 0, :converged\n    end\n\n    Hₖ ./= norm(∇fₖ)  # Initial inverse Hessian approximation\n\n    for k = 1:maxiter\n\n        if norm(∇fₖ, Inf) &lt; ε # Convergence check\n            return xₖ, f(xₖ), k, :converged\n        end\n\n        if verbose\n            println(\"Iteration: \", k,  \" | f(xₖ): \", f(xₖ),\" | ǁ∇f(xₖ)ǁ∞: \", norm(∇fₖ, Inf))\n        end\n\n        # TODO Complete the BFGS update, i.e., compute xₖ₊₁, ∇fₖ₊₁, and Hₖ₊₁\n        # For the linesearch use the Armijo condition (https://hurak.github.io/orr/opt_algo_unconstrained.html#approximate-line-search-backtracking)\n\n        # Prepare for the next iteration\n        xₖ .= xₖ₊₁\n        ∇fₖ .= ∇fₖ₊₁\n        Hₖ .= Hₖ₊₁\n\n    end\n\n    return xₖ, f(xₖ), maxiter, :maxiter_reached\n\nend\n\nTo test your implementation, take a look at common test functions in the Optimization Test Functions Wikipedia page. For example, you can use the Rosenbrock function: \nf(\\bm{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2\\right],\n which has the global minimum at \\bm{x} = (1, 1, \\ldots, 1).",
    "crumbs": [
      "2. Optimization – algorithms",
      "Homework"
    ]
  },
  {
    "objectID": "opt_algo_hw.html#implementation-of-the-bfgs-method-for-unconstrained-optimization",
    "href": "opt_algo_hw.html#implementation-of-the-bfgs-method-for-unconstrained-optimization",
    "title": "Homework",
    "section": "",
    "text": "In this assignment, you will implement the BFGS (Broyden–Fletcher–Goldfarb–Shanno) algorithm—a popular quasi-Newton method for unconstrained optimization—in the Julia programming language. Specifically, you will be implementing the inverse Hessian update variant, which can be, e.g., found in the Broyden–Fletcher–Goldfarb–Shanno algorithm Wikipedia page.\nThe instructions for the assignment are as follows\n\nYour solution should be contained in a single file named hw.jl, which you will upload to the BRUTE system.\nYou should use Automatic Differentiation (AD) to compute the gradients of the objective function. You can use the Zygote package for this purpose.\nYou should use backtracking line search with the Armijo condition to find the step size, see Approximate line search – backtracking.\nThe implementation should be based on the provided template below.\n\n\nusing LinearAlgebra\nusing Zygote # for Automatic Differentiation (AD)\n\n\"\"\"\n    BFGS(J::Function, x₀::Vector{T}, ε::T=sqrt(eps(T)), maxiter::Int=50) where T &lt;: AbstractFloat\n\nPerforms unconstrained optimization using the Broyden–Fletcher–Goldfarb–Shanno (BFGS) quasi-Newton method.\n\n# Arguments\n- `f`: The objective function to be minimized. It should take a vector `x` and return a scalar.\n- `x₀`: A vector representing the initial guess for the optimization. The element type `T` must be a subtype of `AbstractFloat`.\n- `ε`: The convergence tolerance for the gradient norm (default: `√eps(T)`, where `T` is the element type of `x₀`).\n- `maxiter`: The maximum number of iterations allowed (default: 50).\n- `verbose`: A boolean indicating whether to print the optimization progress (default: `false`).\n\n# Returns\nA tuple containing:\n- A vector of type `Vector{T}` representing the optimized solution.\n- The optimal function value at the solution.\n- The number of iterations performed.\n- A symbol indicating the termination status (`:converged` or `:maxiter_reached`)\n\nThis implementation supports arbitrary precision arithmetic if `x₀` is of a higher precision type (e.g., `BigFloat`).\n\"\"\"\nfunction BFGS(f::Function, x₀::Vector{T}, ε::T=√(eps(T)), maxiter::Int=50; verbose::Bool=false) where T &lt;: AbstractFloat\n\n    xₖ = x₀ # Initial guess\n\n    # Preallocate memory - all the variables that will be used in the loop\n    xₖ₊₁ = similar(xₖ)\n    ∇fₖ = zeros(T, length(xₖ))\n    ∇fₖ₊₁ = similar(∇fₖ)\n    Hₖ = Matrix{T}(I, length(x₀), length(x₀)) \n    Hₖ₊₁ = similar(Hₖ)\n    yₖ = similar(∇fₖ)\n    sₖ = similar(xₖ)\n    pₖ = similar(xₖ)\n\n    # TODO Compute ∇f(xₖ) using AD\n    # ∇fₖ =\n\n    if norm(∇fₖ, Inf) &lt; ε # Convergence check - Inital guess is already optimal\n        return xₖ, f(xₖ), 0, :converged\n    end\n\n    Hₖ ./= norm(∇fₖ)  # Initial inverse Hessian approximation\n\n    for k = 1:maxiter\n\n        if norm(∇fₖ, Inf) &lt; ε # Convergence check\n            return xₖ, f(xₖ), k, :converged\n        end\n\n        if verbose\n            println(\"Iteration: \", k,  \" | f(xₖ): \", f(xₖ),\" | ǁ∇f(xₖ)ǁ∞: \", norm(∇fₖ, Inf))\n        end\n\n        # TODO Complete the BFGS update, i.e., compute xₖ₊₁, ∇fₖ₊₁, and Hₖ₊₁\n        # For the linesearch use the Armijo condition (https://hurak.github.io/orr/opt_algo_unconstrained.html#approximate-line-search-backtracking)\n\n        # Prepare for the next iteration\n        xₖ .= xₖ₊₁\n        ∇fₖ .= ∇fₖ₊₁\n        Hₖ .= Hₖ₊₁\n\n    end\n\n    return xₖ, f(xₖ), maxiter, :maxiter_reached\n\nend\n\nTo test your implementation, take a look at common test functions in the Optimization Test Functions Wikipedia page. For example, you can use the Rosenbrock function: \nf(\\bm{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2\\right],\n which has the global minimum at \\bm{x} = (1, 1, \\ldots, 1).",
    "crumbs": [
      "2. Optimization – algorithms",
      "Homework"
    ]
  },
  {
    "objectID": "discr_dir_hw.html",
    "href": "discr_dir_hw.html",
    "title": "Homework",
    "section": "",
    "text": "In this homework, you will be implementing a tracking Model Predictive Controller (MPC) for a linear model in Julia. Your task is to take the Optimal Control Problem of the tracking MPC \n\\begin{align*}\n    \\underset{\\mathbf{u}_k}{\\text{minimize}} \\quad & \\frac{1}{2}\\sum_{k=t}^{t+N-1} (\\mathbf{y}_{k+1} - r_{k+1})^T\\mathbf{Q}(\\mathbf{y}_{k+1} - r_{k+1}) + \\Delta\\mathbf{u}_k^T\\mathbf{R}\\Delta\\mathbf{u}_k\\\\\n    \\text{subject to} \\quad & \\mathbf{x}_{k+1} = \\mathbf{A}\\mathbf{x}_k + \\mathbf{B}\\mathbf{u}_k, \\qquad k = t,\\dots,t+N-1,\\\\\n    & \\mathbf{y}_k = \\mathbf{C}\\mathbf{x}_k, \\qquad k = t+1,\\dots,t+N,\\\\\n    &\\mathbf{u}_\\mathrm{min} \\leq \\mathbf{u}_{k} \\leq \\mathbf{u}_\\mathrm{max}, \\qquad k =t,\\dots,t+N-1,\n\\end{align*}\n where \\Delta\\mathbf{u}_k = \\mathbf{u}_k - \\mathbf{u}_{k-1}, and reformulate it as a Quadratic Program (QP) of the form \n\\begin{array}{rl}\n\\underset{\\mathbf{z}}{\\text{minimize}} \\quad &  \\frac{1}{2}\\mathbf{z}^T \\mathbf{H} \\mathbf{z} + [\\mathbf{x}_t^T \\: \\mathbf{u}_{t-1}^T \\: \\mathbf{r}_{t+1,\\dots,t+N}^T]\\,\\mathbf{F}\\,\\mathbf{z} \\\\\n\\text{subject to} \\quad &\\mathbf{G}\\mathbf{z} \\leq \\mathbf{W} + \\mathbf{S}\\left[\\begin{array}{c}\\mathbf{x}_t\\\\ \\mathbf{u}_{t-1}\\end{array}\\right].\n\\end{array}\n where \\mathbf{z}=\\left[\\begin{array}{c}\\Delta \\mathbf{u}_t^\\mathrm{T} & \\ldots & \\Delta \\mathbf{u}_{t+N-1}^\\mathrm{T}\\end{array}\\right]^\\mathrm{T}, \\mathbf{x}_t is the current state value of the model, \\mathbf{u}_{t-1} is the most recently applied input and \\mathbf{r}_{t+1,\\dots,t+N} is the reference over the current prediction horizon. You should then implement the MPC controller using this QP formulation.\nWe recommend starting by writing down the QP formulation on paper and only then proceeding to implement the MPC construction in code. In case of doubts, we advise you to consult the lecture notes, especially the section on the Sequential (Dense) formulation of direct discrete-time optimal control problems, as well as the related MPC video lectures. Another source which may serve you well is slides from a doctoral course on MPC by Bemporad, which can be found here.\nThe MPC controller should be based on the template provided below. Your goal is to complete the implementation by filling in the missing parts, specifically\n\nComplete the setup_mpc function that constructs the matrices \\mathbf{H}, \\mathbf{F}, \\mathbf{S}, \\mathbf{W}, and \\mathbf{G} for the QP formulation.\nComplete the solve! function that does the single MPC step, that is, it solves the QP for the given initial state, input, and reference trajectory, and returns the optimal control input.\n\nYour solution should be contained in a single file named hw.jl, which you will upload to the BRUTE system. You should use the COSMO.jl package for solving the QP.\n\nusing LinearAlgebra, SparseArrays, COSMO\n\n# You might find these other packages useful \n# using ToeplitzMatrices, BlockArrays\n\nmutable struct MPCProblem{T &lt;: AbstractFloat}\n    model::COSMO.Workspace{T}\n    F::Matrix{T}\n    W::Vector{T}\n    S::Matrix{T}\nend\n\n\"\"\"\nSets up the necessary matrices for a tracking Model Predictive Controller (MPC).\n\nGiven a discrete-time linear system with state-space representation:\n    x(k+1) = A*x(k) + B*u(k)\n    y(k) = C*x(k)\n\nThis function constructs the required matrices to reformulate the MPC problem as a quadratic program (QP):\n\n    min_z  (1/2) * z' * H * z + [x_t', u_(t-1)', r_(t+1:t+N)'] * F * z\n    subject to: G * z ≤ W + S * [x_t, u_(t-1)]\n\nwhere:\n    - z = [Δu_t; ...; Δu_(t+N-1)] (control input changes over the horizon)\n    - x_t: current state\n    - u_(t-1): most recently applied input\n    - r_(t+1:t+N): reference trajectory\n\nArguments:\n    - A, B, C: System matrices defining dynamics and output equations\n    - Q: State tracking cost matrix\n    - R: Control input cost matrix\n    - N: Prediction horizon length\n    - u_min, u_max: Input constraints\n\nReturns:\n    A dictionary containing the constructed QP matrices:\n    - H: Quadratic cost matrix\n    - F: Linear cost term matrix\n    - S: Constraint matrix for state and past input\n    - W: Constraint bounds vector\n    - G: Inequality constraint matrix\n\"\"\"\nfunction setup_mpc(A::Matrix{T}, B::Matrix{T}, C::Matrix{T}, Q::Matrix{T}, R::Matrix{T}, N::Int, u_min::Vector{T}, u_max::Vector{T}) where T &lt;: AbstractFloat\n\n    nx = size(A, 1)\n    nu = size(B, 2)\n    ny = size(C, 1)\n\n\n    # TODO Construct the matrices H, F, S, W, and G\n    H = zeros(T, N * nu, N * nu)\n    F = zeros(T, nx + nu + N * ny, N * nu)\n    S = zeros(T, 2N * nu, nx + nu)\n    W = zeros(T, 2N * nu)\n    G = zeros(T, 2N * nu, N * nu) \n   \n\n    return H, F, S, W, G\nend\n\n\"\"\"\nConstructs a tracking Model Predictive Controller (MPC) problem.\n\nArguments:\n    - A, B, C: System matrices defining dynamics and output equations\n    - Q: State tracking cost matrix\n    - R: Control input cost matrix\n    - N: Prediction horizon length\n    - u_min, u_max: Input constraints\n\nReturns:\n    An instance of `MPCProblem` containing the COSMO model and the matrices F, W, and S.\n\"\"\"\nfunction MPCProblem(A::Matrix{T}, B::Matrix{T}, C::Matrix{T}, Q::Matrix{T}, R::Matrix{T}, N::Int, u_min::Vector{T}, u_max::Vector{T}) where T &lt;: AbstractFloat\n    H, F, S, W, G = setup_mpc(A, B, C, Q, R, N, u_min, u_max)\n\n    model = COSMO.Model{T}() # COSMO model\n\n    ## We use COSMO with the following QP formulation:\n    ## min 1/2 x' * P * x + q' * x\n    ## s.t. A_constr * x - b_constr ≥ 0\n\n    # Dummy variables - just for illustration and to provide correct stuff to COSMO\n    x₀ = zeros(T, size(A, 1))\n    r = zeros(T, N * size(C, 1))\n    u₀ = zeros(T, size(B, 2)) \n\n    q = [x₀; u₀; r]' * F \n\n    P = sparse(H)\n\n    A_constr = -G \n    b_constr = W + S * [x₀; u₀] \n    \n    constr = COSMO.Constraint(A_constr, b_constr, COSMO.Nonnegatives);\n\n    COSMO.assemble!(model, sparse(H), q, constr, settings = COSMO.Settings(verbose=true)) # Assemble the QP\n\n    return MPCProblem(model, F, W, S)\nend\n\n\"\"\"\nSolves the MPC problem for the given initial state, input, and reference trajectory.\n\nArguments:\n    - mpc: An instance of `MPCProblem` containing the COSMO model and the matrices F, W, and S.\n    - xₖ: The current state vector.\n    - uₖ₋₁: The most recently applied input vector.\n    - r: The reference trajectory vector.\n\nReturns:\n    The optimal control input vector at the current time step.\n\"\"\"\nfunction solve!(mpc::MPCProblem{T}, xₖ::Vector{T}, uₖ₋₁::Vector{T}, r::Matrix{T}) where T &lt;: AbstractFloat\n\n    # TODO implement the updates for the COSMO model (q and b_constr) \n    q = zeros(T, size(mpc.F, 2))\n    b_constr = zeros(T, size(mpc.S, 1))\n\n    COSMO.update!(mpc.model, q = q, b = b_constr)\n\n    result = COSMO.optimize!(mpc.model)\n\n    # TODO Extract the optimal control input and return it\n    Δu_opt = result.x\n \n    return zeros(T, size(uₖ₋₁, 1))\nend\n\nYou should test your implementation using the following example we prepared for you.\n\nusing ControlSystemsBase\nusing Plots\nAc = [-.0151 -60.5651 0 -32.174;\n     -.0001 -1.3411 .9929 0;\n     .00018 43.2541 -.86939 0;\n      0      0       1      0];\n\nBc = [-2.516 -13.136;\n     -.1689 -.2514;\n     -17.251 -1.5766;\n     0        0];\nCc = [0 1 0 0;\n     0 0 0 1];\nDc = [0 0;\n     0 0];\n\nsys=ss(Ac,Bc,Cc,Dc)\n\nTs = .05; # Sampling time\nmodel = c2d(sys,Ts)\n\nA = model.A\nB = model.B\nC = model.C\n\nN = 10 # Prediction horizon\nQ = diagm([10.0, 10.0]); # Tracking weight matrix\nR = Matrix(0.1I, 2, 2); # Input increment weight matrix\n\nu_max = 25.0*[1, 1];\nu_min = -25.0*[1, 1];\n\nmpc = MPCProblem(A, B, C, Q, R, N, u_min, u_max)\n\nTf = 75*Ts;\nt = 0:Ts:Tf; \n\nref = [2*ones(1, length(t)+N); 10*ones(1, length(t)+N)] # Reference trajectory\n\nref[:, Int(round(end/2)):end] ./= 2\n\nxs = zeros(4, length(t)+1)\nus = zeros(2, length(t))\nys = zeros(2, length(t))\n\nfor k = 2:length(t)\n    u = solve!(mpc, xs[:, k], us[:, k-1], ref[:, k:k+N-1])\n    xs[:, k+1] = A*xs[:, k] + B*u\n    ys[:, k] = C*xs[:, k]\n    us[:, k] = u\nend\n\n## Visualize the results\nusing Plots\np1= plot(t, ys[1, :], label=\"y₁\", linetype=:steppre, linewidth=2)\nplot!(t, ys[2, :], label=\"y₂\", linetype=:steppre, linewidth=2)\n\nplot!(t, ref[1, 1:end-N], linestyle=:dash, label=\"r₁\", linetype=:steppre, linewidth=2)\nplot!(t, ref[2, 1:end-N], linestyle=:dash, label=\"r₂\", linetype=:steppre, linewidth=2)\n\nxlabel!(\"Time [s]\")\nylabel!(\"Output\")\n\np2 = plot(t[1:end], us[1, :], label=\"u₁\", linetype=:steppre, linewidth=2)\nplot!(t[1:end], us[2, :], label=\"u₂\", linetype=:steppre, linewidth=2)\n\nplot!(t, u_max[1]*ones(length(t)), linestyle=:dash, label=\"u max\", linetype=:steppre, linewidth=2)\nplot!(t, u_min[1]*ones(length(t)), linestyle=:dash, label=\"u min\", linetype=:steppre, linewidth=2)\n\nxlabel!(\"Time [s]\")\nylabel!(\"Input\")\n\nplot(p1, p2, layout=(2,1), size=(800, 600))\n\n\n\n\n Back to top",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Homework"
    ]
  },
  {
    "objectID": "roban_software.html",
    "href": "roban_software.html",
    "title": "Software",
    "section": "",
    "text": "The primary tool for us in this part of the course is the Robust Control Toolbox for Matlab. A nice benefit is that accompanying video tutorials by Brian Douglas are available.\nAlternatives in other languages exist, but very often are less well developed and/or documented. A notable exception is RobustAndOptimalControl.jl for Julia.\n\n\n\n Back to top",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Software"
    ]
  },
  {
    "objectID": "discr_indir_LQR_fin_horizon.html",
    "href": "discr_indir_LQR_fin_horizon.html",
    "title": "Discrete-time LQR on a finite horizon",
    "section": "",
    "text": "We consider a linear time-invariant (LTI) system described by the state equation \n\\bm x_{k+1} = \\mathbf A \\bm x_{k} + \\mathbf B \\bm u_k, \\qquad \\bm x_0 = \\mathbf x_0,\n and our goal is to find a (vector) control sequence \\bm u_0, \\bm u_{1},\\ldots, \\bm u_{N-1} that minimizes \nJ_0^N = \\frac{1}{2}\\bm x_N^\\top\\mathbf S_N\\bm x_N + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[\\bm x_k^\\top \\mathbf Q \\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right],\n where the quadratic cost function is parameterized the matrices that must be symmetric and at least positive semidefinite, otherwise the corresponding quadratic terms will not play a good role of penalizing the (weighted) distance from zero.\nWe will see in a moment that the matrix \\mathbf R must comply with an even stricter condition – it must be positive definite. To summarize the assumptions about the matrices, we require \n\\mathbf S_N\\succeq 0, \\mathbf Q\\succeq 0, \\mathbf R\\succ 0.\nThe Hamiltonian for our problem is \n\\boxed{\nH(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}) = \\frac{1}{2}\\left(\\bm x_k^\\top \\mathbf Q\\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right) + \\boldsymbol \\lambda_{k+1}^\\top\\left(\\mathbf A\\bm x_k+\\mathbf B\\bm u_k\\right).\n}\nIn the following derivations we use the shorthand notation H_k for H(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}).\nSubstituting into the general necessary conditions derived in the previous section we obtain \n\\begin{aligned}\n\\mathbf x_{k+1} &= \\nabla_{\\boldsymbol \\lambda_{k+1}}H_k=\\mathbf A\\bm x_k+\\mathbf B\\bm u_k,\\\\\n\\boldsymbol\\lambda_k &= \\nabla_{\\mathbf x_{k}}H_k=\\mathbf Q\\bm x_k+\\mathbf A^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\mathbf 0 &= \\nabla_{\\mathbf u_{k}}H_k = \\mathbf R\\bm u_k + \\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n0 &= (\\mathbf S_N \\bm x_N - \\boldsymbol \\lambda_N)^\\top\\; \\text{d} \\bm x_N,\\\\\n\\bm x_0 &= \\mathbf x_0.\n\\end{aligned}\nThe last two equations represent the boundary conditions. Note that here we have already fixed the initial state. If this is not appropriate in a particular scenario, go back and adjust the boundary equation accordingly.\nThe third equation above – the stationarity equation – can be used to extract the optimal control \n\\bm u_k = -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1}.\nThe need for nonsingularity of \\mathbf R is now obvious. Upon substituting the recipe for the optimal \\bm u_k into the state and the co-state equations, two recursive (or recurrent or just discrete-time) equations result \n\\begin{bmatrix}\n\\mathbf x_{k+1}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf A & -\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q & \\mathbf A^\\top\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bm x_k \\\\ \\boldsymbol\\lambda_{k+1}\n\\end{bmatrix}.\nThis is a two-point boundary value problem (TP-BVP). The problem is of order 2n, where n is the dimension of the state space. In order to solve it we need 2n boundary values: n boundary values are provided by \\bm x_i = \\mathbf x_0, and n boundary values are given by the other boundary condition, from which \\boldsymbol\\lambda_N must be extracted. Most of our subsequent discussion will revolve around this task.\nAn idea might come into our mind: provided \\mathbf A is nonsingular, we can left-multiply the above equation by the inverse of \\mathbf A to obtain \n\\begin{bmatrix}\n\\mathbf x_{k}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf A^{-1} & \\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q\\mathbf A^{-1} & \\mathbf A^\\top+\\mathbf Q\\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf x_{k+1} \\\\ \\boldsymbol\\lambda_{k+1}\n\\end{bmatrix}\n\\tag{1}\nThis helped at least to have both variable evolving in the same direction in time (both backward) but we do not know \\boldsymbol\\lambda_N anyway. Nonetheless, do not forget this result. We are going to invoke it later.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on a finite horizon"
    ]
  },
  {
    "objectID": "discr_indir_LQR_fin_horizon.html#fixed-final-state-and-finite-time-horizon",
    "href": "discr_indir_LQR_fin_horizon.html#fixed-final-state-and-finite-time-horizon",
    "title": "Discrete-time LQR on a finite horizon",
    "section": "Fixed final state and finite time horizon",
    "text": "Fixed final state and finite time horizon\nBack to the nonzero control case. First we are going to investigate the scenario when the final requested state is given by \\mathbf x^\\text{ref}. The optimal control problem turns into \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x_0, \\bm{x}_{1},\\ldots,\\bm{x}_{N},\\bm{u}_{0},\\ldots,\\bm{u}_{N-1}} &\\; \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[\\bm x_k^T \\mathbf Q \\bm x_k+\\bm u_k^T \\mathbf R\\bm u_k\\right]\\\\\n\\text{s.t. } & \\; \\mathbf x_{k+1} = \\mathbf A \\mathbf x_{k} + \\mathbf B \\bm u_k,\\\\\n&\\; \\bm x_0 = \\mathbf x_0,\\\\\n&\\; \\bm x_N = \\mathbf x^\\text{ref},\\\\\n&\\; \\mathbf Q\\geq 0, \\mathbf R&gt;0.\n\\end{aligned}\n\n\nNote also that the term penalizing the final state is removed from the cost because it is always fixed. After eliminating the controls using the stationarity equation \n\\bm u_k = -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\n and replacing the general boundary condition at the final time by \\bm x_N = \\mathbf x^\\text{ref}, the two-point boundary value problem specializes to \n\\begin{aligned}\n\\mathbf x_{k+1} &=\\mathbf A\\bm x_k-\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\boldsymbol\\lambda_k &= \\mathbf Q\\bm x_k+\\mathbf A^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\bm x_0 &= \\mathbf x_0,\\\\\n\\bm x_N &= \\mathbf x^\\text{ref}.\n\\end{aligned}\n\nThis problem is clearly an instance of a two-point boundary value problem (TP-BVP) as the state vector is specified at both ends of the time interval. The costate is left unspecified, but it is fine because only 2n boundary conditions are needed. While BVP are generally difficult to solve, our problem at hand adds one more layer of complexity. For the state variable its evolution forward in time is specified by the state equation, while for the co-state variable the evolution backward in time is prescribed by the co-state equation.\n\n\\begin{bmatrix}\n\\mathbf x_{k+1}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf A & -\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q & \\mathbf A^\\top\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bm x_k \\\\ \\boldsymbol\\lambda_{k+1}.\n\\end{bmatrix}\n\nThere is not much we can do with these equations in this form. However, in case of a nonsingular matrix \\mathbf A, we can invoke the discrete-time Hamiltonian system (Eq. 1), in which we reorganized the equations so that both state and co-state variables evolve backwards. For convenience we give it here again \n\\begin{bmatrix}\n\\mathbf x_{k}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\\underbrace{\n\\begin{bmatrix}\n\\mathbf A^{-1} & \\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q\\mathbf A^{-1} & \\mathbf A^\\top+\\mathbf Q\\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\n\\end{bmatrix}}_{\\mathbf H}\n\\begin{bmatrix}\n\\mathbf x_{k+1} \\\\ \\boldsymbol\\lambda_{k+1}.\n\\end{bmatrix}\n\nThis can be used to relate the state and costate at the initial and final times of the interval \n\\begin{bmatrix}\n\\mathbf x_{0}\\\\\\boldsymbol\\lambda_0\n\\end{bmatrix}\n=\\underbrace{\n\\begin{bmatrix}\n\\mathbf A^{-1} & \\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q\\mathbf A^{-1} & \\mathbf A^\\top+\\mathbf Q\\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\n\\end{bmatrix}^N}_{\\mathbf M\\coloneqq \\mathbf H^N}\n\\begin{bmatrix}\n\\mathbf x_{N} \\\\ \\boldsymbol\\lambda_{N}\n\\end{bmatrix}.\n\nFrom the first equation we can get \\boldsymbol \\lambda_N. First, let’s rewrite it here \n\\mathbf M_{12}\\boldsymbol \\lambda_N = \\bm x_0-\\mathbf M_{11}\\bm x_N,\n from which (after substituting for the known initial and final states) \n\\boldsymbol \\lambda_N = \\mathbf M_{12}^{-1}(\\mathbf r_0-\\mathbf M_{11}\\mathbf r_N).\n\nHaving the final state and the final co-state, \\bm x_N and \\boldsymbol \\lambda_N, respectively, we can solve the Hamiltonian system backward to get the states and co-states on the whole time interval [0,N-1].\n\nSpecial case: minimum-energy control (\\mathbf Q = \\mathbf 0)\nWe can get some more insight into the problem if we further restrict the class of problems we can treat. Namely, we will assume \n\\mathbf Q = \\mathbf 0.\n\nThis is a significant restriction, nonetheless the resulting problem is still practically reasonable. And we do not need to assume that \\mathbf A is nonsingular. The cost function is then \nJ = \\sum_{k=0}^N \\mathbf u^\\top_k\\;\\bm u_k = \\sum_{k=0}^N \\|\\mathbf u\\|_2^2,   \n which is why the problem is called the minimum-energy control problem. Rewriting the state and co-state equations with the new restriction \\mathbf Q=\\mathbf 0 we get \n\\begin{aligned}\n\\bm x_{k+1} &= \\mathbf A\\bm x_k - \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1}\\\\\n\\boldsymbol \\lambda_k &= \\mathbf A^\\top\\boldsymbol\\lambda_{k+1}.\n\\end{aligned}\n\nIt is obvious why we wanted to enforce the \\mathbf Q=\\mathbf 0 restriction — the co-state equation is now completely decoupled from the state equation and can be solved independently \n\\boldsymbol \\lambda_k = (\\mathbf A^\\top)^{N-k}\\boldsymbol \\lambda_N.\n\nNow substitute this solution of the co-state equation into the state equation \n\\bm x_{k+1} = \\mathbf A\\bm x_k - \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-k-1}\\boldsymbol \\lambda_N.\n\nFinding a solution to the state equation is now straightforward — the second summand on the right is considered as a an “input”. The solution is then \n\\bm x_{k} = \\mathbf A^k\\bm x_0 - \\sum_{i=0}^{k-1}\\mathbf A^{k-1-i}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-i-1}\\boldsymbol \\lambda_N.\n\nThe last step reveals the motivation for all the previous steps — we can now express the state at the final time, and by doing that we introduce some known quantity into the problem \n\\bm x_{N} = \\mathbf x^\\text{ref}= \\mathbf A^N\\bm x_0 - \\underbrace{\\sum_{i=0}^{N-1}\\mathbf A^{N-1-i}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-i-1}}_{G_{0,N,R}}\\boldsymbol \\lambda_N.\n\nThis enables us to calculate \\boldsymbol \\lambda_N directly as a solution to a linear equation. To make the notation simpler, denote the sum in the expression above by \\mathbf G_{0,N,R} (we will discuss this particular object in a while) \n\\boldsymbol \\lambda_N = -\\mathbf G^{-1}_{0,N,R}\\; (\\mathbf x^\\text{ref}-\\mathbf A^N\\bm x_0).\n\nThe rest is quite straightforward as the optimal control depends (through the stationarity equation) on the co-state \n\\boxed{\n\\bm u_k = \\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-k-1}\\mathbf G^{-1}_{0,N,R}\\; (\\mathbf x^\\text{ref}-\\mathbf A^N\\bm x_0).\n}\n\nThis is the desired formula for computation of the optimal control.\nA few observations can be made\n\nThe control is proportional to the difference (\\mathbf x^\\text{ref}-\\mathbf A^N\\bm x_0). The intuitive interpretation is that the further the requested final state is from the state into which the system would finally evolve without any control, the higher the control is needed.\n\nThe control is proportional to the inverse of a matrix \\mathbf G_{0,N,R} which is called weighted reachability Gramian. The standard result from the theory of linear dynamic systems is that nonsingularity of a reachability Gramian is equivalent to reachability of the system. More on this below.\n\n\nWeighted reachability Gramian\nRecall (perhaps from your linear systems course) that there is a matrix called discrete-time reachability Gramian defined as \n\\mathbf G = \\sum_{k=0}^{\\infty} \\mathbf A^{k}\\mathbf B\\mathbf B^\\top(\\mathbf A^\\top)^k\n and the nonsingularity of this matrix serves as a test of reachability for stable discrete-time linear systems.\nHow does this classical object relate to the object \\mathbf G_{0,N,R} introduced in the previous paragraph? First consider the restriction of the summation from the infinite interval [0,\\infty] to [0,N-1]. In other words, we analyze the matrix \n\\mathbf G_{0,N} = \\sum_{k=0}^{N-1} \\mathbf A^{N-1-k}\\mathbf B\\mathbf B^\\top(\\mathbf A^\\top)^{N-1-k}.\n\nRecall that Caley-Hamilton theorem tells us that every higher power of an N\\times N matrix can be expressed as a linear combination of powers of 0 through N-1. In other words, using higher order powers of A than N-1 cannot increase the rank of the matrix.\nFinally, provided \\mathbf R is nonsingular (hence \\mathbf R^{-1} is nonsingular as well), the rank of the Gramian is not changed after introducing the weight\n\n\\mathbf G_{0,N,R} = \\sum_{k=0}^{N-1} \\mathbf A^{N-1-k}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-1-k}.\n\nThe weighted Gramian defined on a finite discrete-time horizon is invertible if and only if the (stable) system is reachable. This conclusion is quite natural: if an optimal control is to be found, first it must be guaranteed that any control can be found which brings the system from an arbitrary initial state into an arbitrary final state on a finite time interval — the very definition of reachability.\nTo summarize the whole fixed-final state case, the optimal control can be computed numerically by solving a TP-BVP. For the minimum-problem even a formula exists and there is no need for a numerical optimization solver. But the outcome is always just a sequence of controls. In this regard, the new (indirect) approach did not offer much more that what the direct approach did. Although the new insight is rewarding, it is paid for by the inability to handle constraints on the control or state variables.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on a finite horizon"
    ]
  },
  {
    "objectID": "discr_indir_LQR_fin_horizon.html#free-final-state-and-finite-time-horizon",
    "href": "discr_indir_LQR_fin_horizon.html#free-final-state-and-finite-time-horizon",
    "title": "Discrete-time LQR on a finite horizon",
    "section": "Free final state and finite time horizon",
    "text": "Free final state and finite time horizon\nThe previous discussion revolved around the task of bringing the system to a given final state exactly. What if we relax this strict requirement and instead just request that the system be eventually brought to the close vicinity of the requested state? How close — this could be affected by the terminal state penalty in the cost function.\n\nThe only change with respect to the previous development is just in the boundary condition — the one at the final time. Now the final state \\bm x_N can also be used as a parameter for our optimization. Hence \\text{d}\\bm x_N\\neq 0 and the other term in the product must vanish. We write down again the full necessary conditions including the new boundary conditions \n\\begin{aligned}\n\\bm x_{k+1} &=\\mathbf A\\bm x_k-\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\boldsymbol\\lambda_k &= \\mathbf Q\\bm x_k+\\mathbf A^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\bm u_k &= -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\mathbf S_N \\bm x_N &= \\boldsymbol \\lambda_N,\\\\\n\\bm x_0 &= \\mathbf x_0.\n\\end{aligned}\n\nWe find ourselves in a pretty much similar trouble as before. The final-time boundary condition refers to the variables whose values we do not know. The solution is provided by the insightful guess, namely, why not trying to extend the linear relationship between the state and the co-state at the final time to all preceding discrete times? That is, we assume \n\\mathbf S_k \\bm x_k = \\boldsymbol \\lambda_k.\n\\tag{2}\nAt first, we can have no idea if it works. But let’s try it and see what happens. Substitute (Eq. 2) into the state and co-state equations. We start with the state equation \n\\bm x_{k+1} =\\mathbf A\\bm x_k-\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\bm x_{k+1}.\n\nSolving for \\bm x_{k+1} yields \n\\bm x_{k+1} =(\\mathbf I+\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1})^{-1}\\mathbf A\\bm x_k.\n\nNow perform the same substitution into the co-state equation \n\\mathbf S_k \\bm x_k = \\mathbf Q\\bm x_k+\\mathbf A^\\top\\mathbf S_{k+1}\\bm x_{k+1},\n and substitute for \\bm x_{k+1} from the state equation into the previous equation to get \n\\mathbf S_k \\bm x_k = \\mathbf Q\\bm x_k+\\mathbf A^\\top\\mathbf S_{k+1}(\\mathbf I+\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1})^{-1}\\mathbf A\\bm x_k.\n\nSince this equation must hold for an arbitrary \\bm x_k, we get an equation in the matrices \\mathbf S_k \n\\boxed{\n\\mathbf S_k = \\mathbf Q+\\mathbf A^\\top\\mathbf S_{k+1}(\\mathbf I+\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1})^{-1}\\mathbf A.\n}\n\nThis is a superfamous equation and is called difference (or discrete-time) Riccati equation. When initialized with \\mathbf S_N, it generates the sequence of matrices \\mathbf S_{N-1}, \\mathbf S_{N-2}, \\mathbf S_{N-3},\\ldots Indeed, a noteworthy feature of this sequence is that it is initialized at the final time and the equation prescribes how the sequence evolves backwards.\nOnce we have generated a sufficiently long sequence (down to \\mathbf S_{1}), the optimal control sequence \\bm u_0, \\bm u_1, \\ldots, \\bm u_{N-1} is then computed using the stationary equation \n\\bm u_k = -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1}=-\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\bm x_{k+1}.\n\nThis suggests that the optimal control is generated using the state but the current scheme is noncausal because the control at a given time depends on the state at the next time. But turning this into a causal one is easy — just substitute the state equation for \\bm x_{k+1} and get \n\\bm u_k =-\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}(\\mathbf A\\bm x_{k}+\\mathbf B\\bm u_{k}).\n\nSolving this equation for \\bm u_k gives \n\\bm u_k = -\\underbrace{(\\mathbf I + \\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf B)^{-1}\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf A}_{\\mathbf K_k}\\mathbf x_{k}.\n\nMission accomplished. This is our desired control. A striking observation is that although we made no specifications as for the controller structure, the optimal control strategy turned out a feedback one! Let’s write it down explicitly \n\\boxed{\n\\bm u_k = -\\mathbf K_k \\bm x_{k}.\n}\n\n\n\n\n\n\n\nLQ-optimal control on a finite time horizon with a free final state is a feedback control\n\n\n\nThe importance of this result can hardly be overstated – the optimal control comes in the form of a proportional state-feedback control law.\n\n\nThe feedback gain is time-varying and deserves a name after its inventor — Kalman gain. Incorporating the knowledge that \\mathbf R is nonsingular, a minor simplification of the lengthy expression can be made \n\\mathbf K_k = (\\mathbf R + \\mathbf B^\\top\\mathbf S_{k+1}\\mathbf B)^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf A.\n\\tag{3}\nBefore we move on, let us elaborate a bit more on the difference Riccati equation. Invoking a popular (but hard to reliably memorize) rule for inversion of a sum of two matrices called matrix inversion lemma, which reads \n(\\mathbf A_{11}^{-1}+\\mathbf A_{12}\\mathbf A_{22}\\mathbf A_{21})^{-1} =\\mathbf A_{11}-\\mathbf A_{11}\\mathbf A_{12}(\\mathbf A_{21}\\mathbf A_{11}\\mathbf A_{12}+\\mathbf A_{22}^{-1})^{-1}\\mathbf A_{21}\\mathbf A_{11},\n the Riccati equation can be rewritten (after multiplying the brackets out) as \n\\boxed{\n\\mathbf S_k = \\mathbf Q + \\mathbf A^\\top\\mathbf S_{k+1}\\mathbf A - \\mathbf A^\\top\\mathbf S_{k+1}\\mathbf B( \\mathbf B^\\top\\mathbf S_{k+1}\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf A,\n}\n which we will regard as an alternative form of difference Riccati equation.\nObserving that the steps of the computation of the Kalman gain \\mathbf K_k reappear in the computation of the solution of the Riccati equation, a more efficient arrangement of the computation in every iteration step is \n\\boxed{\n\\begin{aligned}\n\\mathbf K_k &= \\left(\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B+\\mathbf R\\right)^{-1}\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A\\\\\n\\mathbf S_k &= \\mathbf A^\\top \\mathbf S_{k+1}(\\mathbf A-\\mathbf B\\mathbf K_k) + \\mathbf Q.\n\\end{aligned}\n}\n\nFinally, yet another equivalent version of Riccati equation is known as Joseph stabilized form of Riccati equation \n\\boxed{\n\\mathbf S_k = (\\mathbf A-\\mathbf B\\mathbf K_k)^\\top \\mathbf S_{k+1}(\\mathbf A-\\mathbf B\\mathbf K_k) + \\mathbf K_k^\\top \\mathbf R\\mathbf K_k + \\mathbf Q.\n}\n\\tag{4}\nShowing the equivalence can be an exercise.\n\nSecond order sufficient conditions\nSo far we only found a solution that satisfies the first-order necessary equation but we have been warned at the introductory lessons to optimization that such solution need not necessarily constitute an optimum (minimum in our case). In order to check this, the second derivative (Hessian, curvature matrix) must be found and checked for positive definiteness. Our strategy will be to find the value of the optimal cost first and then we will identify its second derivative with respect to \\bm u_k.\nThe trick to find the value of the optimal cost is from [1] and it is rather technical and it may be hard to learn a general lesson from it. Nonetheless we will need the result. Therefore we swiftly go through the procedure without pretending that we are building a general competence. The trick is based on the observation that \n\\frac{1}{2}\\sum_{k=0}^{N-1}(\\mathbf x^\\top _{k+1}\\mathbf S_{k+1} \\mathbf x_{k+1} - \\mathbf x^\\top _{k}\\mathbf S_{k} \\mathbf x_{k}) = \\frac{1}{2}\\mathbf x^\\top _{N}\\mathbf S_{N} \\mathbf x_{N} - \\frac{1}{2}\\mathbf x^\\top _{0}\\mathbf S_{0} \\mathbf x_{0}.\n\nNow consider our optimization criterion and add zero to it. The value of the cost function does not change. Weird procedure, right? Observing that zero can also be expressed as the right hand side minus the left hand side in the above equation, we get \nJ_0 = \\frac{1}{2}\\bm x_0^\\top\\mathbf S_0\\bm x_0 + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[\\mathbf x^\\top _{k+1}\\mathbf S_{k+1} \\mathbf x_{k+1}+\\bm x_k^\\top (\\mathbf Q - \\mathbf S_k) \\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right].\n\nSubstituting the state equation, the cost function transforms to \n\\begin{aligned}\nJ_0 &= \\frac{1}{2}\\bm x_0^\\top\\mathbf S_0\\bm x_0 + \\frac{1}{2}\\sum_{k=0}^{N-1}[\\mathbf x^\\top _{k}(\\mathbf A^\\top \\mathbf S_{k+1}\\mathbf A + \\mathbf Q - \\mathbf S_k) \\mathbf x_{k}+\\bm x_k^\\top \\mathbf A^\\top \\mathbf S_{k+1}\\mathbf B \\bm u_k\\\\\n&\\qquad\\qquad\\qquad\\qquad+\\bm u_k^\\top \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A \\bm x_k+\\bm u_k^\\top (\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B + \\mathbf R)\\bm u_k].\n\\end{aligned}\n\nSubstituting for \\mathbf S_k from the Riccati equation gives \n\\begin{aligned}\nJ_0 &= \\frac{1}{2}\\bm x_0^\\top\\mathbf S_0\\bm x_0 + \\frac{1}{2}\\sum_{k=0}^{N-1}[\\mathbf x^\\top _{k}(\\mathbf A^\\top \\mathbf S_{k+1}\\mathbf B( \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A) \\mathbf x_{k}+\\bm x_k^\\top \\mathbf A^\\top \\mathbf S_{k+1}\\mathbf B \\bm u_k\\\\\n&\\qquad\\qquad\\qquad\\qquad+\\bm u_k^\\top \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A \\bm x_k+\\bm u_k^\\top (\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B + \\mathbf R)\\bm u_k].\n\\end{aligned}\n\nThe time-varying Hessian with respect to the control \\bm u_k is \n\\nabla_{\\bm u_k}^2 J_0 = \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B + \\mathbf R.\n\nProvided that \\mathbf R\\succ 0, it can be seen that it is always guaranteed that \\nabla_{\\bm u_k}^2 J_0\\succ 0. To prove this it must be shown that \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B\\succeq 0. As usual, let us make things more intuitive by switching to the scalar case. The previous expression simplifies to b^2s_{k+1}. No matter what the value of b is, the square is always nonnegative. It remains to show that s_{k+1}\\geq0 (and in the matrix case \\mathbf S_{k+1}\\succeq 0). This can be seen from the prescription for \\mathbf S_{k} given by the Riccati equation using similar arguments for proving positive semidefiniteness of compound expressions.\nTo conclude, the solution to the first-order necessary conditions represented by the Riccati equation is always a minimizing solution.\nWe can work a bit more with the value of the optimal cost. Substituting the optimal control we can see (after some careful two-line work) that \nJ_0^\\star = \\frac{1}{2}\\bm x_0^\\top  \\mathbf S_0 \\bm x_0.\n\nThe same conclusion can be obtained for any time instant k inside the interval [0,N] \n\\boxed{\nJ_k^\\star = \\frac{1}{2}\\bm x_k^\\top  \\mathbf S_k \\bm x_k.\n}\n\nThis is a result that we have already seen in the no-control case: the optimal cost can be obtained as a quadratic function of the initial state using a matrix obtained as a solution to some iteration. We will use this result in the future derivations.\n\n\nNumerical example with a scalar and first-order system\nAs usual, some practical insight can be developed by analyzing the things when restricted to the scalar case. For this, consider a first order system described by the first-order state equation \nx_{k+1} = ax_k + bu_k\n and the optimization criterion in the form \nJ_0 = \\frac{1}{2}s_N x_N^2 + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[ q x_k^2+r u_k^2\\right ].\n\nThe scalar Riccati equation simplifies to \ns_k = a^2s_{k+1} - \\frac{a^2b^2s_{k+1}^2}{b^2s_{k+1}+r} + q\n or \ns_k = \\frac{a^2rs_{k+1}}{b^2s_{k+1}+r} + q.\n\nJulia code and its outputs follow.\n\n\nShow the code\nfunction dre(a,b,q,r,sN,N)\n    s = Vector{Float64}(undef,N+1)          # the S[1] will then not be needed (even defined) but the indices will fit\n    k = Vector{Float64}(undef,N)\n    s[end] = sN\n    for i=N:-1:1\n        k[i]=(a*b*s[i+1])/(r + s[i+1]*b^2);\n        s[i]= a*s[i+1]*(a-b*k[i]) + q;\n    end\n    return s,k\nend\n\na = 1.05;\nb = 0.01;\nq = 100;\nr = 1;\nx0 = 10;\nsN = 100;\nN = 20;\n\ns,k = dre(a,b,q,r,sN,N);\n\nusing Plots\n\np1 = plot(0:1:N,s,xlabel=\"i\",ylabel=\"RE solution\",label=\"s\",markershape=:circ,markersize=1,linetype=:steppost)\np2 = plot(0:1:N-1,k,xlabel=\"i\",ylabel=\"State-feedback gain\",label=\"k\",markershape=:circ,markersize=1,linetype=:steppost,xlims=xlims(p1))\n\nx = Vector{Float64}(undef,N+1)\nu = Vector{Float64}(undef,N)\n\nx[1]=x0;\n\nfor i=1:N\n    u[i] = -k[i]*x[i];\n    x[i+1] = a*x[i] + b*u[i];\nend\n\np3 = plot(0:1:N,x,xlabel=\"i\",ylabel=\"State\",label=\"x\",markershape=:circ,markersize=1,linetype=:steppost)\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObviously the final state is not particularly close to zero, which is the desired final value. However, increasing the s_N term we can bring the system arbitrarily close, as the next simulation confirms.\n\n\nShow the code\nsN = 10000;\nN = 20;\n\ns,k = dre(a,b,q,r,sN,N);\n\np1 = plot(0:1:N,s,xlabel=\"i\",ylabel=\"RE solution\",label=\"s\",markershape=:circ,markersize=1,linetype=:steppost)\np2 = plot(0:1:N-1,k,xlabel=\"i\",ylabel=\"State-feedback gain\",label=\"k\",markershape=:circ,markersize=1,linetype=:steppost,xlims=xlims(p1))\n\nx = Vector{Float64}(undef,N+1)\nu = Vector{Float64}(undef,N)\n\nx[1]=x0;\n\nfor i=1:N\n    u[i] = -k[i]*x[i];\n    x[i+1] = a*x[i] + b*u[i];\nend\n\np3 = plot(0:1:N,x,xlabel=\"i\",ylabel=\"State\",label=\"x\",markershape=:circ,markersize=1,linetype=:steppost)\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we explore what changes if we make the time horizon longer.\n\n\nShow the code\nN = 100;\n\ns,k = dre(a,b,q,r,sN,N);\n\np1 = plot(0:1:N,s,xlabel=\"i\",ylabel=\"RE solution\",label=\"s\",markershape=:circ,markersize=1,linetype=:steppost)\np2 = plot(0:1:N-1,k,xlabel=\"i\",ylabel=\"State-feedback gain\",label=\"k\",markershape=:circ,markersize=1,linetype=:steppost,xlims=xlims(p1))\n\nx = Vector{Float64}(undef,N+1)\nu = Vector{Float64}(undef,N)\n\nx[1]=x0;\n\nfor i=1:N\n    u[i] = -k[i]*x[i];\n    x[i+1] = a*x[i] + b*u[i];\nend\n\np3 = plot(0:1:N,x,xlabel=\"i\",ylabel=\"State\",label=\"x\",markershape=:circ,markersize=1,linetype=:steppost)\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe last outputs suggests that both s_N and K_k stay constant for most of the time interval and they only change dramatically towards the end of the control interval.\nThe observation in the example poses a question of how much is lost after replacing the optimal control represented by the sequence \\mathbf K_k by some constant value \\mathbf K. A natural candidate is the steady-state value that \\mathbf K_k has as the beginning of the control interval, that is at k=0 in our case.\nObviously, on a finite-horizon there is not much to be investigated, the constant feedback gain is just suboptimal, but things are somewhat more involved as the control horizon stretches to infinity, that is, N\\rightarrow \\infty.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on a finite horizon"
    ]
  },
  {
    "objectID": "cont_indir_LQR_inf_horizon.html",
    "href": "cont_indir_LQR_inf_horizon.html",
    "title": "Indirect approach to LQR on an infinite horizon",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Indirect approach to LQR on an infinite horizon"
    ]
  },
  {
    "objectID": "discr_dir_mpc_software.html",
    "href": "discr_dir_mpc_software.html",
    "title": "Numerical solvers for MPC",
    "section": "",
    "text": "The extra features needed for MPC:\n\nwarmstarting requires fesibility of the previous solution. Some methods the iterations may temporarily lose feasibility, which can be a problem if only a fixed number of iterations is allowed (in favor of predictable timing).\n…\n\nA curated list of QP solvers is maintained at https://github.com/qpsolvers/qpbenchmark. Below are a few most popular from the open-source domain. Most if not all of them can be interfaced from various programming languages.\n\nqpOASES\nOSQP\nDAQP\nqpSWIFT\nProxQP\nPiQP\nECOS\nHPIPM",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Numerical solvers for MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_software.html#qp-solvers-suitable-for-linear-mpc",
    "href": "discr_dir_mpc_software.html#qp-solvers-suitable-for-linear-mpc",
    "title": "Numerical solvers for MPC",
    "section": "",
    "text": "The extra features needed for MPC:\n\nwarmstarting requires fesibility of the previous solution. Some methods the iterations may temporarily lose feasibility, which can be a problem if only a fixed number of iterations is allowed (in favor of predictable timing).\n…\n\nA curated list of QP solvers is maintained at https://github.com/qpsolvers/qpbenchmark. Below are a few most popular from the open-source domain. Most if not all of them can be interfaced from various programming languages.\n\nqpOASES\nOSQP\nDAQP\nqpSWIFT\nProxQP\nPiQP\nECOS\nHPIPM",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Numerical solvers for MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_software.html#higher-level-tools",
    "href": "discr_dir_mpc_software.html#higher-level-tools",
    "title": "Numerical solvers for MPC",
    "section": "Higher-level tools",
    "text": "Higher-level tools\n\nacados – free & open source; interfaces to Octave/Matlab, Python, C++.\nCasADi – free & open source; interfaces to Octave/Matlab, Python, C++.\nModelPredictiveControl.jl – free & open source; Julia.\nModel Predictive Control Toolbox for Matlab – commercial (by The Mathworks); Matlab; the generated C code follows industrial standards.\nMultiparametric Toolbox 3 (MPT3) – free & open source; Matlab.\nYalmip – free & open source; Matlab.\nForcesPro – commercial (by Embotech).",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Numerical solvers for MPC"
    ]
  },
  {
    "objectID": "rocond_H_infinity_control.html",
    "href": "rocond_H_infinity_control.html",
    "title": "Hinfinity-optimal control",
    "section": "",
    "text": "Here we formulate the general problem of \\mathcal{H}_\\infty-optimal control. There are two motivations for this. First, it gives the general framework within which we can formulate and solve the mixed-sensitivity problem defined in the frequency domain that we discused previously. Second, it allows us to consider exploit the time-domain (or signal) interpretation of the \\mathcal{H}_\\infty norm of a system to formulate a new class of problems that can be solved with these optimization tools. For the latter, recall that \n\\|\\mathbf G\\|_{\\infty} = \\sup_{u\\in\\mathcal{L}_{2}\\backslash \\emptyset}\\frac{\\|\\bm y\\|_2}{\\|\\bm u\\|_2},\n in which we allow for vector input and output signals, hence MIMO systems, from the very beginning.\nNow, for particular control requirements, we build the generalized plant \\mathbf P such that after forming the feedback interconnection with the controller \\mathbf K as in Figure 1\n\n\n\n\n\n\nFigure 1: Lower LFT of the generalized plant and the controller\n\n\n\nit makes sense require the stabilizing controller to minimize the amplification of the exogenous inputs (disturbances, references, noises) into the regulated outputs. We want to make the regulated outputs as insensitive as possible to the exogenous inputs and to quantify the sizes of the inputs and outputs, we use the \\mathcal L_2 norm.\nBut then what we have is really the standard \\mathcal{H}_\\infty optimization problem \\boxed\n{\\operatorname*{minimize}_{\\mathbf K \\text{ stabilizing}}\\|\\mathcal{F}_{\\mathrm l}(\\mathbf P,\\mathbf K)\\|_{\\infty}.}\n\nNumerical solvers exist in various software environments.\n\nMixed-sensitivity problem reformulated as the standard \\mathcal{H}_\\infty optimization problem\nWe now show how the mixed-sensitivity problem discussed previously can be reformulated within as the standard \\mathcal{H}_\\infty optimization problem. We consider the full mixed-sensitivity problem for a SISO plant\n\n\\operatorname*{minimize}_{K \\text{ stabilizing}}  \n\\left\\|\n\\begin{bmatrix}\nW_1S\\\\W_2KS\\\\W_3T\n\\end{bmatrix}\n\\right\\|_{\\infty},\n which obviously considers a closed-loop system with one input and three outputs. With only one exogenous input, we must choose its role. Say, the only exogenous input is the reference signal. The closed-loop system for which the norm is minimized is in the following block diagram Figure 2.\n\n\n\n\n\n\nFigure 2: Mixed-sensitivity problem interpreted as the standard \\mathcal{H}_\\infty optimization problem\n\n\n\nThe matrix transfer function for generalized plant \\mathbf P has two inputs and four outputs and it can then be written as \n\\mathbf P = \\left[\\begin{array}{c|c}\nW_1 & -W_1G\\\\\n0 & W_2\\\\\n0 & W_3G\\\\\n\\hline\n1 & -G\n\\end{array}\\right].\n\nA state space realization of this plant \\mathbf P is then used as the input argument to the solver for the \\mathcal{H}_\\infty optimization problem. In fact, we must also tell the solver how the inputs and outputs are structured. In this case, the solver must know that of the two inputs, only the second one can be used by the controller, and of the four outputs, only the fourth one is measured.\n\n\nSignal-based \\mathcal{H}_\\infty-optimal control problem\nBeing able to solve the \\mathcal{H}_\\infty optimization problem, indeed we do not have to restrict ourselves to the generalized plants \\mathbf P that correspond to the mixed-sensitivity problem. We can consider any plant \\mathbf P, for which the problem makes sense. For example, if we want to consider not only references but also disturbances, and possibly even noises, there is no way to formulate this within the mixed-sensitivity framework. But we can still formulate this as the standard \\mathcal{H}_\\infty optimal control problem.\n\n\nWhat is behind the \\mathcal{H}_\\infty solver?\n\n\nStructure of the \\mathcal{H}_\\infty-optimal controller\n\n\n\n\n Back to top",
    "crumbs": [
      "12. Robust control",
      "Hinfinity-optimal control"
    ]
  },
  {
    "objectID": "limitations_MIMO.html",
    "href": "limitations_MIMO.html",
    "title": "Limitations for MIMO systems",
    "section": "",
    "text": "Multiple-input-multiple-output (MIMO) systems are subject to limitations of the same origin as single-input-single-output (SISO) systems: unstable poles, “unstable” zeros, delays, disturbances, saturation, etc. However, the vector character of inputs and outputs introduces both opportunities to mitigate those limitations, and… new limitations.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#directions-in-mimo-systems",
    "href": "limitations_MIMO.html#directions-in-mimo-systems",
    "title": "Limitations for MIMO systems",
    "section": "Directions in MIMO systems",
    "text": "Directions in MIMO systems\nWith vector inputs and vector outputs, the input-output model of an LTI MIMO system is a matrix (of transfer functions). As such, it can be characterized not only by various scalar quantities (like poles, zeros, etc.), but also by the associated directions in the input and output spaces.\n\nExample 1 Consider the transfer function matrix (or matrix of transfer functions) \nG(s) = \\frac{1}{(0.2s+1)(s+1)}\\begin{bmatrix}1 & 1\\\\ 1+2s& 2\\end{bmatrix}.\n\nRecall that a complex number z\\in\\mathbb C is a zero of G if the rank of G(z) is less than the rank of G(s) for most s. While reliable numerical algorithms for computing zeros of MIMO systems work with state-space realizations, in this simple case we can easily verify that there is only one zero z=1/2.\nZeros in the RHP only exhibit themselves in some directions.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#conditioning-of-mimo-systems",
    "href": "limitations_MIMO.html#conditioning-of-mimo-systems",
    "title": "Limitations for MIMO systems",
    "section": "Conditioning of MIMO systems",
    "text": "Conditioning of MIMO systems\n\\boxed{\n\\gamma (G) = \\frac{\\bar{\\sigma}(G)}{\\underline{\\sigma}(G)}\n}\n\n\nIll-conditioned for \\gamma&gt;10\nBut depends on scaling!\n\nTherefore minimized conditioning number \\boxed{\n\\gamma^\\star(G) = \\min_{D_1, D_2}\\gamma(D_1GD_2)\n}\n but difficult do compute (=upper bound on \\mu)\nRGA can be used to give a reasonable estimate.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#relative-gain-array-rga",
    "href": "limitations_MIMO.html#relative-gain-array-rga",
    "title": "Limitations for MIMO systems",
    "section": "Relative gain array (RGA)",
    "text": "Relative gain array (RGA)\nRelative Gain Array (RGA) as an indicator of difficulties with control \\boxed{\\Lambda(G) = G \\circ (G^{-1})^T}\n\nindependent of scaling,\nsum of elements in rows and columns is 1,\nsum of absolute values of elements of RGA is very close to the minimized sensitivity number \\gamma^\\star, hence a system with large RGA entries is always ill-conditioned (but system with large \\gamma can have small RGA),\nRGA for a triangular system is an identity matrix,\nrelative uncertainty of an element of a transfer function matrix equal to (negative) inverse of the corresponding RGA entry makes the system singular.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#functional-controllability",
    "href": "limitations_MIMO.html#functional-controllability",
    "title": "Limitations for MIMO systems",
    "section": "Functional controllability",
    "text": "Functional controllability",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#interpolation-conditions-for-mimo-systems",
    "href": "limitations_MIMO.html#interpolation-conditions-for-mimo-systems",
    "title": "Limitations for MIMO systems",
    "section": "Interpolation conditions for MIMO systems",
    "text": "Interpolation conditions for MIMO systems",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#bandwidth-limitations-due-to-unstable-poles-and-zeros",
    "href": "limitations_MIMO.html#bandwidth-limitations-due-to-unstable-poles-and-zeros",
    "title": "Limitations for MIMO systems",
    "section": "Bandwidth limitations due to unstable poles and zeros",
    "text": "Bandwidth limitations due to unstable poles and zeros",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#limits-given-by-presence-of-disturbance-andor-reference",
    "href": "limitations_MIMO.html#limits-given-by-presence-of-disturbance-andor-reference",
    "title": "Limitations for MIMO systems",
    "section": "Limits given by presence of disturbance and/or reference",
    "text": "Limits given by presence of disturbance and/or reference",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#disturbance-rejection-by-a-plant-with-rhp-zero",
    "href": "limitations_MIMO.html#disturbance-rejection-by-a-plant-with-rhp-zero",
    "title": "Limitations for MIMO systems",
    "section": "Disturbance rejection by a plant with RHP zero",
    "text": "Disturbance rejection by a plant with RHP zero",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#limits-given-by-the-input-constraints-saturation",
    "href": "limitations_MIMO.html#limits-given-by-the-input-constraints-saturation",
    "title": "Limitations for MIMO systems",
    "section": "Limits given by the input constraints (saturation)",
    "text": "Limits given by the input constraints (saturation)",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "limitations_MIMO.html#limits-given-by-uncertainty-in-the-model-in-open-loop",
    "href": "limitations_MIMO.html#limits-given-by-uncertainty-in-the-model-in-open-loop",
    "title": "Limitations for MIMO systems",
    "section": "Limits given by uncertainty in the model: in open loop",
    "text": "Limits given by uncertainty in the model: in open loop\n\nIn open loop\n\n\nIn closed loop",
    "crumbs": [
      "13. Limitations of achievable performance",
      "Limitations for MIMO systems"
    ]
  },
  {
    "objectID": "roban_structured.html#robust-performance-with-a-structured-uncertainty",
    "href": "roban_structured.html#robust-performance-with-a-structured-uncertainty",
    "title": "Robustness analysis for structured uncertainty",
    "section": "Robust performance with a structured uncertainty",
    "text": "Robust performance with a structured uncertainty",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for structured uncertainty"
    ]
  },
  {
    "objectID": "cont_numerical_hw.html",
    "href": "cont_numerical_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Homework"
    ]
  },
  {
    "objectID": "cont_dp_LQR.html",
    "href": "cont_dp_LQR.html",
    "title": "Using HJB equation to solve the continuous-time LQR problem",
    "section": "",
    "text": "As we have already discussed a couple of times, in the LQR problem we consider a linear time invariant (LTI) system modelled by \n\\dot{\\bm x}(t) = \\mathbf A\\bm x(t) + \\mathbf B\\bm u(t),\n and the quadratic cost function \nJ(\\bm x(t_\\mathrm{i}),\\bm u(\\cdot), t_\\mathrm{i}) = \\frac{1}{2}\\bm x^\\top(t_\\mathrm{f})\\mathbf S_\\mathrm{f}\\bm x(t_\\mathrm{f}) + \\frac{1}{2}\\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}\\left(\\bm x^\\top \\mathbf Q\\bm x + \\bm u^\\top \\mathbf R \\bm u\\right)\\mathrm{d}t.\n\nThe Hamiltonian is \nH(\\bm x,\\bm u,\\bm \\lambda) = \\frac{1}{2}\\left(\\bm x^\\top \\mathbf Q\\bm x + \\bm u^\\top \\mathbf R \\bm u\\right) + \\boldsymbol{\\lambda}^\\top \\left(\\mathbf A\\bm x + \\mathbf B\\bm u\\right).\n\nAccording to the HJB equation our goal is to minimize H at a given time t, which enforces the condition on its gradient \n\\mathbf 0 = \\nabla_{\\bm u} H = \\mathbf R\\bm u + \\mathbf B^\\top \\boldsymbol\\lambda,\n from which it follows that the optimal control must necessarily satisfy \n\\bm u^\\star = -\\mathbf R^{-1} \\mathbf B^\\top \\boldsymbol\\lambda.\n\nSince the Hessian of the Hamiltonian is positive definite by our assumption on positive definiteness of \\mathbf R \n\\nabla_{\\bm u \\bm u}^2 \\mathbf H = \\mathbf R &gt; 0,\n Hamiltonian is really minimized by the above choice of \\bm u^\\star.\nThe minimized Hamiltonian is \n\\min_{\\bm u(t)}H(\\bm x, \\bm u, \\bm \\lambda) = \\frac{1}{2}\\bm x^\\top \\mathbf Q \\bm x + \\boldsymbol\\lambda^\\top \\mathbf A \\bm x - \\frac{1}{2}\\boldsymbol\\lambda^\\top \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\boldsymbol\\lambda\n\nSetting \\boldsymbol\\lambda = \\nabla_{\\bm x} J^\\star, the HJB equation is \\boxed\n{-\\frac{\\partial J^\\star}{\\partial t} = \\frac{1}{2}\\bm x^\\top \\mathbf Q \\bm x + (\\nabla_{\\bm x} J^\\star)^\\top \\mathbf A\\bm x - \\frac{1}{2}(\\nabla_{\\bm x} J^\\star)^\\top \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\nabla_{\\bm x} J^\\star,}\n and the boundary condition is \nJ^\\star(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) = \\frac{1}{2}\\bm x^\\top (t_\\mathrm{f})\\mathbf S_\\mathrm{f}\\bm x(t_\\mathrm{f}).\n\nWe can now proceed by assuming that the optimal cost function is quadratic in \\bm x for all other times t, that is, there must exist a symmetric matrix function \\mathbf S(t) such that \nJ^\\star(\\bm x(t),t) = \\frac{1}{2}\\bm x^\\top (t)\\mathbf S(t)\\bm x(t).\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that we did something similar when making a sweep assumption to derive a Riccati equation following the indirect approach – we just make an inspired guess and see if it solves the equation. Here the inspiration comes from the observation made elsewhere, that the optimal cost function in the LQR problem is quadratic in \\bm x.\n\n\nWe now aim at substituting this into the HJB equation. Observe that \\frac{\\partial J^\\star}{\\partial t}=\\bm x^\\top(t) \\dot{\\mathbf{S}}(t) \\bm x(t) and \\nabla_{\\bm x} J^\\star = \\mathbf S \\bm x. Upon substitution to the HJB equation, we get\n\n-\\bm x^\\top \\dot{\\mathbf{S}} \\bm x = \\frac{1}{2}\\bm x^\\top \\mathbf Q \\bm x + \\bm x^\\top \\mathbf S \\mathbf A\\bm x - \\frac{1}{2}\\bm x^\\top \\mathbf S \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\mathbf S \\bm x.\n\nThis can be reformatted as \n-\\bm x^\\top \\dot{\\mathbf{S}} \\bm x = \\frac{1}{2} \\bm x^\\top \\left[\\mathbf Q + 2 \\mathbf S \\mathbf A - \\mathbf S \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\mathbf S \\right ] \\bm x.\n\nNotice that the middle matrix in the square brackets is not symmetric. Symmetrizing it (with no effect on the resulting value of the quadratic form) we get\n\n-\\bm x^\\top \\dot{\\mathbf{S}} \\bm x = \\frac{1}{2} \\bm x^\\top \\left[\\mathbf Q + \\mathbf S \\mathbf A + \\mathbf A^\\top \\mathbf S  - \\mathbf S \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\mathbf S \\right ] \\bm x.\n\nFinally, since the above single (scalar) equation should hold for all \\bm x(t), the matrix equation must hold too, and we get the familiar differential Riccati equation for the matrix variable \\mathbf S(t) \\boxed\n{-\\dot{\\mathbf S}(t) = \\mathbf A^\\top \\mathbf S(t) + \\mathbf S(t)\\mathbf A - \\mathbf S(t)\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top \\mathbf S(t) + \\mathbf Q}\n initialized at the final time t_\\mathrm{f} by \\mathbf S(t_\\mathrm{f}) = \\mathbf S_\\mathrm{f}.\nHaving obtained \\mathbf S(t), we can get the optimal control by substituting it into \\boxed\n{\n\\begin{aligned}\n    \\bm u^\\star(t) &= - \\mathbf R^{-1}\\mathbf B^\\top \\nabla_{\\bm x} J^\\star(\\bm x(t),t) \\\\\n                   &= - \\underbrace{\\mathbf R^{-1}\\mathbf B^\\top \\mathbf S(t)}_{\\bm K(t)}\\bm x(t).\n\\end{aligned}\n}\n\nWe have just rederived the continuous-time LQR problem using the HJB equation (previously we did it by massaging the two-point boundary value problem that followed as the necessary condition of optimality from the techniques of calculus of variations).\nNote that we have also just seen the equivalence between a first-order linear PDE and first-order nonlinear ODE.\n\n\n\n Back to top",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Using HJB equation to solve the continuous-time LQR problem"
    ]
  },
  {
    "objectID": "cont_dp_DDP.html",
    "href": "cont_dp_DDP.html",
    "title": "Differential dynamic programming (DDP)",
    "section": "",
    "text": "blabla\n\n\n\n Back to top",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Differential dynamic programming (DDP)"
    ]
  },
  {
    "objectID": "cont_indir_trajectory_stabilization.html",
    "href": "cont_indir_trajectory_stabilization.html",
    "title": "Trajectory stabilization and neigboring extremals",
    "section": "",
    "text": "Indirect methods for optimal control reformulate the optimal control problem into a set of equtions – boundary value problems with differential and algebraic equations in the case of continuous-time systems – and by solving these (typically numerically) we obtain the optimal state and control trajectories. Practical usefullness of these is rather limited as such optimal control trajectory constitutes an open-loop control – there is certainly no need to advocate the importance of feedback in this advanced control course.\nOne way to introduce feedback is to regard the computed optimal state trajectory \\bm x^\\star(t) as a reference trajectory and design a feedback controller to track this reference. To our advantage, we already have the corresponding control trajectory \\bm u^\\star(t) too, and theferore we can formulate such reference tracking problem as a problem of regulating the deviation \\delta \\bm x(t) of the state from its reference by means of superposing a feedback control \\delta \\bm u(t) onto the (open-loop) optimal control.\nWhile this problem – also known as the problem of stabilization of a (reference) trajectory – can be solved by basically any feedback control scheme, one elegant way is to linearize the system around the reference trajectory and formulate the problem as the LQR problem for a time-varying linear system.\n\n\n\n\n\n\nLinearization around a trajectory\n\n\n\nDon’t forget that when linearizing a nonlinear system \\dot{\\bm x} = \\mathbf f(\\bm x,\\bm u) around a point that is not equilibrium – and this inevitably happens when linearizing along the state trajectory \\bm x^\\star(t) obtained from indirect approach to optimal control – the linearized system \\frac{\\mathrm d}{\\mathrm d t} \\delta \\bm x= \\mathbf A(t) \\delta \\bm x + \\mathbf B(t) \\delta \\bm u considers not only the state variables but also the control variables as increments \\delta \\bm x(t) and \\delta \\bm u(t) that must be added to the nominal values \\bm x^\\star(t) and \\bm u^\\star(t) of the state and control variables determining the operating point. That is, \\bm x(t) = \\bm x^\\star(t) + \\delta \\bm x(t) and \\bm u(t) = \\bm u^\\star(t) + \\delta \\bm u(t).\n\n\nHaving decided on an LQR framework, we can now come up with the three matrices \\mathbf Q, \\mathbf R and \\mathbf S that set the quadratic cost function. Once this choice is made, we can just invoke the solver for continuous-time Riccati equation with the ultimate goal of finding the time-varying state feedback gain \\mathbf K(t).\n\n\n\n\n\n\nLQR for trajectory stabilization cab be done in discrete time\n\n\n\nIf discrete-time feedback control is eventually desired, which it mostly is, the whole LQR design for a time-varying linear system will have to be done using just periodically sampled state and control trajectories and applying recursive formulas for the discrete-time Riccati equation and state feedback gain.\n\n\nThe three weighting matrices \\mathbf Q, \\mathbf R and \\mathbf S, if chosen arbitrarily, are not related to the original cost function that is minimized by the optimal state and control trajectories. The matrices just parameterize a new optimal control problem. It turns out, however, that there is a clever (and insightful) way of choosing these matrices so that the trajectory stabilization problem inherits the original cost function. In other words, even when the system fails to stay on the optimal trajectory perfectly, the LQR state-feedback controller will keep minimizing the same cost function when regulating the deviation from the optimal trajectory.\nRecall that using the conventional definition of Hamiltonian H(\\bm x, \\bm u, \\bm \\lambda) = L(\\bm x, \\bm u) + \\bm \\lambda^\\top \\mathbf f(\\bm x, \\bm u), in which we now assume time invariance of both the system and the cost function for notational simplicity, the necessary conditions of optimality are \n\\begin{aligned}\n\\dot{\\bm x} &= \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\boldsymbol \\lambda)  = \\mathbf f(\\bm x, \\bm u), \\\\\n\\dot{\\bm \\lambda} &= -\\nabla_{\\bm x} H(\\bm x,\\bm u,\\boldsymbol \\lambda), \\\\\n\\mathbf 0 &= \\nabla_{\\bm u} H(\\bm x,\\bm u,\\boldsymbol \\lambda),\\\\\n\\bm x(t_\\mathrm{i})&=\\mathbf x_\\mathrm{i},\\\\\n\\bm x(t_\\mathrm{f})&=\\mathbf x_\\mathrm{f} \\quad \\text{or}\\quad \\bm \\lambda(t_\\mathrm{f})=\\nabla\\phi(\\bm{x}(t_\\mathrm{f})),\n\\end{aligned}\n where the option on the last line is selected based on whether the state at the final time is fixed or free.\n\n\n\n\n\n\nThe state at the final time can also be restricted by a linear equation\n\n\n\nThe conditions of optimality stated above correspond to one of the two standard situations, in which the state in the final time is either fixed to a single value or completely free. The conditions can also be modified to consider the more general situation, in which the state at the final time is restricted to lie on a manifold defined by an equality constraint \\psi(\\bm x(t_\\mathrm{f})) = 0.\n\n\nLet’s now consider some tiny perturbation to the initial state \\bm x(t_\\mathrm{i}) from its prescribed nominal value \\mathbf x_\\mathrm{i}. It will give rise to deviations all all the variables in the above equations from their nominal – optimal – trajectories. Assuming the deviations are small, linear model suffices to describe them. In other words, what we are now after is linearization of the above equations\n\n\\begin{aligned}\n\\delta \\dot{\\bm x} &= \\nabla_{\\bm x} \\mathbf f \\; \\delta \\bm x + \\nabla_{\\bm u} \\mathbf f \\; \\delta \\bm u, \\\\\n\\delta \\dot{\\bm \\lambda} &= -\\nabla^2_{\\bm{xx}} H \\; \\delta \\bm x -\\nabla^2_{\\bm{xu}} H \\; \\delta \\bm u -\\underbrace{\\nabla^2_{\\bm{x\\lambda}} H}_{(\\nabla_{\\bm x} \\mathbf f)^\\top} \\; \\delta \\bm \\lambda, \\\\\n\\mathbf 0 &= \\nabla^2_{\\bm{ux}} H \\; \\delta \\bm x + \\nabla^2_{\\bm{uu}} H \\; \\delta \\bm u + \\underbrace{\\nabla^2_{\\bm{u\\lambda}} H}_{(\\nabla_{\\bm u} \\mathbf f)^\\top} \\; \\delta \\bm \\lambda,\\\\\n\\delta \\bm x(t_\\mathrm{i}) &= \\text{specified},\\\\\n\\delta \\bm \\lambda(t_\\mathrm{f}) &= \\nabla^2_{\\bm{xx}}\\phi(\\mathbf{x}(t_\\mathrm{f}))\\; \\delta \\bm x(t_\\mathrm{f}).\n\\end{aligned}\n\\tag{1}\n\n\n\n\n\n\nNote on notation\n\n\n\nLet’s recall for convenience here that since \\mathbf f(\\bm x, \\bm u) is a vector function of a vector argument(s), \\nabla_{\\bm x} \\mathbf f is a matrix whose columns are gradients of individual components of \\mathbf f. Equivalently, (\\nabla_{\\bm x} \\mathbf f)^\\top stands for the Jacobian of the function \\mathbf f with respect to \\bm x. Similarly, \\nabla_{\\bm{xx}} H is the Hessian of \\mathbf f with respect to \\bm x. That is, it is a matrix composed of second derivatives. It is a symmetric matrix, hence no need to transpose it. Finally, the terms \\nabla_{\\bm{ux}} H and \\nabla_{\\bm{xu}} H are matrices containing mixed second derivatives.\n\n\nWith hindsight we relabel the individual terms in Equation 1 as \n\\begin{aligned}\n\\mathbf A(t) &\\coloneqq (\\nabla_{\\bm x} \\mathbf f)^\\top\\\\\n\\mathbf B(t) &\\coloneqq (\\nabla_{\\bm u} \\mathbf f)^\\top\\\\\n\\mathbf Q(t) &\\coloneqq \\nabla^2_{\\bm{xx}} H\\\\\n\\mathbf R(t) &\\coloneqq \\nabla^2_{\\bm{uu}} H\\\\\n\\mathbf N(t) &\\coloneqq \\nabla^2_{\\bm{xu}} H\\\\\n\\mathbf S_\\mathrm{f} &\\coloneqq \\nabla^2_{\\bm{xx}}\\phi(\\mathbf{x}(t_\\mathrm{f})).\n\\end{aligned}\n\nLet’s rewrite the perturbed necessary conditions of optimality using these new symbols\n\n\\begin{aligned}\n\\delta \\dot{\\bm x} &= \\mathbf A(t) \\; \\delta \\bm x + \\mathbf B(t) \\; \\delta \\bm u, \\\\\n\\delta \\dot{\\bm \\lambda} &= - \\mathbf Q(t) \\; \\delta \\bm x - \\mathbf N(t) \\; \\delta \\bm u - \\mathbf A^\\top (t)\\; \\delta \\bm \\lambda, \\\\\n\\mathbf 0 &= \\mathbf N(t) \\; \\delta \\bm x + \\mathbf R(t) \\; \\delta \\bm u + \\mathbf B^\\top (t) \\; \\delta \\bm \\lambda,\\\\\n\\delta \\bm x(t_\\mathrm{i}) &= \\text{specified},\\\\\n\\delta \\bm \\lambda(t_\\mathrm{f}) &= \\mathbf S_\\mathrm{f}    \\; \\delta \\bm x(t_\\mathrm{f}).\n\\end{aligned}\n\nAssuming that \\nabla^2_{\\bm{uu}} H is nonsingular, which can solve the third equation for \\bm u \n\\bm u = -\\nabla^2_{\\bm{uu}} H^{-1} \\left( \\nabla^2_{\\bm{ux}} H \\; \\delta \\bm x + (\\nabla_{\\bm u} \\mathbf f)^\\top \\; \\delta \\bm \\lambda\\right ).\n\n\n\n\n Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Trajectory stabilization and neigboring extremals"
    ]
  },
  {
    "objectID": "ext_LTR.html",
    "href": "ext_LTR.html",
    "title": "Loop transfer recovery (LTR)",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Loop transfer recovery (LTR)"
    ]
  },
  {
    "objectID": "reduction_order_references.html",
    "href": "reduction_order_references.html",
    "title": "References",
    "section": "",
    "text": "The primary reference for our overview of methods for model and controller order reduction is Chapter 11 in [1]. For a more detailed introduction, there a few dedicated monographs such as [2] and [3]. A short extract from the latter is in [4].\nThe latter also excels in that it also admits that the topic of reduction of order of mathematical models formatted as state equations is not only relevant for the control systems community but from a number of other engineering and scientific communities as well. After all, mathematical models are not only for model-based control design but for simulation, optimization, and other purposes. For example, the in [5] they are motivated by fast simulation of VLSI circuits.\n\n\n\n\n Back to topReferences\n\n[1] S. Skogestad and I. Postlethwaite, Multivariable Feedback Control: Analysis and Design, 2nd ed. Wiley, 2005. Available: https://folk.ntnu.no/skoge/book/\n\n\n[2] G. Obinata and B. D. O. Anderson, Model Reduction for Control System Design. New York: Springer, 2000.\n\n\n[3] A. C. Antoulas, Approximation of Large-Scale Dynamical Systems. Philadelphia: Society for Industrial and Applied Mathematics, 2005.\n\n\n[4] A. C. Antoulas and D. C. Sorensen, “Approximation of large-scale dynamical systems: An Overview,” Rice University, Houston, Texas, Technical Report, Feb. 2001. Accessed: May 21, 2024. [Online]. Available: https://hdl.handle.net/1911/101964\n\n\n[5] S. Tan and L. He, Advanced Model Order Reduction Techniques in VLSI Design. Cambridge: Cambridge University Press, 2007.",
    "crumbs": [
      "14. Model and controller order reduction",
      "References"
    ]
  },
  {
    "objectID": "dynamic_programming_hw.html",
    "href": "dynamic_programming_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "Homework"
    ]
  },
  {
    "objectID": "limitations_references.html",
    "href": "limitations_references.html",
    "title": "References",
    "section": "",
    "text": "The material here is mostly based on the chapters 5 (SISO systems) and 6 (MIMO systems) of [1].\nWhile the treatment of the book is more than sufficient for our course, we list here some other resources for further reading. The popular paper [2] provides a nice explanation of the waterbed effect(s).\nInterested readers may want to consult the specialized monograph [3], but it is certainly not necessary for our course.\nMore recent results are surveyed in [4].\n\n\n\n\n Back to topReferences\n\n[1] S. Skogestad and I. Postlethwaite, Multivariable Feedback Control: Analysis and Design, 2nd ed. Wiley, 2005. Available: https://folk.ntnu.no/skoge/book/\n\n\n[2] G. Stein, “Respect the unstable,” IEEE Control Systems, vol. 23, no. 4, pp. 12–25, Aug. 2003, doi: 10.1109/MCS.2003.1213600.\n\n\n[3] M. M. Seron, J. H. Braslavsky, and G. C. Goodwin, Fundamental Limitations in Filtering and Control. in Communications and Control Engineering. London: Springer, 1997. Available: https://doi.org/10.1007/978-1-4471-0965-5\n\n\n[4] J. Chen, S. Fang, and H. Ishii, “Fundamental limitations and intrinsic limits of feedback: An overview in an information age,” Annual Reviews in Control, vol. 47, pp. 155–177, Jan. 2019, doi: 10.1016/j.arcontrol.2019.03.011.",
    "crumbs": [
      "13. Limitations of achievable performance",
      "References"
    ]
  },
  {
    "objectID": "ext_references.html",
    "href": "ext_references.html",
    "title": "References",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "References"
    ]
  },
  {
    "objectID": "cont_numerical_software.html",
    "href": "cont_numerical_software.html",
    "title": "Software",
    "section": "",
    "text": "The methods studied in this chapter are already quite mature and well described. Software implementations exist. Here we enumerate some of them.\nacados Implemented in C but interfaces exist for Matlab and Python FOSS (Verschueren et al. 2022)\nGPOPS-II Matlab\nrockit In Python, built on top of CasADi, interface to Matlab.\n\n\n\n\n Back to topReferences\n\nVerschueren, Robin, Gianluca Frison, Dimitris Kouzoupis, Jonathan Frey, Niels van Duijkeren, Andrea Zanelli, Branimir Novoselnik, Thivaharan Albin, Rien Quirynen, and Moritz Diehl. 2022. “Acados—a Modular Open-Source Framework for Fast Embedded Optimal Control.” Mathematical Programming Computation 14 (1): 147–83. https://doi.org/10.1007/s12532-021-00208-8.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Software"
    ]
  },
  {
    "objectID": "opt_algo_goals.html",
    "href": "opt_algo_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Explain the main principle of descent direction methods for unconstrained optimization. In particular, give the descent direction condition.\nGive an overview of approaches for line search, that is, a one-dimensional optimization.\nExplain the steepest descent (aka gradient) method. Discuss its shortcomings.\nExplain conditioning of a matrix and what impact it has on convergence of steepest descent algorithm. Propose a modification of a steepest descent method that includes scaling of the original matrix such that conditioning is improved.\nExplain the Newton method for unconstrained minimization. Give also the its interpretation as a method for root finding. Discuss its shortcomings.\nDiscuss the issue of solving a set of linear equations (in matrix-vector form) as they appear in the Newton method. Which matrix factorization will be appropriate?\nExplain the key idea behind Quasi-Newton methods.\nExplain the key idea behind trust region methods for unconstrained optimization. What are the advantages with respect to descent direction methods?",
    "crumbs": [
      "2. Optimization – algorithms",
      "Learning goals"
    ]
  },
  {
    "objectID": "opt_algo_goals.html#knowledge-remember-and-understand",
    "href": "opt_algo_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Explain the main principle of descent direction methods for unconstrained optimization. In particular, give the descent direction condition.\nGive an overview of approaches for line search, that is, a one-dimensional optimization.\nExplain the steepest descent (aka gradient) method. Discuss its shortcomings.\nExplain conditioning of a matrix and what impact it has on convergence of steepest descent algorithm. Propose a modification of a steepest descent method that includes scaling of the original matrix such that conditioning is improved.\nExplain the Newton method for unconstrained minimization. Give also the its interpretation as a method for root finding. Discuss its shortcomings.\nDiscuss the issue of solving a set of linear equations (in matrix-vector form) as they appear in the Newton method. Which matrix factorization will be appropriate?\nExplain the key idea behind Quasi-Newton methods.\nExplain the key idea behind trust region methods for unconstrained optimization. What are the advantages with respect to descent direction methods?",
    "crumbs": [
      "2. Optimization – algorithms",
      "Learning goals"
    ]
  },
  {
    "objectID": "opt_algo_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "opt_algo_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nWrite a code implementing a Quasi-Newton method for minimization of a provided function.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Learning goals"
    ]
  },
  {
    "objectID": "opt_theory_goals.html",
    "href": "opt_theory_goals.html",
    "title": "Learning goals",
    "section": "",
    "text": "Give a rigorous definition of a local minimum (and maximum) and explain how it differs from a global minimum (maximum).\nDefine a convex function, convex set and convex optimization problem and explain the impact the convexity has on the optimization.\nState the Weierstrass (extreme value) theorem on the existence of a minimum (maximum).\nExplain the concepts of big O() and little o() within the framework of truncated Taylor series.\nGive first-order necessary conditions of optimality for a scalar function of a scalar argument, and define critical (or stationary) point. Extend these to the vector argument case. Formulate them both using a Fréchet and Gateaux derivatives. Specialize the result to quadratic functions.\nGive second-order sufficient conditions of optimality for a scalar function of a scalar argument. How can we distinguish between a minimum, maximum, and an inflection point? Extend these to the vector case. Define Hessian and show how it can be used to classify the critical points into minimum, maximum, saddle point, and singularity point. Specialize the results to quadratic functions.\nGive first-order necessary condition of optimality for an equality-constrained optimization problem using Lagrange multipliers. Specialize the results to quadratic cost functions and linear constraints.\nCharacterize the regular point (for a given set of equality constraints). Give an example of equality constraints lacking regularity.\nGive second-order sufficient conditions of optimality for an equality-constrained optimization problem using the concept of a projected Hessian.\nState and explain the Karush-Kuhn-Tucker (KKT) conditions for inequality-constrained optimization problems.",
    "crumbs": [
      "1. Optimization – theory",
      "Learning goals"
    ]
  },
  {
    "objectID": "opt_theory_goals.html#knowledge-remember-and-understand",
    "href": "opt_theory_goals.html#knowledge-remember-and-understand",
    "title": "Learning goals",
    "section": "",
    "text": "Give a rigorous definition of a local minimum (and maximum) and explain how it differs from a global minimum (maximum).\nDefine a convex function, convex set and convex optimization problem and explain the impact the convexity has on the optimization.\nState the Weierstrass (extreme value) theorem on the existence of a minimum (maximum).\nExplain the concepts of big O() and little o() within the framework of truncated Taylor series.\nGive first-order necessary conditions of optimality for a scalar function of a scalar argument, and define critical (or stationary) point. Extend these to the vector argument case. Formulate them both using a Fréchet and Gateaux derivatives. Specialize the result to quadratic functions.\nGive second-order sufficient conditions of optimality for a scalar function of a scalar argument. How can we distinguish between a minimum, maximum, and an inflection point? Extend these to the vector case. Define Hessian and show how it can be used to classify the critical points into minimum, maximum, saddle point, and singularity point. Specialize the results to quadratic functions.\nGive first-order necessary condition of optimality for an equality-constrained optimization problem using Lagrange multipliers. Specialize the results to quadratic cost functions and linear constraints.\nCharacterize the regular point (for a given set of equality constraints). Give an example of equality constraints lacking regularity.\nGive second-order sufficient conditions of optimality for an equality-constrained optimization problem using the concept of a projected Hessian.\nState and explain the Karush-Kuhn-Tucker (KKT) conditions for inequality-constrained optimization problems.",
    "crumbs": [
      "1. Optimization – theory",
      "Learning goals"
    ]
  },
  {
    "objectID": "opt_theory_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "href": "opt_theory_goals.html#skills-use-the-knowledge-to-solve-a-problem",
    "title": "Learning goals",
    "section": "Skills (use the knowledge to solve a problem)",
    "text": "Skills (use the knowledge to solve a problem)\n\nFormulate a provided problem as an instance of mathematical optimization: identify the cost function, the constraints, decide if the problems fits into one of the (numerous) families of optimization problems such as linear program, quadratic program (with linear constraints, with quadratic constraints), (general) nonlinear program, …\nSolve a provided linear and/or quadratic programming problem using a solver of your choice.",
    "crumbs": [
      "1. Optimization – theory",
      "Learning goals"
    ]
  },
  {
    "objectID": "cont_indir_hw_2.html",
    "href": "cont_indir_hw_2.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Homework"
    ]
  },
  {
    "objectID": "dynamic_programming.html",
    "href": "dynamic_programming.html",
    "title": "Dynamic programming and discrete-time optimal control",
    "section": "",
    "text": "In the previous two chapters we explained direct and indirect approaches to discrete-time optimal control. While the direct approach conveniently allows for almost arbitrary constraints, it only provides a control trajectory (a finite sequence of values of the control variable). If feedback is needed, the optimization must be performed in every sampling period, thus implementing the concept of receding horizon control, RHC, aka model predictive control, MPC. The indirect approach, in contrast, can lead to a (state) feedback control law, but this only happens in special cases such as a control of a linear system minimizing a quadratic cost (LQR) while assuming no bound constraints on the control or state variables. In the general case it leads to a two-point boundary value problem (TP-BVP), which can only be solved numerically, and trajectories are produced as outcomes.\nIn this chapter we present yet another approach — dynamic programming (DP). It also allows imposing constraints (in fact, even constraints such as integrality of variables, which are not compatible with our derivative-based optimization toolset exploited so far), and yet it directly leads to feedback controllers.\nWhile in the case of a linear system with a quadratic cost function, dynamic programming provides another route to the theoretical results that we already know — Riccati equation based solution to the LQR problem —, in the the case of a general nonlinear dynamical system with a general cost functions, the feedback controller comes in the form of a look-up table. This format of a feedback controller gives some hint about disadvantages of DP, namely, both computation and then the use of these look-up tables do not scale well with the dimension of the state space (aka curse of dimensionality). Various approximation schemes exist — one such branch is known as reinforcement learning.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "DP and discrete-time optimal control"
    ]
  },
  {
    "objectID": "dynamic_programming.html#bellmans-principle-of-optimality-and-dynamic-programming",
    "href": "dynamic_programming.html#bellmans-principle-of-optimality-and-dynamic-programming",
    "title": "Dynamic programming and discrete-time optimal control",
    "section": "Bellman’s principle of optimality and dynamic programming",
    "text": "Bellman’s principle of optimality and dynamic programming\nWe start by considering the following example.\n\nExample 1 (Reusing the plan for a trip from Prague to Ostrava) We are planning a car trip from Prague to Ostrava and you are searching for a route that minimizes the total time. Using the online planner we learn that the fastest route from Prague to Ostrava is — as bizarre as it sounds — via (actually around) Brno.\n\n\nNow, is it possible to reuse this plan for our friends from Brno who are also heading for Ostrava?\n\n\nThe answer is yes, as the planner confirms. Surely did not even need the planner to answer such trivial question. And yet it demonstrates the key wisdom of the whole chapter — the Bellman’s principle of optimality —, which we now state formally.\n\n\nTheorem 1 (Bellman’s principle of optimality) An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\n\nWe now investigate this idea a bit more quantitatively using a simple computational example of finding a shortest path in a graph.\n\nExample 2 (Shortest path in a graph) We consider a directional graph with nodes A, B, C, D, and E and edges with the prescribed lengths as in the figure below.\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA-&gt;B\n\n\n3\n\n\n\nD\n\nD\n\n\n\nA-&gt;D\n\n\n1\n\n\n\nC\n\nC\n\n\n\nB-&gt;C\n\n\n2\n\n\n\nE\n\nE\n\n\n\nB-&gt;E\n\n\n1\n\n\n\nD-&gt;E\n\n\n3\n\n\n\nG\n\nG\n\n\n\nD-&gt;G\n\n\n2\n\n\n\nF\n\nF\n\n\n\nC-&gt;F\n\n\n3\n\n\n\nE-&gt;F\n\n\n3\n\n\n\nH\n\nH\n\n\n\nE-&gt;H\n\n\n2\n\n\n\nG-&gt;H\n\n\n4\n\n\n\nI\n\nI\n\n\n\nF-&gt;I\n\n\n4\n\n\n\nH-&gt;I\n\n\n2\n\n\n\n\n\n\n\n\nThe task is now to find the shortest path from A to I. What are possible solution strategies? We can start enumerating all the possible paths and calculate their costs (by summing the costs of the participating edges). Needless to say, this strategy based on enumeration scales very badly with the growing number of nodes.\nAlternatively, we solve the problem using dynamic programming and relying on Bellman’s principle of optimality. Before we proceed, we need to define the concept of a stage. It is perhaps less common and natural when it comes to solving graph problems, but we introduce it with anticipation of discrete-time optimal control problems. By the kth stage we understand the node at which the kth decision needs to be made. In our case, starting at A, 4 decisions need to be made to reach the final node. But let’s agree that we also denote the final node as the stage, the 5th one, even if no decision is to be made here. The total number of stages is then N=5.\nThe crucial attribute of the strategy based on dynamic programming is that we proceed backwards. We start at the very final stage. At this stage, there is just one node and there is nothing we can do, but note that it also makes sense to formulate problems with several possible nodes at the final stage, each with a different (terminal) costs — we will actually use once we switch to the optimal control setting. Now we proceed backwards to the last but one, that is, the (N-1)th stage.\nThese are F and H nodes at this 4the stage. In these two nodes there is again no freedom as for the actions, but for each of them we can record their respective cost to go: 4 for the F node and 2 for the H node. These costs reflect how costly it is to reach the terminal node from them.\nThings are only getting interesting if we now proceed to the 3rd stage. We now have to consider three possible nodes: C, E and G. For the C and G nodes there is still just one action and we can only record their costs to go. The cost for the C node can be computed as the cost for the immediate transition from C to F plus the cost for the F node, which we recorded previously, that is, 3+4=7. We record the value of 7 with the C node. Similarly for the G node. For the E node there are two possible actions — two possible decisions to be made, two possible paths to choose from. Either to the left (or, actually, up in our orientation of the graph), which would bring us to the node F, or to the right (or down), which would bring us to the node H. We compute the costs to go for both decisions and choose the decision with a smaller cost. Here the cost of the decision to go to the left is composed of the cost of the transition to F plus the cost to go from F, that is, 3+4=7. The cost to go for the decision to go right is composed of the transition cost from E to H plus the cost to go from H, that is, 2+2=4. Obviously, the optimal decision is to go right, that is, to the node H. Here, on top of the value of the optimal (smallest) cost to go from the node we also record the optimal decision (go to the right/down). We do it by coloring the edge in blue.\nNote that in principle we should have highlighted the edges from F to I, from C to F, and from G to H. It was unnecessary here since there were the only possible edges emanating from these nodes.\nWe proceed backwards to the 2nd stage, and we compute the costs to go for the nodes B and D. Again we record their optimal values and the actual optimal decisions.\nOne last shift backwards and we are at the initial node A, for which we can do the same computation of the costs to go. Note that here coincidently both decisions have the same cost to go, hence both possible decisions/actions are optimal and we can just toss a coin.\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n8\n\n\n\nB\n\nB\n5\n\n\n\nA-&gt;B\n\n\n3\n\n\n\nD\n\nD\n7\n\n\n\nA-&gt;D\n\n\n1\n\n\n\nC\n\nC\n7\n\n\n\nB-&gt;C\n\n\n2\n\n\n\nE\n\nE\n4\n\n\n\nB-&gt;E\n\n\n1\n\n\n\nD-&gt;E\n\n\n3\n\n\n\nG\n\nG\n6\n\n\n\nD-&gt;G\n\n\n2\n\n\n\nF\n\nF\n4\n\n\n\nC-&gt;F\n\n\n3\n\n\n\nE-&gt;F\n\n\n3\n\n\n\nH\n\nH\n2\n\n\n\nE-&gt;H\n\n\n2\n\n\n\nG-&gt;H\n\n\n4\n\n\n\nI\n\nI\n0\n\n\n\nF-&gt;I\n\n\n4\n\n\n\nH-&gt;I\n\n\n2\n\n\n\n\n\n\n\n\nMaybe it is not immediately clear from the graph, but when viewed as an itinerary for a trip, it provides a feedback controller. Even if for whichever reason we find ourselves out of the optimal path, we can always have a look at the graph — it will guide us along the path that is optimal from that given node. For example, if we happen to be in node C, we do have a plan. Well, here is misleadingly simple as there is no decision to be made, but you get the point.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "DP and discrete-time optimal control"
    ]
  },
  {
    "objectID": "dynamic_programming.html#bellmans-principle-of-optimality-applied-to-the-discrete-time-optimal-control-problem",
    "href": "dynamic_programming.html#bellmans-principle-of-optimality-applied-to-the-discrete-time-optimal-control-problem",
    "title": "Dynamic programming and discrete-time optimal control",
    "section": "Bellman’s principle of optimality applied to the discrete-time optimal control problem",
    "text": "Bellman’s principle of optimality applied to the discrete-time optimal control problem\nLet’s recapitulate here the problem of optimal control for a discrete-time system. In particular, we consider the system modelled by \n\\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\n defined on the discrete time interval [i,N], with the initial state \\bm x_i fixed (\\bm x_i = \\mathbf x_i) We aim at minimizing the cost function \nJ_i^N\\left(\\bm x_i, \\bm u_i, \\bm u_{i+1}, \\ldots, \\bm u_{N-1}\\right) = \\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1}L_k(\\bm x_k,\\bm u_k).\n\nBefore we proceed, some comments on the notation are in order. Indeed, a well tuned and systematically used notation is instrumental in dynamic programming.\n\n\n\n\n\n\nWe omit the final time from the notation for the cost function\n\n\n\nWhile the cost function does depend on the final time too, in most if not all our analyses we assume that it is fixed and understood from the context. Hence we will not explicitly indicate the dependence on the final time. We will write just J_i(\\ldots) instead of J_i^N(\\ldots). This may help reduce the notational clutter as we are going to need the upper index for something else soon.\n\n\n\n\n\n\n\n\nWe omit the state trajectory from the notation for the cost function and leave just the initial state\n\n\n\nThe cost function is clearly a function of the full sequence \\bm x_i, \\bm x_{i+1},\\ldots, \\bm x_N of the state vectors too. In the previous chapters we handled it systematically (either by considering them as optimization variables in the simultaneous direct approach or by introducing Lagrange multipliers in the indirect approach). But here we want to emphasize the fact that starting with \\bm x_{i+1}, the whole state trajectory is uniquelly determined by the initial state \\bm x_i and the corresponding control trajectory \\bm u_i, \\bm u_{i+1},\\ldots, \\bm u_{N-1}. Therefore, we write the cost function as a function of the initial state, the initial time (we already agreed above not to emphasize the final time), and the sequence of controls.\n\n\n\n\n\n\n\n\nWe use the lower index to display dependence on time\n\n\n\nThe dependence on the discrete time is reflected by the lower indices: not only in \\bm x_k and \\bm u_k but also in \\mathbf f_k(), L_k() and J_k(). We could perhaps write these as \\mathbf f(\\cdot,\\cdot,k), L(\\cdot,\\cdot,k) and J(\\cdot,\\cdot,k) to better indicate that k is really an argument for these functions, but we prefer making it compatible with the way we indicate the time dependence of \\bm x_k and \\bm u_k.\n\n\nHaving introduced the cost function parameterized by the initial state, initial time and the full sequence of controls, we now introduce the optimal cost function\n\n\\boxed{\n    J^\\star_i(\\bm x_i) = \\min_{\\bm u_i,\\ldots, \\bm u_{N-1}} J_i\\left(\\bm x_i, \\bm u_i, \\bm u_{i+1}, \\ldots, \\bm u_{N-1}\\right).}\n\\tag{1}\nThe sequence of controls in the above minimization may be subject to some constraints, but we do not indicate them here for the sake of notational simplicity.\n\n\n\n\n\n\nDifference between the J_i and J^\\star_i functions\n\n\n\nUnderstanding the difference is crucial. While the cost function J_i depends on the (initial) state, the (initial) time and the sequence of controls applied over the whole interval, the optimal cost function J^\\star_i only depends on the (initial) state and the (initial) time.\n\n\nAssume now that we can find an optimal control sequence from any given state \\bm x_{k+1} at time k+1 on, i.e., we can find \\bm u_{k+1}^\\star,\\bm u_{k+2}^\\star,\\ldots, \\bm u_{N-1}^\\star yielding the optimal cost J_{k+1}^\\star(\\bm x_{k+1}). We will soon show how to actually find it, but for the time being we just assume we can have it. We now show how it can be used to find the optimal cost J_k^\\star(\\bm x_k) at state \\bm x_k and time k.\nLet’s now consider the following strategy: with the system at state \\bm x_k and time k we apply some control \\bm u_k, not necessarily an optimal one, which brings the system to the state \\bm x_{k+1} in the next time k+1. But from then on we use the control sequence \\bm u_{k+1}^\\star,\\bm u_{k+2}^\\star,\\ldots, \\bm u_{N-1}^\\star that is optimal from \\bm x_{k+1}. The corresponding cost is \nL_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1}).\n\\tag{2}\nBellman’s principle of optimality states that if we optimize the above expression over \\bm u_k, we get the optimal cost J_k^\\star(\\bm x_k) at time k \n\\boxed{J_k^\\star(\\bm x_k) = \\min_{\\bm u_k}\\left(L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1})\\right).}\n\\tag{3}\nHence, at a given state \\bm x_{k} and time k, the optimization is performed over only one (possibly vector) control \\bm u_k and not the whole trajectory as the definition of the optimal cost in Equation 1 suggests! What a simplification!\n\n\n\n\n\n\nImportant\n\n\n\nThe minimization needs to be performed over the whole sum L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1}), because \\bm x_{k+1} is a function of \\bm u_k (recall that \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k)). We can also write Equation 3 as \nJ_k^\\star(\\bm x_k) = \\min_{\\bm u_k}\\left(L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\mathbf f_k(\\bm x_k,\\bm u_k))\\right),\n which makes it more apparent.\n\n\nOnce we have the optimal cost function J^\\star_{k}, the optimal control \\bm u_k^\\star(x_k) at a given time k and state \\bm x_k is obtained by \n\\boxed{\n    \\bm u_k^\\star(\\bm x_k) = \\arg \\min_{\\bm u_k}\\left(L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\mathbf f_k(\\bm x_k,\\bm u_k))\\right).}\n\n\nAlternative formulation of dynamic programming using Q-factors\nThe cost function in Equation 2 is sometimes called Q-factor and we denote it Q_k^\\star(\\bm x_k,\\bm u_k). We write its definition here for convenience \nQ^\\star_k(\\bm x_k,\\bm u_k) = L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1}).\n\nIt is a cost of choosing the control \\bm u_k at state \\bm x_k at time k and then following the optimal control from the next time on.\nThe optimal cost function J_k^\\star(\\bm x_k) can be recovered from the optimal Q-factor Q_k^\\star(\\bm x_k,\\bm u_k) by taking the minimum over \\bm u_k \nJ_k^\\star(\\bm x_k) = \\min_{\\bm u_k} Q_k^\\star(\\bm x_k,\\bm u_k).\n\nBellman’s principle of optimality can be then expressed using the optimal Q-factor as \n\\boxed{Q_k^\\star(\\bm x_k,\\bm u_k) = L_k(\\bm x_k,\\bm u_k) + \\min_{\\bm u_{k+1}} Q_{k+1}^\\star(\\bm x_{k+1},\\bm u_{k+1})}.\n\nOptimal control is then obtained from the optimal Q-factor as the minimizing control \n\\boxed{\\bm u_k^\\star(\\bm x_k) = \\arg \\min_{\\bm u_k} Q_k^\\star(\\bm x_k,\\bm u_k).}",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming (DP)",
      "DP and discrete-time optimal control"
    ]
  },
  {
    "objectID": "cont_dp_HJB.html",
    "href": "cont_dp_HJB.html",
    "title": "Dynamic programming for continuous-time optimal control",
    "section": "",
    "text": "In the previous sections we investigated both direct and indirect approaches to the optimal control problem. Similarly as in the discrete-time case, complementing the two approaches is the dynamic programming. Indeed, the key Bellmans’s idea, which we previously formulated in discrete time, can be extended to continuous time as well.\nWe consider the continuous-time system \n\\dot{\\bm{x}} = \\mathbf f(\\bm{x},\\bm{u},t)\n with the cost function \nJ(\\bm x(t_\\mathrm{i}), \\bm u(\\cdot), t_\\mathrm{i}) = \\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}}L(\\bm x(t),\\bm u(t),t)\\, \\mathrm d t.\nThe final time can be fixed to a particular value t_\\mathrm{f}, in which case the state at the final time \\bm x(t_\\mathrm{f}) is either free (unspecified but penalized through \\phi(\\bm x(t_\\mathrm{f}))), or it is fixed (specified and not penalized, that is, \\bm x(t_\\mathrm{f}) = \\mathbf x^\\mathrm{ref}).\nThe final time can also be free (regarded as an optimization variable itself), in which case general constraints on the state at the final time can be expressed as \n\\psi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f})=0\n or possibly even using an inequality, which we will not consider here.\nThe final time can also be considered infinity, that is, t_\\mathrm{f}=\\infty, but we will handle this situation later separately.",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Dynamic programming for continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_dp_HJB.html#hamilton-jacobi-bellman-hjb-equation",
    "href": "cont_dp_HJB.html#hamilton-jacobi-bellman-hjb-equation",
    "title": "Dynamic programming for continuous-time optimal control",
    "section": "Hamilton-Jacobi-Bellman (HJB) equation",
    "text": "Hamilton-Jacobi-Bellman (HJB) equation\nWe now consider an arbitrary time t and split the (remaining) time interval [t,t_\\mathrm{f}] into two parts [t,t+\\Delta t] and [t+\\Delta t,t_\\mathrm{f}] , and structure the cost function accordingly \nJ(\\bm x(t),\\bm u(\\cdot),t) = \\int_{t}^{t+\\Delta t} L(\\bm x,\\bm u,\\tau)\\,\\mathrm{d}\\tau + \\underbrace{\\int_{t+\\Delta t}^{t_\\mathrm{f}} L(\\bm x,\\bm u,\\tau)\\,\\mathrm{d}\\tau + \\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f})}_{J(\\bm x(t+\\Delta t), \\bm u(t+\\Delta t), t+\\Delta t)}.\n\nBellman’s principle of optimality gives \nJ^\\star(\\bm x(t),t) = \\min_{\\bm u(\\tau),\\;t\\leq\\tau\\leq t+\\Delta t} \\left[\\int_{t}^{t+\\Delta t} L(\\bm x,\\bm u,\\tau)\\,\\mathrm{d}\\tau + J^\\star(\\bm x+\\Delta \\bm x, t+\\Delta t)\\right].\n\nWe now perform Taylor series expansion of J^\\star(\\bm x+\\Delta \\bm x, t+\\Delta t) about (\\bm x,t) \nJ^\\star(\\bm x,t) = \\min_{\\bm u(\\tau),\\;t\\leq\\tau\\leq t+\\Delta t} \\left[L\\Delta t + J^\\star(\\bm x,t) + (\\nabla_{\\bm x} J^\\star)^\\top \\Delta \\bm x + \\frac{\\partial J^\\star}{\\partial t}\\Delta t + \\mathcal{O}((\\Delta t)^2)\\right].\n\nUsing \n\\Delta \\bm x = \\bm f(\\bm x,\\bm u,t)\\Delta t\n and noting that J^\\star and J_t^\\star are independent of \\bm u(\\tau),\\;t\\leq\\tau\\leq t+\\Delta t, we get \n\\cancel{J^\\star (\\bm x,t)} = \\cancel{J^\\star (\\bm x,t)} + \\frac{\\partial J^\\star }{\\partial t}\\Delta t + \\min_{\\bm u(\\tau),\\;t\\leq\\tau\\leq t+\\Delta t}\\left[L\\Delta t + (\\nabla_{\\bm x} J^\\star )^\\top f\\Delta t\\right].\n\nAssuming \\Delta t\\rightarrow 0 leads to the celebrated Hamilton-Jacobi-Bellman (HJB) equation \\boxed{\n-\\frac{\\partial {\\color{blue}J^\\star (\\bm x(t),t)}}{\\partial t} = \\min_{\\bm u(t)}\\left[L(\\bm x(t),\\bm u(t),t)+(\\nabla_{\\bm x} {\\color{blue} J^\\star (\\bm x(t),t)})^\\top \\bm f(\\bm x(t),\\bm u(t),t)\\right].}\n\nThis is obviously a partial differential equation (PDE) for the optimal cost function J^\\star(\\bm x,t).\n\nBoundary conditions for the HJB equation\nSince the HJB equation is a differential equation, initial/boundary value(s) must be specified to determine a unique solution. In particular, since the equation is first-order with respect to both time and state, specifying the value of the optimal cost function at the final state and the final time is enough.\nFor a fixed-final-time, free-final-state, the optimal cost at the final time is \nJ^\\star (\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) = \\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}).\n\nFor a fixed-final-time, fixed-final-state, since the component of the cost function corresponding to the terminal state is zero, the optimal cost at the final time is zero as well \nJ^\\star (\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) = 0.\n\nWith the general final-state constraints introduced above, the boundary value condition reads \nJ^\\star (\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) = \\phi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}),\\qquad \\text{on the hypersurface } \\psi(\\bm x(t_\\mathrm{f}),t_\\mathrm{f}) = 0.\n\n\n\nOptimal control using the optimal cost (-to-go) function\nAssume now that the solution J^\\star (\\bm x(t),t) to the HJB equation is available. We can then find the optimal control by the minimization \\boxed\n{\\bm u^\\star(t) = \\arg\\min_{\\bm u(t)}\\left[L(\\bm x(t),\\bm u(t),t)+(\\nabla_{\\bm x} J^\\star (\\bm x(t),t))^\\top \\bm f(\\bm x(t),\\bm u(t),t)\\right].}\n\nFor convenience, the minimized function is often labelled as \nQ(\\bm x(t),\\bm u(t),t) = L(\\bm x(t),\\bm u(t),t)+(\\nabla_{\\bm x} J^\\star (\\bm x(t),t))^\\top \\bm f(\\bm x(t),\\bm u(t),t)\n and called just Q-function. The optimal control is then \n\\bm u^\\star(t) = \\arg\\min_{\\bm u(t)} Q(\\bm x(t),\\bm u(t),t).",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Dynamic programming for continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_dp_HJB.html#hjb-equation-formulated-using-a-hamiltonian",
    "href": "cont_dp_HJB.html#hjb-equation-formulated-using-a-hamiltonian",
    "title": "Dynamic programming for continuous-time optimal control",
    "section": "HJB equation formulated using a Hamiltonian",
    "text": "HJB equation formulated using a Hamiltonian\nRecall the definition of Hamiltonian H(\\bm x,\\bm u,\\bm \\lambda,t) = L(\\bm x,\\bm u,t) + \\boldsymbol{\\lambda}^\\top \\mathbf f(\\bm x,\\bm u,t). The HJB equation can also be written as \\boxed\n{-\\frac{\\partial J^\\star (\\bm x(t),t)}{\\partial t} = \\min_{\\bm u(t)}H(\\bm x(t),\\bm u(t),\\nabla_{\\bm x} J^\\star (\\bm x(t),t),t).}\n\nWhat we have just derived is one of the most profound results in optimal control – Hamiltonian must be minimized by the optimal control. We will exploit it next as a tool for deriving some theoretical results.",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Dynamic programming for continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_dp_HJB.html#hjb-equation-vs-pontryagins-principle-of-maximum-minimum",
    "href": "cont_dp_HJB.html#hjb-equation-vs-pontryagins-principle-of-maximum-minimum",
    "title": "Dynamic programming for continuous-time optimal control",
    "section": "HJB equation vs Pontryagin’s principle of maximum (minimum)",
    "text": "HJB equation vs Pontryagin’s principle of maximum (minimum)\nRecall also that we have already encountered a similar results that made statements about the necessary maximization (or minimization) of the Hamiltonian with respect to the control – the celebrated Pontryagin’s principle of maximum (or minimum). Are these two related? Equivalent?",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Dynamic programming for continuous-time optimal control"
    ]
  },
  {
    "objectID": "cont_dp_HJB.html#hjb-equation-for-an-infinite-time-horizon",
    "href": "cont_dp_HJB.html#hjb-equation-for-an-infinite-time-horizon",
    "title": "Dynamic programming for continuous-time optimal control",
    "section": "HJB equation for an infinite time horizon",
    "text": "HJB equation for an infinite time horizon\nWhen both the system and the cost function are time-invariant, and the final time is infinite, that is, t_\\mathrm{f}=\\infty, the optimal cost function J^\\star() must necessarily be independent of time, that is, it’s partial derivative with respect to time is zero, that is, \\frac{\\partial J^\\star (\\bm x(t),t)}{\\partial t} = 0. The HJB equation then simplifies to\n\\boxed{\n0 = \\min_{\\bm u(t)}\\left[L(\\bm x(t),\\bm u(t))+(\\nabla_{\\bm x} {J^\\star (\\bm x(t),t)})^\\top \\bm f(\\bm x(t),\\bm u(t))\\right],}\n or, using a Hamiltonian \\boxed\n{0 = \\min_{\\bm u(t)}H(\\bm x(t),\\bm u(t),\\nabla_{\\bm x} J^\\star (\\bm x(t))).}",
    "crumbs": [
      "9.X. Continuous-time optimal control – dynamic programming",
      "Dynamic programming for continuous-time optimal control"
    ]
  },
  {
    "objectID": "discr_indir_software.html",
    "href": "discr_indir_software.html",
    "title": "Software for LQR and DARE",
    "section": "",
    "text": "ControlSystems.jl\n\nlqr\nare – actually uses MatrixEquations.jl.\n\nMatrixEquations.jl\n\nared",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Software for LQR and DARE"
    ]
  },
  {
    "objectID": "discr_indir_software.html#julia",
    "href": "discr_indir_software.html#julia",
    "title": "Software for LQR and DARE",
    "section": "",
    "text": "ControlSystems.jl\n\nlqr\nare – actually uses MatrixEquations.jl.\n\nMatrixEquations.jl\n\nared",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Software for LQR and DARE"
    ]
  },
  {
    "objectID": "discr_indir_software.html#matlab",
    "href": "discr_indir_software.html#matlab",
    "title": "Software for LQR and DARE",
    "section": "MATLAB",
    "text": "MATLAB\n\nControl System Toolbox\n\nidare (dare deprecated)\ndlqr",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Software for LQR and DARE"
    ]
  },
  {
    "objectID": "discr_indir_software.html#python",
    "href": "discr_indir_software.html#python",
    "title": "Software for LQR and DARE",
    "section": "Python",
    "text": "Python\n\nPython Control\n\ndare\ndlqr",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Software for LQR and DARE"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html",
    "href": "discr_dir_mpc.html",
    "title": "Model predictive control (MPC)",
    "section": "",
    "text": "In the previous section we learnt how to compute an optimal control sequence on a finite time horizon using numerical methods for solving nonlinear programs (NLP), and quadratic programs (QP) in particular. There are two major deficiencies of such approach:\n\nThe control sequence was computed under the assumption that the mathematical model is perfectly accurate. As soon as the reality deviates from the model, either because of some unmodelled dynamics or because of the presence of (external) disturbances, the performance of the system will deteriorate. We need a way to turn the presented open-loop (also feedforward) control scheme into a feedback one.\nThe control sequence was computed for a finite time horizon. It is commonly required to consider an infinite time horizon, which is not possible with the presented approach based on solving finite-dimensional mathematical programs.\n\nThere are several ways to address these issues. Here we introduce one of them. It is known are Model Predictive Control (MPC), also Receding Horizon Control (RHC). Some more are presented in the next two sections (one based on indirect approach, another one based on dynamic programming).",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#deficiencies-of-precomputed-open-loop-optimal-control",
    "href": "discr_dir_mpc.html#deficiencies-of-precomputed-open-loop-optimal-control",
    "title": "Model predictive control (MPC)",
    "section": "",
    "text": "In the previous section we learnt how to compute an optimal control sequence on a finite time horizon using numerical methods for solving nonlinear programs (NLP), and quadratic programs (QP) in particular. There are two major deficiencies of such approach:\n\nThe control sequence was computed under the assumption that the mathematical model is perfectly accurate. As soon as the reality deviates from the model, either because of some unmodelled dynamics or because of the presence of (external) disturbances, the performance of the system will deteriorate. We need a way to turn the presented open-loop (also feedforward) control scheme into a feedback one.\nThe control sequence was computed for a finite time horizon. It is commonly required to consider an infinite time horizon, which is not possible with the presented approach based on solving finite-dimensional mathematical programs.\n\nThere are several ways to address these issues. Here we introduce one of them. It is known are Model Predictive Control (MPC), also Receding Horizon Control (RHC). Some more are presented in the next two sections (one based on indirect approach, another one based on dynamic programming).",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#model-predictive-control-mpc-as-a-way-to-turn-open-loop-control-into-feedback-control",
    "href": "discr_dir_mpc.html#model-predictive-control-mpc-as-a-way-to-turn-open-loop-control-into-feedback-control",
    "title": "Model predictive control (MPC)",
    "section": "Model predictive control (MPC) as a way to turn open-loop control into feedback control",
    "text": "Model predictive control (MPC) as a way to turn open-loop control into feedback control\nThe idea is to compute an optimal control sequence on a finite time horizon using the optimization framework presented in the previous section, but then apply only the first element of the computed control trajectory to the system, and proceed to repeating the whole procedure after shifting the time horizon forward by one time step. The name “model predictive control” expresses the fact that a model-based prediction is the key component of the controller. This is expressed in Fig. 1.\n\n\n\n\n\n\nFigure 1: Diagram describing a single MPC step. Recall that the information carried by the past input trajectories can compressed into the state of the system. A state observer providing an estimate of the state must then be a component of the whole MPC, but then needs not only the input but also the output trajectories.\n\n\n\nThe other name “receding horizon control” is equally descriptive, it emphasizes the phenomenon of the finite time horizon (interval, window) receding (shifting, moving, rolling) as time goes by.\n\n\n\n\n\n\nWe all do MPC in our everyday lives\n\n\n\nIt may take a few moments to comprehend the idea, but then it turns out perfectly natural. As a matter of fact, this is the way most of us control our lifes every day. We plan our actions on a finite time horizon, and to build this plan we use both our understanding (model) of the world and our knowledge of our current situation (state). We then execute the first action from our plan, observe the consequences of our action the and recent changes in the environment, and update our plan accordingly on a new (shifted) time horizon. We repeat this procedure over and over again. It is crucial that the prediction horizon must be long enough so that the full impact of our actions can be observed, but it must not be too long because the planning then becomes too complex and predictions unreliable.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#mpc-regulation",
    "href": "discr_dir_mpc.html#mpc-regulation",
    "title": "Model predictive control (MPC)",
    "section": "MPC regulation",
    "text": "MPC regulation\nWe first investigate the situation when the reference (the set-point, the required final state) is zero.\n\nWe restrict ourselves to a linear system and a quadratic cost function. We impose lower and upper bounds on the control, and optionally on the states as well. We consider a (long) time horizon N. \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_0,\\ldots, \\bm u_{N-1}, \\bm x_{0},\\ldots, \\bm x_N} &\\quad  \\frac{1}{2} \\bm x_N^\\top \\mathbf S \\bm x_N + \\frac{1}{2} \\sum_{k=0}^{N-1} \\left(\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k \\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{k+1} = \\mathbf A\\bm x_k + \\mathbf B\\bm u_k,\\quad k = 0, \\ldots, N-1, \\\\\n                    &\\quad \\bm x_0 = \\mathbf x_0,\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_k \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad (\\mathbf x^{\\min} \\leq \\bm x_k \\leq \\mathbf x^{\\max}).\n\\end{aligned}\n\nIf the time horizon is really long, we can approximate it by \\infty, in which case we omit the terminal state penalty from the overall cost function \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_0,\\bm u_1,\\ldots, \\bm x_{0}, \\bm x_1,\\ldots} &\\quad  \\frac{1}{2} \\sum_{k=0}^{\\infty} \\left(\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k \\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{k+1} = \\mathbf A\\bm x_k + \\mathbf B\\bm u_k,\\quad k = 0, 1, 2, \\ldots, \\\\\n                    &\\quad \\bm x_0 = \\mathbf x_0,\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_k \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad (\\mathbf x^{\\min} \\leq \\bm x_k \\leq \\mathbf x^{\\max}).\n\\end{aligned}\n\nNow, in a model predictive control (MPC) scheme, at the discrete time t, an LQR problem on a finite (and typically only modestly long time prediction horizon N_\\mathrm{p} (typically N_\\mathrm{p}\\approx 20 or 30 or so) is formulated and solved. This time window then moves (recedes, rolls) either forever or untill the the final time N is reached.\nSince the problem is parameterized by the “initial” time t and the state at this time, we need to reflect this in the notation. Namely, \\bm x_{t+k|t} is the predicted state at time t+k as predicted at time t using the information available at that time, that is, \\mathbf x_t (= \\bm x_{t|t}) and the computed control trajectory up to the time just one step before t+k. We can emphasize this dependence by writing it explicitly as \\bm x_{t+k|t}(\\bm x_t, \\bm u_{t}, \\bm u_{t+1}, \\ldots, \\bm u_{t+k-1}), but then the notation becomes rather convoluted and so we stick to the shorter one. We emphasize that it is really just a prediction. The true state at time t+k is denoted \\bm x_{t+k}. Similarly, \\bm u_{t+k|t} is the future control at t+k as computed at time t using the information available at that time, that is, \\bm x_t.\n\n\n\n\n\n\nAlternative notation for variables in MPC optimization problems\n\n\n\nIf you find the notation \\bm x_{t+k|t} clumsy, feel free to replace it with something simpler that does not explicitly mention the time at which the prediction/optimization is made but does not clash with the true state at the given time. Perhaps just using a different letter with a simple lower index such as \\bm z_k for the state and \\bm v_k for the control, while understanding k relative with respect to the current discrete time.\n\n\nThe optimization problem to be solved at every discrete- time t is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_{t|t},u_{t+1}\\ldots, \\bm u_{t+N_\\mathrm{p}-1}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}} &\\quad  \\frac{1}{2} \\bm x_{t+N_\\mathrm{p}|t}^\\top \\mathbf S \\bm x_{t+N_\\mathrm{p}|t} + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left(\\bm x_{t+k|t}^\\top \\mathbf Q \\bm x_{t+k|t} + \\bm u_{t+k|t}^\\top \\mathbf R \\bm u_{t+k|t} \\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t, \\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_{t+k|t} \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\bm x_{t+k|t} \\leq \\mathbf x^{\\max}.\n\\end{aligned}\n\nNote that by using the upright front in \\mathbf x_t we emphasize that the current state plays (measured or estimated) the role of a parameter and not an optimization variable within this optimization problem.\nPreviously we have learnt how to rewrite this finite-horizon optimal control problem as a QP problem, which can then be solved with a dedicated QP solver. However, it is worth analyzing the case without the inequality constraints. We know that we can formulate a system of linear equations and solvem, which we formally write at \n\\begin{bmatrix} \\bm u_{t|t} \\\\ \\bm u_{t+1|t} \\\\ \\vdots \\\\ \\bm u_{t+N-1|t} \\end{bmatrix}\n=\n\\mathbf{H}^{-1} \\mathbf{F} \\mathbf x_t,\n but since we only indend to apply the first element of the control trajectory, we can write \n\\bm u_{t|t}\n=\n\\underbrace{\\begin{bmatrix} \\mathbf I & \\mathbf 0 & \\mathbf 0 & \\ldots & \\mathbf 0 \\end{bmatrix}\n\\mathbf{H}^{-1} \\mathbf{F}}_{\\mathbf K_t} \\mathbf x_t,\n in which we can recognize the classical state feedback with the (time-varying) gain \\mathbf K_t. This is a very useful observation – the MPC strategy, when not considering inequality constraints (aka bounds) on the control or state variables, is just a time-varying state feedback. This observation will turn crucial in later chapters when we come back to MPC and analyze its stability.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#mpc-tracking",
    "href": "discr_dir_mpc.html#mpc-tracking",
    "title": "Model predictive control (MPC)",
    "section": "MPC tracking",
    "text": "MPC tracking\n\n(Nonzero) state reference tracking\nAn immediate extension of the regulation problem is that we replace the desired target (reference) state \\mathbf x^\\mathrm{ref} = \\mathbf 0 with some nonzero \\mathbf x^\\mathrm{ref} \\neq \\mathbf 0. The cost function then changes to \n\\begin{aligned}\nJ(\\ldots) &= \\frac{1}{2} (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf S \\, (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})\\\\\n& \\qquad\\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}}) + \\bm u_{t+k|t}^\\top \\mathbf R \\bm u_{t+k|t}\\right).\n\\end{aligned}\n\nIt seems that we are done, that the only change that had to be made was replacement of the predicted state by the prediction of tracking error in the cost function. But not so fast! Recall that unless the nonzero reference state qualifies as an equilibrium, the control needed to keep the system at the corresponding state is nonzero. Namely, for a general discrete-time linear system \\bm x_{k+1} = \\mathbf A \\bm x_k + \\mathbf B \\bm u_k, at an equilibrium (a steady state) \\mathbf x^\\mathrm{ss}, the following must be satisfied by definition \n\\bm x^\\mathrm{ss} = \\mathbf A \\bm x^\\mathrm{ss} + \\mathbf B \\bm u^\\mathrm{ss}.\n\nRewriting this, we get \n(\\mathbf A - \\mathbf I) \\bm x^\\mathrm{ss} + \\mathbf B \\bm u^\\mathrm{ss} = \\mathbf 0.\n\nFixing the value of the steady state vector \\bm x^\\mathrm{ss} to \\mathbf x^\\mathrm{ref}, we see that \\bm u^\\mathrm{ss} = \\mathbf 0 if (\\mathbf A - \\mathbf I) \\bm x^\\mathrm{ref}, that is, if the desired steady state \\bm x^\\mathrm{ref} is in the null space of (\\mathbf A - \\mathbf I). If the desired steady state does not satisfy the condition, which will generally be the case, the corresponding control is necessarily nonzero. It then makes no sense to penalize the control itself. Instead, its deviation from the compatible nonzero value should be penalized. As a consequence, the cost function should be modified to\n\n\\begin{aligned}\nJ(\\ldots) &= \\frac{1}{2} (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf S \\, (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})\\\\\n& \\qquad\\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}}) + (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})^\\top \\mathbf R (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})\\right),\n\\end{aligned}\n in which we regard {\\color{blue}\\mathbf x^\\mathrm{ref}} as an input to the optimal control problem, and {\\color{ss}\\bm u^\\mathrm{ss}} is a new unknown.\nThe optimization problem to be solved at every discrete- time t is then \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_{t|t},u_{t+1}\\ldots, \\bm u_{t+N_\\mathrm{p}-1}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}, {\\color{red}\\bm u^\\mathrm{ss}}} &\\quad  \\frac{1}{2} (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf S \\, (\\bm x_{t+N_\\mathrm{p}|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})\\\\\n& \\qquad\\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\bm x_{t+k|t}-{\\color{blue}\\mathbf x^\\mathrm{ref}}) + (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})^\\top \\mathbf R (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})\\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t, \\\\\n                    &\\quad \\mathbf B {\\color{red}\\bm u^\\mathrm{ss}} = (\\mathbf I-\\mathbf A) {\\color{blue}\\mathbf x^\\mathrm{ref}},\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_{t+k|t} \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\bm x_{t+k|t} \\leq \\mathbf x^{\\max}.\n\\end{aligned}\n\nNote that compared to the regulation problem, here the optimization problem contains a new (vector) optimization variable \\bm u^\\mathrm{ss}, and a new system of linear equations.\n\n\nOutput reference tracking\nOftentimes we do not have reference values for all the state variables but only for the output variables. These are given by the output equation \n\\bm y_k = \\mathbf C \\bm x_k + \\mathbf D \\bm u_k.\n\nFor notational convenience we restrict ourselves to the case of no feedthrough, that is, \\mathbf D = \\mathbf 0.\nThe goal for the controller is then to make the difference between \\bm y_k = \\mathbf C \\bm x_k and \\mathbf y^\\mathrm{ref} (often named just \\mathbf r if it is clear from the context the reference value of which variables it represents) go to zero.\nSimilarly as in the case of (nonzero) reference state tracking, nonzero control \\bm u^\\mathrm{ss} must be expected in steady state.\nNote, however, that the steady state \\bm x^\\mathrm{ss} is not provided this time – we must relate it to the provided reference output. But it is straightforward:\n\n\\mathbf y^\\mathrm{ref} = \\mathbf C \\bm x^\\mathrm{ss}\\qquad\\qquad (\\text{or}\\; \\mathbf y^\\mathrm{ref} = \\mathbf C \\bm x^\\mathrm{ss} + \\mathbf D \\bm u^\\mathrm{ss}\\; \\text{in general}).\n\nNow we have all that is needed to formulate the MPC problem for output reference tracking:\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_{t|t},u_{t+1}\\ldots, \\bm u_{t+N_\\mathrm{p}-1}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}, {\\color{red}\\bm u^\\mathrm{ss}}, {\\color{blue}\\bm x^\\mathrm{ss}}} &\\quad  \\frac{1}{2} (\\mathbf C\\bm x_{t+N_\\mathrm{p}|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})^\\top \\mathbf S \\, (\\mathbf C\\bm x_{t+N_\\mathrm{p}|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})\\\\\n& \\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\mathbf C\\bm x_{t+k|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\mathbf C\\bm x_{t+k|t}-{\\color{green}\\mathbf y^\\mathrm{ref}}) + (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})^\\top \\mathbf R (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})\\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t, \\\\\n                    &\\quad (\\mathbf A-\\mathbf I) {\\color{blue}\\bm x^\\mathrm{ss}} + \\mathbf B {\\color{red}\\bm u^\\mathrm{ss}} = 0,\\\\\n                    &\\quad \\mathbf C {\\color{blue}\\bm x^\\mathrm{ss}} = {\\color{green}\\mathbf y^\\mathrm{ref}},\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_{t+k|t} \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\bm x_{t+k|t} \\leq \\mathbf x^{\\max}.\n\\end{aligned}\n\nThe tricky part is the terminal term in the cost function. Note that if we consider a nonzero \\mathbf D matrix, the cost function is \n\\begin{aligned}\nJ(\\ldots) &= \\xcancel{\\frac{1}{2} (\\bm y_{t+N_\\mathrm{p}|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})^\\top \\mathbf S \\, (\\bm y_{t+N_\\mathrm{p}|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})}\\\\\n& \\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\bm y_{t+k|t}-{\\color{green}\\mathbf y^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\bm y_{t+k|t}-{\\color{green}\\mathbf y^\\mathrm{ref}}) + (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})^\\top \\mathbf R (\\bm u_{t+k|t}-{\\color{red}\\bm u^\\mathrm{ss}})\\right),\n\\end{aligned}\n but the trouble is that we only consider \\bm u_{t+k|t} for k = 0, \\ldots, N_\\mathrm{p}-1, and therefore \\bm y_{t+N_\\mathrm{p}-1|t} is the last available predicted output – it does not provide \\bm y_{t+N_\\mathrm{p}|t}. The terminal penalty must then be omitted from the cost function, which we have already indicated in the formula by crossing it out.\n\n\nPenalizing the control increments\nThere is an alternative way how to handle the need to consider a nonzero control at steady state. We can penalize not the control itselft but its change, its increment \n\\Delta \\bm u_{t+k|t} = \\bm u_{t+k|t} - \\bm u_{t+k-1|t}.\n\nThe rationale behind this is that once at steady state, the control does not change any longer.\nBut then if \\Delta \\bm u_{t+k|t} is the new optimization variable that replaces \\bm u_{t+k|t} in the cost function, we must still be able to express the control \\bm u_{t+k|t}\n\n\\bm u_{t+k|t} = \\bm u_{t+k-1|t} + \\Delta \\bm u_{t+k|t},\n for which we need to keep track of the control at the previous time step. This can be done systematically just by augmenting the state model with an auxillary state variable \\bm x_{t+k|t}^\\mathrm{u} \\coloneqq \\bm u_{t+k-1|t} \n\\begin{bmatrix} \\bm x_{t+k+1|t} \\\\ \\bm x_{t+k+1|t}^\\mathrm{u} \\end{bmatrix} = \\underbrace{\\begin{bmatrix} \\mathbf A & \\mathbf B \\\\ \\mathbf 0 & \\mathbf I \\end{bmatrix}}_{\\mathbf{\\widetilde{A}}} \\underbrace{\\begin{bmatrix} \\bm x_{t+k|t} \\\\ \\bm x_{t+k|t}^\\mathrm{u} \\end{bmatrix}}_{\\tilde{\\bm{x}}_{t+k|t}} + \\underbrace{\\begin{bmatrix} \\mathbf B \\\\ \\mathbf I \\end{bmatrix}}_{\\mathbf{\\widetilde{B}}} \\Delta \\bm u_{t+k|t}.\n\nWith this new augmented state vector, the output is \n\\bm y_{t+k|t} = \\underbrace{\\begin{bmatrix} \\mathbf C & \\mathbf 0 \\end{bmatrix}}_{\\mathbf{\\widetilde{C}}} \\begin{bmatrix} \\bm x_{t+k|t} \\\\ \\bm x_{t+k|t}^\\mathrm{u} \\end{bmatrix}.\n\nWith this new system we can now proceed to formulate the MPC tracking problem \n\\begin{aligned}\n\\operatorname*{minimize}_{\\Delta \\bm u_{t|t},\\Delta u_{t+1}\\ldots, \\Delta \\bm u_{t+N_\\mathrm{p}-1}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}} &\\quad  \\frac{1}{2} (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+N_\\mathrm{p}|t} - \\mathbf y^\\mathrm{ref})^\\top \\mathbf S \\, (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+N_\\mathrm{p}|t} - \\mathbf y^\\mathrm{ref}) \\\\\n&\\qquad \\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+k|t} - \\mathbf y^\\mathrm{ref})^\\top \\mathbf Q \\, (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+k|t} - \\mathbf y^\\mathrm{ref}) + \\Delta \\bm u_{t+k|t}^\\top \\mathbf R \\, \\Delta \\bm u_{t+k|t} \\right)\\\\\n\\text{subject to}   &\\quad \\tilde{\\bm{x}}_{t+k+1|t} = \\mathbf{\\widetilde{A}}\\tilde{\\bm{x}}_{t+k|t} + \\mathbf{\\widetilde{B}}\\Delta \\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\tilde{\\bm{x}}_{t|t} = \\begin{bmatrix}\\mathbf x_t\\\\ \\mathbf u_{t-1}\\end{bmatrix}, \\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\begin{bmatrix}\\bm I & \\mathbf 0 \\end{bmatrix} \\tilde{\\bm x}_{t+k|t} \\leq \\mathbf x^{\\max},\\quad k = 0, \\ldots, N_\\mathrm{p},\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\begin{bmatrix}\\bm 0 & \\mathbf I \\end{bmatrix} \\tilde{\\bm x}_{t+k|t} +  \\Delta \\bm u_{t+k|t}\\leq \\mathbf u^{\\max},\\quad k = 0, \\ldots, N_\\mathrm{p}-1.\n\\end{aligned}\n\nA bonus of this formulation is that we can also impose contraints on \\Delta \\bm u_{t+k|t}, which effectively implements rate constraints on the control variables. This is useful because sometimes we want to restrict how fast the control variable (say, valve opening) changes in order to save the actuators.\nNote that similarly as before, we cannot include the terminal cost if the output equation contains a feedthrough term, that is if \\mathbf D\\neq 0, in which case \\bm u_{t+N|t} would be needed, while it is not available in our optimization problem. This does not pose a singificant trouble purely from the viewpoint of expressing the control requirements by the cost function, but we only mention in passing that it does pose a problem when it comes to guaranteening the closed-loop stability. We postpone our discussion of closed-loop stability till later chapters, but here we only mention that the terminal penalty (on the prediction horizon), through which we penalize the deviation of the system from the desired state, is one of the mechanisms for achieving closed-loop stability. Within this \\Delta \\bm u framework we eliminated the need to compute the steady state \\bm x^{ss}, but if stability guarantees are needed, it must be reintroduced to the problem.\n#TODO Show the optimization problem including the computation of the steady state and the terminal penalty on the deviation from the steady state (even though performance-wise we are only interested in the output reference tracking).\n\n\nOutput reference tracking for general references (preview control)\nFinally, we consider not just a single value of the output vector \\mathbf y^\\mathrm{ref} towards which the system should ultimately be steered, but we consider an arbitrary reference trajectory {\\color{orange}\\mathbf y_k^\\mathrm{ref}}. The MPC problem changes (slightly) to \n\\begin{aligned}\n\\operatorname*{minimize}_{\\Delta \\bm u_{t|t},\\Delta u_{t+1}\\ldots, \\Delta \\bm u_{t+N_\\mathrm{p}-1}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}} &\\quad  \\frac{1}{2} (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+N_\\mathrm{p}|t} - {\\color{orange}\\mathbf y_{t+N_\\mathrm{p}}^\\mathrm{ref}})^\\top \\mathbf S \\, (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+N_\\mathrm{p}|t} - {\\color{orange}\\mathbf y_{t+N_\\mathrm{p}}^\\mathrm{ref}}) \\\\\n&\\qquad \\qquad + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\left((\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+k|t} - {\\color{orange}\\mathbf y_{t+k}^\\mathrm{ref}})^\\top \\mathbf Q \\, (\\mathbf{\\widetilde{C}}\\tilde{\\bm{x}}_{t+k|t} - {\\color{orange}\\mathbf y_{t+k}^\\mathrm{ref}}) + \\Delta \\bm u_{t+k|t}^\\top \\mathbf R \\, \\Delta \\bm u_{t+k|t} \\right)\\\\\n\\text{subject to}   &\\quad \\tilde{\\bm{x}}_{t+k+1|t} = \\mathbf{\\widetilde{A}}\\tilde{\\bm{x}}_{t+k|t} + \\mathbf{\\widetilde{B}}\\Delta \\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\tilde{\\bm{x}}_{t|t} = \\begin{bmatrix}\\mathbf x_t\\\\ \\mathbf u_{t-1}\\end{bmatrix}, \\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\begin{bmatrix}\\bm I & \\mathbf 0 \\end{bmatrix} \\tilde{\\bm x}_{t+k|t} \\leq \\mathbf x^{\\max},\\quad k = 0, \\ldots, N_\\mathrm{p},\\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\begin{bmatrix}\\bm 0 & \\mathbf I \\end{bmatrix} \\tilde{\\bm x}_{t+k|t} +  \\Delta \\bm u_{t+k|t}\\leq \\mathbf u^{\\max},\\quad k = 0, \\ldots, N_\\mathrm{p}-1.\n\\end{aligned}\n\nOne last time we repeat that if a general output equation with a feedthrough term is considered, the terminal term in the cost function should be omitted.\n#TODO",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#hard-constraints-vs-soft-constraints-on-state-variables",
    "href": "discr_dir_mpc.html#hard-constraints-vs-soft-constraints-on-state-variables",
    "title": "Model predictive control (MPC)",
    "section": "Hard constraints vs soft constraints on state variables",
    "text": "Hard constraints vs soft constraints on state variables\nWhile it is fairly natural to encode the lower and upper bounds on the state variables as inequality constraints in the optimal control problem, this approach comes with a caveat – the corresponding optimization problem can be infeasible. This is a major trouble if the optimization problem is solved online (in real time), which is the case of an MPC controller. The infeasibility of the optimization problem then amounts to the controller being unable to provide its output.\n\nFor example, we may require that the error of regulating the intervehicular gap by an adaptive cruise control (ACC) system is less then 1 m. At one moment, this requirement may turn out unsatisfiable, while, say, 1.1 m error could be achievable, which would cause no harm. And yet the controller would essentially give up and produce no command to the engine. A major trouble.\nAn alternative is to move the requirement from the constraints to the cost function as an extra term. This way, however, the original hard constraint turns into a soft one, by which we mean that we do not guarantee that the requirement is satisfied, but we discourage the optimization algorithm from breaking it by imposing a penalty proportional to how much the constraint is exceeded.\nWe sketch the scheme here. For the original problem formulation with the hard constraints on the output variables\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_1,\\ldots, \\bm u_{N-1}} &\\quad \\sum_k^N \\left[\\ldots \\right]\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N-1, \\\\\n                    &\\quad \\bm y_{t+k|t} = \\mathbf C\\bm x_{t+k|t} + \\mathbf D\\bm u_{t+k|t},\\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t,\\\\\n                    &\\quad \\ldots \\\\\n                    &\\quad \\mathbf y^{\\min} \\leq \\bm y_{t+k|t} \\leq \\mathbf y^{\\max}.\n\\end{aligned}\n\nwe propose the version with soft constraints \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_1,\\ldots, \\bm u_{N-1}, {\\color{red}\\epsilon}} &\\quad \\sum_k^N  \\left[\\ldots {+ \\color{red}\\gamma \\epsilon} \\right]\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N-1, \\\\\n                    &\\quad \\bm y_{t+k|t} = \\mathbf C\\bm x_{t+k|t} + \\mathbf D\\bm u_{t+k|t},\\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t,\\\\\n                    &\\quad \\ldots \\\\\n                    &\\quad \\mathbf y^{\\min} {\\color{red}- \\epsilon \\mathbf v} \\leq \\bm y_k \\leq \\mathbf y^{\\max} {\\color{red}+ \\epsilon \\mathbf v},\n\\end{aligned}\n where \\gamma &gt; 0 and \\mathbf v\\in\\mathbb R^p are fixed parameters and \\epsilon is the additional optimization variable.\n\n\n\n\n\n\nRequirements expressed through constraints or an extra term in the cost function\n\n\n\nWe have just encountered another instance of the classical dillema in optimization and optimal control that we have had introduced previously. Indeed, it is fairly fundamental and appears both in applications and in development of theory. Keep this degree of freeedom in mind on your optimization and optimal control journey.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#prediction-horizon-vs-control-horizon",
    "href": "discr_dir_mpc.html#prediction-horizon-vs-control-horizon",
    "title": "Model predictive control (MPC)",
    "section": "Prediction horizon vs control horizon",
    "text": "Prediction horizon vs control horizon\nOne of the key parameters of a model predictive control is the prediction horizon N_\\mathrm{p}. It must be long enough so that the key dynamics of the system has enough time to exhibit, and yet it must not be too long, because the computational load will be then too high. A rule of thumb (but certainly not a law) is N_\\mathrm{p}\\approx 20. There is one simple way to reduce the computational load – consider the control trajectory defined on a much shorted time horizon than the predicted state trajectory. Namely, we introduce the control horizon N_\\mathrm{c} &lt; N_\\mathrm{p} (typically N_\\mathrm{c} can be as small as 2 or 3 or so), and we only consider the control as optimizable on this short horizon. Of course, we must provide some values after this horizon as well (untill the end of the prediction horizon). The simplest strategy is to set it to the last value on the control horizon. The MPC problem then changes to \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_{t|t},u_{t+1|t}\\ldots, \\bm u_{t+{\\color{red}N_\\mathrm{c}}-1|t}, \\bm x_{t|t},\\ldots, \\bm x_{t+N_\\mathrm{p}|t}} &\\quad  \\frac{1}{2} \\bm x_{t+N_\\mathrm{p}|t}^\\top \\mathbf S \\bm x_{t+N_\\mathrm{p}|t} + \\frac{1}{2} \\sum_{k=0}^{N_\\mathrm{p}-1} \\bm x_{t+k|t}^\\top \\mathbf Q \\bm x_{t+k|t} + + \\frac{1}{2} \\sum_{k=0}^{{\\color{red}N_\\mathrm{c}}-1} \\bm u_{t+k|t}^\\top \\mathbf R \\bm u_{t+k|t}\\\\\n\\text{subject to}   &\\quad \\bm x_{t+k+1|t} = \\mathbf A\\bm x_{t+k|t} + \\mathbf B\\bm u_{t+k|t},\\quad k = 0, \\ldots, N_\\mathrm{p}-1, \\\\\n                    &\\quad \\bm x_{t|t} = \\mathbf x_t, \\\\\n                    &\\quad \\mathbf u^{\\min} \\leq \\bm u_{t+k|t} \\leq \\mathbf u^{\\max},\\\\\n                    &\\quad \\mathbf x^{\\min} \\leq \\bm x_{t+k|t} \\leq \\mathbf x^{\\max},\\\\\n                    &\\quad {\\color{red} \\bm u_{t+k|t} = \\bm u_{t+N_\\mathrm{c}-1|t}, \\quad k=N_\\mathrm{c}, N_\\mathrm{c}+1, \\ldots, N_\\mathrm{p}}.\n\\end{aligned}",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#move-blocking",
    "href": "discr_dir_mpc.html#move-blocking",
    "title": "Model predictive control (MPC)",
    "section": "Move blocking",
    "text": "Move blocking\nAnother strategy for reducing the number of control variables is known as move blocking. In this approach, the control inputs are held constant over several time steps (they are combined into blocks), effectively reducing the number of optimization variables. More on this in [1].",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#open-questions-for-us-at-this-moment",
    "href": "discr_dir_mpc.html#open-questions-for-us-at-this-moment",
    "title": "Model predictive control (MPC)",
    "section": "Open questions (for us at this moment)",
    "text": "Open questions (for us at this moment)\n\nCan the stability of a closed-loop system with an MPC controller be guaranteed? Even in the linear case we cannot just have a look at some poles to make a conclusion.\nWhat is the role of the terminal state cost in the MPC problem? And how shall we choose it? Apparently, if the original time horizon is finite but very long (say, 1000), with the prediction horizon set to 20, we can hardly argue that the corresponding term in the MPC cost function expresses our requirements on the behaviour of the system at time 1000.\n\nWe are going to come back to these after we investigate the other two approaches to discrete-time optimal control – the indirect approach and the dynamical programming.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "ext_software.html",
    "href": "ext_software.html",
    "title": "Software",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Software"
    ]
  },
  {
    "objectID": "roban_uncertainty.html",
    "href": "roban_uncertainty.html",
    "title": "Uncertainty (in) modelling",
    "section": "",
    "text": "Through this chapter we are stepping into the domain of robust control. We need to define a few keywords first.\nWhile these two terms are used in many other fields, here we are tailoring them to the discipline of control systems, in particular their model-based design.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Uncertainty (in) modelling"
    ]
  },
  {
    "objectID": "roban_uncertainty.html#origins-of-uncertainty-in-models",
    "href": "roban_uncertainty.html#origins-of-uncertainty-in-models",
    "title": "Uncertainty (in) modelling",
    "section": "Origins of uncertainty in models?",
    "text": "Origins of uncertainty in models?\n\nPhysical parameters are not known exactly (say, they are known to be within ±10% or ±3σ interval around the nominal value).\nEven if the physical parameters are initially known with a high accuracy, they can evolve in time, unmeasured.\nThere may be variations among the individual units of the same product.\nIf a nonlinear system is planned to be operated around a given operating point, it can be linearized aroud that operating point, which gives a nominal linear model. If the system is then operated in a significantly different operating point, the corresponding linear model is different from the nominal one.\nOur understanding of the underlying physics (or chemistry or biology or …) is imperfect, hence our model is imperfect too. In fact, our understading can even be incorrect, in which case the model contains some discrepancies too. The imperfections of the model are typically observed at higher frequencies (referring to the frequency-domain modeling such as transfer functions).\nEven if we are able to eventually capture full dynamics of the system in a model, we may opt not to do so. We may want to keep the model simple, even if less accurate, because time invested into modelling is not for free.\nEven if we can get a high-fidelity model with a reasonable effort, we may still prefer using a simpler (and less accurate) model for a controller design. The reason is that very often the complexity of the model used for model-based control design is reflected by the complexity of the controller – and high-complexity controllers are not particularly appreciated in industry.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Uncertainty (in) modelling"
    ]
  },
  {
    "objectID": "roban_uncertainty.html#models-of-uncertainty",
    "href": "roban_uncertainty.html#models-of-uncertainty",
    "title": "Uncertainty (in) modelling",
    "section": "Models of uncertainty",
    "text": "Models of uncertainty\nThere are several approaches to model the uncertainty (or, in other words, to characterize the uncertainty in the model). They all aim – in one way or another – to express that the controller has to deal no only with the single nominal system, for which it was designed, but a family of systems. Depending on the mathematical frameworks used for characterization of such a family, there are two major classes of approaches.\n\nWorst-case models of uncertainty\nProbabilistic models of uncertainty\n\nThe former assumes sets of systems with no additional information about the structure of such sets. The latter imposes some probability structure on the set of systems – in other words, although in principle any member of the set possible, some may be more probable than the others. In this course we are focusing on the former, which is also the mainstream in the robust control literature, but note that the latter we already encountered while considering control for systems exposed to random disturbances, namely the LQG control. A possible viewpoint is that as a consequence of the random disturbance, the controller has to deal with a family of systems.\nAnother classification of models of uncertainty is according to the actual quantity that is uncertain. We distinguish these two\n\nParametric uncertainty\nFrequency-dependent (aka dynamical) uncertainty\n\nUnstructured uncertainty\nStructured uncertainty\n\n\n\nParametric uncertainty\nThis is obviously straightforward to state: some (real/physical) parameters are uncertain. The conceptually simplest way to characterize such uncertain parameters is by considering intervals instead of just single (nominal) values.\n\nExample 1 (A pendulum on a cart) \n\\begin{aligned}\n{\\color{red} m_\\mathrm{l}} & \\in [m_\\mathrm{l}^{-},m_\\mathrm{l}^{+}],\\\\\n{\\color{red} l} & \\in [l^{-}, l^{+}],\n\\end{aligned}\n\n\n\\dot{\\bm x}(t) =\n\\begin{bmatrix}\n0 & 1 & 0 & 0\\\\\n0 & 0 & \\frac{\\textcolor{red}{m_\\mathrm{l}}}{m_\\mathrm{c}} g & 0\\\\\n0 & 0 & 0 & 1\\\\\n0 & 0 & -\\frac{(\\textcolor{red}{m_\\mathrm{l}}+m_\\mathrm{c})g}{m_\\mathrm{c}\\textcolor{red}{l}} & 0\n\\end{bmatrix}\n\\bm x(t)\n+\n\\begin{bmatrix}\n0\\\\\n\\frac{1}{m_\\mathrm{c}}\\\\\n0\\\\\n-\\frac{1}{m_\\mathrm{c}\\textcolor{red}{l}}\n\\end{bmatrix}\nu(t).\n\n\n\n\nUnstructured frequency-dependent uncertainty\nNot only some enumerated physical parameters but even the order of the system can be uncertain. In other words, there may be some phenomena exhibitted by the system that is not captured by the model at all. Possibly some lightly damped modes, possibly some time delay here and there. The system contains uncertain dynamics. In the linear case, all this can be expressed by regarding the magnitude and phase responses uncertain without mapping these to actual physical parameters.\n\n\n\n\n\n\nFigure 1: A whole subsystem is uncertain\n\n\n\nA popular model for the uncertain subsystem is that of a transfer function \\Delta(s), about which we know only that it is stable and that its magnitude is bounded by 1 \\boxed\n{\\sup_{\\omega}|\\Delta(j\\omega)|\\leq 1,\\;\\;\\Delta \\;\\text{stable}. }\n\nBut typically the uncertainty is higher at higher frequencies. This can be expressed by using some weighting function w(\\omega).\nFor later theoretical and computational purposes we approximate the real weighting function using a low-order rational stable transfer function W(s). That is, W(j\\omega)\\approx w(\\omega) for \\omega \\in \\mathbb R, that is for s=j\\omega on the imaginary axis.\nThe ultimate transfer function model of the uncertainty is then\n\\boxed{\nW(s)\\;\\Delta(s),\\quad \\max_{\\omega}|\\Delta(j\\omega)|\\leq 1,\\;\\;\\Delta\\; \\text{stable}. }\n\n\n\\mathcal H_\\infty norm of an LTI system\n\nH-infinity norm of an LTI system interpreted in frequency domain\n\nDefinition 4 (\\mathcal H_\\infty norm of a SISO LTI system) For a stable LTI system G with a single input and single output, the \\mathcal H_\\infty norm is defined as \n\\|G\\|_{\\infty} = \\sup_{\\omega\\in\\mathbb{R}}|G(j\\omega)|.\n\n\n\n\n\n\n\n\nWhy supremum and not maximum?\n\n\n\nSupremum is uses in the definition because it is not guaranteed that the peak value of the magnitude frequency response is attained at a single frequency. Consider an example of a first-order system G(s) = \\frac{s}{Ts+1}. The peak gain of 1/T is not attained at a single finite frequency.\n\n\nHaving just defined the \\mathcal H_\\infty norm, the uncertainty model can be expressed compactly as \\boxed{\nW(s)\\;\\Delta(s),\\quad \\|\\Delta(j\\omega)\\|\\leq 1. }\n\n\n\n\n\n\n\n\\mathcal H_\\infty as a space of functions\n\n\n\n\\mathcal H_\\infty denotes a normed vector space of functions that are analytic in the closed extended right half plane (of the complex plane). In parlance of control systems, \\mathcal H_\\infty is the space of proper and stable transfer functions. Poles on the imaginary axis are not allowed. The functions do not need to be rational, but very often we do restrict ourselves to rational functions, in which case we typically write such space as \\mathcal{RH}_\\infty.\n\n\nWe now extend the concept of the \\mathcal H_\\infty norm to MIMO systems. The extension is perhaps not quite intuitive – certainly it is not computed as the maximum of the norms of individual transfer functions, which may be the first guess.\n\nDefinition 5 (\\mathcal H_\\infty norm of a MIMO LTI system) For a stable LTI system \\mathbf G with multiple inputs and/or multiple outputs, the \\mathcal H_\\infty norm is defined as \n\\|\\mathbf G\\|_{\\infty} = \\sup_{\\omega\\in\\mathbb{R}}\\bar{\\sigma}(\\mathbf{G}(j\\omega))\n where \\bar\\sigma is the largest singular value.\n\nHere we include a short recap of singular values and singular value decomposition (SVD) of a matrix. Consider a matrix \\mathbf M, possibly a rectangular one. It can be decomposed as a product of three matrices \n\\mathbf M = \\mathbf U\n\\underbrace{\n\\begin{bmatrix}\n\\sigma_1 & & & &\\\\\n  & \\sigma_2 & & &\\\\\n  & &\\sigma_3 & &\\\\\n\\\\\n  & & & & \\sigma_n\\\\\n\\end{bmatrix}\n}_{\\boldsymbol\\Sigma}\n\\mathbf V^{*}.\n\nThe two square matrices \\mathbf V and \\mathbf U are unitary, that is, \n\\mathbf V\\mathbf V^*=\\mathbf I=\\mathbf V^*\\mathbf V\n and \n\\mathbf U\\mathbf U^*=\\mathbf I=\\mathbf U^*\\mathbf U.\n\nThe nonnegative diagonal entries \\sigma_i \\in \\mathbb R_+, \\forall i of the (possibly rectangular) matrix \\Sigma are called singular values. Commonly they are ordered in a nonincreasing order, that is \n\\sigma_1\\geq \\sigma_2\\geq \\sigma_3\\geq \\ldots \\geq \\sigma_n.\n\nIt is also a common notation to denote the largest singular value as \\bar \\sigma, that is, \\bar \\sigma \\coloneqq \\sigma_1.\n\n\n\\mathcal{H}_{\\infty} norm of an LTI system interpreted in time domain\nWe can also view the dynamical system G with inputs and outputs as an operator mapping from some chosen space of functions to another space of functions. A popular model for these spaces are the spaces of square-integrable functions, denoted as \\mathcal{L}_2, and sometimes interpreted as bounded-energy signals \nG:\\;\\mathcal{L}_2\\rightarrow \\mathcal{L}_2.\n\nIt is a powerful fact that the \\mathcal{H}_{\\infty} norm of the system is then defined as the induced norm of the corresponding operator \\boxed{\n\\|G(s)\\|_{\\infty} = \\sup_{u(t)\\in\\mathcal{L}_2\\setminus 0}\\frac{\\|y(t)\\|_2}{\\|u(t)\\|_2}}.\n\nWith the energy interpretation of the input and output variables, this system norm can also be interpreted as the worst-case energy gain of the system.\nScaling necessary to get any useful info from MIMO models! See Skogestad’s book, section 1.4, pages 5–8. \n\n\n\nHow does the uncertainty enter the model of the system?\n\nAdditive uncertainty\nThe transfer function of an uncertain system can be written as a sum of a nominal system and an uncertainty \nG(s) = \\underbrace{G_0(s)}_{\\text{nominal model}}+\\underbrace{W(s)\\Delta(s)}_{\\text{additive uncertainty}}.\n\nThe block diagram interpretation is in\n\n\n\n\n\n\nFigure 2: Additive uncertainty\n\n\n\nThe magnitude frequency response of the weighting filter W(s) then serves as an upper bound on the absolute error in the magnitude frequency responses \n|G(j\\omega)-G_0(j\\omega)|&lt;|W(j\\omega)|\\quad \\forall \\omega\\in\\mathbb R.\n\n\n\nMultiplicative uncertainty\n\nG(s) = (1+W(s)\\Delta(s))\\,G_0(s).\n\nThe block diagram interpretation is in\n\n\n\n\n\n\nFigure 3: Multiplicative uncertainty\n\n\n\n\n\n\n\n\n\nFor SISO transfer functions no need to bother about the order of terms in the products\n\n\n\nSice we are considering SISO transfer functions, the order of terms in the products is not important. We will have to be more alert to the order of terms when we move to MIMO systems.\n\n\nThe magnitude frequency response of the weighting filter W(s) then serves as an upper bound on the relative error in the magnitude frequency responses \\boxed\n{\\frac{|G(j\\omega)-G_0(j\\omega)|}{|G_0(j\\omega)|}&lt;|W(j\\omega)|\\quad \\forall \\omega\\in\\mathbb R.}\n\n\nExample 2 (Uncertain first-order delayed system) We consider a first-order system with a delay described by \nG(s) = \\frac{k}{T s+1}e^{-\\theta s}, \\qquad 2\\leq k,\\tau,\\theta,\\leq 3.\n\nWe now need to choose the nominal model G_0(s) and then the uncertainty weighting filter W(s). The nominal model corresponds to the nominal values of the parameters, therefore we must choose these. There is no single correct way to do this. Perhaps the most intuitive way is to choose the nominal values as the average of the bounds. But we can also choose the nominal values in a way that makes the nominal system simple. For example, for this system with a delay, we can even choose the nominal value of the delay as zero, which makes the nominal system a first-order system without delay, hence simple enough for application of some basic linear control system design methods. Of course, the price to pay is that the resulting model of an uncertain system, which is actually a set of systems, contains even models of a plant that were not prescribed.\n\n\nShow the code\nusing ControlSystems\nusing Plots\n\nfunction uncertain_first_order_delayed()\n    kmin = 2; kmax = 3; \n    θmin = 2; θmax = 3; \n    Tmin = 2; Tmax = 3;\n\n    k₀ = (kmin+kmax)/2; \n    θ₀ = (θmin+θmax)/2;\n    θ₀ = 0 \n    T₀ = (Tmin+Tmax)/2;\n\n    G₀ = tf(k₀,[T₀, 1])*delay(θ₀)  \n\n    ω = exp10.(range(-2, 2, length=50))\n    G₀ω = freqresp(G₀,ω)\n    G₀ω = vec(G₀ω)\n\n    EEω_db = [];\n    for k in range(kmin,kmax,length=10)\n        for θ in range(θmin,θmax,length=10)\n            for T in range(Tmin,Tmax,length=10)\n                G = tf(k,[T, 1])*delay(θ)  \n                Gω = freqresp(G,ω)\n                Gω = vec(Gω)            \n                Eω = abs.(Gω-G₀ω)./abs.(G₀ω)\n                Eω_db = 20 * log10.(Eω)\n                push!(EEω_db,Eω_db)\n            end\n        end\n    end\n\n    plot(ω,EEω_db,xscale=:log10,label=\"\",ylims=(-40,20))\n    xlabel!(\"Frequency [rad/s]\")\n    ylabel!(\"Relative error [dB]\")\nend\nuncertain_first_order_delayed()\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we need to find some upper bound on the relative error. Simplicity is a virtue here too, hence we are looking for a rational filter of very low order, say 1 or 2. Speaking of the first-order filter, one useful way to format it is \n\\boxed{\nW(s) = \\frac{\\tau s+r_0}{(\\tau/r_{\\infty})s+1}}\n where r_0 is uncertainty at steady state, 1/\\tau is the frequency, where the relative uncertainty reaches 100%, r_{\\infty} is relative uncertainty at high frequencies, often r_{\\infty}\\geq 2.\nFor our example, the parameters of the filter are in the code below and the frequency response follows.\n\n\nShow the code\nusing ControlSystems\nusing Plots\n\nfunction uncertain_first_order_delayed_with_weights()\n    kmin = 2; kmax = 3; \n    θmin = 2; θmax = 3; \n    Tmin = 2; Tmax = 3;\n\n    k₀ = (kmin+kmax)/2; \n    θ₀ = (θmin+θmax)/2;\n    θ₀ = 0 \n    T₀ = (Tmin+Tmax)/2;\n\n    G₀ = tf(k₀,[T₀, 1])*delay(θ₀)  \n\n    ω = exp10.(range(-2, 2, length=50))\n    G₀ω = freqresp(G₀,ω)\n    G₀ω = vec(G₀ω)\n\n    EEω_db = [];\n    for k in range(kmin,kmax,length=10)\n        for θ in range(θmin,θmax,length=10)\n            for T in range(Tmin,Tmax,length=10)\n                G = tf(k,[T, 1])*delay(θ)  \n                Gω = freqresp(G,ω)\n                Gω = vec(Gω)            \n                Eω = abs.(Gω-G₀ω)./abs.(G₀ω)\n                Eω_db = 20 * log10.(Eω)\n                push!(EEω_db,Eω_db)\n            end\n        end\n    end\n\n    plot(ω,EEω_db,xscale=:log10,label=\"\",ylims=(-40,20))\n    xlabel!(\"Frequency [rad/s]\")\n    ylabel!(\"Relative error [dB]\")\n\n    τ = 1/0.25\n    r₀ = 0.2\n    r∞ = 10^(8/20)\n    W = tf([τ, r₀],[τ/r∞, 1])\n    magW, phaseW = bode(W,ω)\n    plot!(ω,20*log10.(vec(magW)),xscale=:log10,lw=3,color=:red,label=\"W\")\n\n    # W2 = W*tf([1, 1.6, 1],[1, 1.4, 1]); \n    # magW2, phaseW2 = bode(W2,ω)\n    # plot!(ω,20*log10.(vec(magW2)),xscale=:log10,lw=3,color=:blue,label=\"W₂\")\nend\nuncertain_first_order_delayed_with_weights()\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObviously the filter does not capture the family of systems perfectly. It is now up to the control engineer to decide if this is a problem. If yes, if the control design should be really robust against all uncertainties in the considered set, some more complex (higher-order) filter is needed to described the uncertainty more accurately. The source code shows (in commented lines) one particular candidate, but in general the whole problem boils down to designing a stable filter with a prescribed magnitude frequency response.\n\n\n\nInverse additive uncertainty\n…\n\n\nInverse multiplicative uncertainty\n…\n\n\nLinear fractional transformation (LFT)\nFor a matrix \\mathbf P sized (n_1+n_2)\\times(m_1+m_2) and divided into blocks like \n\\mathbf P=\n\\begin{bmatrix}\n\\mathbf P_{11} & \\mathbf P_{12}\\\\\n\\mathbf P_{21} & \\mathbf P_{22}\n\\end{bmatrix},\n and a matrix \\mathbf K sized m_2\\times n_2, the lower LFT of \\mathbf P with respect to \\mathbf K is \n\\boxed{\n\\mathcal{F}_\\mathbf{l}(\\mathbf P,\\mathbf K) = \\mathbf P_{11}+\\mathbf P_{12}\\mathbf K(\\mathbf I-\\mathbf P_{22}\\mathbf K)^{-1}\\mathbf P_{21}}.\n\nIt can be viewed as a feedback interconnection of the plant \\mathbf P and the controller \\mathbf K, in which not all plant inputs are used as control inputs and not all plant outputs are measured, as depicted in Figure 4\n\n\n\n\n\n\nFigure 4: Lower LFT of \\mathbf P with respect to \\mathbf K\n\n\n\nSimilarly, for a matrix \\mathbf N sized (n_1+n_2)\\times(m_1+m_2) and a matrix \\boldsymbol\\Delta sized m_1\\times n_1, the upper LFT of \\mathbf N with respect to \\mathbf K is \n\\boxed{\n\\mathcal{F}_\\mathbf{u}(\\mathbf N,\\boldsymbol\\Delta) = \\mathbf N_{22}+\\mathbf N_{21}\\boldsymbol\\Delta(\\mathbf I-\\mathbf N_{11}\\boldsymbol\\Delta)^{-1}\\mathbf N_{12}}.\n\nIt can be viewed as a feedback interconnection of the nominal plant \\mathbf N and the uncertainty block \\boldsymbol\\Delta, as depicted in Figure 5\n\n\n\n\n\n\nFigure 5: Upper LFT of \\mathbf N with respect to \\boldsymbol \\Delta\n\n\n\nHere we already anticipated MIMO uncertainty blocks. One motivation for them is explained in the very next section on structured uncertainties, another one is appearing once we start formulating robust performance within the same analytical framework as robust stability.\n\n\n\n\n\n\nWhich is lower and which is upper is a matter of convention, but a useful one\n\n\n\nOur usage of the lower LFT for a feedback interconnection of a (generalized) plant and a controller and the upper LFT for a feedback interconnection of a nominal system and and uncertainty is completely arbitrary. We could easily use the lower LFT for the uncertainty. But it is a convenient convention to adhere to. The more so that it allows for the combination of both as in the diagram Figure 6 below, which corresponds to composition of the two LFTs.\n\n\n\n\n\n\nFigure 6: Combination of the lower and upper LFT\n\n\n\n\n\n\n\n\n\nStructured frequency-domain uncertainty\nNot just a single \\Delta(s) but several \\Delta_i(s), i=1,\\ldots,n are considered. Some of them scalar-valued, some of them matrix-valued.\nIn the upper LFT, all the individual \\Delta_is are collected into a single overall \\boldsymbol \\Delta, which then exhibits some structure. Typically it is block-diagonal as in \n\\boldsymbol\\Delta =\n\\begin{bmatrix}\n\\Delta_1& 0 & \\ldots & 0\\\\\n0 & \\Delta_2 & \\ldots & 0\\\\\n\\vdots\\\\\n0 & 0 & \\ldots & \\boldsymbol\\Delta_n\n\\end{bmatrix},\n with each block (including the MIMO blocks) satisfying the usual condition \n\\|\\Delta_i\\|_{\\infty}\\leq 1, \\; i=1,\\ldots, n.\n\n\nStructured singular value (SSV, \\mu, mu)\nWith this structured uncertainty, how does the small gain theorem look like?",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Uncertainty (in) modelling"
    ]
  },
  {
    "objectID": "intro_similar.html",
    "href": "intro_similar.html",
    "title": "Similar courses at other universities",
    "section": "",
    "text": "Below we give a selection of courses on optimal and robust (oftentimes not together) control that are given by excellent instructors at research universities abroad:\n\nS. Boyd. EE363 - Linear Dynamical Systems. Stanford. Winter 2008-2009.\nR. D’Andrea. 151-0563-01L – Dynamic Programming and Optimal Control. ETH Zurich. Fall 2024.\nM. Diehl. Numerical Optimal Control. University of Freiburg, Winter semester 2024/2025.\nM. Egerstedt. ECE6553 - Optimal Control and Optimization. Georgia Tech. Spring 2017.\nT. Faulwasser, Y. Jiang. EE-736 – Control for Dynamic Systems. EPFL. 2024.\nA. Hanson. TSRT08 – Optimal control. Linkoping University. Fall 2024.\nM. Hovd. Robust control. Norwegian University of Science Technology, Trondheim, Norway.\nJ. How. 16323 – Principles of Optimal Control. MIT. Spring 2008.\nA. Karimi. Robust control. EPFL, Lausanne.\nS. Lall. Engr210a - Robust Control Analysis and Synthesis. Stanford. Autumn 2001-2002.\nZ. Manchester. 16-745 – Optimal Control and Reinforcement Learning. Carnegie Mellon University. 2025.\nA. Megretski. 6.245 - Multivariable Control Systems. MIT. Spring 2004.\nT. A. E. Oomen, and J.W. van Wingerden. Design Methods for Control Systems. Dutch Institute of Systems and Control (DISC), The Netherlands. 2025.\nM. Pavone, D. Gammelli, D. Morton, M. Foutter. AA 203 – Optimal and Learning-Based Control. Stanford University. Spring 2024.\nA. Rantzer. Robust Control. Lund University. 2015.\nM. Salapaka. EE5235/AEM8425 - Robust Control. University of Minnesota.\nC.W. Scherer. Robust control. TU Delft. Since 2006 not accessible outside TU Delft.\nS. Skogestad. Multivariable feedback control using frequency-domain methods. Norwegian University of Science Technology, Trondheim, Norway. Replaced by the course given by M. Hovd, last run 1999.\n\n\n\n\n Back to top",
    "crumbs": [
      "0. Introduction",
      "Similar courses"
    ]
  },
  {
    "objectID": "discr_dir_references.html",
    "href": "discr_dir_references.html",
    "title": "References",
    "section": "",
    "text": "The key concept of this chapter — model predictive control (MPC), aka receding horizon control (RHC) — has been described in a number of dedicated monographs and textbooks. Particularly recommendable are [1] and [2]. They are not only reasonably up-to-date, written by leaders in the field, but they are also available online for free.\nSome updates as well as additional tutorials are in [3], which seems to be available to CTU students through the institutional access.\nThere is no shortage of lecture notes and slides as well. Particularly recommendable are the course slides [4], and [5].\nExtensions towards nonlinear systems (nonlinear model predictive control, NMPC) are described in [6], which is also available to CTU students through the institutional access. Alternatively, concise introductions are [7] and [8, Ch. 15].\nSince MPC essentially boils down to solving optimization problems in real time on some industrial device, the topic of embedded optimization is important. An overview is given in [9]. Although some new solvers appeared since its publication, the practical considerations highlighted in the paper are still valid.\n\n\n\n\n Back to topReferences\n\n[1] J. B. Rawlings, D. Q. Mayne, and M. M. Diehl, Model Predictive Control: Theory, Computation, and Design, 2nd ed. Madison, Wisconsin: Nob Hill Publishing, LLC, 2017. Available: http://www.nobhillpublishing.com/mpc-paperback/index-mpc.html\n\n\n[2] F. Borrelli, A. Bemporad, and M. Morari, Predictive Control for Linear and Hybrid Systems. Cambridge, New York: Cambridge University Press, 2017. Available: http://cse.lab.imtlucca.it/~bemporad/publications/papers/BBMbook.pdf\n\n\n[3] S. V. Raković and W. S. Levine, Eds., Handbook of Model Predictive Control. in Control Engineering. Birkhäuser Basel, 2019. Accessed: Mar. 06, 2019. [Online]. Available: https://www.springer.com/us/book/9783319774886\n\n\n[4] A. Bemporad, “Model predictive control.” May 2021. Available: http://cse.lab.imtlucca.it/~bemporad/teaching/mpc/imt/1-linear_mpc.pdf\n\n\n[5] S. Boyd, “Model Predictive Control (EE364b - Convex Optimization II.).” Stanford University. Accessed: Feb. 25, 2019. [Online]. Available: https://stanford.edu/class/ee364b/lectures/mpc_slides.pdf\n\n\n[6] L. Grüne and J. Pannek, Nonlinear Model Predictive Control: Theory and Algorithms, 2nd ed. in Communications and Control Engineering. Cham: Springer, 2017. Available: https://doi.org/10.1007/978-3-319-46024-6\n\n\n[7] S. Gros, M. Zanon, R. Quirynen, A. Bemporad, and M. Diehl, “From linear to nonlinear MPC: Bridging the gap via the real-time iteration,” International Journal of Control, vol. 93, no. 1, pp. 62–80, Jan. 2020, doi: 10.1080/00207179.2016.1222553.\n\n\n[8] S. Gros and M. Diehl, “Numerical Optimal Control (draft).” KU Leuven, May 2020. Available: https://www.syscop.de/teaching/ss2017/numerical-optimal-control\n\n\n[9] H. J. Ferreau et al., “Embedded Optimization Methods for Industrial Automatic Control,” IFAC-PapersOnLine, vol. 50, no. 1, pp. 13194–13209, Jul. 2017, doi: 10.1016/j.ifacol.2017.08.1946.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "References"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html",
    "href": "cont_numerical_direct.html",
    "title": "Numerical methods for direct approach",
    "section": "",
    "text": "We have already seen that direct methods for discrete-time optimal control problems essentially just regroup and reshape the problem data so that a nonlinear programming solver can accept them. In order to follow the same approach with continous-time optimal control problems, some kind of discretization is inevitable in order to formulate a nonlinear program. Depending of what is discretized and how, several groups of methods can be identified\nThe core principles behind the methods are identical to those discussed in the previous section on numerical methods for indirect approaches, but the adjective “direct” suggests that there must be some modifications.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-shooting-methods",
    "href": "cont_numerical_direct.html#direct-shooting-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct shooting methods",
    "text": "Direct shooting methods\nIn contrast with the application of shooting methods to the TP-BVP originating from the indirect approach, here the variables through whose values we aim the fictitious cannon are not the initial co-state vector but the (possibly long) vector parameterizing the whole control trajectory.\nIn the simplest – and fairly practical – case of a piecewise contstant control trajectory, which is motivated by the eventual discrete-time implementation using a zero-order hold (ZOH), the control trajectory is parameterized just by the sequence of values u_0, u_1, \\ldots, u_{N-1}.\n\n\n\n\n\n\nFigure 1: Direct shooting – only the control trajectory is discretized and the parameters of this discretization serve as optimization variables\n\n\n\nThe state trajectory x(t) corresponding to the fixed initial state \\mathrm x_\\mathrm{i} and the sequence of controls is obtained by using some IVP ODE numerical solver.\nFor the chosen control trajectory and the simulated state trajectory, the cost function J(u(\\cdot);x_\\mathrm{i}) = \\phi(x(t_\\mathrm{f}),t_\\mathrm{f}) + \\int L(x,u) \\mathrm d t is then evaluated. This can also only be done numerically, typically using methods for numerical integration.\nOnce the cost function is evaluated, numerical optimization solver is used to update the vector parameterizing the control trajectory so that the cost is reduced. For the new control trajectory, the state response is simulated, … and so on.\nSince optimization is only done over the optimization variables that parameterize the control trajectory, the method resembles the sequential method of the direct approach to discrete-time optimal control.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-multiple-shooting-methods",
    "href": "cont_numerical_direct.html#direct-multiple-shooting-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct multiple shooting methods",
    "text": "Direct multiple shooting methods\n…",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-transcription-methods",
    "href": "cont_numerical_direct.html#direct-transcription-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct transcription methods",
    "text": "Direct transcription methods\nIn contrast with the direct shooting methods, here both the control and state trajectories are discretized. There is then no need for a numerical solver for IVP ODE.\n\n\n\n\n\n\nFigure 2: Direct transcription/discretization – both the control and state trajectories are discretized and the resulting vectors of parameterers of this discretization serve as optimization variables\n\n\n\nThe optimization is then done over the optimization variables that parameterize both the control trajectory and the state trajectory. In this regard the methods resemble the simultaneous method of the direct approach to discrete-time optimal control.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-collocation-methods",
    "href": "cont_numerical_direct.html#direct-collocation-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct collocation methods",
    "text": "Direct collocation methods\nSimilarly as in the direct transcription methods, both the control and state trajectories are discretized and the parameterization of this discretization enters the optimization.\nBut here it is not just the values of the state and the control variables at the discretization points that are the optimization variables, but rather the coefficients of the polynomials that approximate the corresponding variables at the intermediate times between the discretization points.\n\n\n\n\n\n\nFigure 3: Direct collocation – both the control and state trajectories are approximated by piecewise polynomials and parameters of these polynomials serve as optimization variables\n\n\n\nHowever, similarly as we have already seen while discussing the collocation methods for the indirect approach, every direct collocation methods is equivalent to some direct transcription method. For example, collocation with a quadratic polynomial and the collocation points at the beginning and end of the interval is equivalent to the implicit trapezoidal rule. Some people even say that the implicit trapezoidal method is a collocation method.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#minimumtime-optimal-control-in-direct-approach",
    "href": "cont_numerical_direct.html#minimumtime-optimal-control-in-direct-approach",
    "title": "Numerical methods for direct approach",
    "section": "Minimum‐time optimal control in direct approach",
    "text": "Minimum‐time optimal control in direct approach\nWhile the indirect approach to optimal control allows for relaxing the final time t_\\mathrm{f} (and possibly including it in the cost function), the direct approach does not seem to support it. The final time has to be finite and constant.\nHere we show one way to turn the final time is t_\\mathrm{f} into an optimization variable. We achieve it by augmenting the state equations with a new variable… guess what… t_\\mathrm{f} accompanied with the corresponding state equation \\dot t_\\mathrm{f} = 0.\nWe now introduce a new normalized time variable \\tau\\in [0,1] such that \n  t = \\tau t_\\mathrm{f}.\n\nThe differentials of time then relate correspondingly \n  \\mathrm{d} t = t_\\mathrm{f}\\mathrm{d}\\tau.\n\nThe original state equation \\frac{\\mathrm{d}x}{\\mathrm{d}t} = f(x) then modifies to \\boxed{\n  \\frac{\\mathrm{d}x}{\\mathrm{d}\\tau} = t_\\mathrm{f} f(x).}\n\nStrictly speaking this should be \\frac{\\mathrm{d}\\hat x}{\\mathrm{d}\\tau} = t_\\mathrm{f} f(\\hat x), reflecting that x(t) = x(t_\\mathrm{r} \\tau) = \\hat x(\\tau) . But the abuse of notation is perhaps acceptable.\nThe cost function will then include a term penalizing t_\\mathrm{f}. Or perhaps the whole cost function is just t_\\mathrm{f} .Or perhaps the final time does not even have to be penalized at all, it is just an optimization variable, it just adds a degree of freedom to the optimal control problem. Of course, in the latter case this is no longer a minimum-time problem, just a free-final time problem.\nIn direct transcription methods for optimal control, the number of discretization/integration (sub)intervals is fixed and not subject to optimization, otherwise the resulting NLP would have a varying size for a varying final time, which is not convenient. Instead, the length of the integration interval, or the discretization/sampling interval is varied hN = t_\\mathrm{f}.\nRecall that all (single-step) discretization methods can be interpreted as an approximate computation of the definite integral in x_{k+1} = x_k + \\int_{t_k}^{t_{k+1}} f(x(t),t)\\mathrm{d}t, which in s-stage Runge-Kutta methods with a fixed time step h always looks like x_{k+1} = x_k + h \\sum_{j=1}^s b_{j} f_{kj}.\nSince the length of the integration interval h is now an optimization variable, even a problem that is originally linear is now turned into a nonlinear one. Furthermore, we can see consistency between x_{k+1} = x_k + \\frac{t_\\mathrm{f}}{N} \\sum_{j=1}^s b_{j} f_{kj} and the above result for the continuous-time problem that does not consider discretization.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "intro.html#what-is-optimal-control",
    "href": "intro.html#what-is-optimal-control",
    "title": "What is optimal and robust control?",
    "section": "What is optimal control?",
    "text": "What is optimal control?\nWhat does the adjective “optimal” stand for? According to the authoritative Oxford English Dictionary (OED), the word is defined as: best, most favourable, especially under a particular set of circumstance.\nOptimal control is then simply… the best control.\n\n\n\n\n\n\nCaution\n\n\n\nSince the word optimal already denotes the best, there is no point in saying “more optimal” or “the most optimal” (in Czech: “optimálnější” or “nejoptimálnější”). People using these terms simply do not know what they are talking about.",
    "crumbs": [
      "0. Introduction",
      "What is optimal and robust control?"
    ]
  },
  {
    "objectID": "intro.html#cost-function",
    "href": "intro.html#cost-function",
    "title": "What is optimal and robust control?",
    "section": "Cost function",
    "text": "Cost function\nOptimality of a (control) design can only be meaningfully considered if some quantitative measure of the quality of the design is specified. In control engineering it is typically called cost function, or simply cost. It is either provided by the customer, or chosen by the control engineer so that it reflects the customer’s requirements. An optimal controller then minimizes this cost. Alternatively, instead of a cost to be minimized, some kind of value to be maximized can be considered. But is a lot more common in control engineering to minimize a cost.\nAmong the classical examples of a cost function are the following:\n\nrise time, overshoot, settling time, …\nclassical integral criteria:\n\nintegral of the square of the error (ISE): J = \\int_0^\\infty e^2(t) \\mathrm{d}t, (or J = \\int_0^\\infty \\bm e^\\top\\bm e \\,\\mathrm{d}t in the vector case),\nintegral of the absolute error (IAE): J = \\int_0^\\infty |e(t)| \\mathrm{d}t,\nintegral of the absolute error weighted by time (ITAE): J = \\int_0^\\infty t|e(t)| \\mathrm{d}t,\n…\n\n\nThese are all quantifying in one way or another the performance, that is, how small the regulation error is, or how fast it gets small. But oftentimes the control effort must also be kept small. This necessity of a trade-off between performance and control effort is standard in optimal control theory. One way to handle it is through the celebrated LQR cost\n\nquadratic cost function: J = \\int_0^\\infty (\\bm x^\\top \\mathbf Q \\bm x + \\bm u^\\top \\mathbf R \\bm u) \\mathrm{d}t, where \\bm x and \\bm u are the (possibly vector) state and control variables, respectively, and \\mathbf Q and \\mathbf R are positive semidefinite weighting matrices that serve as the “tuning knobs” through which we express the trade-off.\n\nFinally, an alternative – and nicely unifying – approach to expressing cost functions is through the use of system norms.\n\nsystem norms: \\mathcal H_2 system norm, \\mathcal H_\\infty system norm, \\ell_1 system norm, …\n\nThe time to define these will only come later in the course, but at this moment we just mention it here.",
    "crumbs": [
      "0. Introduction",
      "What is optimal and robust control?"
    ]
  },
  {
    "objectID": "intro.html#why-do-we-need-optimal-control",
    "href": "intro.html#why-do-we-need-optimal-control",
    "title": "What is optimal and robust control?",
    "section": "Why do we need optimal control?",
    "text": "Why do we need optimal control?\nThere are three slightly different motivations for using the methods of optimal control, all equally useful:\n\nThe best performance is really needed. This is the most obvious opportunity. It occurs when, for example, the time for a robot to finish the assembly process must be as short as possible, or the fuel consumed by a vehicle to reach the final destination should be as low as possible.\nKnowledge of the best achievable performance is needed. Frequently, the technology chosen for the project imposes stringent coinstraints on the class of a controller that can be implemented. Say, just a PI or PID controller is allowed. Since the methods of optimal control typically yield controllers of higher complexity (higher-order, or nonlinear, or employing online optimization), an optimal controller would not be implementable with the chosen technology. However, even in these situations it may be useful to have an optimal controller as a baseline for comparison. If we know the minimum possible cost needed to accomplish the control task, we can compare it with the cost incurred by the current (setting of the) controller. Is the difference acceptable? If not, we can perhaps try to convice the customer to allow a more complex controller. If yes, we know that there is no need to keep on tuning the PID controller manually.\nAny reasonable controller would suffice, but it is difficult to find. Even if optimality of the control design is not a strict requirement, and it is only required to make the system “just work”, say, just to stable, decently fast, and free of large oscillations, even these modest requirements may turn out rather challenging in many situations. In particular, if the system has several inputs and outputs, the more so if it is unstable, or non-minimum phase, or nonlinear, or when the system is subject to disturbances or uncertainty. The methods of optimal control provide a systematic way to design a controller in these situations.\n\n\nWhat is robust control?\nRobust control system, as we understand it in this course, is capable of maintaining its performance even though the controlled system is subject to changes in its parameters, structure, or operating conditions.\n\n\nRobust control vs adaptive control\nIt is commonly understood that a robust controller is a controller with a fixed structure and fixed values of its parameters. As such it does not adapt to the aformentioned changes of the controlled system. This is in contrast with another important branch of control engineering, adaptive control.\n\n\nRobustness just as one aspect of a control system\nRobustness of a control system is certainly an important practical property, and as such it has been discussed in a number of publications and courses under the name of “robust control”. But we emphasize it here that we really view robustness as just another, however important, aspect of any control system. There is no point in seriously considering a nonrobust controller. Recall that even those classical concepts such as gain and phase margins, GM and PM, respectively, that you are familiar with from the introductory courses on automatic control, do characterize robustness. In our course, we will be investigating robustness in a systematic way, see the next paragraph.\n\n\nRobustness as an optimization criterion\nThe reason why we address this particular aspect of a control system in our course is that robustness can be naturally incorporated into the design of a controller using the methods of optimal control. Namely, the frameworks \\mathcal H_\\infty-optimal control and \\mu-synthesis are the most prominent examples of such optimization-based robust control design. While it used to be possible to design, say, a PID controller while keeping an eye on the gain and phase margins, we aim at attaining robustness as an outcome of optimization procedures.",
    "crumbs": [
      "0. Introduction",
      "What is optimal and robust control?"
    ]
  },
  {
    "objectID": "discr_indir_references.html",
    "href": "discr_indir_references.html",
    "title": "References",
    "section": "",
    "text": "While the indirect approaches to optimal control constitute the classical core of the optimal control theory, most treatments of the subject consider continuous-time systems. Our treatment was based on Chapter 2 in [1], which is one of a few resources that discuss discrete-time optimal control too.\n\n\n\n\n Back to topReferences\n\n[1] F. L. Lewis, D. Vrabie, and V. L. Syrmo, Optimal Control, 3rd ed. John Wiley & Sons, 2012. Accessed: Mar. 09, 2022. [Online]. Available: https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "References"
    ]
  },
  {
    "objectID": "cont_numerical_indirect.html",
    "href": "cont_numerical_indirect.html",
    "title": "Numerical methods for indirect approach",
    "section": "",
    "text": "The indirect approach to optimal control reformulates the optimal control problem as a system of differential and algebraic equations (DAE) with the values of some variables specified at both ends of the time interval – the two-point boundary value problem (TP–BVP). It is only in special cases that we are able to reformulate the TP–BVP as an initial value problem (IVP), the prominent example of which is the LQR problem and the associate differential Riccati equation solved backwards in time. However, generally we need to solve the TP–BVP DAE and the only way to do it is by numerical methods. Here we give some.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for indirect approach"
    ]
  },
  {
    "objectID": "cont_numerical_indirect.html#gradient-method-for-the-tp-bvp-dae-for-free-final-state",
    "href": "cont_numerical_indirect.html#gradient-method-for-the-tp-bvp-dae-for-free-final-state",
    "title": "Numerical methods for indirect approach",
    "section": "Gradient method for the TP-BVP DAE for free final state",
    "text": "Gradient method for the TP-BVP DAE for free final state\nRecall that with the Hamiltonian defined as H(\\bm x, \\bm u, \\bm \\lambda) = L(\\bm x, \\bm u) + \\bm \\lambda^\\top \\mathbf f(\\bm x, \\bm u), the necessary conditions of optimality for the fixed final time and free final state are are given by the following system of differential and algebraic equations (DAE) \n\\begin{aligned}\n\\dot{\\bm{x}} &= \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\bm \\lambda) \\\\\n\\dot{\\bm{\\lambda}} &= -\\nabla_{\\bm x} H(\\bm x,\\bm u,\\bm \\lambda) \\\\\n\\mathbf 0 &= \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)\\qquad (\\text{or} \\qquad \\bm u^\\star = \\text{argmax } H(\\bm x^\\star,\\bm u, \\bm\\lambda^\\star),\\quad \\bm u \\in\\mathcal{U})\\\\\n\\bm x(t_\\mathrm{i}) &=\\mathbf x_\\mathrm{i}\\\\\n\\bm \\lambda(t_\\mathrm{f}) &= \\nabla\\phi(\\bm{x}(t_\\mathrm{f})).\n\\end{aligned}\n\nOne idea to solve this is to guess at the trajectory \\bm u(t) on a grid of the time interval, use it to solve the state and costate equations, and then with all the three variables \\bm x, \\bm u, and \\bm u evaluate how much the stationarity equation is actually not satisfied. Based on the this, modify \\bm u and go for another iteration. Formally this is expressed as the algorithm:\n\nSet some initial trajectory \\bm u(t),\\; t\\in[t_\\mathrm{i},t_\\mathrm{f}] on a grid of points in [t_\\mathrm{i},t_\\mathrm{f}].\nWith the chosen \\bm u(\\cdot) and the initial state \\bm x(t_\\mathrm{i}), solve the state equation \n\\dot{\\bm{x}} = \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\bm \\lambda) = \\mathbf f(\\bm x, \\bm u)\n for \\bm x(t) using a solver for initial value problem ODE, that is, on a grid of t\\in[t_\\mathrm{i},t_\\mathrm{f}].\nHaving the control and state trajectories, \\bm u(\\cdot) and \\bm x(\\cdot), solve the costate equation \n\\dot{\\bm{\\lambda}} = -\\nabla_{\\bm x} H(\\bm x,\\bm u,\\bm \\lambda)\n\nfor the costates \\bm \\lambda(t), starting at the final time t_\\mathrm{f}, invoking the boundary condition \\bm \\lambda(t_\\mathrm{f}) = \\nabla\\phi(\\bm{x}(t_\\mathrm{f})).\nEvaluate \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda) for all t\\in[t_\\mathrm{i}, t_\\mathrm{f}].\nIf \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda) \\approx  \\mathbf 0 for all t\\in[t_\\mathrm{i}, t_\\mathrm{f}], quit, otherwise modify \\bm u(\\cdot) and go to the step 2.\n\nThe question is, of course, how to modify \\bm u(t) for all t \\in [t_\\mathrm{i}, t_\\mathrm{f}] in the step 4. Recall that the variation of the (augmented) cost functional is \n\\begin{aligned}\n\\delta  J^\\text{aug} &= [\\nabla \\phi(\\bm x(t_\\mathrm{f})) - \\bm\\lambda(t_\\mathrm{f})]^\\top \\delta \\bm{x}(t_\\mathrm{f})\\\\\n& \\qquad + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\dot{\\bm{\\lambda}} +\\nabla_{\\bm x} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm x(t)\\mathrm{d}t + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\dot{\\bm{x}} - \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm\\lambda(t)  \\mathrm{d}t\\\\\n& \\qquad + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm u(t) \\mathrm{d}t\n\\end{aligned},\n and for state and costate variables satisfying the state and costate equations this variation simplifies to \n\\delta  J^\\text{aug} = \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm u(t) \\mathrm{d}t.\n\nSince our goal is to minimize J^\\text{aug}, we need to make \\Delta  J^\\text{aug}\\leq0. Provided the increment \n\\delta \\mathbf u(t)=\\bm u^{(i+1)}(t)-\\bm u^{(i)}(t)\n is small, we can consider the linear approximation \\delta  J^\\text{aug} instead. We choose \n\\delta \\bm u(t)  = -\\alpha \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)\n for \\alpha&gt;0, which means that the control trajectory in the next iteration is \\boxed\n{\\bm u^{(i+1)}(t)  = \\bm u^{(i)}(t) -\\alpha \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda),}\n and the variation of the augmented cost funtion is\n\n\\delta  J^\\text{aug} = -\\alpha\\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)]^2 \\mathrm{d}t \\leq 0,\n and it is zero only for \\nabla_{\\bm u} H(t,\\bm x,\\bm u,\\bm \\lambda) = \\mathbf 0 for all t\\in[t_\\mathrm{i}, t_\\mathrm{f}].",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for indirect approach"
    ]
  },
  {
    "objectID": "cont_numerical_indirect.html#methods-for-solving-tp-bvp-ode",
    "href": "cont_numerical_indirect.html#methods-for-solving-tp-bvp-ode",
    "title": "Numerical methods for indirect approach",
    "section": "Methods for solving TP-BVP ODE",
    "text": "Methods for solving TP-BVP ODE\nHere we assume that from the stationarity equation \n\\mathbf 0 = \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)\n we can express \\bm u(t) as a function of the the state and costate variables, \\bm x(t) and \\bm \\lambda(t), respectively. In fact, Pontryagin’s principles gives this expression as \\bm u^\\star(t) = \\text{arg} \\min_{\\bm u(t) \\in\\mathcal{U}} H(\\bm x^\\star(t),\\bm u(t), \\bm\\lambda^\\star(t)). And we substitute for \\bm u(t) into the state and costate equations. This way we eliminate \\bm u(t) from the system of DAEs and we are left with a system of ODEs for \\bm x(t) and \\bm \\lambda(t) only. Formally, the resulting Hamiltonian is a different function as it is now a functio of two variables only.\n\n\\begin{aligned}\n\\dot{\\bm{x}} &= \\nabla_{\\bm\\lambda} \\mathcal H(\\bm x,\\bm \\lambda) \\\\\n\\dot{\\bm{\\lambda}} &= -\\nabla_{\\bm x} \\mathcal H(\\bm x,\\bm \\lambda) \\\\\n\\bm x(t_\\mathrm{i}) &=\\mathbf x_\\mathrm{i}\\\\\n\\bm x(t_\\mathrm{f}) &= \\mathbf x_\\mathrm{f} \\qquad \\text{or} \\qquad \\bm \\lambda(t_\\mathrm{f}) = \\nabla\\phi(\\bm{x}(t_\\mathrm{f})).\n\\end{aligned}\n\nAlthough we now have an ODE system, it is still a BVP. Strictly speaking, from now on, arbitrary reference on numerical solution of boundary value problems can be consulted to get some overview – we no longer need to restrict ourselves to the optimal control literature and software. On the other hand, the right sides are not quite arbitrary – these are Hamiltonian equations – and this property could and perhaps even should be exploited by the solution methods.\nThe methods for solving general BVPs are generally divided into\n\nshooting and multiple shooting methods,\ndiscretization methods,\ncollocation methods.\n\n\nShooting methods\n\nShooting method outside optimal control\nHaving made the diclaimer that boundary value problems constitute a topic indenendent of the optimal control theory, we start their investigation within a control-unrelated setup. We consider a system of two ordinary differential equations in two variables with the value of the first variable specified at both ends while the value of the other variable is left unspecified \n\\begin{aligned}\n\\begin{bmatrix}\n  \\dot y_1(t)\\\\\n  \\dot y_2(t)\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nf_1(\\bm y,t)\\\\\nf_2(\\bm y,t)\n\\end{bmatrix}\\\\\ny_1(t_\\mathrm{i}) &= \\mathrm y_{1\\mathrm{i}},\\\\\ny_1(t_\\mathrm{f}) &= \\mathrm y_{1\\mathrm{f}}.\n\\end{aligned}\n\nAn idea for a solution method is this:\n\nGuess at the missing (unspecified) value y_{2\\mathrm{i}} of y_2 at the initial time t_\\mathrm{i},\nUse an IVP solver (for example ode45 in Matlab) to find the values of both variables over the whole interval [t_\\mathrm{i},t_\\mathrm{f}].\nCompare the simulated value of the state variable y_1 at the final time t_\\mathrm{f} and compare it with the boundary value .\nBased on the error e = y_1(t_\\mathrm{f})-\\mathrm y_{1\\mathrm{f}}, update y_{2\\mathrm{i}} and go back to step 2.\n\nHow shall the update in the step 4 be realized? The value of y_1 at the final time t_\\mathrm{f} and therefore the error e are functions of the value y_{2\\mathrm{i}} of y_2 at the initial time t_\\mathrm{i}. We can formally express this upon introducing a map F such that e = F(y_{2\\mathrm{i}}). The problem now boils down to solving the nonlinear equation \\boxed\n{F(y_{2\\mathrm{i}}) = 0.}\n\nIf Newton’s method is to be used for solving this equation, the derivative of F is needed. Most often than not, numerical solvers for IVP ODE have to be called in order to evaluate the function F, in which case the derivative cannot be determined analytically. Finite difference (FD) and algorithmic/automatic differentiation (AD) methods are available.\nIn this example we only considered y_1 and y_2 as scalar variables, but in general these could be vector variables, in which case a system of equations in the vector variable has to be solved. Instead of a single scalar derivative, its matrix version – Jacobian matrix – must be determined.\nBy now the reason for calling this method shooting is perhaps obvious. Indeed, the analogy with aiming and shooting a cannon is illustrative.\nAs another example, we consider the BVP for a pendulum.\n\nExample 1 (BVP for pendulum) For an ideal pendulum described by the second-order model \\ddot \\theta + \\frac{b}{ml^2}\\dot \\theta + \\frac{g}{l} \\sin(\\theta) = 0 and for a final time t_\\mathrm{f}, at which some prescribed value of \\theta(t_\\mathrm{f}) must be achieved, compute by the shooting method the needed value of the initial angle \\theta_\\mathrm{i}, while assuming the initial angular rate \\omega_\\mathrm{i} is zero.\n\n\nShow the code\nusing DifferentialEquations\nusing Roots\nusing Plots\n\nfunction demo_shoot_pendulum()\n    θfinal = -0.2;\n    tfinal = 3.5;\n    tspan = (0.0,tfinal)\n    tol = 1e-5\n    function pendulum!(dx,x,p,t)\n        g = 9.81\n        l = 1.0;\n        m = 1.0;\n        b = 0.1;\n        a₁ = g/l\n        a₂ = b/(m*l^2)\n        θ,ω = x\n        dx[1] = ω\n        dx[2] = -a₁*sin(θ) - a₂*ω\n    end\n    prob = ODEProblem(pendulum!,zeros(Float64,2),tspan)\n    function F(θ₀::Float64)\n        xinitial = [θ₀,0.0]\n        prob = remake(prob,u0=xinitial)\n        sol = solve(prob,Tsit5(),reltol=tol/10,abstol=tol/10)\n        return θfinal-sol[end][1]\n    end\n    θinitial = find_zero(F,(-pi,pi))    # Solving the equation F(θ)=0 using Roots package. In general can find more solutions.\n    xinitial = [θinitial,0.0]\n    prob = remake(prob,u0=xinitial)     # Already solved in F(), but we solve it again for plotting.\n    sol = solve(prob,Tsit5())\n    p1 = plot(sol,lw=2,xlabel=\"Time\",ylabel=\"Angle\",label=\"θ\",idxs=(1))\n    scatter!([tfinal],[θfinal],label=\"Required terminal θ\")\n    p2 = plot(sol,lw=2,xlabel=\"Time\",ylabel=\"Angular rate\",label=\"ω\",idxs=(2))\n    display(plot(p1,p2,layout=(2,1)))\nend\n\ndemo_shoot_pendulum()\n\n\nGKS: cannot open display - headless operation mode active\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: State responses for a pendulum on a given time interval, with zero initial angular rate and the initial angle solved for numerically so that the final angle attains a give value\n\n\n\n\n\nA few general comments to the above code:\n\nThe function F(\\theta_\\mathrm{i}) that defines the nonlinear equation F(\\theta_\\mathrm{i})=0 calles a numerical solver for an IVP ODE. The latter solver then should have the numerical tolerances set more stringent than the former.\nThe ODE problem should only be defined once and then in each iteration its parameters should be updated. In Julia, this is done by the remake function, but it may be similar for other languages.\n\n\n\nShooting method for indirect approach to optimal control\nWe finally bring the method into the realm of indirect approach to optimal control – it is the initial value \\lambda_\\mathrm{i} of the costate variable that serves as an optimization variable, while the initial value x_\\mathrm{i} of the state variable is known and fixed. The final values of both the state and costate variables are the outcomes of numerical simulation obtained using a numerical solver for an IVP ODE. Based on these, the residual is computed. Either as e = x(t_\\mathrm{f})-x_\\mathrm{f} if the final state is fixed, or as e = \\lambda(t_\\mathrm{f}) - \\nabla \\phi(x(t_\\mathrm{f})) if the final state is free. Based on this residual, the initial value of the costate is updated and another iteration of the algorithm is entered.\n\n\n\n\n\n\nFigure 2: Indirect shooting\n\n\n\n\nExample 2 (Shooting for indirect approach to LQR) Standard LQR optimal control for a second-order system on a fixed finite interval with a fixed final state.\n\n\nShow the code\nusing LinearAlgebra\nusing DifferentialEquations\nusing NLsolve\n\nfunction shoot_lq_fixed(A,B,Q,R,xinitial,xfinal,tfinal)\n    n = size(A)[1]\n    function statecostateeq!(dw,w,p,t)\n        x = w[1:n]\n        λ = w[(n+1):end]\n        dw[1:n] = A*x - B*(R\\B'*λ)\n        dw[(n+1):end] = -Q*x - A'*λ\n    end\n    λinitial = zeros(n)\n    tspan = (0.0,tfinal)\n    tol = 1e-5\n    function F(λinitial)\n        winitial = vcat(xinitial,λinitial)\n        prob = ODEProblem(statecostateeq!,winitial,tspan)\n        dsol = solve(prob,Tsit5(),abstol=tol/10,reltol=tol/10)\n        xfinalsolved = dsol[end][1:n]\n        return (xfinal-xfinalsolved)\n    end\n    nsol = nlsolve(F,λinitial,xtol=tol)                 # Could add autodiff=:forward.\n    λinitial = nsol.zero                                # Solving once again for plotting.\n    winitial = vcat(xinitial,λinitial)\n    prob = ODEProblem(statecostateeq!,winitial,tspan)\n    dsol = solve(prob,Tsit5(),abstol=tol/10,reltol=tol/10)\n    return dsol\nend\n\nfunction demo_shoot_lq_fixed()\n    n = 2                                               # Order of the system.\n    m = 1                                               # Number of inputs.\n    A = rand(n,n)                                       # Matrices modeling the system.\n    B = rand(n,m)\n        \n    Q = diagm(0=&gt;rand(n))                               # Weighting matrices for the quadratic cost function.\n    R = rand(1,1)\n\n    xinitial = [1.0, 2.0]\n    xfinal = [3.0, 4.0]\n    tfinal = 5.0 \n\n    dsol = shoot_lq_fixed(A,B,Q,R,xinitial,xfinal,tfinal)\n\n    p1 = plot(dsol,idxs=(1:2),lw=2,legend=false,xlabel=\"Time\",ylabel=\"State\")\n    p2 = plot(dsol,idxs=(3:4),lw=2,legend=false,xlabel=\"Time\",ylabel=\"Costate\")\n    display(plot(p1,p2,layout=(2,1)))\nend\n\ndemo_shoot_lq_fixed()\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Shooting method applied to the indirect approach to LQR problem\n\n\n\n\n\n\n\n\nMultiple shooting methods\nThe key deficiency of the shooting method is that the only source of the error is the error in the initial condition, this error then amplifies as it propagates over the whole time interval as the numerical integration proceeds, and consequently the residual is very sensitive to tiny changes in the initial value. The multiple shooting method is a remedy for this. The idea is to divide the interval [t_\\mathrm{i},t_\\mathrm{f}] into N subintervals [t_k,t_{k+1}] and to introduce the values of the state and co-state variable at the beginning of each subinterval as additional variables. Additional equations are then introduced that enforce the continuity of the variable at the end of one subinterval and at the beginning of the next subinterval.\n\n\n\n\n\n\nFigure 4: Indirect multiple shooting\n\n\n\n\n\nDiscretization methods\nShooting methods take advantage of availability of solvers for IVP ODEs. These solvers produce discret(ized) trajectories, proceeding (integration) step by step, forward in time. But they do this in a way hidden from users. We just have to set the initial conditions (possibly through numerical optimization) and the solver does the rest.\nAlternatively, the formulas for the discrete-time updates are not evaluated one by one, step by step, running forward in time, but are assembled to form a system of equations, in general nolinear ones. Appropriate boundary conditions are then added to these nonlinear equations and the whole system is then solved numerically, yielding a discrete approximation of the trajectories satisfying the BVP.\nSince all those equatins are solved simultaneously (as a system of equations), there is no advantage in using explicit methods for solving ODEs, and implicit methods are used instead.\nIt is now time to recall some crucial results from the numerical methods for solving ODEs. First, we start with the popular single-step methods known as the Runge-Kutta (RK) methods.\nWe consider the standard ODE \\dot x(t) = f(x(t),t).\nand we define the Butcher tableau as \n  \\begin{array}{ l | c c c c }\n    c_1 & a_{11} & a_{12} & \\ldots & a_{1s}\\\\\n    c_2 & a_{21} & a_{22} & \\ldots & a_{2s}\\\\\n    \\vdots & \\vdots\\\\\n    c_s & a_{s1} & a_{s2} & \\ldots & a_{ss}\\\\\n    \\hline\n      & b_{1} & b_{2} & \\ldots & b_{s}\n  \\end{array}.\n such that c_i = \\sum_{j=1}^s a_{ij}, and 1 = \\sum_{j=1}^s b_{j}.\nReffering to the particular Butcher table, a single step of the method is \n  \\begin{aligned}\n  f_{k1} &= f(x_k + h_k \\left(a_{11}f_{k1}+a_{12}f_{k2} + \\ldots + a_{1s}f_{ks}),t_k+c_1h_k\\right)\\\\\n  f_{k2} &= f(x_k + h_k \\left(a_{21}f_{k1}+a_{22}f_{k2} + \\ldots + a_{2s}f_{ks}),t_k+c_2h_k\\right)\\\\\n  \\vdots\\\\\n  f_{ks} &= f(x_k + h_k \\left(a_{s1}f_{k1}+a_{s2}f_{k2} + \\ldots + a_{ss}f_{ks}),t_k+c_sh_k\\right)\\\\\n  x_{k+1} &= x_k + h_k \\left(b_1 f_{k1}+b_2f_{k2} + \\ldots + b_sf_{ks}\\right).\n  \\end{aligned}\n\nIf the matrix A is strictly lower triangular, that is, if a_{ij} = 0 for all i&lt;j , the method belongs to explicit Runge-Kutta methods, otherwise it belongs to implicit Runge-Kutta methods.\nA prominent example of explicit RK methods is the 4-stage RK method (oftentimes referred to as RK4).\n\nExplicit RK4 method\nThe Buttcher table for the method is \n      \\begin{array}{ l | c c c c }\n        0 & 0 & 0 & 0 & 0\\\\\n        1/2 & 1/2 & 0 & 0 & 0\\\\\n        1/2 & 0 & 1/2 & 0 & 0\\\\\n        1 & 0 & 0 & 1 & 0\\\\\n        \\hline\n          & 1/6 & 1/3 & 1/3 & 1/6\n      \\end{array}.\n\nFollowing the Butcher table, a single step of this method is \n      \\begin{aligned}\n      f_{k1} &= f(x_k,t_k)\\\\\n      f_{k2} &= f\\left(x_k + \\frac{h_k}{2}f_{k1},t_k+\\frac{h_k}{2}\\right)\\\\\n      f_{k3} &= f\\left(x_k + \\frac{h_k}{2}f_{k2},t_k+\\frac{h_k}{2}\\right)\\\\\n      f_{k4} &= f\\left(x_k + h_k f_{k3},t_k+h_k\\right)\\\\\n      x_{k+1} &= x_k + h_k \\left(\\frac{1}{6} f_{k1}+\\frac{1}{3}f_{k2} + \\frac{1}{3}f_{k3} + \\frac{1}{6}f_{k4}\\right)\n      \\end{aligned}.\n\nBut as we have just mentions, explicit methods are not particularly useful for solving BVPs. We prefer implicit methods. One of the simplest is the implicit midpoint method.\n\n\nImplicit midpoint method\nThe Butcher tableau is \n  \\begin{array}{ l | c r }\n    1/2 & 1/2 \\\\\n    \\hline\n      & 1\n  \\end{array}\n\nA single step is then \\begin{aligned}\n  f_{k1} &= f\\left(x_k+\\frac{1}{2}f_{k1} h_k, t_k+\\frac{1}{2}h_k\\right)\\\\\n  x_{k+1} &= x_k + h_k f_{k1}.\n  \\end{aligned}\nBut adding to the last equation x_k we get x_{k+1} + x_k = 2x_k + h_k f_{k1}.\nDividing by two we get \\frac{1}{2}(x_{k+1} + x_k) = x_k + \\frac{1}{2}h_k f_{k1} and then it follows that \\boxed{x_{k+1} = x_k + h_k f\\left(\\frac{1}{2}(x_k+x_{k+1}),t_k+\\frac{1}{2}h_k\\right).}\nThe right hand side of the last equation explains the “midpoint” in the name. It can be viewed as a rectangular approximation to the integral in x_{k+1} = x_k + \\int_{t_k}^{t_{k+1}} f(x(t),t)\\mathrm{d}t as the integral is computed as an area of a rectangle with the height determined by f() evaluated in the middle point.\nAlthough we do not explain the details here, let’s just note that it is the simplest of the collocation methods. In particular it belongs to Gauss (also Gauss-Legandre) methods.\n\n\nImplicit trapezoidal method\nThe method can be viewed both as a single-step (RK) method and a multi-step method. When viewed as an RK method, its Butcher table is \n  \\begin{array}{ l | c r }\n    0 & 0 & 0 \\\\\n    1 & 1/2 & 1/2 \\\\\n    \\hline\n      & 1/2 & 1/2 \\\\\n  \\end{array}\n   Following the Butcher table, a single step of the method is then \n  \\begin{aligned}\n  f_{k1} &= f(x_k,t_k)\\\\\n  f_{k2} &= f(x_k + h_k \\frac{f_{k1}+f_{k2}}{2},t_k+h_k)\\\\\n  x_{k+1} &= x_k + h_k \\left(\\frac{1}{2} f_{k1}+\\frac{1}{2} f_{k2}\\right).\n  \\end{aligned}\n\nBut since the collocation points are identical with the nodes (grid/mesh points), we can relabel to \\begin{aligned}\n  f_{k} &= f(x_k,t_k)\\\\\n  f_{k+1} &= f(x_{k+1},t_{k+1})\\\\\n  x_{k+1} &= x_k + h_k \\left(\\frac{1}{2} f_{k}+\\frac{1}{2} f_{k+1}\\right).\n  \\end{aligned}\n\nThis possibility is a particular advantage of Lobatto and Radau methods, which contain both end points of the interval or just one of them among the collocation points. The two symbols f_k and f_{k+1} are really just shorthands for values of the function f at the beginning and the end of the integration interval, so the first two equations of the triple above are not really equations to be solved but rather definitions. And we can assemble it all into just one equation \\boxed{\n      x_{k+1} = x_k + h_k \\frac{f(x_k,t_k)+f(x_{k+1},t_{k+1})}{2}.\n      }\n\nThe right hand side of the last equation explains the “trapezoidal” in the name. It can be viewed as a trapezoidal approximation to the integral in x_{k+1} = x_k + \\int_{t_k}^{t_{k+1}} f(x(t),t)\\mathrm{d}t as the integral is computed as an area of a trapezoid.\nWhen it comes to building a system of equations within transcription methods, we typically move all the terms just on one side to obtain the defect equations x_{k+1} - x_k - h_k \\left(\\frac{1}{2} f(x_k,t_k)+\\frac{1}{2} f(x_{k+1},t_{k+1})\\right) = 0.\n\n\nHermite-Simpson method\nIt belongs to the family of Lobatto III methods, namely it is a 3-stage Lobatto IIIA method. Butcher tableau \n  \\begin{array}{ l | c c c c }\n    0 & 0 &0 & 0\\\\\n    1/2 & 5/24 & 1/3 & -1/24\\\\\n    1 & 1/6 & 2/3 & 1/6\\\\\n    \\hline\n      & 1/6 & 2/3 & 1/6\n  \\end{array}\n\nHermite-Simpson method can actually come in three forms (this is from Betts (2020)):\n\nPrimary form\nThere are two equations for the given integration interval [t_k,t_{k+1}] x_{k+1} = x_k + h_k \\left(\\frac{1}{6}f_k + \\frac{2}{3}f_{k2} + \\frac{1}{6}f_{k+1}\\right), x_{k2} = x_k + h_k \\left(\\frac{5}{24}f_k + \\frac{1}{3}f_{k2} - \\frac{1}{24}f_{k+1}\\right), where the f symbols are just shorthand notations for values of the function at a certain point f_k = f(x_k,u(t_k),t_k), f_{k2} = f(x_{k2},u(t_{k2}),t_{k2}), f_{k+1} = f(x_{k+1},u(t_{k+1}),t_{k+1}), and the off-grid time t_{k2} is given by t_{k2} = t_k + \\frac{1}{2}h_k.\nThe first of the two equations can be recognized as Simpson’s rule for computing a definite integral. Note that while considering the right hand sides as functions of the control inputs, we also correctly express at which time (the collocation time) we consider the value of the control variable.\nBeing this general allows considering general control inputs and not only piecewise constant control inputs. For example, if we consider piecewise linear control inputs, then u(t_{k2}) = \\frac{u_k + u_{k+1}}{2}. But if we stick to the (more common) piecewise constant controls, not surprisingly u(t_{k2}) = u_k. Typically we format the equations as defect equations, that is, with zero on the right hand side \n\\begin{aligned}\nx_{k+1} - x_k - h_k \\left(\\frac{1}{6}f_k + \\frac{2}{3}f_{k2} + \\frac{1}{6}f_{k+1}\\right) &= 0,\\\\\nx_{k2} - x_k - h_k \\left(\\frac{5}{24}f_k + \\frac{1}{3}f_{k2} - \\frac{1}{24}f_{k+1}\\right) &= 0.\n\\end{aligned}\n\nThe optimization variables for every integration interval are x_k,u_k,x_{k2}, u_{k2}.\n\n\nHermite-Simpson Separated (HSS) method\nAlternatively, we can express f_{k2} in the first equation as a function of the remaining terms and then substitute to the second equation. This will transform the second equation such that only the terms indexed with k and k+1 are present. \n\\begin{aligned}\nx_{k+1} - x_k - h_k \\left(\\frac{1}{6}f_k + \\frac{2}{3}f_{k2} + \\frac{1}{6}f_{k+1}\\right) &= 0,\\\\\nx_{k2} - \\frac{x_k + x_{k+1}}{2} - \\frac{h_k}{8} \\left(f_k - f_{k+1}\\right) &= 0.\n\\end{aligned}\n\nWhile we already know (from some paragraph above) that the first equation is Simpson’s rule, the second equation is an outcome of Hermite intepolation. Hence the name. The optimization variables for every integration interval are the same as before, that is, x_k,u_k,x_{k2}, u_{k2} .\n\n\nHermite-Simpson Condensed (HSC) method\nYet some more simplification can be obtained from HSS. Namely, the second equation can be actually used to directly prescribe x_{k2} x_{k2} = \\frac{x_k + x_{k+1}}{2} + \\frac{h_k}{8} \\left(f_k - f_{k+1}\\right), which is used in the first equation as an argument for the f() function (represented by the f_{k2} symbol), by which the second equation and the term x_{k2} are eliminated from the set of defect equations. The optimization variables for every integration interval still need to contain u_{k2} even though x_{k2} was eliminated, because it is needed to parameterize f_{k2} . That is, the optimization variables then are x_k,u_k, u_{k2} . Reportedly (by Betts) this has been widely used and historically one of the first methods. When it comes to using it in optimal control, it turns out, however, that the sparsity pattern is better for the HSS.\n\n\n\n\nCollocation methods\nYet another family of methods for solving BVP ODE \\dot x(t) = f(x(t),t) are collocation methods. They are also based on discretization of independent variable – the time t. That is, on the interval [t_\\mathrm{i}, t_\\mathrm{f}], discretization points (or grid points or nodes or knots) are chosen, say, t_0, t_1, \\ldots, t_N, where t_0 = t_\\mathrm{i} and t_N = t_\\mathrm{f}. The solution x(t) is then approximated by a polynomial p_k(t) of a certain degree s on each interval [t_k,t_{k+1}] of length h_k=t_{k+1}-t_k\np_k(t) = p_{k0} + p_{k1}(t-t_k) + p_{k2}(t-t_k)^2+\\ldots + p_{ks}(t-t_k)^s.\nThe degree of the polynomial is low, say s=3 or so, certainly well below 10. With N subintervals, the total number of coefficients to parameterize the (approximation of the) solution x(t) over the whole interval is then N(s+1). For example, for s=3 and N=10, we have 40 coefficients: p_{00}, p_{01}, p_{02}, p_{03}, p_{10}, p_{11}, p_{12}, p_{13},\\ldots, p_{90}, p_{91}, p_{92}, p_{93}.\n\n\n\n\n\n\nFigure 5: Indirect collocation\n\n\n\nFinding a solution amounts to determining all those coefficients. Once we have them, the (approximate) solution is given by a piecewise polynomial.\nHow to determine the coefficients? By interpolation. But we will see in a moment that two types of interpolation are needed – interpolation of the value of the solution and interpolation of the derivative of the solution.\nThe former is only performed at the beginning of each interval, that is, at every discretization point (or grid point or node or knot). The condition reads that the polynomial p_{k-1}(t) approximating the solution x(t) on the (k-1)th interval should attain the same value at the end of that interval, that is, at t_{k-1} + h_{k-1}, as the polynomial p_k(t) approximating the solution x(t) on the kth interval attains at the same point, which from its perspective is the beginning of the kth interval, that is, t_k. We express this condition formally as \\boxed{p_{k-1}(\\underbrace{t_{k-1}+h_{k-1}}_{t_{k}}) = p_k(t_k).}\nExpanding the two polynomials, we get p_{k-1,0} + p_{k-1,1}h_{k-1} + p_{k-1,2}h_{k-1}^2+\\ldots + p_{k-1,s}h_{k-1}^s = p_{k0}.\n\n\n\n\n\n\nSubscripts in the coefficients\n\n\n\nWe adopt the notational convention that a coefficient of a polynomial is indexed by two indices, the first one indicating the interval and the second one indicating the power of the corresponding term. For example, p_{k-1,2} is the coefficient of the quadratic term in the polynomial approximating the solution on the (k-1)th interval. For the sake of brevity, we omit the comma between the two subscripts in the cases such as p_{k1} (instead of writing p_{k,1}). But we do write p_{k-1,0}, because here omiting the comma would introduce ambiguity.\n\n\nGood, we have one condition (one equation) for each subinterval. But we need more, if polynomials of degree at least one are considered (we then parameterize them by two parameters, in which case one more equation is needed for each subinterval). Here comes the opportunity for the other kind of interpolation – interpolation of the derivative of the solution. At a given point (or points) that we call collocation points, the polynomial p_k(t) approximating the solution x(t) on the kth interval should satisfy the same differential equation \\dot x(t) = f(x(t),t) as the solution does. That is, we require that at\nt_{kj} = t_k + h_k c_{j}, \\quad j=1,\\ldots, s, which we call collocation points, the polynomial satisfies \\boxed{\\dot p_k(t_{kj}) = f(p_k(t_{kj}),t_{kj}), \\quad j=1,\\ldots, s.}\nExpressing the derivative of the polynomial on the left and expanding the polynomial itself on the right, we get \n\\begin{aligned}\np_{k1} + &2p_{k2}(t_{kj}-t_k)+\\ldots + s p_{ks}(t_{kj}-t_k)^{s-1} = \\\\ &f(p_{k0} + p_{k1}(t_{kj}-t_k) + p_{ks}(t_{kj}-t_k)^2 + \\ldots + p_{ks}(t_{kj}-t_k)^s), \\quad j=1,\\ldots, s.\n\\end{aligned}\n\nThis gives us the complete set of equations for each interval. For the considered example of a cubic polynomial, we have one interpolation condition at the beginning of the interval and then three collocation conditions at the collocation points. In total, we have four equations for each interval. The number of equations is equal to the number of coefficients of the polynomial. Before the system of equations can solved for the coefficients, we must specifies the collocation points. Based on these, the collocation methods split into three families:\n\nGauss or Gauss-Legendre methods – the collocation points are strictly inside each interval.\nLobatto methods – the collocation points include also both ends of each interval.\nRadau methods – the collocation points include just one end of the interval.\n\n\n\n\n\n\n\nFigure 6: Single (sub)interval in indirect collocation – a cubic polynomial calls for three collocation points, two of which coincide with the discretization points (discrete-times); the continuity is enforced at the discretization point at the beginning of the interval\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAlthough in principle the collocation points could be arbitrary (but distinct), within a given family of methods, and for a given number of collocation points, some clever options are known that maximize accuracy.\n\n\n\nLinear polynomials\nBesides the piecewise constant approximation, which is too crude, not to speak of the discontinuity it introduces, the next simplest approximation of a solution x(t) on the interval [t_k,t_{k+1}] of length h_k=t_{k+1}-t_k is a linear (actually affine) polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k).\nOn the given kth interval it is parameterized by two parameters p_{k0} and p_{k1}, hence two equations are needed. The first equation enforces the continuity at the beginning of the interval \\boxed\n{p_{k-1,0} + p_{k-1,1}h_{k-1} = p_{k0}.}\n\nThe remaining single equation is the collocation condition at a single collocation point t_{k1} = t_k + h_k c_1, which remains to be chosen. One possible choice is c_1 = 1/2, that is \nt_{k1} = t_k + \\frac{h_k}{2}\n\nIn words, the collocation point is chosen in the middle of the interval. The collocation condition then reads \\boxed\n{p_{k1} = f\\left(p_{k0} + p_{k1}\\frac{h_k}{2}\\right).}\n\n\n\nQuadratic polynomials\nIf a quadratic polynomial is used to approximate the solution, the condition at the beginning of the interval is \\boxed\n{p_{k-1,0} + p_{k-1,1}h_{k-1} + p_{k-1,2}h_{k-1}^2 = p_{k0}.}\n\nTwo more equations – collocation conditions – are needed to specify all the three coefficients that parameterize the aproximating polynomial on a given interval [t_k,t_{k+1}]. One intuitive (and actually clever) choice is to place the collocation points at the beginning and the end of the interval, that is, at t_k and t_{k+1}. The coefficient that parameterize the relative position of the collocation points with respect to the interval are c_1=0 and c_2=1 The collocation conditions then read \\boxed\n{\\begin{aligned}\np_{k1}  &= f(p_{k0}),\\\\\np_{k1} + 2p_{k2}h_{k} &= f(p_{k0} + p_{k1}h_k + p_{k2}h_k^2).\n\\end{aligned}}\n\n\n\nCubic polynomials\nWhen a cubic polynomial is used, the condition at the beginning of the kth interval is \\boxed\n{p_{k-1,0} + p_{k-1,1}h_{k-1} + p_{k-1,2}h_{k-1}^2+p_{k-1,3}h_{k-1}^3 = p_{k0}.}\n\nThree more equations are needed to determine all the four coefficients of the polynomial. Where to place the collocations points? One intuitive (and clever too) option is to place them at the beginning, in the middle, and at the end of the interval. The relative positions of the collocation points are then given by c_1=0, c_2=1/2, and c_3=1. The collocation conditions then read \\boxed\n{\\begin{aligned}\np_{k1} &= f\\left(p_{k0} + p_{k1}(t_{k1}-t_k) + p_{k2}(t_{k1}-t_k)^2 + p_{k3}(t_{k1}-t_k)^3\\right),\\\\\np_{k1} + 2p_{k2}\\frac{h_k}{2} + 3 p_{k3}\\left(\\frac{h_k}{2}\\right)^{2} &= f\\left(p_{k0} + p_{k1}\\frac{h_k}{2} + p_{k2}\\left(\\frac{h_k}{2}\\right)^2 + p_{k3}\\left(\\frac{h_k}{2} \\right)^3\\right),\\\\\np_{k1} + 2p_{k2}h_k + 3 p_{k3}h_k^{2} &= f\\left(p_{k0} + p_{k1}h_k + p_{k2}h_k^2 + p_{k3}h_k^3\\right).\n\\end{aligned}}\n\n\n\n\nCollocation methods are implicit Runge-Kutta methods\nAn important observation that we are goint to make is that collocation methods can be viewed as implicit Runge-Kutta methods. But not all IRK methods can be viewed as collocation methods. In this section we show that the three implicit RK methods that we covered above are indeed (equivalent to) collocation methods. By the equivalence we mean that there is a linear relationship between the coefficients of the polynomials that approximate the solution on a given (sub)interval and the solution at the discretization point together with the derivative of the solution at the collocation points.\n\nImplicit midpoint method as a Radau collocation method\nFor the given integration interval [t_k,t_{k+1}], we write down two equations that relate the two coefficients of the linear polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k) and an approximation x_k of x(t) at the beginning of the interval t_k, as well as an approximation of \\dot x(t) at the (single) collocation point t_{k1} = t_{k} + \\frac{h_k}{2}.\nIn particular, the first interpolation condition is p_k(t_k) = \\textcolor{red}{p_{k0} = x_k} \\approx x(t_k).\nThe second interpolation condition, the one on the derivative in the middle of the interval is \\dot p_k\\left(t_k + \\frac{h_k}{2}\\right) = \\textcolor{red}{p_{k1} = f(x_{k1},t_{k1})} \\approx f(x(t_{k1}),t_{k1}).\nNote that here we introduced yet another unknown – the approximation x_{k1} of x(t_{k1}) at the collocation point t_{k1}. We can write it using the polynomial p_k(t) as \nx_{k1} = p_k\\left(t_k + \\frac{h_k}{2}\\right) = p_{k0} + p_{k1}\\frac{h_k}{2}.\n\nSubstituting for p_{k0} and p_{k1}, we get \nx_{k1} = x_k + f(x_{k1},t_{k1})\\frac{h_k}{2}.\n\nWe also introduce the notation f_{k1} for f(x_{k1},t_{k1}) and we can write an equation \nf_{k1} = f\\left(x_k + f_{k1}\\frac{h_k}{2}\\right).\n\nBut we want to find x_{k+1}, which we can accomplish by evaluating the polynomial p_k(t) at t_{k+1} = t_k+h_k \nx_{k+1} = x_k + f_{k1}h_k.\n\nCollecting the last two equations, we rederived the good old friend – the implicit midpoint method.\n\n\nImplicit trapezoidal method as a Lobatto collocation method\nFor the given integration interval [t_k,t_{k+1}], we write down three equations that relate the three coefficients of the quadratic polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k) + p_{k2}(t-t_k)^2 and an approximation x_k of x(t) at the beginning of the interval t_k, as well as approximations to \\dot x(t) at the two collocations points t_k and t_{k+1}.\nIn particular, the first interpolation condition is p_k(t_k) = \\textcolor{red}{p_{k0} = x_k} \\approx x(t_k).\nThe second interpolation condition, the one on the derivative at the beginning of the interval, the first collocation point, is \\dot p_k(t_k) = \\textcolor{red}{p_{k1} = f(x_k,t_k)} \\approx f(x(t_k),t_k).\nThe third interpolation condition, the one on the derivative at the second collocation point \\dot p_k(t_k+h_k) = \\textcolor{red}{p_{k1} + 2p_{k2} h_k = f(x_{k+1},t_{k+1})} \\approx f(x(t_{k+1}),t_{k+1}).\nAll the three conditions (emphasized in color above) can be written together as \n      \\begin{bmatrix}\n      1 & 0 & 0\\\\\n      0 & 1 & 0\\\\\n      0 & 1 & 2 h_k\\\\\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k+1},t_{k+1})\n      \\end{bmatrix}.\n\nThe above system of linear equations can be solved by inverting the matrix \n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      1 & 0 & 0\\\\\n      0 & 1 & 0\\\\\n      0 & -\\frac{1}{2h_k} & \\frac{1}{2h_k}\\\\\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k+1},t_{k+1})\n      \\end{bmatrix}.\n\nWe can now write down the interpolating/approximating polynomial p_k(t) = x_{k} + f(x_{k},t_{k})(t-t_k) +\\left[-\\frac{1}{2h_k}f(x_{k},t_{k}) + \\frac{1}{2h_k}f(x_{k+1},t_{k+1})\\right](t-t_k)^2.\nThis polynomial can now be used to find an (approximation of the) value of the solution at the end of the interval x_{k+1} = p_k(t_k+h_k) = x_{k} + f(x_{k},t_{k})h_k +\\left[-\\frac{1}{2h_k}f(x_{k},t_{k}) + \\frac{1}{2h_k}f(x_{k+1},t_{k+1})\\right]h_k^2, which can be simplified nearly upon inspection to x_{k+1} = x_{k} + \\frac{f(x_{k},t_{k}) + f(x_{k+1},t_{k+1})}{2} h_k, but this is our good old friend, isn’t it? We have shown that the collocation method with a quadratic polynomial with the collocation points chosen at the beginning and the end of the interval is (equivalent to) the implicit trapezoidal method. The method belongs to the family of Lobatto IIIA methods, which are all known to be collocation methods.\n\n\nHermite-Simpson method as a Lobatto collocation method\nHere we show that Hermite-Simpson method also qualifies as a collocation method. In particular, it belongs to the family of Lobatto IIIA methods, similarly as implicit trapezoidal method. The first condition, the one on the value of the cubic polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k) + p_{k2}(t-t_k)^2+ p_{k3}(t-t_k)^3 at the beginning of the interval is p_k(t_k) = \\textcolor{red}{p_{k0} = x_k} \\approx x(t_k).\nThe three remaining conditions are imposed at the collocation points, which for the integration interval [t_k,t_{k+1}] are t_{k1} = t_k , t_{k2} = \\frac{t_k+t_{k+1}}{2} , and t_{k3} = t_{k+1}. With the first derivative of the polynomial given by \\dot p_k(t) = p_{k1} + 2p_{k2}(t-t_k) + 3p_{k3}(t-t_k)^2, the first collocation condition \\dot p_k(t_k) = \\textcolor{red}{p_{k1} = f(x_k,t_k)} \\approx f(x(t_k),t_k).\nThe second collocation condition – the one on the derivative in the middle of the interval – is \\dot p_k\\left(t_k+\\frac{1}{2}h_k\\right) = \\textcolor{red}{p_{k1} + 2p_{k2} \\frac{h_k}{2} + 3p_{k3} \\left(\\frac{h_k}{2}\\right)^2 = f(x_{k2},t_{k2})} \\approx f\\left(x\\left(t_{k}+\\frac{h_k}{2}\\right),t_{k}+\\frac{h_k}{2}\\right).\nThe color-emphasized part can be simplified to \\textcolor{red}{p_{k1} + p_{k2} h_k + \\frac{3}{4}p_{k3} h_k^2 = f(x_{k2},t_{k2})}.\nFinally, the third collocation condition – the one imposed at the end of the interval – is \\dot p_k(t_k+h_k) = \\textcolor{red}{p_{k1} + 2p_{k2} h_k + 3p_{k3} h_k^2 = f(x_{k+1},t_{k+1})} \\approx f(x(t_{k+1}),t_{k+1}).\nAll the four conditions (emphasized in color above) can be written together as \n      \\begin{bmatrix}\n      1 & 0 & 0 & 0\\\\\n      0 & 1 & 0 & 0\\\\\n      0 & 1 & h_k & \\frac{3}{4} h_k^2\\\\\n      0 & 1 & 2 h_k & 3h_k^2\\\\\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2} \\\\p_{k3}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k2},t_{k2}) \\\\ f(x_{k+1},t_{k+1}).\n      \\end{bmatrix}\n\nInverting the matrix analytically, we get \n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2}\\\\ p_{k3}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      1 & 0 & 0 & 0\\\\\n      0 & 1 & 0 & 0\\\\\n      0 & -\\frac{3}{2h_k} & \\frac{2}{h_k} & -\\frac{1}{2h_k}\\\\\n      0 & \\frac{2}{3h_k^2} & -\\frac{4}{3h_k^2} & \\frac{2}{3h_k^2}\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k2},t_{k2})\\\\ f(x_{k+1},t_{k+1}).\n      \\end{bmatrix}.\n\nWe can now write down the interpolating/approximating polynomial \n      \\begin{aligned}\n      p_k(t) &= x_{k} + f(x_{k},t_{k})(t-t_k) +\\left[-\\frac{3}{2h_k}f(x_{k},t_{k}) + \\frac{2}{h_k}f(x_{k2},t_{k2}) -\\frac{1}{2h_k}f(x_{k+1},t_{k+1}) \\right](t-t_k)^2\\\\\n      & +\\left[\\frac{2}{3h_k^2}f(x_{k},t_{k}) - \\frac{4}{3h_k^2}f(x_{k2},t_{k2}) +\\frac{2}{3h_k^2}f(x_{k+1},t_{k+1}) \\right](t-t_k)^3.\n      \\end{aligned}\n\nWe can use this prescription of the polynomial p_k(t) to compute the (approximation of the) value of the solution at the end of the kth interval \n      \\begin{aligned}\n      x_{k+1} = p_k(t_k+h_k) &= x_{k} + f(x_{k},t_{k})h_k +\\left[-\\frac{3}{2h_k}f(x_{k},t_{k}) + \\frac{2}{h_k}f(x_{k2},t_{k2}) -\\frac{1}{2h_k}f(x_{k+1},t_{k+1}) \\right]h_k^2\\\\\n      & +\\left[\\frac{2}{3h_k^2}f(x_{k},t_{k}) - \\frac{4}{3h_k^2}f(x_{k2},t_{k2}) +\\frac{2}{3h_k^2}f(x_{k+1},t_{k+1}) \\right]h_k^3,\n      \\end{aligned}\n which can be simplified to \n      \\begin{aligned}\n      x_{k+1} &= x_{k} + f(x_{k},t_{k})h_k +\\left[-\\frac{3}{2}f(x_{k},t_{k}) + \\frac{2}{1}f(x_{k2},t_{k2}) -\\frac{1}{2}f(x_{k+1},t_{k+1}) \\right]h_k\\\\\n      & +\\left[\\frac{2}{3}f(x_{k},t_{k}) - \\frac{4}{3}f(x_{k2},t_{k2}) +\\frac{2}{3}f(x_{k+1},t_{k+1}) \\right]h_k,\n      \\end{aligned}\n which further simplifies to \n      x_{k+1}  = x_{k} + h_k\\left[\\frac{1}{6}f(x_{k},t_{k}) + \\frac{2}{3}f(x_{k2},t_{k2}) + \\frac{1}{6}f(x_{k+1},t_{k+1}) \\right],\n which can be recognized as the Simpson integration that we have already seen in implicit Runge-Kutta method described above.\nObviously f_{k2} needs to be further elaborated on, namely, x_{k2} needs some prescription too. We know that it was introduced as an approximation to the solution x in the middle of the interval. Since the value of the polynomial in the middle is such an approximation too, we can set x_{k2} equal to the value of the polynomial in the middle. \n      \\begin{aligned}\n      x_{k2} = p_k\\left(t_k+\\frac{1}{2}h_k\\right) &= x_{k} + f(x_{k},t_{k})\\frac{h_k}{2} +\\left[-\\frac{3}{2h_k}f(x_{k},t_{k}) + \\frac{2}{h_k}f(x_{k2},t_{k2}) -\\frac{1}{2h_k}f(x_{k+1},t_{k+1}) \\right]\\left(\\frac{h_k}{2}\\right)^2\\\\\n      & +\\left[\\frac{2}{3h_k^2}f(x_{k},t_{k}) - \\frac{4}{3h_k^2}f(x_{k2},t_{k2}) +\\frac{2}{3h_k^2}f(x_{k+1},t_{k+1}) \\right]\\left(\\frac{h_k}{2}\\right)^3,\n      \\end{aligned}\n which without further ado simplifies to \n      x_{k2} = x_{k} + h_k\\left( \\frac{5}{24}f(x_{k},t_{k}) +\\frac{1}{3}f(x_{k2},t_{k2}) -\\frac{1}{24}f(x_{k+1},t_{k+1}) \\right),\n which can be recognized as the other equation in the primary formulation of Implicit trapezoidal method described above.\n\n\n\nPseudospectral collocation methods\nThey only consider a single polynomial over the whole interval. The degree of such polynomial, in contrast with classical collocation methods, rather high, therefore also the number of collocation points is high, but their location is crucial.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for indirect approach"
    ]
  },
  {
    "objectID": "ext_H2.html",
    "href": "ext_H2.html",
    "title": "H2-optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "H2-optimal control"
    ]
  },
  {
    "objectID": "cont_indir_Pontryagin.html",
    "href": "cont_indir_Pontryagin.html",
    "title": "Pontryagin’s maximum principle",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Pontryagin's maximum principle"
    ]
  },
  {
    "objectID": "opt_theory_references.html",
    "href": "opt_theory_references.html",
    "title": "References",
    "section": "",
    "text": "If a single reference book on nonlinear optimization is to be recommended, be it [1] that sits on your book shelf.\nIf one or two more can still fit, [2], [3] are classical comprehensive references on nonlinear programming (the latter covers linear programming too).\nWhile all the three books are only available for purchase, there is a wealth of resources that are freely available online such as the notes [4] accompanying a course on optimal control, which do a decent job of introduction to a nonlinear programming, and beautifully typeset modern textbooks [5] and [6], the former based on Julia language. Yet another high-quality textbook that is freely available online is [7].\nWhen restricting to convex optimization, the bible of this field [8] is also freely available online. It is a must-have for everyone interested in optimization. Yet another advanced and treatment of convex optimization is [9], which is also freely available online.\nMaybe a bit unexpected resources on theory are materials accompanying some optimization software. Partilarly recommendable is [10], it is very useful even if you do not indend to use their software. In particular, their introduction to conic optimization is very well written and easy to follow.\n\n\n\n\n Back to topReferences\n\n[1] J. Nocedal and S. Wright, Numerical Optimization, 2nd ed. in Springer Series in Operations Research and Financial Engineering. New York: Springer, 2006. Available: https://doi.org/10.1007/978-0-387-40065-5\n\n\n[2] D. Bertsekas, Nonlinear Programming, 3rd ed. Belmont, Mass: Athena Scientific, 2016. Available: http://www.athenasc.com/nonlinbook.html\n\n\n[3] D. G. Luenberger and Y. Ye, Linear and Nonlinear Programming, 5th ed. in International Series in Operations Research & Management Science, no. 228. Cham, Switzerland: Springer, 2021. Available: https://doi.org/10.1007/978-3-030-85450-8\n\n\n[4] S. Gros and M. Diehl, “Numerical Optimal Control (Draft).” Systems Control; Optimization Laboratory IMTEK, Faculty of Engineering, University of Freiburg, Apr. 2022. Available: https://www.syscop.de/files/2020ss/NOC/book-NOCSE.pdf\n\n\n[5] M. J. Kochenderfer and T. A. Wheeler, Algorithms for Optimization. The MIT Press, 2019. Accessed: Dec. 29, 2020. [Online]. Available: https://algorithmsbook.com/optimization/\n\n\n[6] J. R. R. A. Martins and A. Ning, Engineering Design Optimization. Cambridge ; New York, NY: Cambridge University Press, 2022. Available: https://mdobook.github.io/\n\n\n[7] M. Bierlaire, Optimization: Principles and Algorithms, 2nd ed. Lausanne: EPFL Press, 2018. Available: https://transp-or.epfl.ch/books/optimization/html/OptimizationPrinciplesAlgorithms2018.pdf\n\n\n[8] S. Boyd and L. Vandenberghe, Convex Optimization, Seventh printing with corrections 2009. Cambridge, UK: Cambridge University Press, 2004. Available: https://web.stanford.edu/~boyd/cvxbook/\n\n\n[9] A. Ben-Tal and A. Nemirovski, “Lectures on Modern Convex Optimization - 2020/2021/2022/2023 Analysis, Algorithms, Engineering Applications,” Technion & Georgia Institute of Technology, 2023. Available: https://www2.isye.gatech.edu/~nemirovs/LMCOLN2023Spring.pdf\n\n\n[10] “MOSEK Modeling Cookbook.” Mosek ApS, Sep. 2024. Available: https://docs.mosek.com/MOSEKModelingCookbook-a4paper.pdf",
    "crumbs": [
      "1. Optimization – theory",
      "References"
    ]
  },
  {
    "objectID": "roban_hw.html",
    "href": "roban_hw.html",
    "title": "Homework",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Homework"
    ]
  }
]